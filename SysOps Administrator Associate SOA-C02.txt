




======================================================== Amazon EC2 for SysOps ========================================================





----------------------------------------------- EC2 Changing InstanceType




-- t2.micro ----> t2.small

• This only works for EBS backed instances

     - create t2.micro and create some files in it and change instance type the file will be there coz it is EBS Backed instance

• Stop the instance

• Instance Settings => Change InstanceType 

• Start Instance     







-- A retail company has realized that their Amazon EBS volume backed EC2 instance is consistently over-utilized and needs an upgrade. A developer has connected with you to understand the key parameters to be considered when changing the instance type.

As a SysOps Administrator, which of the following would you identify as correct regarding the instance types for the given use-case? 


ANS : 

        1 Resizing of an instance is only possible if the root device for your instance is an EBS volume

                - If the root device for your instance is an EBS volume, you can change the size of the instance simply by changing its instance type, which is known as resizing it.

                - If the root device for your instance is an instance store volume, you must migrate your application to a new instance with the instance type that you need.



        2 You must stop your Amazon EBS–backed instance before you can change its instance type. AWS moves the instance to new hardware; however, the instance ID does not change                

                - You must stop your Amazon EBS–backed instance before you can change its instance type.

                - When you stop and start an instance, AWS moves the instance to new hardware; however, the instance ID does not change.



        3 If your instance is in an Auto Scaling group, the Amazon EC2 Auto Scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance                 

                -  If your instance is in an Auto Scaling group, the Amazon EC2 Auto Scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance.

                - To prevent this, you can suspend the scaling processes for the group while you're resizing your instance.






----------------------------------------------- Enhanced Networking



-- So it is a way for you to get better networking performance for your EC2 instances.


• EC2 Enhanced Networking (SR-IOV)

        • Higher bandwidth, higher PPS (packet per second), lower latency

        • Option 1: Elastic Network Adapter (ENA) up to 100 Gbps

        • Option 2: Intel 82599 VF up to 10 Gbps – LEGACY

        • Works for newer generation EC2 Instances



• Elastic Fabric Adapter (EFA)


        • Improved ENA for HPC, only works for Linux

        • Great for inter-node communications, tightly coupled workloads

        • Leverages Message Passing Interface (MPI) standard

        • Bypasses the underlying Linux OS to provide low-latency, reliable transport








----------------------------------------------- Placement Groups



• Sometimes you want control over the EC2 Instance placement strategy

• That strategy can be defined using placement groups

     - So we don't get direct interaction with the hardware of AWS, but we let AWS know how we would like our EC2 instance to be placed compared to one another.


• When you create a placement group, you specify one of the following strategies for the group:

    • Cluster — clusters instances into a low-latency group in a single Availability Zone (This is going to give you high performance but high risk.)

    • Spread — spreads instances across underlying hardware (max 7 instances per group per AZ) – critical applications

    • Partition—spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)






----------------------------------------------- Placement Groups Cluster(Same AZ)



• Pros: Great network (10 Gbps bandwidth between instances with Enhanced Networking enabled - recommended)

• Cons: If the AZ fails, all instances fails at the same time

• Use case:

     • Big Data job that needs to complete fast

     • Application that needs extremely low latency and high network throughput





----------------------------------------------- Placement Groups Spread



-- so in this case, when we ask for spread placement group, all the EC2 instances are going to be located on different hardware.

• Pros:

        • Can span across Availability Zones (AZ)

        • Reduced risk is simultaneous failure

        • EC2 Instances are on different physical hardware


• Cons:

        • Limited to 7 instances per AZ per placement group


• Use case:
        • Application that needs to maximize high availability

        • Critical Applications where each instance must be isolated from failure from each other








----------------------------------------------- Placements Groups Partition




-- for the partition placement group, we can have instances spread across partitions in multiple availability-zones.

-- So why do we use a partition placement group?

        - Well, each partition represents a rack in AWS. And so by having many partitions, you're making sure that your instances are distributed across many hardware racks, 
        
        - and so therefore, they're safe from a rack failure from one another.

• Up to 7 partitions per AZ

• Can span across multiple AZs in the same region

• Up to 100s of EC2 instances

• The instances in a partition do not share racks with the instances in the other partitions

• A partition failure can affect many EC2 but won’t affect other partitions

• EC2 instances get access to the partition information as metadata

• Use cases: HDFS, HBase, Cassandra, Kafka






----------------------------------------------- Shutdown Behavior of your EC2 instances.




1 Shutdown Behavior



-- So the question is, how should the instance react when the shutdown is done from within the operating system?


• Shutdown Behavior: How should the instance react when shutdown is done using the OS?  not within the console, but within the OS.

        • Stop (default) : That means that if you stop the operating system, then the EC2 instance will stop as well.

        • Terminate 

             - That means that if you have an EC2 instance running and you initiate a shutdown command from within,then two options.

             1 you stop your EC2 instance, which is what we've seen so far.

             2 terminate the instance.That means that it will be gone.



• This is not applicable when shutting down from AWS console.
 
       - So, if you stop the instance by doing right-click, stop instance, it's not going to terminate it.

       - It's only when you initiate the shutdown from within the EC2 instance.


• CLI Attribute:  InstanceInitiatedShutdownBehavior       






2 Termination Protection


• Enable termination protection:

    - To protect against accidental termination in AWS Console or CLI



• Exam Tip:

        • We have an instance where shutdown behavior = terminate and enable terminate protection is ticked

        • We shutdown the instance from the OS, what will happen ?

        ANS: • The instance will still be terminated! , coz you haven't been doing this from the console, but you've done this from within the OS.








----------------------------------------------- Shutdown Behavior of your EC2 instances Hands ON




-- create an ec2 --> Advanced details --> Shutdown behavior = Terminate --> Termination protection = Enabled --> launch instance 

        - even though we have enabled termination protection when we shut down from within the instance, then the instance itself is going to terminate,


-- try to delete the ec2 u can't coz we have enabled termination protection.

-- therefore, what you can do is that you could change from the instance setting the termination protection to disable it.

-- then the instance can be terminated using the console or an API code.

-- But, we don't want to disable it, So we want to keep it enabled.

-- even though we have the termination protection currently enabled on this instance, we can still terminate it by shutting it down

-- to do so, connect to instance and type 

        sudo shutdown 

--  So this says an operating system type of shutdown. Okay, it happens from within the operating system of your instance.       

-- wait for some time the ec2 instance will get terminated 








----------------------------------------------- EC2 Launch Troubleshooting


1

• # InstanceLimitExceeded: if you get this error, it means that you have reached your limit of max number of vCPUs per region


• On-Demand instance limits are set on a per-region basis

• Example: If you run on-demand (A, C, D, H, I, M, R,T, Z) instance types you’ll have 64 vCPUs (default)

• Resolution: Either launch the instance in a different region or request AWS to increase your limit of the region

• NOTE: vCPU-based limits only apply to running On-Demand instances and Spot instances




2 


• # InsufficientInstanceCapacity : if you get this error, it means AWS does not have that enough On-Demand capacity in the particular AZ where the instance is launched.

-- This is not a problem on you, this is a problem for AWS.

• Resolution :

     • Wait for few mins before requesting again. in case there is more capacity added to that particular AZ.

     • If requesting more than 1 requests, break down the requests. If you need 5 instances, rather than a single request of 5, request one by one.

     • If urgent, submit a request for a different instance type now, which can be resized later.

     • Also, can request the EC2 instance in a different AZ



https://aws.amazon.com/premiumsupport/knowledge-center/ec2-insufficient-capacity-errors/




3


• # InstanceTerminates Immediately (goes from pending to terminated)

        • You've reached your EBS volume limit.

        • An EBS snapshot is corrupt.

        • The root EBS volume is encrypted and you do not have permissions to access the KMS key for decryption.

        • The instance store-backed AMI that you used to launch the instance is missing a required part (an image.part.xx file).



• To find the exact reason, check out the EC2 console of AWS - instances - Description tab, note the reason next to the State transition reason label.










----------------------------------------------- EC2 SSH troubleshooting



• Make sure the private key (pem file) on your linux machine has 400 permissions, else you will get “Unprotected private key file” error

• Make sure the username for the OS is given correctly when logging via SSH, else you will get “Host key not found”, “Permission denied”, or “Connection closed by [instance] port 22” error

• Possible reasons for “Connection timed out” to EC2 instance via SSH:

        • SG is not configured correctly

        • NACL is not configured correctly

        • Check the route table for the subnet (routes traffic destined outside VPC to IGW)

        • Instance doesn’t have a public IPv4

        • CPU load of the instance is high






-----------------------------------------------  SSH vs. EC2 Instance Connect (check in course)



Connect using SSH



Inbound Rules:

Type Protocol Port  Source
SSH   TCP      22  1.2.3.4/32


-- allow only one user 










-----------------------------EC2 Instances Purchasing Options


• On-Demand Instances – short workload, predictable pricing, pay by second

• Reserved (1 & 3 years)
   • Reserved Instances – long workloads
   • Convertible Reserved Instances – long workloads with flexible instances

• Savings Plans (1 & 3 years) –commitment to an amount of usage, long workload

• Spot Instances – short workloads, cheap, can lose instances (less reliable)

• Dedicated Hosts – book an entire physical server, control instance placement

• Dedicated Instances – no other customers will share your hardware

• Capacity Reservations – reserve capacity in a specific AZ for any duration



              1  ON-Demand instances: pay as u go 

-fixed price , pay per hour 
- no upfront payments
- no predictable
- no long-term commitment
- short term committment

• Pay for what you use:
   • Linux or Windows - billing per second, after the first minute
   • All other operating systems - billing per hour

• Has the highest cost but no upfront payment

• Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave


             2  Reserved instances 

- Long term commitent 
- 1 or 3 years
- upfront payments(full , partial)
- 75% discount approx

------- we have 3 types of RI

a   Standard RI : where u get 75% discount

b   convertible RI : to change the capacity of the instance 66% discount 
    • Can change the EC2 instance type, instance family, OS, scope and tenancy

    
c   Schedule RI :  reserve it for short term like fraction of day , week, or month


• Up to 72% discount compared to On-demand

• You reserve a specific instance attributes (Instance Type, Region,Tenancy, OS)

• Reservation Period – 1 year (+discount) or 3 years (+++discount)

• Payment Options – No Upfront (+), Partial Upfront (++), All Upfront (+++)

• Reserved Instance’s Scope – Regional or Zonal (reserve capacity in an AZ)

• Recommended for steady-state usage applications (think database)

• You can buy and sell in the Reserved Instance Marketplace




            3 Spot Instances: 

• Can get a discount of up to 90% compared to On-demand

• Instances that you can “lose” at any point of time if your max price is less than the current spot price

• The MOST cost-efficient instances in AWS

• Useful for workloads that are resilient to failure
  • Batch jobs
  • Data analysis
  • Image processing
  • Any distributed workloads
  • Workloads with a flexible start and end time

• Not suitable for critical jobs or databases
  
- Biding 
- huge capacity for cheaper price
- 90% discount


-- Spot blocks allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted

   EXP : Spot blocks are designed not to be interrupted and will run continuously for the duration you select (1 to 6 hours), independent of the Spot market price.

   -- In rare situations, Spot blocks may be interrupted due to Amazon Web Services' capacity needs. In these cases, AWS will provide a two-minute warning before it terminates your instance and you will not be charged for the affected instance(s).


-- Spot Instances with a defined duration (also known as Spot blocks) are designed not to be interrupted and will run continuously for the duration you select. 

-- This makes them ideal for jobs that take a finite time to complete, such as batch processing, encoding and rendering, modeling and analysis, and continuous integration.

-- Running our load on a Spot Instance with Spot Block sounds like the perfect use case, as we can block the spot instance for 1 hour, run the script there, and then the instance will be terminated.





some point to know about sopt instances : 

1  If a spot request is persistent, then it is opened again after your Spot Instance is interrupted

EXP : -- A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. 

      --  If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance.

2  Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated

EXP : -- The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.

      -- You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request.

3  When you cancel an active spot request, it does not terminate the associated instance

EXP : -- If your Spot Instance request is active and has an associated running Spot Instance, or your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance; 

      -- you must terminate the running Spot Instance manually. Moreover, to cancel a persistent Spot request and terminate its Spot Instances, you must cancel the Spot request first and then terminate the Spot Instances.





                4 Dedicated Host

• A physical server with EC2 instance capacity fully dedicated to your use

• Allows you address compliance requirements and use your existing server- bound software licenses (per-socket, per-core, pe—VM software licenses)

• Purchasing Options:
   • On-demand – pay per second for active Dedicated Host
   • Reserved - 1 or 3 years (No Upfront, Partial Upfront,All Upfront)

• The most expensive option

• Useful for software that have complicated licensing model (BYOL – Bring Your
Own License)

• Or for companies that have strong regulatory or compliance needs

-- With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance


- if u need a physical machine with VM's for this model 

- privaacy

- high security

- Dedicated Hosts enable you to use your existing server-bound software licenses like Windows Server and address corporate compliance and regulatory requirements.





                   5  EC2 dedicated instances 

• Instances run on hardware that’s dedicated to you

• May share hardware with other instances in same account

• No control over instance placement (can move hardware after Stop / Start)

-  Dedicated instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. 

- Your dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. 

- Dedicated instances may share hardware with other instances from the same AWS account that are not dedicated instances. 

- Dedicated instances cannot be used for existing server-bound software licenses.



EPV : A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines.

Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?

ANS : dedicated instances 




- Explantion b/w EC2 dedicated instances  vs EC2 dedicated Hosts

https://stackoverflow.com/questions/64309679/aws-dedicated-host-vs-dedicated-instance-why-the-first-is-more-expensive-than






                   6  savings plans 

- Savings Plans are a flexible pricing model that offer low prices on EC2, Fargate and Lambda usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term. 

- Savings Plans provide you the flexibility to use the compute option that best suits your needs and automatically save money, all without having to perform exchanges or modifications. When you sign up for a Savings Plan, you will be charged the discounted Savings Plans price for your usage up to your commitment.

- Savings Plans allow you to easily reduce your bill by making a commitment to compute usage (e.g. $10/hour) instead of making commitments to specific instance configurations or compute services.

- Amazon Web Services offers two types of Savings Plans - Compute Savings Plans and EC2 Instance Savings Plans.

1 Flexible

-- Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to:

   - EC2 instance usage regardless of instance family, size, Availability Zone, Region, OS, or tenancy
   - Fargate usage
   - Lambda usage for Duration, Provisioned Concurrency, and Provisioned Duration

   EG : For example, with Compute Savings Plans, you can change from C4 to M5 instances,, shift a workload from EU (Ireland) to EU (London), or move a workload from EC2 to Fargate or Lambda at any time and automatically continue to pay the Savings Plans price.



2 Significant Discounts

  - EC2 Instance Savings Plans provide the lowest prices, offering savings up to 72% in exchange for commitment to usage of individual instance families in a region (e.g. M5 usage in N. Virginia). 

  - This automatically reduces your cost on the selected instance family in that region regardless of AZ, size, OS or tenancy. 

  -  EC2 Instance Savings Plans give you the flexibility to change your usage between instances within a family in that region.

  EG : For example, you can move from c5.xlarge running Windows to c5.2xlarge running Linux and automatically benefit from the Savings Plan prices.


• Get a discount based on long-term usage (up to 72% - same as RIs)

• Commit to a certain type of usage ($10/hour for 1 or 3 years)

• Usage beyond EC2 Savings Plans is billed at the On-Demand price

• Locked to a specific instance family & AWS region (e.g., M5 in us-east-1)

• Flexible across:
   • Instance Size (e.g., m5.xlarge, m5.2xlarge)
   • OS (e.g., Linux, Windows)
   • Tenancy (Host, Dedicated, Default)

- it has same as RI but difernet starategy



IMP to KNOW : You can use Dedicated Hosts and Dedicated instances to launch Amazon EC2 instances on physical servers that are dedicated for your use.

-- An important difference between a Dedicated Host and a Dedicated instance is that a Dedicated Host gives you additional visibility and control over how instances are placed on a physical server, and you can consistently deploy your instances to the same physical server over time.

-- As a result, Dedicated Hosts enable you to use your existing server-bound software licenses and address corporate compliance and regulatory requirements.




                   7  EC2 Capacity Reservations

• Reserve On-Demand instances capacity in a specific AZ for any duration

• You always have access to EC2 capacity when you need it

• No time commitment (create/cancel anytime), no billing discounts

• Combine with Regional Reserved Instances and Savings Plans to benefit from billing discounts

• You’re charged at On-Demand rate whether you run instances or not

Suitable for short-term, uninterrupted workloads that needs to be in a "specific AZ"





EXP : A media publishing company is using Amazon EC2 instances for running their business-critical applications. Their IT team is looking at reserving capacity apart from savings plans for the critical instances.

      As a Developer Associate, which of the following reserved instance types you would select to provide capacity reservations?

ANS : Zonal Reserved Instances

EXP : When you purchase a Reserved Instance for a specific Availability Zone, it's referred to as a Zonal Reserved Instance. Zonal Reserved Instances provide capacity reservations as well as discounts.

- A zonal Reserved Instance provides a capacity reservation in the specified Availability Zone. Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances.

- Regional reserved nstance cannot provide capacity reservation














Which purchasing option is right for me? 


• On demand: coming and staying in resort whenever we like, we pay the full price

• Reserved: like planning ahead and if we plan to stay for a long time, we may get a good discount.

• Savings Plans: pay a certain amount per hour for certain period and stay in any room type (e.g., King, Suite, Sea View, ...)

• Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms.You can get kicked out at any time

• Dedicated Hosts: We book an entire building of the resort

• Capacity Reservations: you book a room for a period with full price even you don’t stay in it







---------------------------------------------- EC2 Spot Instance Requests and Spot Fleets




• Can get a discount of up to 90% compared to On-demand


• Define max spot price and get the instance while current spot price < max


        • The hourly spot price varies based on offer and capacity

        • If the current spot price > your max price you can choose to stop or terminate your instance with a 2 minutes grace period.

        - if one day the spot price goes below your max price, then you can restart your instance and continue where you left it off


• Other strategy: Spot Block
        
        - if you don't want your spot instance to be reclaimed by AWS, it use a spot block.

        • “block” spot instance during a specified time frame (1 to 6 hours) without interruptions

        • In rare situations, the instance may be reclaimed

IMP :   Spot Block are no longer availabe to new AWS customers since July 2021 , and won't be supported afer Dec 2022



• Used for batch jobs, data analysis, or workloads that are resilient to failures.

• Not great for critical jobs or databases







---------------------------------------------- How to terminate Spot Instances?



-- So, we have to first understand how a spot request works. And so for this, let's consider a spot request.

-- So, with the spot request, you are defining how many instances you want, your maximum price you're going to pay,the launch specification, so the AMI and so on,

-- then the request type.

        - it's very important to understand there's two types of requests.

        - You can do a one-time request for spot instances

        - or a persistent request for spot instances.

        - So, if it's a "one-time" request, as soon as your spot request is fulfilled, your instances are going to be launched and then your spot request will go away because it was a one-time request type.

        - But if it's a "persistent request" type, that means that we want this number of instances to be valid as long as the spot request is valid from to valid until.

            - so, that means that if somehow your instances do get stopped or interrupted based on the spot price, then your spot request will go back into action.

            - And when things can be validated, we'll restart spot instances for you.    

            - So if somehow you stop a spot instance in persistent mode and your spot request is still active, your spot request automatically will be smart enough to restart a launch and instance for you.


-- You can only cancel Spot Instance requests that are open, active, or disabled.

-- Cancelling a Spot Request does not terminate instances You must first cancel a Spot Request, and then terminate the associated Spot Instances

    - Because if you were to terminate the spot instances first, remember it goes back into the spot request and the spot request says, "Okay, you wanted six instances "but I can see you have zero right now. "I'm going to launch six instances for you."








-------------------------- Spot Fleets

• Spot Fleets = set of Spot Instances + (optional) On-Demand Instances

• The Spot Fleet will try to meet the target capacity with price constraints

   • Define possible launch pools: instance type (m5.large), OS, Availability Zone
   • Can have multiple launch pools, so that the fleet can choose
   • Spot Fleet stops launching instances when reaching capacity or max cost

• Strategies to allocate Spot Instances:

   • lowestPrice: from the pool with the lowest price (cost optimization, short workload)
   • diversified: distributed across all pools (great for availability, long workloads)
   • capacityOptimized: pool with the optimal capacity for the number of instances
   • priceCapacityOptimized (recommended): pools with highest capacity available, then select the pool with the lowest price (best choice for most workloads)

• Spot Fleets allow us to automatically request Spot Instances with the lowest price







---------------------------------------------- Burstable Instances (T2/T3)



• AWS has the concept of burstable instances (T2/T3 machines)

• Burst means that overall, the instance has OK CPU performance.

• When the machine needs to process something unexpected (a spike in load for example), it can burst, and CPU can be VERY good.

• If the machine bursts, it utilizes “burst credits”

• If all the credits are gone, the CPU becomes BAD

• If the machine stops bursting, credits are accumulated over time

• Burstable instances can be amazing to handle unexpected traffic and getting the insurance that it will be handled correctly

• If your instance consistently runs low on credit, you need to move to a different kind of non-burstable instance







---------------------------------------------- What happens when credit are exhausted? 



• Experiment: run a CPU stress command (to peak at 100%)

• After the credits are exhausted, the measured CPU utilization drops





---------------------------------------------- T2/T3 Unlimited


• It is possible to have an “unlimited burst credit balance”

• You pay extra money if you go over your credit balance, but you don’t lose in performance

• If average CPU usage over a 24-hour period exceeds the baseline, the instance is billed for additional usage per vCPU/hour

• Be careful, costs could go high if you’re not monitoring the CPU health of your instances





-- ec2 console --> launch ec2 --> advanced options ---> credit specification (std and unlimited)--> 








---------------------------------------------- Elastic IPs



• When you stop and then start an EC2 instance, it changes its public IP

• If you need to have a fixed public IP, you need an Elastic IP

• An Elastic IP is a public IPv4 you own as long as you don’t delete it

• You can attach it to one instance at a time

• You can remap it across instances

• You don’t pay for the Elastic IP if it’s attached to a server

• You pay for the Elastic IP if it’s not attached to a server

• With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.

• You can only have 5 Elastic IP in your account (you can ask AWS to increase that).

• How you can avoid using Elastic IP:

        • Always think if other alternatives are available to you

        • You could use a random public IP and register a DNS name to it

        • Or use a Load Balancer with a static hostname






---------------------------------------------- CloudWatch Metrics for EC2


• AWS Provided metrics (AWS pushes them):

        • Basic Monitoring (default): metrics are collected at a 5 minute internal

        • Detailed Monitoring (paid): metrics are collected at a 1 minute interval

        • Includes CPU, Network, Disk and Status Check Metrics


• Custom metric (yours to push):

        • Basic Resolution: 1 minute resolution

        • High Resolution: all the way to 1 second resolution

        • Include RAM, application level metrics

        • Make sure the IAM permissions on the EC2 instance role are correct !





 ---------------------------------------------- EC2 included metrics



 • CPU: CPU Utilization + Credit Usage / Balance

 • Network: Network In / Out

 • Status Check:

        • Instance status = check the EC2 VM
        • System status = check the underlying hardware
        • Attached EBS status = check attached EBS volumes       


• Disk: Read / Write for Ops / Bytes (only for instance store)


• RAM is NOT included in the AWS EC2 metrics





----------------------------------------------  Unified CloudWatch Agent


• For virtual servers (EC2 instances, on-premises servers, ...)

• Collect additional system-level metrics such as RAM, processes, used disk space, etc.

• Collect logs to send to CloudWatch Logs

        • No logs from inside your EC2 instance will be sent to CloudWatch Logs without using an agent

• Centralized configuration using SSM Parameter Store

• Make sure IAM permissions are correct


IMP 

• Default namespace for metrics collected by the Unified CloudWatch agent is CWAgent (can be configured/changed)





-- A retail company has built its server infrastructure on Amazon EC2 instances that run on Windows OS. The development team has defined a few custom metrics that need to be collected by the unified CloudWatch agent.

As a SysOps Administrator, can you identify the correct configuration to be used for this scenario?


ANS : Configure the CloudWatch agent with StatsD protocol to collect the necessary system metrics 


EXP : You can retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers. Here, the instances are running on Windows servers, hence StatsD is the right protocol.


https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html






----------------------------------------------  Unified CloudWatch Agent – procstat Plugin


• Collect metrics and monitor system utilization of individual processes

• Supports both Linux and Windows servers

• Example: amount of time the process uses CPU, amount of memory the process uses, ...

• Select which processes to monitor by

        • pid_file: name of process identification number (PID) files they create
        • exe: process name that match string you specify (RegEx)
        • pattern: command lines used to start the processes (RegEx)


• Metrics collected by procstat plugin begins with “procstat” prefix (e.g., procstat_cpu_time, procstat_cpu_usage, ...)





https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-procstat-process-metrics.html




---------------------------------------------- Status Checks


• Automated checks to identify hardware and software issues

• System Status Checks

        • Monitors problems with AWS systems(software/hardware issues on the physical host, loss of system power, ...)

        • Check Personal Health Dashboard for any scheduled critical maintenance by AWS to your instance’s host

        • Resolution:stop and start the instance(instancemigratedtoa new host)


• Instance Status Checks

        • Monitors software/network configuration of your instance (invalid network configuration, exhausted memory, ...)

        • Resolution:reboot the instance or change instance configuration


• Attached EBS Status Checks

        • Monitors EBS volumes attached to your instance (reachable & complete I/O Operations)

        • Resolution:reboot the instance or replace affected EBS volumes



But there is a way for you to automate the recovery








---------------------------------------------- Status Checks - CW Metrics & Recovery



• CloudWatch Metrics (1 minute interval)

        • StatusCheckFailed_System
        • StatusCheckFailed_Instance
        • StatusCheckFailed_AttachedEBS
        • StatusCheckFailed (for any)


• Option 1: CloudWatch Alarm to recover your EC2 instance.

        • Recover EC2 instance with the same private/publicIP,EIP, metadata, and Placement Group

        • Send notifications using SNS


• Option 2: Auto Scaling Group


        • Set min/max/desired 1 to recover an instance but won't keep the same private and elastic IP

        - What will happen is that in case there is a issue with your EC2 instance, it will be terminated by your auto scaling group, and therefore because we have a min, max and desire to one, a new EC2 instance will be launched within your same auto scaling group.

        - So in this case, you don't have the same EBS volumes, you don't have the same private IP, you don't have the same elastic IP, but at least your instance is back up and running.







---------------------------------------------- Status Checks HAnds ON


-- create one ec2 instance 

-- go to status check tab --> actions --> create an alarm for status check --> Alarm notification = select SNS topic --> enable Alarm action = recover --> Type of data to sample = StatusCheckFailed:System -->create an alarm

-- go to cloudwatch and wait for some time it will turn to OK state or 

     - aws cloudwatch set-alarm-state --alarm-name "awsec2-i-0e842938fa0f9c6d8-GreaterThanOrEqualToThreshold-StatusCheckFailed_System" --state-reason "Testing" --state-value ALARM

-- run aboce cmnd and check in history of CW alarm

-- so it'll take a bit of time to be recovered entirely,




 






---------------------------------------------- EC2 Instance Status Checks - MUST KNOW



1 SYSTEM status checks


-- System status checks monitor the AWS systems on which your instance runs

- Problem with the underlying host. Example: 

        - Loss of network connectivity

        - Loss of system power

        - Software issues on the physical host

        - Hardware issues on the physical host that impact network reachability

        - Either wait for AWS to fix the host, OR

        - Move the EC2 instance to a new host = STOP & START the instance (if EBS backed)





2 INSTANCE status checks



- Instance status checks monitor the software and network configuration of your individual instance

Example of issues


    - Incorrect networking or startup configuration

    - Exhausted memory

    - Corrupted file system

    - Incompatible kernel

    - Requires your involvement to fix

    - Restart the EC2 instance, OR

    - Change the EC2 instance configuration





Attached EBS status checks


-- Attached EBS status checks monitor the EBS volumes attached to your individual instance

Example of issues

    - Hardware or software issues on the storage subsystems underlying the EBS volumes
    
    - Hardware issues on the physical host that impact reachability of the EBS volumes

    - Connectivity issues between the instance and EBS volumes

    - Requires your involvement to fix

    - Restart the EC2 instance, OR

    - Replace the affected EBS volumes







---------------------------------------------- EC2 Hibernate



• We know we can stop, terminate instances

    • Stop – the data on disk (EBS) is kept intact in the next start

    • Terminate – any EBS volumes (root) also set-up to be destroyed is lost



• On start, the following happens:

    • First start: the OS boots & the EC2 User Data script is run

    • Following starts: the OS boots up

    • Then your application starts, caches get warmed up, and that can take time!



-- But the idea is that with Hibernate, we want to achieve a new state.


• Introducing EC2 Hibernate:


        • The in-memory (RAM) state is preserved

        • The instance boot is much faster! (the OS is not stopped / restarted)

        • Under the hood: the RAM state is written to a file in the root EBS volume

        • The root EBS volume must be encrypted



• Use cases:

        • Long-running processing

        • Saving the RAM state

        • Services that take time to initialize






 ---------------------------------------------- Limitations on the EC2 instances enabled for hibernation



    1 You cannot increase/decrease the size of hibernated instance.

    2 Cannot enable snapshot or AMIs from instance in hibernation state.

    3 You can’t enable hibernation after launch.

    4 If an EC2 instance is enabled for auto-scaling group or used by Amazon ECS, then you cannot hibernate that instance.

    5 An EC2 instance cannot be kept hibernated for a period of more than 60 days. To keep the instance for longer than 60 days, you must start the hibernated instance, stop the instance, and start it.

    6 To hibernate an instance that was launched using your own AMI, you must first configure your AMI to support hibernation.




 ---------------------------------------------- EC2 Hibernate – Good to know


 • Supported Instance Families – C3, C4, C5, I3, M3, M4, R3, R4,T2,T3, ...

 • Instance RAM Size – must be less than 150 GB.

 • Instance Size – not supported for bare metal instances.

 • AMI – Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS & Windows...

 • Root Volume – must be EBS, encrypted, not instance store, and large

 • Available for On-Demand, Reserved and Spot Instances

 • An instance can NOT be hibernated more than 60 days






---------------------------------------------- EC2 Hibernate  Hands ON 



 -- create one instance A with hibernation , do encrypt ur EBS volume while creating and use aws key for encryption

-- create another instance B  , without hibernation 

-- do connect A and type uptime , it will shows the time from how much u are active state 

-- do same for instance B 

-- do stop the instances and wait for 1-2 min 

-- do start the instance again 

-- connect the instance A and do uptime it will give the time from the time when u started the instance A , coz it is hibernated.

--  do same with the instance B , when u do for the instance B , it will show the time from 0 , coz it is not hibernated 










======================================================== Amazon Machine Image (AMI) ========================================================






---------------------------------------------- AMI Overview



• AMI = Amazon Machine Image

• AMI are a customization of an EC2 instance

     • You add your own software, configuration, operating system, monitoring...

     • Faster boot / configuration time because all your software is pre-packaged



• AMI are built for a specific region (and can be copied across regions)

• You can launch EC2 instances from:

        • A Public AMI: AWS provided

        • Your own AMI: you make and maintain them yourself

        • An AWS Marketplace AMI: an AMI someone else made (and potentially sells)








---------------------------------------------- AMI Process (from an EC2 instance)



• Start an EC2 instance and customize it

• Stop the instance (for data integrity)

• Build an AMI – this will also create EBS snapshots

• Launch instances from other AMIs




US-EAST-1A  --------- Create AMI ------> Custom AMI -----------> Launch from AMI ------> US-EAST-1B






---------------------------------------------- AMI No-Reboot Option



• Enables you to create an AMI without shutting down your instance

• By default, it’s not selected (AWS will shut down the instance before creating an AMI to maintain the file system integrity)




1 With No-Reboot Disabled (default)


    - if we want to initiate creating an AMI, the instance is first shut shutdown. Then after shutdown, the attached EBS volume will get a snapshot taken into an EBS snapshot.

    - then the EBS snapshot will be converted into an AMI.



2  With No-Reboot Enabled

    - your EC2 instance that is currently running, will get a snapshot directly made onto its running attached EBS volume and then the image will be created.

    - So the risk here is to not have a file system integrity.

    - Also, any OS operating system buffer will not be flushed to the disc before the snapshot is created.







---------------------------------------------- AWS Backup Plans to create AMI 



-- you can create a backup plan and this allows you to create an AMI.

• AWS Backup doesn't reboot the instances while taking EBS snapshots (no-reboot behavior)

     - so by default, the only option actually is to have the no-reboot behavior.

     - That means that when you use AWS backup on Amazon EC2, the AMIs will be created using the parameter no-reboots that the instances are not being interrupted while they're functioning.

• This won't help you to create an AMI that guarantees file system integrity since you need to reboot the instance

• To maintain integrity you need to provide the reboot parameter while taking images (EventBridge + Lambda + CreateImage API with reboot)

    - you could create a schedule which will invoke the Lambda function once every week.

    - The Lambda function will have its own code to create an AMI, with this time with the reboot option.

    - In this case, then the Amazon EC2 will be rebooting and an AMI will be created.







---------------------------------------------- EC2 Instance Migration between AZ


-- if you wanted to migrate an EC2 instance from one AZ to another, well the way you would do it is using an AMI.

-- So, in this example, we want to migrate our EC2 instance from us-east-1a to us-east-1b, 

-- first we take an AMI from our EC2 instance and then restore that AMI into a new EC2 instance, in a different AZ







---------------------------------------------- Cross-Account AMI Sharing


• You can share an AMI with another AWS account

• Sharing an AMI does not affect the ownership of the AMI

-- you can share an AMI for in two cases.

    • You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer managed key

    • If you share an AMI with encrypted volumes, you must also share any customer managed keys used to encrypt them.




-- So, let's take an example, 

   - We have account A and this is an unencrypted AMI in your source accounts and you're just going to share it with account B.

   - And then the account B can launch directly an EC2 instance from that source AMI,

   - Now, if you add KMS encryption we have the same use case, but this time your AMI is actually encrypted with your CMK-A

   - you're going to share this AMI with your account B but also you're going to share the KMS key.

   - And you're going to give permissions to the target accounts to describe the key to decrypt to re-encrypt and so on.

   - this will allow the target accounts to launch your custom AMI, even though it was encrypted with a key from your accounts,








---- A firm uses Amazon EC2 instances for running its flagship application. With new business expansion plans, the firm is looking at a bigger footprint for its AWS infrastructure. The development team needs to share Amazon Machine Images (AMIs) across AZs, AWS accounts and Regions.

ANS : 

        1 You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer-managed CMK 

                - You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer-managed CMK.

                - If you share an AMI with encrypted volumes, you must also share any CMKs used to encrypt them.

        2 You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI

                - You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI. 

                - Only the AMI itself needs to be shared; the system automatically provides the access to the referenced Amazon EBS snapshots for the launch.                






---------------------------------------------- Cross-Account AMI Copy



• If you copy an AMI that has been shared with your account, you are the owner of the target AMI in your account

• The owner of the source AMI must grant you read permissions for the storage that backs the AMI (EBS Snapshot)

• If the shared AMI has encrypted snapshots, the owner must share the key or keys with you as well

• Can encrypt the AMI with your own CMK while copying






---------------------------------------------- AMI Copy with KMS Encryption


Cross-Region / Cross-Account Encrypted AMI Copy


-- let's have a look if we have KMS encryption

        - we are sharing the underlying EBS snapshot and we still give KMS key permissions to the target accounts, 

        - And the target accounts can issue a copy command to, for example, re-encrypt the EBS snapshot by decrypting it using the CMK-A that was given access to and re-encrypt it with CMK-B and its own accounts

        - which will give a custom AMI that will be owned by the target account B with its own encryption mechanism








---------------------------------------------- EC2 Image Builder


• Used to automate the creation ofVirtual Machines or container images

• => Automate the creation, maintain, validate and test EC2 AMIs

• Can be run on a schedule (weekly, whenever packages are updated, etc...)

• Free service (only pay for the underlying resources)






                    create      Build Components applied (customize software on instance)                               Test suite is run(is the AMI working, secure?)
EC2 Image Builder -------->  Builder EC2 Instance ------------------------------------------> create ----> New AMI ----> Test EC2 Instance -------------------------------> AMI is distributed(can be multiple regions)



EXP :

-- So we have the EC2 Image Builder service and it is automatically, when it's going to run, it is going to create an EC2 instance called a Builder EC2 instance,

-- that EC2 instance is going to build components and customize the software, for example, install Java, update the CLI, update the software system, maybe install firewalls, whatever you define to happen on that EC2 instance, maybe install your application.

-- then once this is done, then an AMI is going to be created out of that EC2 instance, but all of this is obviously automated.

-- Then the AMI is created, but we want to validate it.

-- So EC2 Image Builder will automatically create a test EC2 instance from that AMI and going to run a bunch of tests that you are defining in advance.

--  if you don't wanna run any tests, obviously you can just skip that test,

-- but the test can be asking, is the AMI working, is it secure? Is my application running correctly? All these kinds of things.

-- then once the AMI is tested, then the AMI is going to be distributed.

-- So while EC2 Image Builder is Regional service, it is possible for you to take that AMI and distribute it to multiple regions, therefor allowing your application and your workflow to be truly global.




-- Next, EC2 Image Builder can be run on a schedule. So you can define a weekly schedule

-- or you can say you can run whenever packages are updated or you can run it manually, 







---------------------------------------------- EC2 Image Builder Hands ON



-- open ec2 image builder in console

-- create pipeline --> Build schedule  = manual --> Choose recipe = create new --> Image type = AMI ---> give name and version --> Base image = Select managed images --> os = amazon linux --> Image origin = Quick start (Amazon-managed) --> Image name = Amazon Linux x86 --> \

--> Components  = (So we can apply the build components that are pre-created by AWS,) select amazon-corretto-11-headless (you would have Java 11 being installed on your AMI,) --> click on 2 page on Right top select aws-cli-version-2-linux \

--> Type = Default workflows --> Infrastructure configuration = Create a new infrastructure configuration --> create new role (aws service --> ec2 --> EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore managed policy.-->role name = EC2InstanceProfileForImageBuilder-->create role) \

--> IAM role = choose that we created just now --> AWS infrastructure --> Instance type = t2.micro --> nxt --> nxt --> create pipeline 



-- open pipeline --> actions --> run pipeline --> click on the version like 1.0.0/1

-- I'm going to wait until the build starts.

-- check ec2 console , ec2 get created ,So this instance was created by EC2 Image Builder, and you can verify it by going into tags, 

-- the whole Pipeline process will take 20-25 minutes t create 

-- u can also click on the CW logs link and see how the behaviour is ...

-- So if I go into my instances and refresh this, we can see that my builder instance has now been terminated,

-- because we have built the AMI from it, and my test instance is now running.

-- if I look at my test instance and scroll down, we can see that the AMI right here,

-- the AMI is a new AMI that has been created. , and contains the timestamp of when it was created.

-- So now the test instance is actually launched from this new AMI, and is being tested.

-- check tags for the AMI 

-- after 10-15 min Image status = testing it will come 

-- now we can see that the AMI has been automatically created, and now it's in the test phase.

-- after 20-25 min the Image status = distribution 

-- the test instance will also get teminated once Image status = distribution  

-- now got created my AMI and AMI is availabe

-- now launch an instance --> select AMI create by the EC2 Image Builder --> without key pair --> create ec2 instance 

-- connect to the ec2 instance --> Username = ec2-user(because we created a custom AMI, we need to tell AWS that we still want to use the ec2-user user which is coming from Amazon Linux 2.) 

-- we can verify two things.

        - aws --version  it will give 2nd version 

        - java --version  , u will get this 

            openjdk 11.0.24 2024-07-16 LTS
            OpenJDK Runtime Environment Corretto-11.0.24.8.1 (build 11.0.24+8-LTS)
            OpenJDK 64-Bit Server VM Corretto-11.0.24.8.1 (build 11.0.24+8-LTS, mixed mode)

-- OpenJDK Runtime Environment Corretto-11.0.24.8.1 (build 11.0.24+8-LTS)     what exactly we want 

-- delete ec2 , snapshot , AMI and pipeline 







---------------------------------------------- AMI in Production



• You can force users to only launch EC2 instances from pre-approved AMIs (AMIs tagged with specific tags) using IAM policies




{
...
"Condition": {
"StringEquals": { "ec2:ResourceTag/Environment": "Prod" }
} }



• Combine with AWS Config to find non- compliant EC2 instance (instances launched with non-approved AMIs)









========================================================  Managing EC2 at Scale - Systems Manager (SSM) ========================================================





---------------------------------------------- AWS Systems Manager Overview


• Helps you manage your EC2 and On-Premises systems at scale

• Get operational insights about the state of your infrastructure

• Easily detect problems

• Patching automation for enhanced compliance

• Works for both Windows and Linux OS

• Integrated with CloudWatch metrics / dashboards

• Integrated with AWS Config

• Free service





----------------------------------------------  AWS Systems Manager Features



• Resource Groups  -IMP 

• Operations Management

    • Explorer
    • OpsCenter
    • CloudWatch Dashboard 
    • PHD
    • Incident Manager


• Shared Resources

    • Documents  - IMP 


• Change Management

        • Change Manager
        • Automation    - imp 
        • Change Calendar
        • Maintenance Windows - imp




• Application Management


        • Application Manager
        • AppConfig
        • Parameter Store - IMP 



• Node Management


        • Fleet Manager
        • Compliance
        • Inventory   - imp 
        • Hybrid Activations 
        • Session Manager   - imp 
        • Run Command     - imp 
        • State Manager      - imp 
        • Patch Manager      - imp 
        • Distributer







---------------------------------------------- How Systems Manager works



• We need to install the SSM agent onto the systems we control

• Installed by default on Amazon Linux 2 AMI & some Ubuntu AMI

• If an instance can’t be controlled with SSM, it’s probably an issue with the SSM agent!

• Make sure the EC2 instances have a proper IAM role to allow SSM actions







---------------------------------------------- AWS Tags


• You can add text key-value pairs called Tags to many AWS resources

• Commonly used in EC2

• Free naming, common tags are Name, Environment,Team ...

• They’re used for

    • Resource grouping 
    • Automation
    • Cost allocation


• Better to have too many tags than too few!

-- So with these tags now what we can do, is that we can leverage them to create resource groups








---------------------------------------------- Resource Groups



• Create, view or manage logical group of resources thanks to tags

• Allows creation of logical groups of resources such as

        • Applications
        • Different layers of an application stack
        • Production versus development environments


• Regional service

• Works with EC2, S3, DynamoDB, Lambda, etc...



-- create 3 instance and give tags with proper naming convenient


-- go to Resource Groups & Tag Editor in console --> Tags --> Grouping criteria --> Resource types = ec2::instace --> give tag and value (eg ; env = dev) --> it will gives all dev instances --> create group and give name 

-- Now, the reason why we do these resource groups is that we wanna be able to operate SSM directly at the group level.







---------------------------------------------- SSM – Documents



• Documents can be in JSON or YAML

• You define parameters

• You define actions

• Many documents already exist in AWS


-- but also these documents can be applied to other SSM features such as state manager, patch manager, automation, and documents can even retrieve some data from the SSM parameter store to be able to give you some kind of modularity and dynamicity




EG :

---
schemaVersion: '2.2'
description: Sample YAML template to install Apache
parameters: 
  Message:
    type: "String"
    description: "Welcome Message"
    default: "Hello World"
mainSteps:
- action: aws:runShellScript
  name: configureApache
  inputs:
    runCommand:
    - 'sudo yum update -y'
    - 'sudo yum install -y httpd'
    - 'sudo systemctl start httpd'
    - 'sudo systemctl enable httpd'
    - 'echo "{{Message}} from $(hostname -f)" > /var/www/html/index.html'





---------------------------------------------- SSM – Run Command


IMP : make sure u have the IAM Role that has "AmazonSSMManagedInstanceCore" Policy , attach this role to all instance that you want to run ur document 

-- do not open 22 port on SG , ssm will take care of this 



• Execute a document (= script) or just run a command 

• Run command across multiple instances (using resource groups)

• Rate Control / Error Control

    - So imagine you're applying a command to 1,000 instances and it will take them down for a minute or two. 
    
    - Then you want to make sure you do this progressively, and that in case you have any errors, you are able to stop running the command in your fleets.


• Integrated with IAM & CloudTrail

• No need for SSH

• Command Output can be shown in the Console, sent to S3 bucket or CloudWatch Logs

• Send notifications to SNS about command statues (In progress, Success, Failed, ...)

• Can be invoked using EventBridge






---------------------------------------------- SSM – Run Command Hands ON 



-- i wan to install Apache on my servers 

-- go to system Manager --> documents --> create document --> command or session --> Target type = ec2 instance --> yaml (paste below code) -->  create document 





---
schemaVersion: '2.2'
description: Sample YAML template to install Apache
parameters: 
  Message:
    type: "String"
    description: "Welcome Message"
    default: "Hello World"
mainSteps:
- action: aws:runShellScript
  name: configureApache
  inputs:
    runCommand:
    - 'sudo yum update -y'
    - 'sudo yum install -y httpd'
    - 'sudo systemctl start httpd'
    - 'sudo systemctl enable httpd'
    - 'echo "{{Message}} from $(hostname -f)" > /var/www/html/index.html'




-- So this document is now owned by me

-- go to owned by me , u can find ur document 


-- now go to Run Command --> Owner: Owned by me --> Target selection = choose as u want --> Timeout (seconds) = 600 --> Rate control = set as u want --> Output options = Enable CloudWatch logs , give name of log group--> Run 


        - Timeout (seconds) : So if the commands don't finish within 600 seconds, so 10 minutes, then you should fail the command.

        - Concurrency : So do we want to run the commands on 50 targets at a time or maybe one target at a time, so one by one Or maybe percentage, 

        - error threshold : So that means just after one error, you know, on the first error, then this will stop the entire task,so maybe you're saying that as long as 5% of my instances don't error out, this is fine, please keep on going, but if you go above this 5% of error threshold, then stop running the command.


-- if you want to run through the AWS CLI , at last coloumn it will generate the cmnd to run , paste in cloudshell it will run 

-- select instance --> view output to see the o/P

-- copy ip of ec2 and check in browser u will able to see the o/p 




IMP 


-- here we are able to run a command across three EC2 Instances, but remember, these EC2 Instances do not have the SSH port open, 

-- So what happens is that the SSM agent did run the commands for us, which is super helpful because we don't compromise on security.









---------------------------------------------- SSM - Automation




• Simplifies common maintenance and deployment tasks of EC2 instances and other AWS resources

• Example: restart instances, create an AMI, EBS snapshot

• Automation Runbook : is the name of the document for SSM that are going to be of type Automation

        • SSM Documents of type Automation

        • Defines actions performed on your EC2 instances or AWS resources

        • Pre-defined runbooks (AWS) or create custom runbooks


• Can be triggered

        • Manually using AWSConsole ,AWSCLI or SDK

        • By Amazon EventBridge

        • On a schedule using Maintenance Windows

        • By AWS Config for rules remediations






---------------------------------------------- SSM - Automation Hands ON 



-- open system manager in console --> Automation (left side) --> Instance management --> u have so many automation books pre defined --> search "aws-restart" \

--> select AWS-RestartEC2Instance --> Runbook version = latest version --> Rate Control --> Parameter = instanceid --> Targets = All instances --> execute 



--> So effectively, what we can do, for example, using this automation is just restart a full fleet of EC2 Instances, without enabling SSH access.









---------------------------------------------------------- SSM Parameter Store -------------------------------- 



• Secure storage for configuration and secrets

• Optional Seamless Encryption using KMS

• Serverless, scalable, durable, easy SDK

• Version tracking of configurations / secrets

• Security through IAM

• Notifications with Amazon EventBridge

• Integration with CloudFormation





---------------------------------------------------------- SSM Parameter Store Hierarchy


• /my-department/
    • my-app/
        • dev/
            • db-url
            • db-password

        • prod/
            • db-url
            • db-password

• other-app/



- You also have the opportunity to access Secrets of Secrets Manager through the Parameter Store by using this reference right here.

        • /aws/reference/secretsmanager/secret_ID_in_Secrets_Manager


- there are something called Public Parameters that are issued by AWS that you can use.

        • /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 (public)
           







---------------------------------------------------------- SSM Parameter Standard and advanced parameter tiers




Standard :

- Total number of parameters allowed (per AWS account and Region) :  10,000

- Maximum size of a parameter value                               :  4 KB

- Parameter policies available                                    :  No

- Cost                                                            :  No additional charge

- Storage Pricing                                                 :  Free






Advanced :

- Total number of parameters allowed (per AWS account and Region) :  100,000

- Maximum size of a parameter value                               :  8 KB

- Parameter policies available                                    : yes

- Cost                                                            :  charges Apply

- Storage Pricing                                                 :  $0.05 per advanced parameter per month






---------------------------------------------------------- Parameters Policies (for advanced parameters)



• Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as passwords

• Can assign multiple policies at a time




1 Expiration (to delete a parameter) : 

{
    "Type": "Expiration",
    "Version": "1.0",
    "Attributes": {
        "Timestamp": "2018-12-02T21:34:33.000Z"
    }
}





2 ExpirationNotification (EventBridge)



{
    "Type": "ExpirationNotification",
    "Version": "1.0",
    "Attributes": {
        "Before": "15",
        "Unit": "Days"
    }
}


-- So in this example, 15 days before the parameter expires we'll receive notification in EventBridge which gives us enough time to actually update it

-- and make sure the parameter is not getting deleted because of the TTL.





3 NoChangeNotification (EventBridge)


{
    "Type": "NoChangeNotification",
    "Version": "1.0",
    "Attributes": {
        "After": "20",
        "Unit": "Days"
    }
}


-- maybe sometimes you wanna make sure the parameters change once in a while.

-- So you can have a no change notification in EventBridge so that if a parameter has not been updated for 20 days, then you will be notified as well.










---------------------------------------------------------- SSM Parameter Store Hands ON CLI



-- open ssm in console --> choose parameter store on left side panel ---> give path to store value (/my-app/dev/db-url) ---> string --> value = dev.database.subbu.com:3306 --> create parameter

-- dev.database.subbu.com:3306 = u can give any value 

-- now do create dev password
  
-- give path to store value (/my-app/dev/db-password) ---> securestring --> value = give password here --> KMS Key ID = i am using my own key (eg: tutorial) -----> create parameter       

-- now do create for prod environment also same like as Dev

EG : /my-app/prod/db-url , /my-app/prod/db-password


-- So we are going to use this CLI to get the parameters.

-- open cloudshell

       aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password


-- for the password it's a SecureString, and here is the value of it, which is an encrypted value.

-- So for this, you basically need to decrypt it.

-- for this you have a special parameter and it's called with-decryption,

-- so this will check whether or not I have the KMS permission to decrypt this secret that was encrypted with the KMS tutorial key.

         aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password --with-decryption


-- now observe changes 


  aws ssm get-parameters-by-path --path /my-app/dev     =  u will get all parameters from specific path if u want 

   aws ssm get-parameters-by-path --path /my-app/ --recursive  --with-decryption     = u will get all parameters under /my-app/ 












---------------------------------------------------------- SSM Parameter Store Hands ON with LAMBDA 


-- create one function with py 3.8 runtime 




import json

import boto3

ssm = boto3.client('ssm', region_name="ap-south-1")

def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"])
    print(db_password)
    return "Worked!"



-- now go to configuration ---> permissons --->  create inline policy --> system manager ---> give all access --> all resources --> nxt 

-- now do refresht he lambda page 

-- if u get errror , after adding permissions , then wait for 5 min 

-- now u will get this

-- u can see 'SecureString' is encrypted here 

-- So what we'd like to do is now decrypt it,

-- so in code for password decrypt , add 

        db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"], WithDecryption = True)


-- now do test , u will get (AccessDeniedException) error 

-- because we're not allowed to use the customer master key and decrypt our secrets.

-- So it turns out that because having given KMS access to my IAM role we're not allowed to decrypt the secrets,

-- so this is a good proof that even though I have access to this database password, because it's encrypted and I don't have access to KMS I'm not able to decrypt it,

-- and so that DB password is really safe and secure.

-- to fix this add permissions 

     permissons --> create inline policy --> kms --> add all permissions --> all resources--> create 


-- now do test , u will get decrypted values 


-- now to access through the Environment Variables 

-- create  Environment Variable --> DEV_OR_PROD	 = dev

-- add this in code 



import json

import boto3
import os 

ssm = boto3.client('ssm', region_name="ap-south-1")
dev_or_prod = os.environ['DEV_OR_PROD']


def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-password"], WithDecryption = True)
    print(db_password)
    return "Worked!"




-- if u test this u will get dev values 

--  go to env variable , change to prod (DEV_OR_PROD	 = prod )

--  do test again , u will prod values 

        





---------------------------------------------------------- SSM – Inventory




• Collect metadata from your managed instances (EC2/On-premises)

• Metadata includes installed software, OS drivers, configurations, installed updates, running services ...

        - It creates an inventory of what's happening on your managed instances.


• View data in AWS Console or store in S3 and query and analyze using Athena(for serverless.) and QuickSight(if you want to build some dashboards)

• Specify metadata collection interval (minutes, hours, days)

• Query data from multiple AWS accounts and regions

• Create Custom Inventory for your custom metadata (e.g., rack location of each managed instance)








---------------------------------------------------------- SSM - State Manager




• Automate the process of keeping your managed instances (EC2/On- premises) in a state that you define

• Use cases: bootstrap instances with software, patch OS/software updates on a schedule ...

• State Manager Association:

        • Defines the state that you want to maintain to your managed instances

        • Example: port 22 must be closed, antivirus must be installed ...

        • Specify a schedule when this configuration is applied


• Uses SSM Documents to create an Association (e.g., SSM Document to configure CW Agent)

-- So state manager is to ensure that your fleet of instances are all in a state that you desire.






---------------------------------------------------------- SSM – Patch Manager



• Automates the process of patching managed instances

• OS updates, applications updates, security updates, ...

• Supports both EC2 instances and on-premises servers

• Supports Linux, macOS, and Windows

• Patch on-demand or on a schedule using "Maintenance Windows"

• Scan instances and generate patch compliance report (missing patches)

• Patch compliance report can be sent to S3



----------------------- So patch manager has two components




• Patch Baseline

        • Defines which patches should and shouldn’t be installed on your instances

        • Ability to create custom Patch Baselines (specify approved/rejected patches)

        • Patches can be auto-approved within days of their release

        • By default, install only critical patches and patches related to security onto your SSM managed instances.


• Patch Group

        • Associate a set of instances with a specific Patch Baseline

        • Example: create Patch Groups for different environments (dev, test, prod)

        • Instances should be defined with the tag key Patch Group

        • An instance can only be in one Patch Group

        • Patch Group can be registered with only one Patch Baseline




----------------------- SSM – Patch Manager Patch Baselines



• Pre-Defined Patch Baseline

        • Managed by AWS for different Operating Systems (can’t be modified)

        • AWS-RunPatchBaseline (SSM Document) – apply both operating system and application patches (Linux, macOS,Windows Server)


• Custom Patch Baseline

        • Create your own Patch Baseline and choose which patches to auto-approve

        • Operating System, allowed patches, rejected patches, ...

        • Ability to specify custom and alternative patch repositories






---------------------------------------------------------- SSM – Maintenance Windows



• Defines a schedule for when to perform actions on your instances

• Example: OS patching, updating drivers, installing software, ...

• Maintenance Window contains

        • Schedule
        • Duration
        • Set of registered instances 
        • Set of registered tasks







---------------------------------------------------------- SSM – Patch Manager and Maintenance Windows Hands ON





-- open system manager --> patch manager --> create policy --> Scanning and installation = Scan and install --> all values are default or select as ur wish --> create 


        - we have the option to either do a scan to have a look at all my instances and see if some patches are missing,

        - or we can do a scan and install.



-- now open Maintenance Windows ,we can create a maintenance window to run our patches.

-- create window with the values of you want 

-- within this window, what we can do is that we can register specific tasks that will be run.

-- open window id --> Tasks --> register tasks --> Register Run command task --> Command document = AWS-RunPatchBaseline --> targets = select ec2 instance u want --> give rate control and error threshold --> create 

-- So now, that means that within my maintenance window, this run patch baseline will be run, and it will happen only during this window.

-- give time of execution and check the instaces patched Successfully








---------------------------------------------------------- SSM – Session Manager



• Allows you to start a secure shell on your EC2 and on- premises servers

• Access through AWS Console, AWS CLI, or Session Manager SDK

• Does not need SSH access, bastion hosts, or SSH keys

• Supports Linux, macOS, and Windows

• Log connections to your instances and executed commands

• Session log data can be sent to S3 or CloudWatch Logs

• CloudTrail can intercept StartSession events



• IAM Permissions

        • Control which users/groups can access Session Manager and which instances

        • Use tags to restrict access to only specific EC2 instances

        • Access SSM + write to S3 + write to CloudWatch


• Optionally, you can restrict commands a user can run in a session


EG : 


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ssm:SendCommand",
                "ssm:GetCommandInvocation",
                "ssm:DescribeInstanceInformation",
                "ssm:ListCommands",
                "ssm:ListCommandInvocations"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "ec2:ResourceTag/Environment": "DEV"
                }
            }
        }
    ]
}



-- this is an IAM policy that allows you to connect to any instance, as long as the resource type of environment is named dev.












======================================================== High Availability & Scalability ========================================================





---------------------------------------------------------- Scalability & High Availability



• Scalability means that an application / system can handle greater loads by adapting.

• There are two kinds of scalability: 

        • Vertical Scalability
        • Horizontal Scalability (= elasticity)


• Scalability is linked but different to High Availability




---------------------------------------------------------- Vertical Scalability



• Vertically scalability means increasing the size of the instance

• For example, your application runs on a t2.micro

• Scaling that application vertically means running it on a t2.large

• Vertical scalability is very common for non distributed systems, such as a database.

• RDS, ElastiCache are services that can scale ver tically.

• There’s usually a limit to how much you can vertically scale (hardware limit)





---------------------------------------------------------- Horizontal Scalability



• Horizontal Scalability means increasing the number of instances / systems for your application

• Horizontal scaling implies distributed systems.

• This is very common for web applications / modern applications

• It’s easy to horizontally scale thanks the cloud offerings such as Amazon EC2






---------------------------------------------------------- High Availability



• High Availability usually goes hand in hand with horizontal scaling

• High availability means running your application / system in at least 2 data centers (== Availability Zones)

• The goal of high availability is to survive a data center loss

• The high availability can be passive (for RDS Multi AZ for example)

• The high availability can be active (for horizontal scaling)





---------------------------------------------------------- High Availability & Scalability For EC2



• Vertical Scaling: Increase instance size (= scale up / down)

        • From: t2.nano - 0.5G of RAM, 1 vCPU

        • To: u-12tb1.metal – 12.3 TB of RAM, 448 vCPUs



• Horizontal Scaling: Increase number of instances (= scale out / in)

        • Auto Scaling Group
        • Load Balancer


• High Availability: Run instances for the same application across multi-AZ

        • Auto Scaling Group multi-AZ
        • Load Balancer multi-AZ







---------------------------------------------------------- What is load balancing?



• Load Balances are servers that forward traffic to multiple servers (e.g., EC2 instances) downstream



Why use a load balancer?


        • Spread load across multiple downstream instances

        • Expose a single point of access (DNS) to your application

        • Seamlessly handle failures of downstream instances

        • Do regular health checks to your instances

        • Provide SSL termination (HTTPS) for your websites

        • Enforce stickiness with cookies

        • High availability across zones

        • Separate public traffic from private traffic







----------------------------------------------- Why use an Elastic Load Balancer?



• An Elastic Load Balancer is a managed load balancer

        • AWS guarantees that it will be working

        • AWS takes care of upgrades, maintenance, high availability

        • AWS provides only a few configuration knobs



• It costs less to setup your own load balancer but it will be a lot more effort on your end


• It is integrated with many AWS offerings / services


        • EC2, EC2 Auto Scaling Groups, Amazon ECS

        • AWS Certificate Manager (ACM), CloudWatch

        • Route53,AWSWAF,AWSGlobalAccelerator








-- A data analytics company has its server infrastructure built on Amazon EC2 instances fronted with Elastic Load Balancers (ELBs). The ELBs are maintained in two AZs with each ELB having two EC2 instances registered with it. Both the instances in one AZ have been recorded as unhealthy.

What is the status of traffic that flows to the ELB connected to unhealthy instances?


ANS : The Load Balancer routes requests to the unhealthy targets 

EXP :

      - If there is at least one healthy target in a target group, the load balancer routes requests only to the healthy targets. 

      - If a target group contains only unhealthy targets, the load balancer routes requests to the unhealthy targets. 

      - Hence, it is advised to configure an Auto Scaling Group, if the instances are hosting a business-critical application.











----------------------------------------------- Health Checks



• Health Checks are crucial for Load Balancers

• They enable the load balancer to know if instances it forwards traffic to are available to reply to requests

• The health check is done on a port and a route (/health is common)

• If the response is not 200 (OK), then the instance is unhealthy







---------------------------------------------------- Types of load balancer on AWS



• AWS has 4 kinds of managed Load Balancers

• Classic Load Balancer (v1 - old generation) – 2009 – CLB 

        • HTTP, HTTPS,TCP,SSL(secureTCP)


• Application Load Balancer (v2 - new generation) – 2016 – ALB

        • HTTP, HTTPS,WebSocket


• Network Load Balancer (v2 - new generation) – 2017 – NLB

        • TCP,TLS(secureTCP),UDP


• Gateway Load Balancer – 2020 – GWLB

        • Operates at layer 3 (Network layer) – IP Protocol




• Overall, it is recommended to use the newer generation load balancers as they provide more features

• Some load balancers can be setup as internal (private) or external (public) ELBs







---------------------------------------------------- Load Balancer Security Groups



-- So the users can access your load balancer from anywhere using HTTP or HTTPS.

-- therefore the security group rule is going to look like something like this, where the port range is going to be 80 or 443. And the source is going to be 0.0.0.0/0, which means anywhere. And so we allow the users to connect to our load balancer,

-- but then the cool thing is that EC2 instances should only allow traffic coming directly from the load balancer.

-- therefore the security group rule of your EC2 instances is going to look a little bit different.

-- So it's going to allow HTTP traffic on port 80 and the source of it is not going to be an IP range is going to be a security group.

-- So we're going to link the security group of the EC2 instance, to the security group of the load balancer.

-- effectively what this will do is that it will say that the EC2 instance is only allowing traffic if the traffic originates from the load balancer, which is an enhanced security mechanism.





---------------------------------------------------- Classic Load Balancers (v1)



• Suppor ts TCP (Layer 4), HTTP & HTTPS (Layer 7)

• Health checks are TCP or HTTP based

• Fixed hostname XXX.region.elb.amazonaws.com






---------------------------------------------------- Application Load Balancer (v2)


• Application load balancers is Layer 7 (HTTP)

• Load balancing to multiple HTTP applications across machines (target groups)

• Load balancing to multiple applications on the same machine (ex: containers)

• Suppor t for HTTP/2 and WebSocket

• Support redirects (from HTTP to HTTPS for example)



• Routing tables to different target groups:

        • Routing based on path in URL (example.com/users & example.com/posts)

        • Routing based on hostname in URL (one.example.com & other.example.com)

        • Routing based on Query String, Headers

                (example.com/users?id=123&order=false)


• ALB are a great fit for micro services & container-based application (example: Docker & Amazon ECS)

• Has a port mapping feature to redirect to a dynamic port in ECS

• In comparison, we’d need multiple Classic Load Balancer per application






---------------------------------------------------- Application Load Balancer (v2) Target Groups



• EC2 instances (can be managed by an Auto Scaling Group) – HTTP

• ECS tasks (managed by ECS itself) – HTTP

• Lambda functions – HTTP request is translated into a JSON event

• IP Addresses – must be private IPs

• ALB can route to multiple target groups

• Health checks are at the target group level






---------------------------------------------------- Application Load Balancer (v2) Good to Know


• Fixed hostname (XXX.region.elb.amazonaws.com)

• The application servers don’t see the IP of the client directly

        • The true IP of the client is inserted in the header X-Forwarded-For

        • We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)





Hands on ---- refer SAA Course 







---------------------------------------------------- Network Load Balancer (v2)



• Network load balancers (Layer 4) allow to:

        • Forward TCP & UDP traffic to your instances

        • Handle millions of request per seconds

        • Ultra-low latency



• NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP)


• NLB are used for extreme performance,TCP or UDP traffic

• Not included in the AWS free tier






---------------------------------------------------- Network Load Balancer – Target Groups 



• EC2 instances

• IP Addresses – must be private IPs

• Application Load Balancer

        - it's also possible for you to have a Network Load Balancer in front of an Application Load Balancer.

        - So in that case, the NLB is in front of your ALB.

        - WHY? for example, the fixed IP addresses, and then, thanks to the Application Load Balancer, you can get all the rules that you have around handling HTP type of traffic, so it is a valid combination.


-- Network Load Balancer target groups are support three different kind of protocols.

        • Health Checks support the TCP, HTTP and HTTPS Protocols





--------- Hands on See SAA course







---------------------------------------------------- Gateway Load Balancer



• Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS

• Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, payload manipulation, ...

• Operates at Layer 3 (Network Layer) – IP Packets

• Combines the following functions:

        • Transparent Network Gateway – single entry/exit for all traffic

        • Load Balancer – distributes traffic to your virtual appliances


• Uses the GENEVE protocol on port 6081        







---------------------------------------------------- Gateway Load Balancer –Target Groups




• EC2 instances

• IP Addresses – must be private IPs







---------------------------------------------------- Sticky Sessions (Session Affinity)




• It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer

• This works for Classic Load Balancer, Application Load Balancer, and Network Load Balancer

• For both CLB & ALB, the “cookie” used for stickiness has an expiration date you control

• Use case: make sure the user doesn’t lose his session data

• Enabling stickiness may bring imbalance to the load over the backend EC2 instances

 



---------------------------------------------------- Sticky Sessions – Cookie Names



• Application-based Cookies

        • Custom cookie

                • Generated by the target
                • Can include any custom attributes required by the application
                • Cookie name must be specified individually for each target group
                • Don’t use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)

        • Application cookie

                • Generated by the load balancer
                • Cookie name is AWSALBAPP



• Duration-based Cookies

        • Cookie generated by the load balancer

        • Cookie name is AWSALB for ALB, AWSELB for CLB






---------------------------------------------------- Cross-Zone Load Balancing


With Cross Zone Load Balancing:

        - each load balancer instance distributes evenly across all registered instances in all AZ

Without Cross Zone Load Balancing:

        - Requests are distributed in the instances of the node of the Elastic Load Balancer




• Application Load Balancer

        • Enabled by default (can be disabled at the Target Group level)

        • No charges for inter AZ data


• Network Load Balancer & Gateway Load Balancer

        • Disabled by default

        • You pay charges ($) for inter AZ data if enabled


• Classic Load Balancer

        • Disabled by default

        • No charges for inter AZ data if enabled





 

 ---------------------------------------------------- SSL/TLS - Basics




• An SSL Certificate allows traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption)

• SSL refers to Secure Sockets Layer, used to encrypt connections

• TLS refers to Transport Layer Security, which is a newer version

• Nowadays, TLS cer tificates are mainly used, but people still refer as SSL

• Public SSL certificates are issued by Certificate Authorities (CA)

• Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc...

• SSL certificates have an expiration date (you set) and must be renewed





----------------------------- Load Balancer - SSL Certificates




       HTTPS(encrypted)                      HTTP Over private VPC 
       Over www
Users ----------------->      Load balancer  ---------------------->     ec2 instances 
      <-----------------                     <----------------------  


• The load balancer uses an X.509 certificate (SSL/TLS server certificate)

• You can manage certificates using ACM (AWS Certificate Manager)

• You can create upload your own certificates alternatively

• HTTPS listener:
  • You must specify a default certificate
  • You can add an optional list of certs to support multiple domains
  • Clients can use SNI (Server Name Indication) to specify the hostname they reach
  • Ability to specify a security policy to support older versions of SSL /TLS (legacy clients)





---------------------------------------------------- SSL – Server Name Indication (SNI)



• SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)

• It’s a “newer” protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake

• The server will then find the correct certificate, or return the default one


Note:

        • Only works for ALB & NLB (newer generation), CloudFront

        • Does not work for CLB (older gen)





---------------------------------------------------- Elastic Load Balancers – SSL Certificates

• Classic Load Balancer (v1)
  
   • Support only one SSL certificate
   • Must use multiple CLB for multiple hostname with multiple SSL certificates

• Application Load Balancer (v2)

   • Supports multiple listeners with multiple SSL certificates
   • Uses Server Name Indication (SNI) to make it work

• Network Load Balancer (v2)
 
   • Supports multiple listeners with multiple SSL certificates
   • Uses Server Name Indication (SNI) to make it work






---------------------------------------------------- Connection Draining




• Feature naming

   • Connection Draining – for CLB
   • Deregistration Delay – for ALB & NLB

• Time to complete “in-flight requests” while the instance is de-registering or unhealthy

• Stops sending new requests to the EC2 instance which is de-registering

• Between 1 to 3600 seconds (default: 300 seconds)

• Can be disabled (set value to 0)

• Set to a low value if your requests are short






---------------------------------------------------- ELB Health Checks



• Target Health Status

        • Initial: registering the target
        • Healthy
        • Unhealthy
        • Unused: target is not registered
        • Draining: de-registering the target
        • Unavailable: health checks disabled


• If a target group contains only unhealthy targets, ELB routes requests across its unhealthy targets




Setting                         Value           Description

HealthCheckProtocol             HTTP            Protocol used to perform health checks

HealthCheckPort                 80              Port used to perform health checks

HealthCheckPath                 /               Destination for health checks on targets

HealthCheckTimeoutSeconds       5               Consider the health check failed if no response after 5 seconds

HealthCheckIntervalSeconds      30              Send health check every 30 seconds

HealthyThresholdCount           3               Consider the target healthy after 3 successful health checks

UnhealthyThresholdCount         5               Consider the target unhealthy after 5 failed health checks










---------------------------------------------------- Load Balancer Error codes



• Successful request : Code 200.

• Unsuccessful at client side : 4XX code.

        • Error 400 : Bad Request
        • Error 401 : Unauthorized
        • Error 403 : Forbidden
        • Error 460 : Client closed connection.
        • Error 463 : X-Forwarded For header with >30 IP (Similar to malformed request).


• Unsuccessful at server side : 5xx code.

        • An error 500 / Internal server error would mean some error on the ELB itself. • Error 502 : Bad Gateway
        • An error 503 / Service Unavailable
        • Error 504 / Gateway timeout : probably an issue within the server.
        • Error 561 : Unauthorized





---------------------------------------------------- Load Balancers Monitoring



• All Load Balancer metrics are directly pushed to CloudWatch metrics

        • BackendConnectionErrors 

        • HealthyHostCount /UnHealthyHostCount 

        • HTTPCode_Backend_2XX: Successful request.

        • HTTPCode_Backend_3XX: redirected request

        • HTTPCode_ELB_4XX: Client error codes

        • HTTPCode_ELB_5XX: Server error codes generated by the load balancer.

        • Latency

        - RequestCount

        - RequestCountPerTarget

        - SurgeQueueLength: The totalnumber of requests (HTTPlistener) or connections (TCPlistener) that are pending routing to a healthy instance.Help to scale out ASG. Max value is 1024

        - SpilloverCount: The total number of requests that were rejected because the surge queue is full.







---------------------------------------------------- Load Balancer troubleshooting using metrics



• HTTP 400: BAD_REQUEST => The client sent a malformed request that does not meet HTTP specifications.

• HTTP 503: Service Unavailable => Ensure that you have healthy instances in every Availability Zone that your load balancer is configured to respond in. Look for HealthyHostCount in CloudWatch

• HTTP 504: Gateway Timeout => Check if keep-alive settings on your EC2 instances are enabled and make sure that the keep-alive timeout is greater than the idle timeout settings of load balancer.

• Set alarms & look at the documentation for troubleshooting: 

        https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ts-elb-error- message.html





---------------------------------------------------- Load Balancers Access Logs



• Access logs from Load Balancers can be stored in S3 and contain:

        • Time
        • Client IP address 
        • Latencies
        • Request paths
        • Server response 
        • Trace Id



• Only pay for the S3 storage

• Helpful for compliance reason

• Helpful for keeping access data even after ELB or EC2 instances are terminated

• Access Logs are already encrypted

-- go to ALB --> Attributes --> edit --> enable access logs and give bucket name if u want to store 








---------------------------------------------------- Application Load Balancer RequestTracing



• Request tracing – Each HTTP request has an added custom header ‘X-Amzn-Trace-Id’

• Example:

        X-Amzn-Trace-Id: Root=1-67891233-abcdef012345678912345678


• This is useful in logs / distributed tracing platform to track a single request

• Application Load Balancer is not (yet) integrated with X-Ray








---------------------------------------------------- Target Groups Settings


• deregisteration_delay.timeout_seconds: time the load balancer waits before deregistering a target

• slow_start.duration_seconds: 

• load_balancing.algorithm.type: how the load balancer selects targets when routing requests (Round Robin, Least Outstanding Requests)

• stickiness.enabled

• stickiness.type: application-based or duration-based cookie

• stickiness.app_cookie.cookie_name: name of the application cookie

• stickiness.app_cookie.duration_seconds: application-based cookie expiration period

• stickiness.lb_cookie.duration_seconds: duration-based cookie expiration period






---------------------------------------------------- Slow Start Mode 


• By default, a target receives its full share of requests once it’s registered with the target group

• if you enable Slow Start Mode gives healthy targets time to warm-up before the load balancer sends them a full share of requests

• The load balancer linearly increases the number of requests that it sends to the target

        - If you don't have slow start mode, all of a sudden, as soon as your EC2 instance is part of your target group is going to receive a full share of requests, which may overload the instance directly.

        - But with slow start mode, there's going to be a gradual increase. So at first it will receive one request, then it will receive two requests, then it will receive three requests and so on up until the slow start is over and then the EC2 instance will be at full capacity.


• A target exits Slow Start Mode when:

        • The duration period elapses

        • The target becomes unhealthy


• To disable, set Slow star t duration value to 0





---------------------------------------------------- Request Routing Algorithms – Least Outstanding Requests



• The next instance to receive the request is the instance that has the lowest number of pending/unfinished requests

• Works with Application Load Balancer and Classic Load Balancer (HTTP/HTTPS)




---------------------------------------------------- Request Routing Algorithms – Round Robin


• Equally choose the targets from the target group

• Works with Application Load Balancer and Classic Load Balancer(TCP)



---------------------------------------------------- Request Routing Algorithms – Flow Hash   (NLB)


• Selects a target based on the protocol, source/destination IP address, source/destination port, and TCP sequence number

• Each TCP/UDP connection is routed to a single target for the life of the connection which is sort of the equivalent for the sticky sessions

• Works with Network Load Balancer




-- go to target groups --> actions --> edit attributes --> deregistration delay,slow start ,algorith options 







---------------------------------------------------- ALB – Listener Rules


• Processed in order (with Default Rule)

• Supported Actions (forward, redirect,fixed-response)

• Rule Conditions:

        • host-header
        • http-request-method
        • path-pattern
        • source-ip
        • http-header
        • query-string


-- each rule will have a specific target








---------------------------------------------------- Target Group Weighting



• Specify weight for each Target Group on a single Rule

• Example: multiple versions of your app, blue/green deployment

• Allows you to control the distribution of the traffic to your applications









---------------------------------------------------- What’s an Auto Scaling Group?





• In real-life, the load on your websites and application can change

• In the cloud, you can create and get rid of servers very quickly

• The goal of an Auto Scaling Group (ASG) is to:
   • Scale out (add EC2 instances) to match an increased load
   • Scale in (remove EC2 instances) to match a decreased load
   • Ensure we have a minimum and a maximum number of EC2 instances running
   • Automatically register new instances to a load balancer
   • Re-create an EC2 instance in case a previous one is terminated (ex: if unhealthy)

• ASG are free (you only pay for the underlying EC2 instances)

IMP : When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the earlier ones. This way, rebalancing does not compromise the performance or availability of your application.

-- Because Amazon EC2 Auto Scaling attempts to launch new instances before terminating the earlier ones, being at or near the specified maximum capacity could impede or completely halt rebalancing activities.






-- After configuring Amazon EC2 Auto Scaling, a systems administrator had tried to launch the Auto Scaling Group. But, the following launch failure message was displayed - Client.InternalError: Client error on launch.

What is the cause of this error and how can it be fixed?


ANS : This error can be caused when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer-managed CMK used to encrypt it

EXP :

        - Client.InternalError: Client error on launch error is caused when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer-managed CMK used to encrypt it. Additional setup is required to allow the Auto Scaling group to launch instances.

        - There are two scenarios possible:

                1) CMK and Auto Scaling group are in the same AWS account,

                2) CMK and Auto Scaling group are in different AWS accounts.


                https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-12   - ref for 2 cases 







---------------------------------------------------- Auto Scaling Group Attributes



• A Launch Template (older “Launch Configurations” are deprecated)

        • AMI + InstanceType
        • EC2 User Data 
        • EBSVolumes
        • Security Groups 
        • SSH Key Pair
        • IAM Roles for your EC2 Instances
        • Network + Subnets Information 
        • Load Balancer Information


• Min Size / Max Size / Initial Capacity

• Scaling Policies





---------------------------------------------------- Auto Scaling - CloudWatch Alarms & Scaling



• It is possible to scale an ASG based on CloudWatch alarms

• An alarm monitors a metric (such as Average CPU, or a custom metric)

• Metrics such as Average CPU are computed for the overall ASG instances

• Based on the alarm:
  • We can create scale-out policies (increase the number of instances)
  • We can create scale-in policies (decrease the number of instances)









-- create a Load Balancer with  empty TG

-- u have to create Launch template (user-data), u can't edit LT once u created , u have create new LT if u want new version of ur appn 

-- c.o on ASG and Give name as you want and select Launch Template(IMP:Make sure AMI Version is with Kernal 5.10.....) ,select the version of template you want

-- using with LT, create ASG 

-- ASG use LT to launch ec2 instance 

-- if u want ur big Application like tomact on ec2 instance(ASG), cretae ec2 first ,deploy the appn and create AMI 

-- use this custom AMI in LT instead on Linux and create LT 

-- u provide min , max and desired capacity while creating ASG 

-- ASG will launch ec2 in TG automatically 

-- Always Turn on ALB Health checks 'coz, 

Elastic Load Balancing monitors whether instances are available to handle requests. When it reports an unhealthy instance, EC2 Auto Scaling can replace it on its next periodic check.

-- here we need to use sns ( simple notification servie) 

-- in SNS , we have TOPIC and SUBSCRIPTION 

-- in sns u need to crate TOPIC , once u created u need to subscribe

-- u can add this topic in ASG creation

-- create LB with simple TG no need to add any instances as targets ,this job will do by ASG for us during scale out and scale in process

-- now launch template with user-data

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html

-- create auto scaling group 

-- give LT and subnets as u preffered 

-- in notification topic u jst create topic to get notifications 

-- crate ASG 

-- what this ASG will do is that , it wil take LT and it will launch ec2 automatically according to ur desired capacity , so LT has user data so then our ec2 have index.html appn,all the ec2 instances will registered in the target groups automatically  and these TG are linked to Load balancer , so if u can access LB u will get appn 

-- all the things u have do in the launch template only , based on these LT ec2 will created 

-- go n check in target group 

-- in targets all ec2 are gettig registered 

-- There are 3 types of scaling 

1  manual scaling : u can change manually capapcity 

2  schedule Scaling : do schedule when u want to scale out happens when u feel the traffic will more o the certain days or time 

3  dynamic scalng : automatically happens 


-- u can check in activity section how scale out and scle in happens 

-- once it reach to max capaicty it wont go more than that as th given capacity , u can edit once if u want more capacity to scale out 





----------------------------------------------------- scaleout practicals 

-- (IMP:Make sure AMI Version is with Kernal 5.10.....) ,select the version of template you want

-- conect one instace to SSH and install manual load to the EC2 instance by install stress commands manually

--  enter the command

 sudo amazon-linux-extras install epel -y 


 - if it is not working go to 2nd command directly

-- once avove command is successfully installed , then enter this command

  sudo yum install stress -y

-- once you installed, then try to push the load manually by entering the command

  stress -c 5

--  wait for sometime(4-5 min) to updating the data , then check in ASG and see how the changs are happening

-- u can see the scale out is happening and meet the max capacity







----------------------------------------------------- Auto Scaling Groups – Scaling Policies



• Dynamic Scaling

        • Target Tracking Scaling

                • Simple to set-up
                • Example: I want the average ASG CPU to stay at around 40%

        • Simple / Step Scaling

                • When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units

                • When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1


• Scheduled Scaling

        • Anticipate a scaling based on known usage patterns

        • Example: increase the min capacity to 10 at 5 pm on Fridays





------------- Good metrics to scale on :



-- CPU utilization       : AVg CPU across ur instance 
-- REquestCountPertarget : to makesure the no.of request per ec2 instance is stable
-- AVG Network in/out    : if u r appn is network bound 


IMP NOTE : the ASG can not go over the max capacity that u have mentioned during the creation time 





----------------------------------------------------- Auto Scaling Groups - Scaling Cooldowns



• After a scaling activity happens, you are in the cooldown period (default 300 seconds)

• During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize)

• Advice: Use a ready-to-use AMI to reduce configuration time in order to be serving request fasters and reduce the cooldown period







----------------------------------------------------- ASG For SysOps  -----------------------------------------------------



---------------------------- ASG – Lifecycle Hooks


-- So, the lifecycle hooks is a way for you to hook into the lifecycle of an ASG instances.

• By default, as soon as an instance is launched in an ASG it’s in service

-- But you can set up an lifecycle hook for that effect.

• You can perform extra steps before the instance goes in service (Pending state)

        • Define a script to run on the instances as they start

        - So when you're done with the initial setup of your EC2 instances, you make them go into a pending proceed state. And then after that, they will be moved into the in service states.        
     
        - So this lifecycle hook really allows you to perform some kind of custom logic between the pending and the in service states.

• You can perform some actions before the instance is terminated(Terminating state)        

        • Pause the instances before they’re terminated for troubleshooting

        - this will give you is the opportunity, for example, to take the logs out of your instances.

        - So let's say that instance goes from in service to terminating, then in part of your lifecycle hook, okay, you can go into the terminating wait state, okay. 
        
        - And then when you're there, you can execute again, some scripts, or get some logs out, or do whatever you want, or even get some information out, take an AMI, whatever you want, really. Or take an EBS snapshot, and then go into terminating proceed.



-- the use-cases for all these lifecycle hooks is really to do cleanup, log extraction, or special health checks, before your instance is started and goes in service.

• Integration with EventBridge, SNS, and SQS






----------------------------------------------------- Launch Configuration vs. Launch Template



• Both:

        • ID of the AmazonMachineImage(AMI),the instancetype,a keypair,securitygroups,and the other parameters that you use to launch EC2 instances (tags, EC2 user-data...)

        • You can’t edit both LaunchConfigurations and LaunchTemplates



• Launch Configuration (legacy):

        • Must be re-created every time


• LaunchTemplate(newer):

        • Can have multiple versions
        • Create parameters subsets (partial configuration for re-use and inheritance) 
        • Provision using both On-Demand and Spot instances (or a mix)
        • Suppor ts Placement Groups, Capacity Reser vations, Dedicated hosts,
        • CanuseT2unlimitedburstfeature
        • RecommendedbyAWSgoingforward






Launch Configuration :

-- Launch configuration is an instance configuration template that an Auto Scaling Group uses to launch EC2 instances.

-- Launch configuration is similar to EC2 configuration and involves the selection of the Amazon Machine Image (AMI), block devices, key pair, instance type, security groups, user data, EC2 instance monitoring, instance profile, kernel, ramdisk, the instance tenancy, whether the instance has a public IP address, and is EBS-optimized.

-- Launch configuration can be associated with multiple ASGs

-- Launch configuration can’t be modified after creation and needs to be created new if any modification is required.

-- Basic or detailed monitoring for the instances in the ASG can be enabled when a launch configuration is created.

-- By default, basic monitoring is enabled when you create the launch configuration using the AWS Management Console, and detailed monitoring is enabled when you create the launch configuration using the AWS CLI or an API

-- AWS recommends using Launch Template instead.







Launch Template :

-- A Launch Template is similar to a launch configuration, with additional features, and is recommended by AWS.

-- Launch Template allows multiple versions of a template to be defined.

-- With versioning, a subset of the full set of parameters can be created and then reused to create other templates or template versions for e.g, a default template that defines common configuration parameters can be created and allow the other parameters to be specified as part of another version of the same template.

-- Launch Template allows the selection of both Spot and On-Demand Instances or multiple instance types.

-- Launch templates support EC2 Dedicated Hosts. Dedicated Hosts are physical servers with EC2 instance capacity that are dedicated to your use.

-- Launch templates provide the following features

   - Support for multiple instance types and purchase options in a single ASG.

   - Launching Spot Instances with the capacity-optimized allocation strategy.
   - Support for launching instances into existing Capacity Reservations through an ASG.
   - Support for unlimited mode for burstable performance instances.
   - Support for Dedicated Hosts.
   - Combining CPU architectures such as Intel, AMD, and ARM (Graviton2)
   - Improved governance through IAM controls and versioning.
   - Automating instance deployment with Instance Refresh.








----------------------------------------------------- SQS with Auto Scaling Group (ASG)



-- So how to scale, and it's gonna group, based on an SQS queue status.

-- So, in this example we have an SQS queue, and a bunch of EC2 instances processing messages from it.

-- And what you want to do, is to scale your ASG based on whether or not you have more messages in your SQS queue.

-- So for this, you can create a CloudWatch metric. For example, on the queue length.

        - So the metric name is ApproximateNumberOfMessages

-- whenever that queue length is too big, that means you have too many messages to process.        

-- Then you can create an alarm, and that alarm will go into CloudWatch alarm, and would be triggering a scaling policy on your ASG.






----------------------------------------------------- ASG Health Checks



• To make sure you have high availability, means you have least 2 instances running across 2 AZ in your ASG (must configure multi-AZ ASG)

• Health checks available:

        • EC2 Status Checks: So, to make sure that the underlying software and hardware of your EC2 instances are still functioning, which is enabled by default.

        • ELB Health Checks: So this is to make sure that your application, if linked to a target group and an ALB will be checked, will be having its health checked by the ELB as well.

        • Custom Health Checks: send instance’s health to ASG using AWS CLI or AWS SDK       


• ASG will launch a new instance after terminating an unhealthy one

• ASG will not reboot unhealthy hosts for you

• Good to know CLI:

        • set-instance-health (use with Custom Health Checks)

        • terminate-instance-in-auto-scaling-group






----------------------------------------------------- Troubleshooting ASG issues



• <number of instances> instance(s) are already running. Launching EC2 instance failed.

        • The Auto Scaling group has reached the limit set by the MaximumCapacity parameter. Update your Auto Scaling group by providing a new value for the maximum capacity.


• Launching EC2 instances is failing:

        • The security group does not exist. SG might have been deleted

        • The key pair does not exist. The key pair might have been deleted


• If the ASG fails to launch an instance for over 24 hours, it will automatically suspend the processes (administration suspension)






----------------------------------------------------- CloudWatch Metrics for ASG



• Metrics are collected every 1 minute

• ASG-level metrics: (opt-in)

        • GroupMinSize, GroupMaxSize, GroupDesiredCapacity : which represents the value of these parameters over time.

        • GroupInServiceInstances, GroupPendingInstances, GroupStandbyInstances 

        • GroupTerminatingInstances, GroupTotalInstances

        • You should enable metric collection to see these metrics


• EC2-level metrics (enabled): CPU Utilization, etc...

        • Basic monitoring: 5 minutes granularity

        • Detailed monitoring: 1 minute granularity







----------------------------------------------------- AWS Auto Scaling



• Backbone service of auto scaling for scalable resources in AWS:

• Amazon EC2 Auto Scaling groups: Launch or terminate EC2 instances

• Amazon EC2 Spot Fleet requests: Launch or terminate instances from a Spot Fleet request, or automatically replace instances that get interrupted for price or capacity reasons.

• Amazon ECS:Adjust the ECS service desired count up or down

• Amazon DynamoDB (table or global secondary index):WCU & RCU

• Amazon Aurora: Dynamic Read Replicas Auto Scaling




----------------------------------------------------- AWS Auto Scaling – Scaling Plans



• Dynamic scaling: creates a target tracking scaling policy

        • Optimize for availability => 40% of resource utilization

        • Balance availability and cost => 50% of resource utilization

        • Optimize for cost => 70% of resource utilization

        • Custom => choose own metric and target value

        • Options:Disablescale-in,cooldownperiod,warmup time (for ASG)



• Predictive scaling: continuously forecast load and schedule scaling ahead







======================================================== ElasticBeanStalk (see previous course) ========================================================







======================================================== Cloudformation For SysOps ========================================================



-- check the below topics in previous course 


        - Resources 
        - parameters
        - Mappings 
        - Outputs&exports 
        - conditions
        - intrinsic functions
        - rollbacks 
        - service role 
        - service role 
        - capabilities
        - Deletion Policy 
        - Stack policy
        - termination protection
        - custom resources 
        - dynamic References






======================================================== SysOps-specific lectures below (CloudFormation) ========================================================



-- check cloudformation course for the below topics 

        - UserData 
        - cfn-init
        - cfn-signal & wait condition 
        - cfn-signal Failures
        - Nested Stacks 
        - Depends ON
        - Stack Sets 
        - troubleshooting
        







======================================================== AWS Lambda ========================================================






------------------------------------------------------ Why AWS Lambda


Amazon EC2 :

  • Virtual Servers in the Cloud
  • Limited by RAM and CPU
  • Continuously running
  • Scaling means intervention to add / remove servers


Amazon Lambda : 

   • Virtual functions – no servers to manage!
   • Limited by time - short executions
   • Run on-demand
   • Scaling is automated!








------------------------------------------------------ Benefits of AWS Lambda


• Easy Pricing:

    • Pay per request and compute time 

    • Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time


• Integrated with the whole AWS suite of services

• Integrated with many programming languages

• Easy monitoring through AWS CloudWatch

• Easy to get more resources per functions (up to 10GB of RAM!)

• Increasing RAM will also improve CPU and network!









------------------------------------------------------ AWS Lambda language support


• Node.js (JavaScript)
• Python
• Java (Java 8 compatible)
• C# (.NET Core)
• Golang
• C# / Powershell
• Ruby
• Custom Runtime API (community supported, example Rust)


• Lambda Container Image : this Lambda container image is quite special.

     • The container image must implement the Lambda Runtime API , so it's not any container image that can run on Lambda. There needs to be some prerequisites about how that container image is built.

     • ECS / Fargate is preferred for running arbitrary Docker images


-- So the exam, if they ask you to run a container on Lambda, unless that container does implement the Lambda runtime API, you will run  that container on ECS or Fargate.












------------------------------------------------------ AWS Lambda Integrations Main ones


1 API Gateway : So API Gateway is to create a REST API, and they will invoke our Lambda functions.


2 Kinesis : Kinesis will be using Lambda to do some data transformations on the fly.


3 DynamoDB :  DynamoDB will be used to create some triggers, so whenever something happens in our database a Lambda function will be triggered.


4 S3 : A Lambda function would be triggered anytime, for example, a file is created in S3.


5 CloudFront :  CloudFront, this will be Lambda@edge,


6 CloudWatch Events : CloudWatch Events or EventBridge. This is whenever things happen in our infrastructure on AWS, and we want to be able to react to things.

             - For example, say we have a cut pipeline, state changes and we want to do some automations based on it, we can use a Lambda function.


7 CloudWatch Logs : CloudWatch Logs, to stream these logs, wherever you want.


8 SNS : SNS to react to notifications and your SNS topics.


9 SQS : SQS to process messages from your SQS queues.


10 Cognito : Cognito to react to whenever, for example, a user login to your database.




-- So, these are just the main ones. There are tons of Lambda integrations.









------------------------------------------------------ Example: Serverless Thumbnail creation


-- So let's say we have an S3 bucket, and we want to create thumbnails on the fly.

-- So there will be an event which is that the new image will be uploaded in Amazon S3.

-- This will trigger, through an S3 event notification, a Lambda function.

-- that Lambda function will have code to generate a thumbnail. That thumbnail maybe pushed and uploaded into another S3 bucket or the same S3 bucket, which would be a smaller version of that image.

-- And also, our Lambda function may want to insert some data into DynamoDB, around some metadata for the image, for example the image, name, size, creation date, etc..




New image in S3 -------------(trigger)----------> AWS Lambda Function Creates a Thumbnail -------------(Push)-------> New thumbnail in S3 / Metadata in DynamoDB









------------------------------------------------------ Example: Serverless CRON Job


-- So CRON is a way on your EC2 instances, for example, to generate jobs every five minutes, or every Monday at 10:00 AM, etc

-- But you need to run CRON on a virtual server. So an EC2 two instance and so on.

-- so while your instance is not running, or at least your CRONs are not doing anything, then your instance time is wasted.

-- so, as such, you can create a CloudWatch Event rule or an EventBridge rule that will be triggered every 1 hour. And every 1 hour it will be integrated with a Lambda function that will perform your task.

-- So this is a way to create a serverless CRON,

-- in this example, CloudWatch Events is serverless and Lambda functions are serverless too.









------------------------------------------------------ AWS Lambda Pricing: example


• You can find overall pricing information here:

     https://aws.amazon.com/lambda/pricing/


• Pay per calls:

     • First 1,000,000 requests are free

     • $0.20 per 1 million requests thereafter ($0.0000002 per request)


• Pay per duration: (in increment of 1 ms)

     • 400,000 GB-seconds of compute time per month for FREE

     • == 400,000 seconds if function is 1GB RAM

     • == 3,200,000 seconds if function is 128 MB RAM

     • After that $1.00 for 600,000 GB-seconds


• It is usually very cheap to run AWS Lambda so it’s very popular









------------------------------------------------------ Lambda Hands On


-- open lambda in console ---> check how it will work on the intro page 

-- c.o scale seamlessly , to observe how this will work and how it will get scale automatically

-- Lambda responds to events : Once you create Lambda functions, you can configure them to respond to events from a variety of sources. Try sending a mobile notification, streaming data to Lambda, or placing a photo in an S3 bucket.


-- Scale seamlessly : Lambda scales up and down automatically to handle your workloads, and you don't pay anything when your code isn't running.

       - * Your first 1 million requests or 400,000 GB-seconds of compute per month are free. Costs in this demo are based on a 128 MB function with a 1 second invocation duration.



-- choose blue print --> hello world python --> this will give lambda function code --> create function


-- create test event and do test and observe the result 







------------------------------------------------------ CloudWatch Events / EventBridge


-- how we can integrate CloudWatch Events or EventBridge with Lambda.

-- 2 ways 


1 CRON or Rate EventBridge Rule ------------------(Trigger Every 1 hour)------------> AWS Lambda Funchon Perform a task


2 CodePipeline EventBridge Rule ------------------(Trigger on State Changes) ------------> AWS Lambda Function Perform a task








------------------------------------------------------  CloudWatch Events / EventBridge (Hands ON)



-- create one function with python 3.9

-- make sure this function is being invoked by EventBridge.

-- Eventbridge --> rule --> create new rule --> Rule type = Schedule --> c.o continue to create rule ---> A schedule that runs at a regular rate, such as every 10 minutes. (1 min)--> choose target as aws lambda fun---> create rule

-- do refresh page of lambda u will see invocation of Eventbridge

-- wait for 1 min --> now check in cloudwatch invocation happens

-- now add print(event) in code and deploy the changes and wait for 1 Min

-- now do check in CW recent logs, u will see the event info

-- do disable our rule 











------------------------------------------------------ lambda and S3 Events Notifications


• S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...

    - So just a reminder on S3 event notifications, it is a way for you to get notified whenever an object is created, removed, restored, when there is a replication happening.

• Object name filtering possible (*.jpg)

    - You can filter by prefix and by suffix. And the use case is, the classic one is to generate thumbnail images of every image uploaded into Amazon S3.
    
    • Use case: generate thumbnails of images uploaded to S3


-- So you have your events into Amazon S3, and S3 can send it to three things,


1 Amazon S3 -------(Events)-------> SNS ---------> SQS

 
EXP : your events into Amazon S3, send to SNS and from an SNS topic, we can do a fan out pattern to send to multiple SQ-Q,  we can sent it into an SQ-Q


2 Amazon S3 -------(Events)-------> SQS ---------> Lambda Function


EXP : we can sent it into an SQ-Q , and have a Lambda function directly read off that SQ-SQ,


3 Amazon S3 -------(Events)-------(async)----------> Lambda Function ----------- (DLQ) ---------> SQS


EXP : we could have an Amazon S3 event notification, directly invoke our Lambda function and this is an asynchronous invocation.

- this Lambda function could do whatever it wants with that data, and then in case things go wrong, we can set up a dead-letter queue, for example an SQS, as we've seen from before.



• S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer

• If two writes are made to a single non- versioned object at the same time, it is possible that only a single event notification will be sent

• If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.









------------------------------------------------------ Simple S3 Event Pattern – Metadata Sync


  S3 bucket -------------(New file event)------------> lambda (Update metadata table) -----------> DynamoDB Table / Table in RDS


EXP : So, here is a simple pattern. An S3 bucket will have a new file event into Lambda. And Lambda function will process that file maybe insert that data into DynamoDB Table or even a table in RDS database.







------------------------------------------------------ lambda and S3 Events Notifications Hands On


-- create one lambda function and one s3 bucket , make sure both are in same region

-- s3 buket --> properties --> event notificatin --> create event notification for all object creation

-- So this event notification is enabled to send data into lambda function.

print(event) in code 


import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }



-- now do upload a file in the s3 bucket

-- now what's going to happen is that this should trigger an event into my Lambda function and to see whether or not this has worked,

-- lambda --> cloudwatch --> u will see that records have created with all the event info 










------------------------------------------------------ Lambda Execution Role (IAM Role)


• Grants the Lambda function permissions to AWS services / resources

• Sample managed policies for Lambda:
  
      • AWSLambdaBasicExecutionRole – Upload logs to CloudWatch.

      • AWSLambdaKinesisExecutionRole – Read from Kinesis

      • AWSLambdaDynamoDBExecutionRole – Read from DynamoDB Streams

      • AWSLambdaSQSQueueExecutionRole – Read from SQS

      • AWSLambdaVPCAccessExecutionRole – Deploy Lambda function in VPC

      • AWSXRayDaemonWriteAccess – Upload trace data to X-Ray.



• When you use an event source mapping to invoke your function, Lambda uses the execution role to read event data.

• Best practice: create one Lambda Execution Role per function







------------------------------------------------------ Lambda Resource Based Policies


• Use resource-based policies to give other accounts and AWS services permission to use your Lambda resources

• Similar to S3 bucket policies for S3 bucket

• An IAM principal can access Lambda:

      • if the IAM policy attached to the principal authorizes it (e.g. user access and we have full permissions, so we can access our Lambda function, this is what we've been doing so far,)

      • OR if the resource-based policy authorizes (e.g. service access) , this is more helpful when you have a service to service access.



• When an AWS service like Amazon S3 calls your Lambda function, the resource-based policy gives it access.







------------------------------------------------------  Lambda Resource Based Policies Hands On


--  go to IAM --> search for lambda permissions --:> explore those permissions it have 









------------------------------------------------------ Lambda Logging & Monitoring


-- how Lambda does logging, monitoring, and tracing.


• CloudWatch Logs: 

      • AWS Lambda execution logs are stored in AWS CloudWatch Logs

      • Make sure your AWS Lambda function has an execution role with an IAM policy that authorizes writes to CloudWatch Logs , this is included in the Lambda basic execution role as we've seen before.


• CloudWatch Metrics: 
   
      • AWS Lambda metrics are displayed in AWS CloudWatch Metrics UI or the Lambda UI.

      • they will represent information about your Invocations, Durations, Concurrent Executions

      • Error count, Success Rates, Throttles

      • Async Delivery Failures

      • Iterator Age (Kinesis & DynamoDB Streams)








------------------------------------------------------ Lambda Tracing with X-Ray


• Enable in Lambda configuration (Active Tracing)

• It will Runs the X-Ray daemon for you

• Use AWS X-Ray SDK in Code

• Ensure Lambda Function has a correct IAM Execution Role

      • The managed policy is called AWSXRayDaemonWriteAccess


IMP • Environment variables to communicate with X-Ray

        • _X_AMZN_TRACE_ID: contains the tracing header 

        • AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR

 IMP    • AWS_XRAY_DAEMON_ADDRESS: the X-Ray Daemon IP_ADDRESS:PORT

     





------------------------------------------------------ Lambda Tracing with X-Ray Hands On


-- lambda function ---> configuration --> monitor --> enable active tracing 

-- take s3 event notifications examples ---> write code for success --> upload object --> wait for 5 minutes --> go n check in x-ray console 









------------------------------------------------------ Lambda Function Improvement



Lambda Function Configuration


• RAM:

     • From 128MB to 10GB in 1MB increments

     • The more RAM you add, the more vCPU credits you get

     • At 1,792 MB of RAM, a function has the equivalent of one full vCPU

     • After 1,792 MB, you get more than one CPU, and need to use multi-threading in your code to benefit from it (up to 6 vCPU)


• If your application is CPU-bound (computation heavy), increase RAM

    - So if your application is CPU bound, that means that it has a lot of computations, and you want to improve the performance of your application, 
    
    - that means to decrease the amount of time your function will run for, then you need to increase your application,your lambda function RAM.


• Timeout: default 3 seconds, maximum is 900 seconds (15 minutes)

    - by default has a timeout of three seconds. That means that if your lambda function runs for more than three seconds, it will error out with a timeout,

    - Anything above 15 minutes is not a good use case for lambda and is something maybe that's going to be better for Fargate, ECS, or EC2.






------------------------------------------------------ Lambda Execution Context 


• The execution context is a temporary runtime environment that initializes any external dependencies of your lambda code

• Great for database connections, HTTP clients, SDK clients...

• The execution context is maintained for some time in anticipation of another Lambda function invocation

• The next function invocation can “re-use” the context to execution time and save time in initializing connections objects

• The execution context includes the /tmp directory

     - which is a space where you can write files and they will be available across executions.



------------------------------------------------------ Initialize outside the handler


BAD : The DB connection is established At every function invocation


import json
import time

def lambda_handler(event, context):
    connect_to_db()
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
def connect_to_db():
      time.sleep(3)




GOOD! : The DB connection is established Once And re-used across invocations

import json
import time

def connect_to_db():
      time.sleep(3)


connect_to_db()

def lambda_handler(event, context):
   
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }







------------------------------------------------------ Lambda Functions /tmp space


-- what if you need to write some temporary files and reuse them?

ANS : You can use the /tmp space.

• If your Lambda function needs to download a big file to work...

• If your Lambda function needs disk space to perform operations...

• You can use the /tmp directory

• Max size is 10GB

• The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations (helpful to checkpoint your work)

• For permanent persistence of object (non temporary), use S3

• To encrypt content on /tmp, you must generate KMS Data Keys




------------------------------------------------------ Lambda Function Improvement Hands On



step 1 :


-- lambda function --> configuraation --> general configuration --> keep timeout sec = 3 

-- now modify code for this demo 


import json
import time

def lambda_handler(event, context):
    time.sleep(2)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }


-- here we make sleep = 2 sec , it will work 'coz we have given timeout sec = 3 , and see the executed time 

-- But what happens if we make the Lambda function sleep five seconds?

-- make sleep(5), do test 

-- the Lambda function will fail. Why? Because it will timeout. So we got an error message here saying, Hey the tasks timed out after three seconds

-- now change timeout = 6 secs in configuration 

-- now u will get response without any error



Step 2 :


------ Last thing to optimize your Lambda function performance is around where you set the initialization of your function.

-- So if you're connecting to a DB, modify code

import json
import time

def lambda_handler(event, context):
    connect_to_db()
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
def connect_to_db():
      time.sleep(3)


-- where we connect to the database within the Lambda handler,

-- that means that every time we invoke our function this function connects to DB is going to be run it's going to take three seconds 

-- because it takes a long time to connect your database and then is going to return the results you have.

-- test again , it will take 3 sec again because we are connecting to database every single time, 



Step 3 :


-- instead of doing the connection to the database within the Lambda handler, you do it outside of it.


import json
import time

def connect_to_db():
      time.sleep(3)


connect_to_db()

def lambda_handler(event, context):
   
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }



-- deploy and run the code , at first INIT time it will take 3 sec , then do test again 

-- it will take less time , now my function is much quicker because we've done the database initialization again outside of the function handler.

-- Imagine that instead of here, instead of sleeping, you actually connect to the database and you get a database object out of it that you can use within your Lambda handler









------------------------------------------------------ Lambda Concurrency



-------------------- Lambda Concurrency and Throttling


-- In Lambda, concurrency is the number of in-flight requests that your function is currently handling. There are two types of concurrency controls available:


1 Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges.


2 Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account.






-- So the more we invoke our Lambda functions, the more we will have concurrent executions of our Lambda functions. We know this because Lambda can scale very, very easily and fast.

• Concurrency limit: up to 1000 concurrent executions

• Can set a “reserved concurrency” at the function level (=limit)

       EG : this Lambda function can only have "up to 50 concurrent executions.


• Each invocation over the concurrency limit will trigger a “Throttle”

• Throttle behavior:

       • If synchronous invocation => return ThrottleError - 429

       • If asynchronous invocation => retry automatically and then go to DLQ


• If you need a higher limit, open a support ticket




-------------------- Lambda Concurrency Issue


• If you don’t reserve (=limit) concurrency, the following can happen:

IMP : the concurrency limit applies to all the functions in your accounts, and so you have to be careful because if one function goes over the limit, it's possible that your other functions get throttled.




-------------------- Concurrency and Asynchronous Invocations


-- let's take the example of S3 event notifications. So we are uploading files into our S3 buckets,

-- and this creates a new file event that will invoke our Lambda functions,

-- say we are putting many, many files at the same time. So we get many, many different Lambda concurrent executions happening.

• If the function doesn't have enough concurrency available to process all events, additional requests are throttled.

• For throttling errors (429) and system errors (500-series), Lambda returns the event to the queue and attempts to run the function again for up to 6 hours. So there's a lot of retries that happens due to the throttling and so on.

       - 'coz it is asynchronous mode


• The retry interval increases exponentially from 1 second after the first attempt to a maximum of 5 minutes.





-------------------- Cold Starts & Provisioned Concurrency


• Cold Start:

      • New instance => code is loaded and code outside the handler run (init)

      • If the init is large (code, dependencies, SDK...) this process can take some time.

      • First request served by new instances has higher latency than the rest , user is unhappy

      - that may impact your users. So if your user is maybe waiting three seconds to get a request response, that may be very, very slow for them and 

      - they may experience a cold start and may be unhappy with your product.


• Provisioned Concurrency:

      • Concurrency is allocated before the function is invoked (in advance)

      • So the cold start never happens and all invocations have low latency

      • Application Auto Scaling can manage concurrency (schedule or target utilization),to make sure that you have enough reserved Lambda functions to be ready to be used and minimize this cold start problem.



• Note:
 
     • Note:cold starts inVPC have been dramatically reduced in Oct & Nov 2019

     - https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/





-------------------- Reserved and Provisioned Concurrency


https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html










------------------------------------------------------------- Lambda Monitoring – CloudWatch Metrics



• Invocations – number of times your function is invoked (success/failure)

• Duration – amount of time your function spends processing an event

• Errors – number of invocations that result in a function error

• Throttles – number of invocation requests that are throttled (no concurrency available)

• DeadLetterErrors – number of times Lambda failed to send an event to a DLQ (async invocations)

• IteratorAge – time between when a Stream receives a record and when the Event Source Mapping sends the event to the function (for Event Source Mapping that reads from Stream)

• ConcurrentExecutions – number of function instances that are processing events






------------------------------------------------------------- Lambda Monitoring – CloudWatch Alarms




• Example 1 – No Lambda Invocations in the last hour using "Invocations" CloudWatch Metric

• Example 2 – When error > 0 using Errors CloudWatch Metric

• Example 3 – When throttles > 0 using Throttles CloudWatch Metric





------------------------------------------------------------- Lambda Monitoring – CloudWatch Logs Insights



• Allows you to search through all your Lambda functions logs

        Example: How many times Lambda function had errors in the last 7 days



-- Example CloudWatch Logs Insights Queries



  Query                                                                                                                          Descripcon


fields Timestamp, LogLevel, Message | filter LogLevel == "ERR”
sort @timestamp desc                                                                                                     The last 100 errors
limit 100



filter @type = "REPORT”
| stats sum(strcontains(@message, "Init Duration"))/count(*) * 100 as                                                      Percentage of cold starts in total invocations
coldStartPct, avg(@duration)by bin(5m)        



filter @type = "REPORT" and @maxMemoryUsed=@memorySize | statscount_distinct(@requestId) by bin(30m)                         Invocations using 100% of assigned memory


avgMemoryUsedPERC,avg(@billedDuration) as avgDurationMSby bin(5m)                                                            Average memory used across invocations


filter @message like /Task timed out/ | stats count() by bin(30m)                                                               Invocations that timed out






------------------------------------------------------------- Lambda Monitoring – Lambda Insights



• Collects, aggregates, and summarizes:

        • System-level Metrics – CPU time, memory, disk, network

        • Diagnostic Information – cold starts, Lambda worker shutdowns



• Helps you isolate issues with your Lambda functions and resolve them quickly

• Uses a CloudWatch Lambda Extension (provided as a Lambda layer)










========================================================  EC2 Storage and Data Management - EBS and EFS ========================================================






------------------------------------------------------------- What’s an EBS Volume?



• An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run

• It allows your instances to persist data, even after their termination

• They can only be mounted to one instance at a time (at the CCP level)

• They are bound to a specific availability zone

• Analogy:Think of them as a “network USB stick”

• Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per month





------------------------------------------------------------- EBS Volume


• It’s a network drive (i.e. not a physical drive)

        • It uses the network to communicate the instance, which means there might be a bit of latency

        • It can be detached from an EC2 instance and attached to another one quickly



• It’s locked to an Availability Zone (AZ)

        • An EBS Volume in us-east-1a cannot be attached to us-east-1b

        • To move a volume across, you first need to snapshot it


• Have a provisioned capacity (size in GBs, and IOPS)

        • You get billed for all the provisioned capacity

        • You can increase the capacity of the drive over time






------------------------------------------------------------- DeleteOnTermination



NOTE : if u terminate the instance all ur root volumes get terminated by default coz, "delete on termination" is checked/enabled / Yes

-  if u want to retain the root volume do uncheck  "delete on termination" while launching te ec2 instance 

- if u terminate ec2 , by default "additional volume will not be deleted coz "DOT" is unchecked/disabled/No


========= IMP : If the instance is already running, you can set DeleteOnTermination to False using the "command line."
    
EG : To modify the deleteOnTermination attribute of the root volume through command line 

-- The following modify-instance-attribute example sets the deleteOnTermination attribute for the root volume of the specified Amazon EBS-backed instance to false. By default, this attribute is true for the root volume.

     cmd : 

     aws ec2 modify-instance-attribute --instance-id i-1234567890abcdef0 --block-device-mappings "[{\"DeviceName\": \"/dev/sda1\",\"Ebs\":{\"DeleteOnTermination\":false}}]"

      This command produces no output






------------------------------------------------------------- EC2 Instance Store


• EBS volumes are network drives with good but “limited” performance

• If you need a high-performance hardware disk, use EC2 Instance Store

• Better I/O performance

• EC2 Instance Store lose their storage if they’re stopped (ephemeral)

• Good for buffer / cache / scratch data / temporary content

• Risk of data loss if hardware fails

• Backups and Replication are your responsibility






------------------------------------------------------------- EBS Volume Types 




• EBS Volumes come in 6 types

        • gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads

        • io1 / io2 Block Express (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads

        • st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput- intensive workloads

        • sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads



• EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)

• When in doubt always consult the AWS documentation – it’s good!

• Only gp2/gp3 and io1/io2 Block Express can be used as boot volumes




------------------------------------------------------------- EBS Volume Types Use cases



1 General Purpose SSD

        • Cost effective storage, low-latency

        • System boot volumes,Virtual desktops, Development and test environments

        • 1 GiB - 16TiB

        • gp3:

                • Baseline of 3,000 IOPS and throughput of 125 MiB/s

                • Can increase IOPS up to 16,000 and throughput up to 1000 MiB/s independently

        • gp2:

                • Small gp2 volumes can burst IOPS to 3,000

                • Size of the volume and IOPS are linked, max IOPS is 16,000

                • 3 IOPS per GB, means at 5,334 GB we are at the max IOPS        





2 Provisioned IOPS (PIOPS) SSD




        • Critical business applications with sustained IOPS performance

        • Or applications that need more than 16,000 IOPS

        • Great for databases workloads (sensitive to storage perf and consistency)

        • io1 (4 GiB - 16TiB):

                • Max PIOPS: 64,000 for Nitro EC2 instances & 32,000 for other

                • Can increase PIOPS independently from storage size


        • io2 Block Express (4 GiB – 64 TiB):

                • Sub-millisecond latency

                • Max PIOPS: 256,000 with an IOPS:GiB ratio of 1,000:1


        • Supports EBS Multi-attach




3 Hard Disk Drives (HDD)

        • Cannot be a boot volume

        • 125 GiB to 16TiB

        • Throughput Optimized HDD (st1)

                • Big Data, Data Warehouses, Log Processing
                • Max throughput 500 MiB/s – max IOPS 500

        • Cold HDD (sc1):

                • For data that is infrequently accessed
                • Scenarios where lowest cost is important
                • Max throughput 250 MiB/s – max IOPS 250







------------------------------------------------------------- EBS Multi-Attach – io1/io2 family




• Attach the same EBS volume to multiple EC2 instances in the same AZ

• Each instance has full read & write permissions to the high-performance volume

• Use case:

        • Achieve higher application availability in clustered Linux applications (ex:Teradata)

        • Applications must manage concurrent write operations

• Up to 16 EC2 Instances at a time

- this Multi-Attach feature is only available from within a specified availability zone, of course. It doesn't allow you to attach an EBS volume from one AZ to another AZ.

• Must use a file system that’s cluster-aware (not XFS, EXT4, etc...)








------------------------------------------------------------- EBS Volume Resizing



• You can only increase the EBS volumes:

        • Size (any volume type)

        • IOPS (only in IO1)


• After resizing an EBS volume, you need to repartition your drive

        - That means that after you increase the size of your volume, there's gonna be more size available, but you're EC2 instance will not know about it until you repartition your drive to tell your instance to use that new space.


• After increasing the size, it’s possible for the volume to be in a long time in the “optimisation” phase.The volume is still usable

• You can’t decrease the size of your EBS volume ((create another smaller volume then migrate data))





------------------------------------------------------------- EBS Volume Resizing Hands ON 


-- create one ec2 instace -> connect ec2 instance , then do 

-- lsblk   , it will show you like this 

NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0   8G  0 disk 
├─xvda1   202:1    0   8G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi


-- now is to go and extend my volume 'cause we want a bigger volume.

-- go to volumes in consle --> select volume --> Actions --> modify change from 8 to 10 GiB --> modify 

-- now type lsblk , u can able to see 10G volume and 8G comes under partition

-- df -h 

Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        4.0M     0  4.0M   0% /dev
tmpfs           475M     0  475M   0% /dev/shm
tmpfs           190M  444K  190M   1% /run
/dev/xvda1      8.0G  1.6G  6.4G  20% /
tmpfs           475M     0  475M   0% /tmp
/dev/xvda128     10M  1.3M  8.7M  13% /boot/efi
tmpfs            95M     0   95M   0% /run/user/1000


-- as you can see again, it's eight gigabytes for X V D A 1. after doing df -h 

-- So we need to perform some operations to use the full 10 gigabytes on my instance.

-- our volume is xvda , df -h it is in /dev/xvda1 so give this cmnd 

        sudo growpart /dev/xvda 1 

-- it will give like this 

        CHANGED: partition=1 start=24576 old: size=16752607 end=16777183 new: size=20946911 end=20971487


-- do lsblk 


NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0  10G  0 disk 
├─xvda1   202:1    0  10G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi


-- as you can see now, we have 10 gigabytes for the main partition, which is the same size as the disc.

-- do df -h 


Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        4.0M     0  4.0M   0% /dev
tmpfs           475M     0  475M   0% /dev/shm
tmpfs           190M  444K  190M   1% /run
/dev/xvda1      8.0G  1.6G  6.4G  20% /
tmpfs           475M     0  475M   0% /tmp
/dev/xvda128     10M  1.3M  8.7M  13% /boot/efi
tmpfs            95M     0   95M   0% /run/user/1000
[ec2-user@ip-172-31-15-215 ~]$ 


      - we only see eight gigabytes. So to fix this, we can run some more commands, or we just reboot it from the console.



-- after doing reeboot , enter lsblk and df -h cmnds u can get 


Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        4.0M     0  4.0M   0% /dev
tmpfs           475M     0  475M   0% /dev/shm
tmpfs           190M  480K  190M   1% /run
/dev/xvda1       10G  1.6G  8.4G  16% /
tmpfs           475M     0  475M   0% /tmp
/dev/xvda128     10M  1.3M  8.7M  13% /boot/efi
tmpfs            95M     0   95M   0% /run/user/1000







------------------------------------------------------------- EBS Snapshots



• Make a backup (snapshot) of your EBS volume at a point in time

• Not necessary to detach volume to do snapshot, but recommended

• Can copy snapshots across AZ or Region





------------------------------------------------------------- Amazon Data Lifecycle Manager


• Automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs

• Schedule backups, cross-account snapshot copies, delete outdated backups, ...

• Uses resource tags to identify the resources (EC2 instances, EBS volumes)

• Can’t be used to manage snapshots/AMIs created outside DLM

• Can’t be used to manage instance-store backed AMIs






------------------------------------------------------------- EBS Snapshots – Fast Snapshot Restore (FSR)



-- one little feature that could be really really helpful but really really expensive

• EBS Snapshots stored in S3, So you don't see them, but this is how they're stored internally in AWS.

• By default, there’s a latency of I/O operations the first time each block is accessed (block must be pulled from S3)

• Solution: force the initialization of the entire volume (using the dd or fio command), 

        - so the idea is that you start an EC2 instance, you attach an EBS volume, and you read the entire volume. Then therefore, all the blocks will be accessed and initiated. 
        
        - or you can enable FSR

• FSR helps you to create a volume from a snapshot that is fully initialized at creation (no I/O latency)

• Enabled for a snapshot in a particular AZ (billed per minute – very expensive $$$) around $500 per month.

• Can be enabled on snapshots created by Data Lifecycle Manager






------------------------------------------------------------- EBS Snapshots Features



• EBS Snapshot Archive

        • Move a Snapshot to an ”archive tier” that is 75% cheaper

        • Takes within 24 to 72 hours for restoring the archive


• Recycle Bin for EBS Snapshots

        • Setup rules to retain deleted snapshots so you can recover them after an accidental deletion

        • Specify retention (from 1 day to 1 year)





------------------------------------------------------------- EBS Snapshots Hands ON 



-- volumes --> select volume --> actions --> create snapshots --> u can see not encrypted coz main volm is not encrypted --> create snapshot

-- go to snapshots 
        
        - we can create a volume back from the snapshot if you want 
        
        - we can change the availability zone if you wanted to.

        - So my main volume was in eu-west-1a. But thanks to the Restore Snapchat option, I can move my volume to eu-west-1b.

        - we can enable encryption for this volume if you wanted to encrypt our EBS volume, the new one with a KMS key , if you wan to do emcryption 


-- snapshots --> actions --> copy snapshots 

        - you can copy it into the same region or a different region for disaster recovery purposes.

        - when we copy the snapshots, we can encrypt it with a KMS key if we wanted to.

        - then any volumes created from the snapshots would of course be encrypted as well.

        - Manage fast snapshot restore 

                - So the idea is that when you want to restore a huge volume, then instead of mounting it and then reading all the blocks, you can enable fast snapshot restore.

        - Storage Tier 

                - my storage tier is currently standard. So that means that the storage tier is standard, but I can actually move the storage tier        

                - so we can archive the snapshots. And when you archive a snapshot, it's going to be in the archived tier which is costing you less money,

                - But if you do so, then it's going to take a little bit of time to restore from the snapshot. So you save on costs, but then if you wanna restore from snapshot, it will take longer.





-- now go to DLM in console 

        - So Lifecycle Manager allows you to create an EBS snapshot policy

                - only the volumes that have been marked with the tags are going to be archived.

                - create schedule as u want 


-- recycle Bin 

        - create rule --> Resource type = EBS or AMI --> apply to all resource --> Rule lock settings = lock /unlock as u want(So you can unlock it, which that means that this rule can be modified at any point by anyone or you can lock it and say that this rule will be locked for a specific delay period and no one can change it.) --> create 

        - now do snaphot delete in console 

        - now to recycle bin and check Resources , u can able to see the snapshots and now do retain if you want 

        






------------------------------------------------------------- EBS Migration



• EBS Volumes are only locked to a specific AZ

• To migrate it to a different AZ (or region):

        • Snapshot the volume

        • (optional) Copy the volume to a different region

        • Create a volume from the snapshot in the AZ of your choice





------------------------------------------------------------- EBS Encryption


• When you create an encrypted EBS volume, you get the following:

        • Data at rest is encrypted inside the volume

        • All the data in flight moving between the instance and the volume is encrypted

        • All snapshots are encrypted

        • All volumes created from the snapshot


• Encryption and decryption are handled transparently (you have nothing to do) It's all handled by EC2 and EBS behind the scenes.

• Encryption has a minimal impact on latency

• EBS Encryption leverages keys from KMS (AES-256)

• Copying an unencrypted snapshot allows encryption

• Snapshots of encrypted volumes are encrypted





------------------------------------------------------------- Encryption: encrypt an unencrypted EBS volume 



• Create an EBS snapshot of the volume

• Encrypt the EBS snapshot ( using copy )

• Create new ebs volume from the snapshot ( the volume will also be encrypted )

• Now you can attach the encrypted volume to the original instance





------------------------------------------------------------- Encryption: encrypt an unencrypted EBS volume Hands ON 




-- any snapshots encrypted from a non-encrypted EBS volume, will be not encrypted.

-- create volm withput encryption 

-- now take snapshot from the volm u have created 

-- this snapshot, as we observe is not encrypted.

-- so, to create an encrypted snapshot, now, what you have to do is to do action --> copy snapshot

-- when you copy the snapshots, you have the option right here to enable encryption into the same destination region.

-- from this encrypted snapshots that is now completed I can create a volume.

-- We saw how we can encrypt one EBS volume this way, by going through a snapshot copying and so on.


-- There's a shortcut.

        - go to unencrypted snapshot in console --> create volumes from snapshot

        - you can actually on the fly enable encryption for the EBS volume directly from here,

        - select an EBS key, and you would create an encrypted EBS volume this way through a unencrypted snapshots.







------------------------------------------------------------- Amazon EFS – Elastic File System 



• Managed NFS (network file system) that can be mounted on many EC2

• EFS works with EC2 instances in multi-AZ

• Highly available, scalable, expensive (3x gp2), pay per use

• Use cases: content management, web serving, data sharing,Wordpress

• Uses NFSv4.1 protocol

• Uses security group to control access to EFS

• Compatible with Linux based AMI (not Windows)

• Encryption at rest using KMS




• POSIX file system (~Linux) that has a standard file API

• File system scales automatically, pay-per-use, no capacity planning!





------------------------------------------------------------- EFS – Performance & Storage Classes



• EFS Scale

        • 1000s of concurrent NFS clients, 10 GB+ /s throughput

        • Grow to Petabyte-scale network file system, automatically


• Performance Mode (set at EFS creation time)

        • General Purpose (default) – latency-sensitive use cases (web server, CMS, etc...)

        • Max I/O – higher latency, throughput, highly parallel (big data, media processing)


• Throughput Mode

        • Bursting – 1TB = 50MiB/s + burst of up to 100MiB/s

        • Provisioned – set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage

        • Elastic – automatically scales throughput up or down based on your workloads

                • Upto 3GiB/s for reads and 1GiB/s for writes

                • Used for unpredictable work loads







------------------------------------------------------------- EFS – Storage Classes



• Storage Tiers (lifecycle management feature – move file after N days)

        • Standard: for frequently accessed files price to store.

        • Archive: rarely accessed data (few times each year), 50% cheaper

        • Archive: rarely accessed data (few times each year), 50% cheaper

        • Implement lifecycle policies to move files between storage tiers


• Availability and durability

        • Standard: Multi-AZ, greatforprod

        • OneZone: OneAZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone-IA)








------------------------------------------------------------- EFS Hands ON 





-- create EFS 

-- throughput : speed b/w ec2 and EFS 

-- EFS is regional 

-- Replicatio is posssible in EFS , cost calulated how much data is transferred 

-- launch 1 ec2 instances 

-- in S.G add NFS inbound rule 

-- connect instance 1 and follow some commands 

1  sudo -s 

2  yum install -y nfs-utils

3   mkdir efs  --- create one folder 

4   now do mount with folder 


mount -t nfs4 fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com:/ efs/


explanation :   fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com  -- u wil get from efs dns name    and efs is folder name 

-- cd efs 

-- now create some files here 


---- now create 2nd instance in diff A.Z 

-- follow the same steps , here u can give mkdir names anything u want 

-- once u do ls u will get same filesin diff A.Z zone instances also 

-- create one file new in ec2 1 and check in 2nd ec2 u will get that file 

-- same for deletion also 

-- this is how u do with Efs amd u can also do repliction EFS 







------------------------------------------------------------- EBS vs EFS – Elastic Block Storage


EBS vs EFS – Elastic Block Storage 


• EBS volumes...

    • one instance (except multi-attach io1/io2)
    • are locked at the Availability Zone (AZ) level 
    • gp2: IO increases if the disk size increases
    • gp3 & io1: can increase IO independently



• To migrate an EBS volume across AZ

   • Take a snapshot
   • Restore the snapshot to another AZ
   • EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic

• Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated. (you can disable that by using "DeleteOnTermination")




EBS vs EFS – Elastic File System

• Mounting 100s of instances across AZ 

• EFS share website files (WordPress)

• Only for Linux Instances (POSIX)

• EFS has a higher price point than EBS 

• Can leverage EFS-IA for cost savings







------------------------------------------------------------- EFS – Access Points





• Easily manage applications access to NFS environments

• Enforce a POSIX user and group to use when accessing the file system

• Restrict access to a directory within the file system and optionally specify a different root directory

• Can restrict access from NFS clients using IAM policies



-- So here's an example, we have an EFS file system that's going to be shared across your entire company but has different folders under the routes.

-- There is a data folder, the secret folder and the config folder, and we want different users to access different parts of your EFS file system.






------------------------------------------------------------- EFS - Operations




• Operations that can be done in place:

        • Lifecycle Policy (enable IA or change IA settings)

        • Throughput Mode and Provisioned Throughput Numbers

        • EFS Access Points


-- But, some operations require a full migration of your EFS file system

• Operations that require a migration using DataSync (replicates all file attributes and metadata)

        • Migration to encrypted EFS

        • Performance Mode (e.g. Max IO)




-- So let's have an example.

        - we have one source file system and destination file system which is encrypted.

        - so to migrate between the two, using the DataSync service,

        - then you can migrate your E2 instances from the first file system to the second file system.






-------------------------------------------------------------  EFS – CloudWatch Metrics



• PercentIOLimit

        • How close the file system reaching the I/O limit(General Purpose)        

        • If at 100%, move to Max I/O (migration) to increase your I/O capacity of your EFS file system.

        - So, though if you wanted to enable Max I/O, if you remember, you would have to perform a migration because this operation cannot be done in place.

        - So that means creating a new file system with a Max I/O already enabled and then migrating from the previous file system to the new file system using a data sync service.        


• BurstCreditBalance

         • The number of burst credits the file system can use to achieve higher throughput levels


• StorageBytes

        • File system’s size in bytes (15 minutes interval)

        • Dimensions: Standard, IA,Total (Standard + IA)



-- go and check efs monitoiring tab 











======================================================== Amazon S3 ========================================================




• Amazon S3 is one of the main building blocks of AWS

• It’s advertised as ”infinitely scaling” storage

• Many websites use Amazon S3 as a backbone

• Many AWS services use Amazon S3 as an integration as well





------------------------------------------------------------- Amazon S3 Use cases


• Backup and storage

• Disaster Recovery

• Archive

• Hybrid Cloud storage 

• Application hosting

• Media hosting

• Data lakes & big data analytics • Software delivery

• Static website







------------------------------------------------------------- Amazon S3 - Buckets



• Amazon S3 allows people to store objects (files) in “buckets” (directories)

• Buckets must have a globally unique name (across all regions all accounts)

• Buckets are defined at the region level

• S3 looks like a global service but buckets are created in a region

• Naming convention

        • No uppercase, No underscore
        • 3-63 characters long
        • Not an IP
        • Must start with lowercase letter or number
        • Must NOT start with the prefix xn--
        • Must NOT end with the suffix -s3alias





------------------------------------------------------------- Amazon S3 - Objects 


• Objects (files) have a Key

• The key is the FULL path:

        • s3://my-bucket/my_file.txt

        • s3://my-bucket/my_folder1/another_folder/my_file.txt


• The key is composed of prefix + object name

        • s3://my-bucket/my_folder1/another_folder/my_file.txt

• There’s no concept of “directories” within buckets (although the UI will trick you to think otherwise)

• Just keys with very long names that contain slashes (“/”)

• Object values are the content of the body:

        • Max. Object Size is 5TB (5000GB)

        • If uploading more than 5GB, must use “multi-part upload”


• Metadata (list of text key / value pairs – system or user metadata)

• Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle

• Version ID (if versioning is enabled)






------------------------------------------------------------- Amazon S3 – Security



• User-Based

        • IAM Policies – which API calls should be allowed for a specific user from IAM


• Resource-Based

        • Bucket Policies – bucket wide rules from the S3 console - allows cross account

        • Object Access Control List (ACL) – finer grain (can be disabled)

        • Bucket Access Control List (ACL) – less common (can be disabled)



• Note: an IAM principal can access an S3 object if

        • The user IAM permissions ALLOW it OR the resource policy ALLOWS it

        • AND there’s no explicit DENY then the IAM principle can access the S3 object on the specified API call.





• Encryption: encrypt objects in Amazon S3 using encryption keys 







------------------------------------------------------------- S3 Bucket Policies




• JSON based policies

        • Resources: buckets and objects

        • Effect: Allow / Deny

        • Actions: Set of API to Allow or Deny

        • Principal:The account or user to apply the policy to


• Use S3 bucket for policy to:

        • Grant public access to the bucket

        • Force objects to be encrypted at upload

        • Grant access to another account (Cross Account)






------------------------------------------------------------- Bucket settings for Block Public Access



• These settings were created to prevent company data leaks

• If you know your bucket should never be public, leave these on

        - So even though you would set an S3 Bucket policy that would make it public, if these settings are enabled, the Bucket will never be public.

• Can be set at the account level






------------------------------------------------------------- S3 Bucket Policies Advanced 


• Use S3 bucket for policy to:

        • Grant public access to the bucket

        • Force objects to be encrypted at upload

        • Grant access to another account (Cross Account)


• Optional Conditions on:

        • Public IP or Elastic IP (not on Private IP)

        • Source VPC or Source VPC Endpoint – only works with VPC Endpoints

        • CloudFront Origin Identity

        • MFA



• Examples here: https://docs.aws.amazon.com/AmazonS3/latest/dev/example- bucket-policies.html




Examples :


1




https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/



• Restrict access to only principals from AWS accounts inside an AWS Organization using aws:PrincipalOrgID condition key


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::2018-Financial-Data/*",
            "Condition": {"StringEquals": 
                             {"aws:PrincipalOrgID": [ "o-yyyyyyyyyy" ]}
                         }
        }
    ]
}





2 





• Prevent uploads of unencrypted objects to an S3 bucket using s3:x-amz-server-side- encryption condition key

https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/#:~:text=The%20policy%20needs%20to%20cover,server%2Dside%2Dencryption%20key.




{
     "Version": "2012-10-17",
     "Id": "PutObjPolicy",
     "Statement": [
           {
                "Sid": "DenyIncorrectEncryptionHeader",
                "Effect": "Deny",
                "Principal": "*",
                "Action": "s3:PutObject",
                "Resource": "arn:aws:s3:::<bucket_name>/*",
                "Condition": {
                        "StringNotEquals": {
                               "s3:x-amz-server-side-encryption": "AES256"
                         }
                }
           },
           {
                "Sid": "DenyUnEncryptedObjectUploads",
                "Effect": "Deny",
                "Principal": "*",
                "Action": "s3:PutObject",
                "Resource": "arn:aws:s3:::<bucket_name>/*",
                "Condition": {
                        "Null": {
                               "s3:x-amz-server-side-encryption": true
                        }
               }
           }
     ]
 }




 -- check above link for more info 






 • Restrict access to specific IP addresses using NotIpAddress condition key




{
  "Id": "Policy1721724823144",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1721724821411",
      "Action": [
        "s3:DeleteObject",
        "s3:GetObject",
        "s3:ListBucket",
        "s3:PutObject"
      ],
      "Effect": "Deny",
      "Resource": ["arn:aws:s3:::hfgvgdgs","arn:aws:s3:::Bucket-Name/*"],
      "Condition": {
        "NotIpAddress": {
          "aws:SourceIp": "3.109.60.179/24"
        }
      },
      "Principal": "*"
    }
  ]
}






4  


• Grant user access to list and download all objects in an S3 bucket


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::your-bucket-name"
        },
        {
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::your-bucket-name/*"
        }
    ]
}






5 



• Restrict access to users authenticated using MFA using MultiFactorAuthPresent condition key





{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::your-bucket-name",
            "Condition": {
                "Bool": {
                    "aws:MultiFactorAuthPresent": "true"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::your-bucket-name/*",
            "Condition": {
                "Bool": {
                    "aws:MultiFactorAuthPresent": "true"
                }
            }
        }
    ]
}









------------------------------------------------------------- Amazon S3 – Static Website Hosting


• S3 can host static websites and have them accessible on the Internet

• The website URL will be (depending on the region)

        • http://bucket-name.s3-website-aws-region.amazonaws.com 

        OR

        • http://bucket-name.s3-website.aws-region.amazonaws.com


• If you get a 403 Forbidden error, make sure the bucket policy allows public reads!






------------------------------------------------------------- Amazon S3 - Versioning



• You can version your files in Amazon S3

• It is enabled at the bucket level

• Same key overwrite will change the “version”: 1, 2, 3....

• It is best practice to version your buckets

        • Protect against unintended deletes (ability to restore a version)

        • Easy roll back to previous version


• Notes:

        • Any file that is not versioned prior to enabling versioning will have version “null”

        • Suspending versioning does not delete the previous versions






-- when u have critical data it is help to bacjup

-- Versionong is like a backup tool 

-- by deafult versioning is not enabled 

-- this cna be also do while creating bucket 

-- enable on bucket level and applied to objects 

-- Version ID is always Unique 

-- if u delete original ,it will have marker (delete marker) applied to  new version 

-- to restore object , delete the mrker and ur object is restored automatically (latest version) 

-- but if u want to previous version to be restored , u have to download it nd upload it again 

-- it is possible to download version files 

-- it is not posible to download delete marker , u can only delete it 

-- delete marker is applied to only latetst version, not for old/previous versions 

-- once u can enable versioning u can't disable it but u can do suspend it 

-- u can not restore the files when u suspend the versioning 

-- when the version is suspended it wont effect on previus objects but coming objects get effected 

-- It is enabled at the bucket level


----IMP POINTS 

-- min object size = 0 bytes , MAx objct size = 5TB

-- you can have unlimited number of objects having 5TB each in a single bucket 

-- if u have file 5TB u can not upload it in one shot , so in aws recommened MPU(multi-part-upload) 

-- this will done through the CLI not from the console 

-- aws recommended , if u have object > 100MB , go for MPU 







------------------------------------------------------------- Amazon S3 - Versioning Hands On 




-- enable versioning for bucket 

-- upload same 2 images again

-- once u toggled show versions options t will show u L with image which means the latest 

-- do delete image 1 

-- now select versions and delete the delete marker u will get back u images 

-- this is how u will get back files if u enebled versioning 


-- Bucket Versioning : properties 

-------------IMP : ------------- MFA delete : additional layer of security that requries MFA for changing bucket versioning settings and permaenetly delete object versions . this feature can be enabled CLI only 


-- -- in S3 , it is possible suspend the versioning 

-- once u suspend versioning u cant get back  the files once u dlete 

-- even though u can see delete marker options but u can not get back those files once u delete , when u suspend the versioning 

-- existing objects do not have any impact once u suspend the vesioning 

eg: for image 1 u have enebled version previously and nxt suspend versioning once u dleete image 1 u will get back those image co existing files wont get any effect 


IMP :  for image 3 nad 4 u did not enable versioning 

-- now do enable versioning , now dlete image 4 and go n check enable versions dlete marker u will get back ur image 4 , coz now versioning is enabled 

-- once the enable is enabled u will get back all files , irresective of uploading time with versioning enable time suspended time 









------------------------------------------------------------- Amazon S3 – Replication (CRR & SRR) 



• Must enable Versioning in source and destination buckets

• Cross-Region Replication (CRR)

• Same-Region Replication (SRR)

• Buckets can be in different AWS accounts

• Copying is asynchronous

• Must give proper IAM permissions to S3


• Use cases:

        • CRR – compliance, lower latency access, replication across accounts

        • SRR – log aggregation, live replication between production and test accounts



• After you enable Replication, only new objects are replicated

• Optionally, you can replicate existing objects using S3 Batch Replication

        • Replicates existing objects and objects that failed replication


• For DELETE operations

        • Can replicate delete markers from source to target (optional setting)

        • Deletions with a version ID are not replicated (to avoid malicious deletes)


• There is no “chaining” of replication

        • If bucket 1 has replication into bucket 2, which has replication into bucket 3

        • Then objects created in bucket 1 are not replicated to bucket 3







------------------------------------------------------------- CRR(Cross-Region Replication)




-- u have bucket in mumbai and u have another bucket in singapore 

-- whatever the thing in b1 it will get relicate in singapore also 

-- for this u have to create CRR rules in Bucket 1

-- CRR is not enebled by default 

-- it is BUcket-level

-- CRR can be shared to different Accounts possible 


Cost : cross-region replication incurs request and transfer fees of $0.005/1000 requests and $0.02/1GB transferred. Standard storage pricing on the replicated side apply and differ by region.  Currently for US locations N. California is 10-60% more expensive than N. Virginia and Oregon, depending on S3, S3-IA, or Glacier.

EG : For an example, replicating 2500GB data containing 5000files breaks down as follows:

5 * $0.005 + 2500 * $0.02 = $50.025 access & transfer fees


-------------------------------- LAB 

Relication Rules for CRR

-- one bucket files in bk1 , this files replicate to bkt 2

-- go to management 

-- enable versioning is maditory 

--public or private it is does not matter 

-- create 2 buckets in two different regions 

-- go to bkt 1 -->> management n create relication rules select destination path 

-- ask aws to create new IAM role

-- now upload some file sin bkt 1 n do refresh in blt 2 u will get same files in bkt 2 


-- if u get delete object in bkt 1 , it won't get delete in the bucket 2 (same for CRR and SRR)


============SRR(same region replication) ==================


-- if u are replicating same region 

-- it is bucket level 


NOTE : versioning is manditory to have CRR/SRR


VERY IMP for EPV :


-- if u want to get logs of ur bucket u have to enable " server Access Logs"

-- object level logs can be captured in CloudTrail








-------------------------------------------------------------  S3 Storage Classes



• Amazon S3 Standard - General Purpose
• Amazon S3 Standard-Infrequent Access (IA) 
• Amazon S3 One Zone-Infrequent Access
• Amazon S3 Glacier Instant Retrieval
• Amazon S3 Glacier Flexible Retrieval
• Amazon S3 Glacier Deep Archive
• Amazon S3 Intelligent Tiering



• Can move between classes manually or using S3 Lifecycle configurations





------------------------------------------------------------- S3 Durability and Availability



• Durability:

        • High durability (99.999999999%, 11 9’s) of objects across multiple AZ

        • If you store 10,000,000 objects with Amazon S3, you can on average expect to

        incur a loss of a single object once every 10,000 years

        • Same for all storage classes


• Availability:

        • Measures how readily available a service is

        • Varies depending on storage class

        • Example: S3 standard has 99.99% availability = not available 53 minutes a year






------------------------------------------------------------- S3 Standard – General Purpose




• 99.99% Availability
• Used for frequently accessed data
• Low latency and high throughput
• Sustain 2 concurrent facility failures


• Use Cases: Big Data analytics, mobile & gaming applications, content distribution...






------------------------------------------------------------- S3 Storage Classes – Infrequent Access


• For data that is less frequently accessed, but requires rapid access when needed

• Lower cost than S3 Standard


• Amazon S3 Standard-Infrequent Access (S3 Standard-IA)

        • 99.9% Availability

        • Use cases: Disaster Recovery, backups


• Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)

        • High durability (99.999999999%) in a single AZ; data lost when AZ is destroyed

        • 99.5% Availability

        • Use Cases: Storing secondary backup copies of on-premises data, or data you can recreate





------------------------------------------------------------- Amazon S3 Glacier Storage Classes



• Low-cost object storage meant for archiving / backup

• Pricing: price for storage + object retrieval cost


• Amazon S3 Glacier Instant Retrieval

        • Millisecond retrieval, great for data accessed once a quarter

        • Minimum storage duration of 90 days


• Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):

        • Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free

        • Minimum storage duration of 90 days


• Amazon S3 Glacier Deep Archive – for long term storage:

        • Standard (12 hours), Bulk (48 hours)

        • Minimum storage duration of 180 days






------------------------------------------------------------- S3 Intelligent-Tiering


• Small monthly monitoring and auto-tiering fee

• Moves objects automatically between Access Tiers based on usage

• There are no retrieval charges in S3 Intelligent-Tiering


• Frequent Access tier (automatic): default tier

• Infrequent Access tier (automatic): objects not accessed for 30 days

• Archive Instant Access tier (automatic): objects not accessed for 90 days

• Archive Access tier (optional): configurable from 90 days to 700+ days

• Deep Archive Access tier (optional): config. from 180 days to 700+ days






https://aws.amazon.com/s3/storage-classes/   - for more info 








======================================================== Amazon S3 – Advanced ========================================================




------------------------------------------------------------- Amazon S3 – Moving between Storage Classes



• You can transition objects between storage classes

• For infrequently accessed object, move them to Standard IA

• For archive objects that you don’t need fast access to, move them to Glacier or Glacier Deep Archive

• Moving objects can be automated using a Lifecycle Rules





------------------------------------------------------------- Amazon S3 – Lifecycle Rules



• Transition Actions – configure objects to transition to another storage class

        • Move objects to Standard IA class 60 days after creation

        • Move to Glacier for archiving after 6 months


• Expiration actions – configure objects to expire (delete) after some time

        • Access log files can be set to delete after a 365 days

        • Can be used to delete old versions of files (if versioning is enabled)

        • Can be used to delete incomplete Multi-Part uploads


• Rules can be created for a certain prefix (example: s3://mybucket/mp3/*)

• Rules can be created for certain objectsTags (example:Department:Finance)










IMP FOR EPV : Amazon S3 supports a waterfall model for transitioning between storage classes,

S3 Standard storage 

    S3 Standard-IA storage class 
     
          S3 Intelligent-Tiering storage class

               S3 One Zone-IA storage class

                    S3 Glacier Instant Retrieval storage class

                           S3 Glacier Flexible Retrieval storage class 

                                    S3 Glacier Deep Archive storage class.




1  Supported lifecycle transitions

-- Amazon S3 supports the following lifecycle transitions between storage classes using an S3 Lifecycle configuration. You can transition from the following:

-  The S3 Standard storage class to any other storage class.

-  The S3 Standard-IA storage class to the S3 Intelligent-Tiering, S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or S3 Glacier Deep Archive storage classes.

-  The S3 Intelligent-Tiering storage class to the S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or S3 Glacier Deep Archive storage classes.

-  The S3 One Zone-IA storage class to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes.

-  The S3 Glacier Instant Retrieval storage class to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes.

-  The S3 Glacier Flexible Retrieval storage class to the S3 Glacier Deep Archive storage class.

-  Any storage class to the S3 Glacier Deep Archive storage class.


2  Unsupported lifecycle transitions

-- Amazon S3 does not support any of the following lifecycle transitions. You can't transition from the following:

- Any storage class to the S3 Standard storage class.

- Any storage class to the Reduced Redundancy Storage (RRS) class.

- The S3 Intelligent-Tiering storage class to the S3 Standard-IA storage class.

- The S3 One Zone-IA storage class to the S3 Intelligent-Tiering, S3 Standard-IA, or S3 Glacier Instant Retrieval storage classes.


Constraints : 


-- When you transition objects from the S3 Standard or S3 Standard-IA storage classes to S3 Intelligent-Tiering, S3 Standard-IA, or S3 One Zone-IA, the following object size constraints apply:

Larger objects  : For the following transitions, there is a cost benefit to transitioning larger objects:

  - From the S3 Standard or S3 Standard-IA storage classes to S3 Intelligent-Tiering.

  - From the S3 Standard storage class to S3 Standard-IA or S3 One Zone-IA.

-- Objects smaller than 128 KiB : For the following transitions, Amazon S3 does not transition objects that are smaller than 128 KiB:

  - From the S3 Standard or S3 Standard-IA storage classes to S3 Intelligent-Tiering or S3 Glacier Instant Retrieval.

  - From the S3 Standard storage class to S3 Standard-IA or S3 One Zone-IA.


ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html










---------------------------- Amazon S3 – Lifecycle Rules (Scenario 1)

Q Your application on EC2 creates images thumbnails after profile photos are uploaded to Amazon S3.These thumbnails can be easily recreated, and only need to be kept for 60 days.The source images should be able to be immediately retrieved for these 60 days, and afterwards, the user can wait up to 6 hours. How would you design this?
 
 Ans : -- S3 source images can be on Standard, with a lifecycle configuration to transition them to Glacier after 60 days

       -- S3 thumbnails can be on One-Zone IA, with a lifecycle configuration to expire them (delete them) after 60 days


----------------------------- Amazon S3 – Lifecycle Rules (Scenario 2)

-- A rule in your company states that you should be able to recover your deleted S3 objects immediately for 30 days, although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable within 48 hours.

Ans : Enable S3 Versioning in order to have object versions, so that “deleted objects” are in fact hidden by a “delete marker” and can be recovered

-- Transition the “noncurrent versions” of the object to Standard IA
-- Transition afterwards the “noncurrent versions” to Glacier Deep Archive







------------------------------------------------------------- Amazon S3 Analytics – Storage Class Analysis



-- Help you decide when to transition objects to the right storage class

-- Recommendations for Standard and Standard IA

-- Does NOT work for One-Zone IA or Glacier

-- Report is updated daily

-- 24 to 48 hours to start seeing data analysis

--  S3 bucket --> Storage Class Analysis --> .csv report 














------------------------------------------------------  S3 Events Notifications


• S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...

    - So just a reminder on S3 event notifications, it is a way for you to get notified whenever an object is created, removed, restored, when there is a replication happening.

• Object name filtering possible (*.jpg)

    - You can filter by prefix and by suffix. And the use case is, the classic one is to generate thumbnail images of every image uploaded into Amazon S3.
    
    • Use case: generate thumbnails of images uploaded to S3


-- So you have your events into Amazon S3, and S3 can send it to three things,


1 Amazon S3 -------(Events)-------> SNS ---------> SQS

 
EXP : your events into Amazon S3, send to SNS and from an SNS topic, we can do a fan out pattern to send to multiple SQ-Q,  we can sent it into an SQ-Q


2 Amazon S3 -------(Events)-------> SQS ---------> Lambda Function


EXP : we can sent it into an SQ-Q , and have a Lambda function directly read off that SQ-SQ,


3 Amazon S3 -------(Events)-------(async)----------> Lambda Function ----------- (DLQ) ---------> SQS


EXP : we could have an Amazon S3 event notification, directly invoke our Lambda function and this is an asynchronous invocation.

- this Lambda function could do whatever it wants with that data, and then in case things go wrong, we can set up a dead-letter queue, for example an SQS, as we've seen from before.



• S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer

• If two writes are made to a single non- versioned object at the same time, it is possible that only a single event notification will be sent

• If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.









------------------------------------------------------ Simple S3 Event Pattern – Metadata Sync


  S3 bucket -------------(New file event)------------> lambda (Update metadata table) -----------> DynamoDB Table / Table in RDS


EXP : So, here is a simple pattern. An S3 bucket will have a new file event into Lambda. And Lambda function will process that file maybe insert that data into DynamoDB Table or even a table in RDS database.







------------------------------------------------------ lambda and S3 Events Notifications Hands On


-- create one lambda function and one s3 bucket , make sure both are in same region

-- s3 buket --> properties --> event notificatin --> create event notification for all object creation

-- So this event notification is enabled to send data into lambda function.

print(event) in code 


import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }



-- now do upload a file in the s3 bucket

-- now what's going to happen is that this should trigger an event into my Lambda function and to see whether or not this has worked,

-- lambda --> cloudwatch --> u will see that records have created with all the event info 









------------------------------------------------------ S3 Event Notifications with Amazon EventBridge





events ---------> s3 bucket ---(ALL Events) -----> Amazon EventBridge -----(Rules)-------> Over 18 AWS services as destinations



• Advanced filtering options with JSON rules (metadata, object size, name...)

• Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose...

• EventBridge Capabilities – Archive, Replay Events, Reliable delivery







------------------------------------------------------ S3 – Baseline Performance


• Amazon S3 automatically scales to high request rates, latency 100-200 ms

• Your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.

• There are no limits to the number of prefixes in a bucket.

• Example (object path => prefix):

        • bucket/folder1/sub1/file => /folder1/sub1/

                - So, in this case it is slash folder one, slash sub one. So, that means that for this file in this prefix, you can get 3,500 Puts and 5,500 Gets per second.

        • bucket/folder1/sub2/file => /folder1/sub2/

        • bucket/1/file => /1/

        • bucket/2/file => /2/


• If you spread reads across all four prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD






------------------------------------------------------ S3 Performance



• Multi-Part upload:

        • recommended for files > 100MB, must use for files > 5GB

        • Can help parallelize uploads (speed up transfers)



• S3Transfer Acceleration

        • Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region

        • Compatible with multi-part upload

        - We have a file in the United States of America, and we want to upload it to S3 bucket in Australia.

        - So, what this will do is that we will upload that file through an edge location in the United States, which will be very, very quick, and then we'll be using the public internet.

        - And then from that edge location to the Amazon S3 bucket in Australia, the edge location will transfer it over the fast, private AWS network. So, this is called transfer acceleration,

        - because we minimized the amount of public internet that we go through and we maximized the amount of private AWS network that we go through.

        - So, transfer acceleration is a great way to speed up transfers.








------------------------------------------------------ S3 Performance – S3 Byte-Range Fetches



• Parallelize GETs by requesting specific byte ranges

• Better resilience in case of failures

- Can be used to speed up downloads

- Can be used to retrieve only partial data (for example the head of a file)









------------------------------------------------------ S3 Batch Operations




-- if u wnat to perform bulk operations on existing S3 object with a single request , example:

• Modify object metadata & properties 
• Copy objects between S3 buckets
• Encrypt un-encrypted objects
• Modify ACLs,tags
• Restore objects from S3 Glacier
• Invoke Lambda function to perform custom action on each lambda object

• A job consists of a list of objects, the action to perform, and optional parameters


• S3 Batch Operations manages retries, tracks progress, sends completion notifications, generate reports ...

• You can use S3 Inventory to get object list and use S3 Select to filter your objects







------------------------------------- LAB 

IMP NOTE : Do not create your own IAM Roles and policies for this project , You just go with "create new Role" option while doing this process


-- create 2 buckets , versioning is must 

-- now in bucket 1 , upload some files 

-- our main aim is to replicate the existing objects from one bukcet to another bucket

-- now in the bucket 1 --> management --> create replication rule 

-- give rule name 

-- Destination : select destination bukcet 

-- IAM role = ask aws to create a role 

-- once u click on save , pop-ip will come and select "Yes, replicate existing objects."

-- Job run options = select any one 

if u choose Automatically run the job when it's ready , it will run automatically once it will get finished 

if u choose Wait to run the job when it's ready , u have to run job manually 

-- Generate completion report = create new bucket to store the report or just simply give bkt 1 for practice only 

-- ask aws to create new IAM role for this job 

-- click on save , once u save this it will create job operations for u and wait for some time , once it get finished , all the objects will get replicated from source to dest bucket

-- now upload file in bucket 1 and it will replicate in dest bucket 'coz we have created replication 



------------------- LAB 2 

-- if u have create only replication rule(SRR, CRR ) already , jst click on view configuration and Click on create manifestjob and provide the details like the above example

-- set Priority b/w 1-5 









------------------------------------------------------ S3 Inventory



• List objects and their corresponding metadata (alternative to S3 List API operation)

• Usage examples:

        • Audit and report on the replication and encryption status of your objects

        • Get the number of objects in an S3 bucket

        • Identify the total storage of previous object versions

• Generate daily or weekly reports

• Output files: CSV, ORC, or Apache Parquet

• You can query all the data using Amazon Athena, Redshift, Presto, Hive, Spark...

• You can filter generated report using S3 Select

• Use cases: Business, Compliance, Regulatory needs, ...






------------------------------------------------------ S3 Inventory Hands on 


-- create one s3 bucket this is ur destination bucket 

-- chooose one bucket and it should have some files in i it , it act as source bucket 

-- note  both are in same region 

-- go to source bucket --> management --> inventory configuration --> create and give values as ur want 

-- it will take 48 hrs to create 

-- u will find all the data once after 48hrs 







------------------------------------------------------ Amazon S3 Glacier



• Low-cost object storage meant for archiving / backup

• Data is retained for the longer term (10s of years)

• Alternative to on-premises magnetic tape storage

• Average annual durability is 99.999999999%

• Cost per storage per month ($0.004 / GB – Standard | $0.00099 / GB Deep Archive)


• Each item in Glacier is called “Archive” (up to 40TB)

• Archives are stored in ”Vaults”

• By default, data encrypted at rest using AES-256 – keys managed by AWS



• Exam tip: archival from S3 after XXX days => use Glacier





------------------------------------------------------ Amazon S3 Glacier Operations



• Vault Operations:

        • Create & Delete – delete only when there’s no archives in it

        • Retrieving Metadata – creation date, number of archives, total size of all archives, ...

        • Download Inventory–list of archives in the vault(archiveID,creationdate,size,...)


• Glacier Operations:

        • Upload - single operation or by parts (MultiPart upload) for larger archives

        • Download - first initiate a retrieval job for the archive, Glacier then prepares it for download.

                - User then has a limited time to download the data from staging server. (optionally, specify a range or portion of bytes to retrieve)

        • Delete - use Glacier Rest API or AWS SDKs by specifying archive ID




• Restore links have an expiry date

• Retrieval Options:

        • Expedited (1 to 5 minutes retrieval) – $0.03 per GB and $10 per 1000 requests

        • Standard (3 to 5 hours) - $0.01 per GB and 0.03 per 1000 requests

        • Bulk (5 to 12 hours) - $0.0025 per GB and $0.025 per 1000 requests



 



------------------------------------------------------ Amazon S3 Glacier - Vault Policies & Vault Lock



• Each Vault has:

        • ONE vault access policy

        • ONE vault lock policy


• Vault Policies are written in JSON

• Vault Access Policy is like a bucket policy (restrict user / account permissions)

• Vault Lock Policy is a policy you lock, for regulatory and compliance requirements.

• The policy is immutable, it can never be changed (that’s why it’s call LOCK)

        • Example 1: forbid deleting an archive if less than 1 year old

        • Example 2: implement WORM policy (write once read many)






------------------------------------------------------ Glacier – Notifications for Restore Operations



-- So, because glacier is asynchronous because you have to initiate resource and then they will happen in time in the future. Then you need to have notification


• Vault Notification Configuration

        - Configure a vault so that when a job completes, a message is sent to SNS

        - Optionally, specify an SNS topic when you initiate a job



• S3 Event Notifications

        - S3 supports the restoration of objects archived to S3 Glacier storage classes

        - s3:ObjectRestore:Post => notify when object restoration initiated

        - s3:ObjectRestore:Completed => notify when object restoration completed






------------------------------------------------------ Amazon S3 Glacier vault Hands ON 


-- go to s3 glacier in console 

-- create a vault --> turnoff notificatin

-- open vault --> vault policies --> Vault Lock policy 

        - https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html?icmpid=docs_console_unmapped

        - u will find examples of vault policies

        - copy lock id eg : hlL937ZBSpye2jq1t4sCcuLP



{
               "Version":"2012-10-17",
               "Statement":[
                  {
                     "Sid": "deny-based-on-archive-age",
                     "Principal": "*",
                     "Effect": "Deny",
                     "Action": "glacier:DeleteArchive",
                     "Resource": [
                        "arn:aws:glacier:us-west-2:123456789012:vaults/examplevault"
                     ],
                     "Condition": {
                        "NumericLessThan" : {
                              "glacier:ArchiveAgeInDays" : "365"
                              }
                           }
                        }
                     ]
                  }
            

-- so, we'll never ever ever be able to delete an archive that is less than 365 days, ever. I cannot change this. And so this is why vault lock policies are so important,



-- review and create lock , u cannot delete once u create the lock 

-- IMP 

        - The last thing to know is that through this UI, you're not able to upload files that relate to Glacier. You will have to use the SDK, or the CLI,
        
        - Here we have to use the API if wanting to upload file into this our vault.






------------------------------------------------------ S3 Multi Part Upload – Deep Dive



• Upload large objects in parts (any order)

• Recommended for files > 100MB, must use for files > 5GB

• Can help parallelize uploads (speed up transfers)

• Max. parts: 10,000

• Failures: restart uploading ONLY failed parts (improves performance)

• Use Lifecycle Policy to automate old parts deletion of unfinished upload after x days (e.g., network outage)

• Upload using AWS CLI or AWS SDK







------------------------------------------------------ S3 Multi Part Upload – hands On 




-- open cloudshell 

1  give bucket name 

       BUCKET_NAME=mymultipartupload

2  create bucket using cmnd 

        aws s3 mb s3://$BUCKET_NAME


3 genearate 100mb file 

        dd if=/dev/zero of=100MB.txt bs=1MB count=100

        - it will create file with name 100MB.txt


4 split into 3 parts of 35MB each 

        split -b 35m 100MB.txt 100MB_part_

5 we're going to create a multipart upload 

        aws s3api create-multipart-upload --bucket $BUCKET_NAME --key 100MB.txt


{
    "ServerSideEncryption": "AES256",
    "Bucket": "mymultipartupload",
    "Key": "100MB.txt",
    "UploadId": "XjrPzuYVjz5E4zpYwF2c508KgxUs37kC75.5HEpHUXPJxQ0ymi136A0.04bsT4b1JPOsA5deI8fRFtMbrRzdpV1HCyoh6gECHFVxuxqlWOMBvGy7yFgrwnc2yCQ27TtG"
}



6  upload to s3 

        UPLOAD_ID=XjrPzuYVjz5E4zpYwF2c508KgxUs37kC75.5HEpHUXPJxQ0ymi136A0.04bsT4b1JPOsA5deI8fRFtMbrRzdpV1HCyoh6gECHFVxuxqlWOMBvGy7yFgrwnc2yCQ27TtG 



7 list all multipart uplaod 

        aws s3api list-multipart-uploads --bucket $BUCKET_NAME



{
    "Uploads": [
        {
            "UploadId": "XjrPzuYVjz5E4zpYwF2c508KgxUs37kC75.5HEpHUXPJxQ0ymi136A0.04bsT4b1JPOsA5deI8fRFtMbrRzdpV1HCyoh6gECHFVxuxqlWOMBvGy7yFgrwnc2yCQ27TtG",
            "Key": "100MB.txt",
            "Initiated": "2024-10-09T11:20:33+00:00",
            "StorageClass": "STANDARD",
            "Owner": {
                "DisplayName": "dharmasathya33",
                "ID": "4e15fbb634d441b5553de0d6d654e0bc5f544f8b41a564724037046d275ac33c"
            },
            "Initiator": {
                "ID": "arn:aws:iam::298132369629:user/demo-copilot",
                "DisplayName": "demo-copilot"
            }
        }
    ],
    "RequestCharged": null
}





8  upload the parts 

        aws s3api upload-part --bucket $BUCKET_NAME --key 100MB.txt --part-number 1 --body 100MB_part_aa --upload-id $UPLOAD_ID

        aws s3api upload-part --bucket $BUCKET_NAME --key 100MB.txt --part-number 2 --body 100MB_part_ab --upload-id $UPLOAD_ID

        aws s3api upload-part --bucket $BUCKET_NAME --key 100MB.txt --part-number 3 --body 100MB_part_ac --upload-id $UPLOAD_ID

        - if I go into my S3 bucket now and refresh we currently don't see any objects. That's because the parts don't appear in the console UI. The parts are not yet objects.


9 list the parts 

         aws s3api list-parts --upload-id $UPLOAD_ID --bucket $BUCKET_NAME --key 100MB.txt



{
    "Parts": [
        {
            "PartNumber": 1,
            "LastModified": "2024-10-09T11:27:13+00:00",
            "ETag": "\"cdbd372cb84b1720b94c30cc95d5c4cf\"",
            "Size": 36700160
        },
        {
            "PartNumber": 2,
            "LastModified": "2024-10-09T11:27:58+00:00",
            "ETag": "\"cdbd372cb84b1720b94c30cc95d5c4cf\"",
            "Size": 36700160
        },
        {
            "PartNumber": 3,
            "LastModified": "2024-10-09T11:28:04+00:00",
            "ETag": "\"53d2747cf38ca67cfb9c165d90ff896f\"",
            "Size": 26599680
        }
    ],
    "ChecksumAlgorithm": null,
    "Initiator": {
        "ID": "arn:aws:iam::298132369629:user/demo-copilot",
        "DisplayName": "demo-copilot"
    },
    "Owner": {
        "DisplayName": "dharmasathya33",
        "ID": "4e15fbb634d441b5553de0d6d654e0bc5f544f8b41a564724037046d275ac33c"
    },
    "StorageClass": "STANDARD"
}



10 complete the multipart upload in single command 


                  aws s3api complete-multipart-upload --bucket $BUCKET_NAME --key 100MB.txt --upload-id $UPLOAD_ID --multipart-upload "{\"Parts\":[{\"ETag\":\"etag1\",\"PartNumber\":1},{\"ETag\":\"etag2\",\"PartNumber\":2},{\"ETag\":\"etag3\",\"PartNumber\":3}]}"


-- in above cmnd change etag based on previous cmnd  , for eg 



                aws s3api complete-multipart-upload --bucket $BUCKET_NAME --key 100MB.txt --upload-id $UPLOAD_ID --multipart-upload "{\"Parts\":[{\"ETag\":\"cdbd372cb84b1720b94c30cc95d5c4cf\",\"PartNumber\":1},{\"ETag\":\"cdbd372cb84b1720b94c30cc95d5c4cf\",\"PartNumber\":2},{\"ETag\":\"53d2747cf38ca67cfb9c165d90ff896f\",\"PartNumber\":3}]}"


-- go n see console it will upload to s3 bucket 



-- now delete bucket and file 








------------------------------------------------------ Amazon Athena



• Serverless query service to analyze data stored in Amazon S3

• Uses standard SQL language to query the files (built on Presto)

• SupportsCSV,JSON,ORC,Avro,andParquet

• Pricing: $5.00 per TB of data scanned

• Commonly used with Amazon Quicksight for repor ting/dashboards

• Use cases: Business intelligence / analytics / reporting, analyze & queryVPC Flow Logs,ELB Logs,CloudTrail trails,etc...



• Exam Tip: analyze data in S3 using serverless SQL, use Athena





------------------------------------------------------ Amazon Athena – Performance Improvement



• Use columnar data for cost-savings (less scan)

        • Apache Parquet or ORC is recommended

        • Huge performance improvement

        • Use Glue to convert your data to Parquet or ORC


• Compress data for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd...)

• Partition datasets in S3 for easy querying on virtual columns

        • s3://yourBucket/pathToTable

                 /<PARTITION_COLUMN_NAME>=<VALUE>
                  /<PARTITION_COLUMN_NAME>=<VALUE>
                    /<PARTITION_COLUMN_NAME>=<VALUE>

        • Example:s3://athena-examples/flight/parquet/year=1991/month=1/day=1/


• Use larger files (> 128 MB) to minimize overhead







------------------------------------------------------ Amazon Athena – Federated Query



• Allows you to run SQL queries across data stored in relational, non-relational, object, and custom data sources (AWS or on-premises)

• Uses Data Source Connectors that run on AWS Lambda to run Federated Queries (e.g., CloudWatch Logs, DynamoDB, RDS, ...)

• Store the results back in Amazon S3






------------------------------------------------------ Amazon Athena Hands ON 






Step 1 : create s3 bucket and insert some .csv data 

--  insert some  data in the bucket 

--   create one folder inside bucket and go to google and search for sample .csv files downlaod and upload in the s3 bucket 

Step 2 : Glue

-- open glue in console

-- glue --> from left panel data Catalog --> crawlers --> add crawlers --> give name of ur crawler --> not yet --> add a data source --> choose s3 --> copy uri of ur bucket and paste in S3 path --> click on create new role , give name of ur role and select nxt --> click on create new database --> give name of ur database , nxt --> frequency = on-demand --> create crawler

-- now try to run the crawler , 

--  u can also check the logs in cloudwatch 

-- once u done with the running part , a table will added to the database , table name is same as the folder name of s3 that u have uploaded 

--  if ur data is not match , then u can also do edit ur schema 


step 3 : Athena

-- open Athena in the console

-- in the left panel u will see the database and table , u can check all the coloums of ur table

-- now do queries , before that u should add destination to store our outputs in the s3 bucket 

-- go to s3 buckets create one folder in the bucket to store the outputs

-- now try to query in athena editor

-- SELECT * FROM "weather-database"."weather_csv" limit 10;

above is eg query , u can replace with ur details 

weather-database = database name 

weather_csv = table name


NOTE : if u r getting zero records but query is successfull then do 

ur s3 location is like this 

s3://doc-example-bucket/table1.csv 

-- to get avoid from this error , jst create one sub-folder and upload .csv file in that subfolder 

s3://doc-example-bucket/table1/table1.csv



-- in the crawler location u have to give like this , then u won't get any error

s3://doc-example-bucket/table1/










======================================================== Amazon S3 – Security ========================================================





------------------------------------------------------ Amazon S3 – Object Encryption



• You can encrypt objects in S3 buckets using one of 4 methods


• Server-Side Encryption (SSE)

        • Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) – Enabled by Default

                 • Encrypts S3objects using key shandled,managed,and owned by AWS

        • Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)

                • Leverage AWS KeyManagement Service(AWSKMS)to manage encryption keys

        • Server-Side Encryption with Customer-Provided Keys (SSE-C)

                • When you want to manage your own encryption keys



• Client-Side Encryption

• It’s important to understand which ones are for which situation for the exam







------------------------------------------------------ Amazon S3 Encryption – SSE-S3



- it is used Algorith called AES-256 (advanced Encryption Standard)

- by deafult , bucket encryption is enabled 

- Object is encrypted server-side

- Must set header "x-amz-server-side-encryption": "AES256"


explanation : Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in AWS data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. For example, if you share your objects by using a presigned URL, that URL works the same way for both encrypted and unencrypted objects. Additionally, when you list objects in your bucket, the list API operations return a list of all objects, regardless of whether they are encrypted.


-- When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates.







------------------------------------------------------ Amazon S3 Encryption – SSE-KMS




- KMS advantages: user control + audit key usage using CloudTrail

- Object is encrypted server side

- Must set header "x-amz-server-side-encryption": "aws:kms"

explanation : All Amazon S3 buckets have encryption configured by default, and all new objects that are uploaded to an S3 bucket are automatically encrypted at rest. Server-side encryption with Amazon S3 managed keys (SSE-S3) is the default encryption configuration for every bucket in Amazon S3. To use a different type of encryption, you can either specify the type of server-side encryption to use in your S3 PUT requests, or you can set the default encryption configuration in the destination bucket.


---- SSE-KMS Limitation

-- If you use SSE-KMS, you may be impacted by the KMS limits

-- When you upload, it calls the GenerateDataKey KMS API

-- When you download, it calls the Decrypt KMS API

-- Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)

-- You can request a quota increase using the Service Quotas Console






------------------------------------------------------ Amazon S3 Encryption – SSE-C



-- SSE-C you can only do from the CLI not from console 

- Server-Side Encryption using keys fully managed by the customer outside of AWS

- Amazon S3 does NOT store the encryption key you provide

- HTTPS must be used

- Amazon S3 will reject any requests made over HTTP when using SSE-C. For security considerations, AWS recommends that you consider any key you send erroneously using HTTP to be compromised.

- Encryption key must provided in HTTP headers, for every HTTP request made

explanation : Server-side encryption is about protecting data at rest. Server-side encryption encrypts only the object data, not the object metadata. By using server-side encryption with customer-provided keys (SSE-C), you can store your own encryption keys. With the encryption key that you provide as part of your request, Amazon S3 manages data encryption as it writes to disks and data decryption when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing that you need to do is manage the encryption keys that you provide.

When you upload an object, Amazon S3 uses the encryption key that you provide to apply AES-256 encryption to your data. Amazon S3 then removes the encryption key from memory. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key that you provided matches, and then it decrypts the object before returning the object data to you.



EPV : A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?

ANS : Configure the bucket policy to deny if the PutObject does not have an "x-amz-server-side-encryption" header set

EXP : 

-- Server-side encryption is about data encryption at rest—that is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.

-- As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.

-- To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.

-- In order to enforce object encryption, create an Amazon S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. 

-- There are two possible values for the x-amz-server-side-encryption header

    1 AES256, which tells S3 to use S3-managed keys, 

    2 aws:kms, which tells Amazon S3 to use AWS KMS–managed keys.









------------------------------------------------------ Amazon S3 Encryption – Client-Side Encryption



• Use client libraries such as Amazon S3 Client-Side Encryption Library

• Clients must encrypt data themselves before sending to Amazon S3

• Clients must decrypt data themselves when retrieving from Amazon S3

• Customer fully manages the keys and encryption cycle



Client-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects.

The Amazon S3 Encryption Client works as an intermediary between you and Amazon S3. After you instantiate the Amazon S3 Encryption Client, your objects are automatically encrypted and decrypted as part of your Amazon S3 PutObject and GetObject requests. Your objects are all encrypted with a unique data key. The Amazon S3 Encryption Client does not use or interact with bucket keys, even if you specify a KMS key as your wrapping key.






------------------------------------------------------ Amazon S3 – Encryption in transit (SSL/TLS)





-- Encryption in flight is also called SSL/TLS

-- Amazon S3 exposes two endpoints:
  • HTTP Endpoint – non encrypted
  • HTTPS Endpoint – encryption in flight

-- HTTPS is recommended

-- HTTPS is mandatory for SSE-C

-- Most clients would use the HTTPS endpoint by default

Encrypting Data-at-Rest and Data-in-Transit

To protect data in transit, AWS encourages customers to leverage a multi-level approach. All network traffic between AWS data centers is transparently encrypted at the physical layer. All traffic within a VPC and between peered VPCs across regions is transparently encrypted at the network layer when using supported Amazon EC2 instance types. At the application layer, customers have a choice about whether and how to use encryption using a protocol like Transport Layer Security (TLS). All AWS service endpoints support TLS to create a secure HTTPS connection to make API requests.






       

------------------------------------------------------ Amazon S3 – Force Encryption in Transit aws:SecureTransport




{
  "Version": "2012-10-17",
  "Id": "ExamplePolicy01",
  "Statement": [
    {
      "Sid": "VisualEditior0",
      "Effect": "Allow",
      "Principal": {
        "AWS": "*"
      },
      "Action": [
"elasticfilesystem:ClientRootAccess",
"elasticfilesystem:ClientMount",
"elasticfilesystem:ClientWrite"
      ],
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "true"
        }
      }
    }
  ]
}



-- So SecureTransport is going to be true whenever using HTTPS and false whenever you're not using an encryption,

--  so, therefore, any user trying to use HTTP on your bucket is going to be blocked, but users using HTTPS may be allowed.







------------------------------------------------------ What is CORS?



• Cross-Origin Resource Sharing (CORS)

• Origin = scheme (protocol) + host (domain) + port

        • example: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)

• Web Browser based mechanism to allow requests to other origins while visiting the main origin        

• Same origin: http://example.com/app1 & http://example.com/app2

• Different origins: http://www.example.com & http://other.example.com

• The requests won’t be fulfilled unless the other origin allows for the requests, using CORS Headers (example: Access-Control-Allow-Origin)


• If a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers

• It’s a popular exam question

• You can allow for a specific origin or for * (all origins)







------------------------------------------------------ CORS Hands ON 





-- create 2 public buckets 

-- create 2 html files 

1 index.html 

2 load.html 


1  

<html>
<head>
<title>AWS s3 Cors Demo </title>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
</head>
<body>
<h1>AWS S3 CORS DEMO</h1>
<div id="loadDiv"></div>
</body>
</html>
<script type="text/javascript">
$("#loadDiv").load("load.html");
</script>


2

this is load from same bucket 



-- upload 2 htm files and make them as public in bucket 1

-- enable static web hosting 

-- now try to access link u will get o/p 2 lines calling load.html 

-- now create another bucket 2 (public bucket) and upload only load.html 

-- make file public 

-- copy object url of load.html from bucket 1


<html>
<head>
<title>AWS S3 CORS DEMO </title>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
</head>
<body>
<h1>AWS S3 CORS DEMO</h1>
<div id="loadDiv"></div>
</body>
</html>
<script type="text/javascript">
$("#loadDiv").load("https://cors-dem2.s3.ap-south-1.amazonaws.com/load.html");
</script>

-- now upload back index.html(updated) to s3 again and make public in bucket 1

-- make them public 

-- now do refresh the link in browser  

-- u can not able to see load.html content in the browser coz u are calling from the other bucket and u did not enable CORS so u are not getting content 

-- now go and create CORS in Bucket2 

-- open 2 bkt --> permissions --> CORS c.o learn more --> copy snippet from document in google 

[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [],
        "AllowedMethods": [
            "GET"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    }
]


-- now do refrsh the link u will get content of load.html 








------------------------------------------------------ Amazon S3 – MFA Delete


• MFA (Multi-Factor Authentication) – force users to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3

• MFA will be required to:

        • Permanently delete an object version

        • Suspend Versioning on the bucket


• MFA won’t be required to:

        • EnableVersioning

        • List deleted versions


• To use MFA Delete, Versioning must be enabled on the bucket

• Only the bucket owner (root account) can enable/disable MFA Delete





-- Your company has decided that certain users should have Multi-Factor Authentication (MFA) enabled for their sign-in credentials. A newly hired manager has a Gemalto MFA device that he used in his earlier company. He has approached you to configure it for his AWS account.

How will you configure his existing Gemalto MFA device so he can seamlessly connect with AWS services in the new company?


ANS : AWS MFA does not support the use of your existing Gemalto device

EXP : 

        - AWS MFA relies on knowing a unique secret associated with your hardware MFA (Gemalto) device in order to support its use.

        - Because of security constraints that mandate such secrets never be shared between multiple parties, AWS MFA cannot support the use of your existing Gemalto device.

        - Only a compatible hardware MFA device purchased from Gemalto can be used with AWS MFA. You can re-use an existing U2F security key with AWS MFA, as U2F security keys do not share any secrets between multiple parties.







------------------------------------------------------ Amazon S3 – MFA Delete Hands ON 



-- create bucket and eneble version

-- go to bucket --> properties --> edit versioning --> usee MFA option 

-- you cannot change this through the UI of Amazon console, for some reason. So maybe someday they will allow us to enable it. But for now, what you have to do is to enable it directly using the AWS CLI.

-- so to do this , u have to login through the root account --> security credential --> MFA 

-- create new access key and secret key for this demo 

-- go to server or cmd promt in your local machine 

        aws configure --profile root-mfa-delete-demo

        - give access key and secret key and configure 

        - aws s3 ls --profile root-mfa-delete-demo     , able to see buckets if you have any 


-- # enable mfa delete

        aws s3api put-bucket-versioning --bucket mfa-demo-stephane --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa "arn-of-mfa-device mfa-code" --profile root-mfa-delete-demo

        - change ur bucket name 

        - change ARN of device u will get from the IAM MFA console 

        - MFA code you will get from your application directly 


-- if I go into my bucket versioning and refresh, as we can see now, bucket versioning, it says bucket versioning is enabled as well as Multi-Factor authentication,MFA delete is enabled.




-- so, how do we know if it worked?

        - let's say I'm going to my objects and I'm going to upload objects. So let me upload, for example, a copy of the file,

        - I will upload it, so this is working

        - If I go back to my buckets, take that objects and delete it. Okay, we're going to delete it,

        - but we have enabled versioning, so this is just going to add a delete marker. This is working as well.

        - if I list my bucket versions now, okay I have two versions for my file, but now if I wanted to, for example, delete this specific version ID.

        - It says, you cannot delete object because Multi-Factor authentication, MFA deletes is enabled for this bucket.(pop-up)

        - So we can just go ahead and disable MFA delete, using cli / server 

        -  # disable mfa delete

                aws s3api put-bucket-versioning --bucket mfa-demo-stephane --versioning-configuration Status=Enabled,MFADelete=Disabled --mfa "arn-of-mfa-device mfa-code" --profile root-mfa-delete-demo

        - chnage mfa code 

        - now try to delete the version id , u are able to delete 

        - # delete the root credentials in the IAM console!!!






------------------------------------------------------ S3 Access Logs



• For audit purpose, you may want to log all access to S3 buckets

• Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket

• That data can be analyzed using data analysis tools...

• The target logging bucket must be in the same AWS region



• The log format is at: https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html





------------------------------------------------------ S3 Access Logs:Warning



• Do not set your logging bucket to be the monitored bucket

• It will create a logging loop, and your bucket will grow exponentially






------------------------------------------------------ S3 Access Logs Hands ON 


-- create 2 buckets 

-- open source bkt and enable server access logs and choose destination bkt 

-- upload some files in source bucket  wait for some time to generate logs in destination bkt 

-- after hours u will see the logs of each actions 








------------------------------------------------------ Amazon S3 – Pre-Signed URLs



• Generate pre-signed URLs using the S3 Console, AWS CLI or SDK

• URL Expiration

        • S3 Console – 1 min up to 720 mins (12 hours)

        • AWS CLI – configure expiration with --expires-in parameter in seconds (default 3600 secs, max. 604800 secs ~ 168 hours)


• Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET / PUT


• Examples:

        • Allow only logged-in users to download a premium video from your S3 bucket

        • Allow an ever-changing list of users to download files by generating URLs dynamically

        • Allow temporarily a user to upload a file to a precise location in your S3 bucket

 



 ------------------------------------------------------ Amazon S3 – Pre-Signed URLs Hands ON 






-- if u want to generate Pre-signed url through cli

          aws s3 presign s3://hfgvgdgs/ec2.py --expires-in 604800 --region ap-south-1 --endpoint-url https://s3.ap-south-1.amazonaws.com

                                               or 

          aws s3 presign s3://hfgvgdgs/ec2.py --expires-in 3600                                               



-- if u want to create with python



import boto3 


def lambda_handler(event, context):
    url = boto3.client('s3').generate_presigned_url(
    ClientMethod='get_object',
    Params={'Bucket': 'hfgvgdgs', 'Key': 'ec2.py'},
    ExpiresIn = 3600)
    
    
    print("the url is " + url)






-- Downloading the File

         curl -o "output-file-name" "presigned-url"



-- Customizing File Name and Location

         curl -o "/path/to/destination/file-name.extension" "presigned-url"

          - Ensure that the directory exists before executing the command.






------------------------------------------------------ S3 Glacier Vault Lock



• Adopt a WORM (Write Once Read Many) model

• Create a Vault Lock Policy

• Lock the policy for future edits (can no longer be changed or deleted)

• Helpful for compliance and data retention




------------------------------------------------------ S3 Object Lock (versioning must be enabled)



• Adopt a WORM (Write Once Read Many) model

• Block an object version deletion for a specified amount of time

• Retention mode - Compliance:

        • Object versions can't be overwritten or deleted by any user, including the root user

        • Objects retention modes can't be changed, and retention periods can't be shortened

• Retention mode - Governance:

        • Most users can't overwrite or delete an object version or alter its lock settings

        • Some users have special permissions to change the retention or delete the object


• Retention Period: protect the object for a fixed period, it can be extended

• Legal Hold:

        • protect the object indefinitely, independent from retention period

        • can be freely placed and removed using the s3:PutObjectLegalHold IAM permission







------------------------------------------------------ S3 – Access Points



• Access Points simplify security management for S3 Buckets

• Each Access Point has:

        • its own DNS name (Internet Origin or VPC Origin)

        • an access point policy (similar to bucket policy) – manage security at scale







------------------------------------------------------ S3 – Access Points - VPC Origin



• We can define the access point to be accessible only from within the VPC

• You must create a VPC Endpoint to access the Access Point (Gateway or Interface Endpoint)

• The VPC Endpoint Policy must allow access to the target bucket and Access Point




- The following example policy statement configures a VPC endpoint to allow calls to GetObject for a bucket named awsexamplebucket1 and an access point named example-vpc-ap.



{
    "Version": "2012-10-17",
    "Statement": [
    {
        "Principal": "*",
        "Action": [
            "s3:GetObject"
        ],
        "Effect": "Allow",
        "Resource": [
            "arn:aws:s3:::awsexamplebucket1/*",
            "arn:aws:s3:us-west-2:123456789012:accesspoint/example-vpc-ap/object/*"
        ]
    }]
}



 

 ------------------------------------------------------ S3 – Access Points Hands ON check in course 






------------------------------------------------------ S3 – Multi-Region Access Points


• Provide a global endpoint that span S3 buckets in multiple AWS regions

• Dynamically route requests to the nearest S3 bucket (lowest latency)

• Bi-directional S3 bucket replication rules are created to keep data in sync across regions

• Failover Controls – allows you to shift requests across S3 buckets in different AWS regions within minutes (Active-Active or Active- Passive)








======================================================== Advanced Storage Solutions ========================================================






------------------------------------------------------ AWS Snow Family


• Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS



-- Data migration:

1 Snowcone
2 Snowball Edge
3 Snowmobile



-- Edge computing:
1 Snowcone
2 Snowball Edge




                                Snowcone                      Snowball Edge

Storage Capacity                8 TB HDD-14 TB SSD            80 TB-210 TB
Migration Size                  Up to terabytes               Up to petabytes






---------------- Data Migrations with AWS Snow Family

                  Time to Transfer

            100 Mbps     1Gbps      10Gbps
10 TB       12 days      30 hours    3 hours
100 TB      124 days     12 days     30 hours
1PB         3 years      124 days    12 days



-- Challenges:
• Limited connectivity
• Limited bandwidth
• High network cost
• Shared bandwidth (can’t maximize the line)
• Connection stability



-- AWS Snow Family: offline devices to perform data migrations

-- If it takes more than a week to transfer over the network, use Snowball devices!






------------------------------------------------------ Snow Family – Usage Process




1. Request Snowball devices from the AWS console for delivery

2. Install the snowball client / AWS OpsHub on your servers

3. Connect the snowball to your servers and copy files using the client

4. Ship back the device when you’re done (goes to the right AWS facility)

5. Data will be loaded into an S3 bucket

6. Snowball is completely wiped







------------------------------------------------------ What is Edge Computing?

-- Process data while it’s being created on an edge location


• A truck on the road,a ship on the sea,a mining station underground...

-- These locations may have 
• Limited / no internet access
• Limited / no easy access to computing power


--- We setup a Snowball Edge / Snowcone device to do edge computing

-- Use cases of Edge Computing:
• Preprocess data
• Machine learning at the edge 
• Transcoding media streams

-- Eventually (if need be) we can ship back the device to AWS (for transferring data for example)



• We setup a Snowball Edge / Snowcone device to do edge computing

        • Snowcone: 2 CPUs, 4 GB of memory, wired or wireless access

        • Snowball Edge Compute Optimized (dedicated for that use case) & Storage Optimized

        • Run EC2 Instances or Lambda functions at the edge





------------------------------------------------------   1 Snowball Edge (for data transfers)





-- The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. 

-- Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. 

-- AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.

-- Physical data transport solution : moveTBs or PBs of data in or out of AWS

-- Alternative to moving data over the network (and paying network fees)

-- Pay per data transfer job

-- Provide block storage and Amazon S3-compatible object storage

-- Snowball Edge Storage Optimized
   • 80 TB of HDD capacity for block volume and S3 compatible object storage

Migration Size : Up to petabytes, offline

-- Snowball Edge Compute Optimized
   • 42 TB of HDD or 28TB NVMe capacity for block volume and S3 compatible object storage

-- Usecases:large data cloud migrations,DC decommission,disaster recovery

-- no pre installation of data sync





------------------------------------------------------  2 AWS Snowcone & Snowcone SSD




-- Small, portable computing, anywhere, rugged & secure, withstands harsh environments

-- Light (4.5 pounds, 2.1 kg)

-- Device used for edge computing, storage, and data transfer

-- Snowcone – 8 TB of HDD Storage

-- Snowcone SSD – 14 TB of SSD Storage

-- Use Snowcone where Snowball does not fit (space- constrained environment)

-- Must provide your own battery / cables

-- Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data

-- Migration Size : Up to 24 TB, online and offline




  ------------------------------------------------------ 3 AWS Snowmobile




-- Transfer exabytes of data (1 EB = 1,000 PB = 1,000,000 TBs)

-- Each Snowmobile has 100 PB of capacity (use multiple in parallel)

-- High security: temperature controlled, GPS, 24/7 video surveillance

-- Better than Snowball if you transfer more than 10 PB

-- Migration Size : Up to exabytes, offline

-- no pre installation of data sync

-- AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. 

-- Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. 

-- Transferring data with Snowmobile is more secure, fast, and cost-effective. AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location.

-- For datasets less than 10PB or distributed in multiple locations, you should use Snowball.





------------------------------------------------------ Amazon FSx – Overview


• Launch 3rd party high-performance file systems on AWS

• Fully managed service

        FSx for Lustre

        FSx for Windows File Server

        FSx for NetApp ONTAP

        FSx for OpenZFS







---------------- 1 Amazon FSx for Lustre



• Lustre is a type of parallel distributed file system, for large-scale computing

• The name Lustre is derived from “Linux” and “cluster

• Machine Learning, High Performance Computing (HPC)

• Video Processing, Financial Modeling, Electronic Design Automation

• Scales up to 100s GB/s, millions of IOPS, sub-ms latencies

• Storage Options:
     • SSD – low-latency, IOPS intensive workloads, small & random file operations
     • HDD – throughput-intensive workloads, large & sequential file operations


• Seamless integration with S3 
     • Can “read S3” as a file system (through FSx)
     • Can write the output of the computations back to S3 (through FSx)


• Can be used from on-premises servers (VPN or Direct Connect)



----- FSx Lustre - File System Deployment Options


1 • Scratch File System
     • Temporary storage
     • Data is not replicated (doesn’t persist if file server fails)
     • High burst (6x faster, 200MBps per TiB)
     • Usage: short-term processing, optimize costs


2 • Persistent File System

     • Long-term storage
     • Data is replicated within same AZ
     • Replace failed files within minutes
     • Usage: long-term processing, sensitive data





------------------- 2 Amazon FSx for Windows (File Server)

• FSx for Windows is a fully managed Windows file system share drive

• Suppor ts SMB protocol & Windows NTFS

• Microsoft ActiveDirectory integration, ACLs, userquotas

• Can be mounted on Linux EC2 instances

• Supports Microsoft's Distributed File System (DFS) Namespaces (group files across multiple FS)

• Scale up to 10s of GB/s, millions of IOPS, 100s PB of data

• Storage Options:
    • SSD – latency sensitive workloads (databases, media processing, data analytics, ...)
    • HDD – broad spectrum of workloads (home directory, CMS, ...)

• Can be accessed from your on-premises infrastructure (VPN or Direct Connect)

• Can be configured to be Multi-AZ (high availability)

• Data is backed-up daily to S3



--------------------- 3 Amazon FSx for NetApp ONTAP

• Managed NetApp ONTAP on AWS

• File System compatible with NFS, SMB, iSCSI protocol

• Move workloads running on ONTAP or NAS to AWS

• Works with:
      • Linux
      • Windows
      • MacOS
      • VMware Cloud on AWS
      • Amazon Workspaces & AppStream 2.0
      • Amazon EC2, ECS and EKS


• Storage shrinks or grows automatically

• Snapshots,replication,low-cost,compression and data de-duplication

• Point-in-time instantaneous cloning (helpful for testing new workloads)


----- Scheduled replication using NetApp SnapMirror

-- You can use NetApp SnapMirror to schedule periodic replication of your FSx for ONTAP file system to or from a second file system. This capability is available for both in-Region and cross-Region deployments.

-- NetApp SnapMirror replicates data at high speeds, so you get high data availability and fast data replication across ONTAP systems, whether you're replicating between two Amazon FSx file systems in AWS, or from on-premises to AWS.

-- Replication can be scheduled as frequently as every 5 minutes, although intervals should be carefully chosen based on RPOs (Recovery Point Objectives), RTOs (Recovery Time Objectives), and performance considerations.

-- There are two types of SnapMirror replication: Volume-level SnapMirror and SVM Disaster Recovery (SVMDR). Only volume-level SnapMirror replication is supported by FSx for ONTAP.




----------------------- 4 Amazon FSx for OpenZFS

• Managed OpenZFS file system on AWS

• File System compatible with NFS (v3, v4, v4.1, v4.2)

• Move workloads running on ZFS to AWS

• Works with:
      • Linux
      • Windows
      • MacOS
      • VMwareCloudonAWS
      • AmazonWorkspaces&AppStream2.0 
      • AmazonEC2, ECS and EKS


• Up to 1,000,000 IOPS with < 0.5ms latency

• Snapshots, compression and low-cost

• Point-in-time instantaneous cloning (helpful for testing new workloads)










------------------------------------------------------ FSx for SysOps



• FSx for Windows – Single-AZ

        • Automatically replicates data within an AZ

        • Two generations: Single-AZ 1 (SSD), Single-AZ 2 (SSD & HDD)



• FSx for Windows – Multi-AZ

        • Automatically replicates data across AZs (synchronous)

        • Standby file server in a different AZ (automatic failover)








------------------------------------------------------ Hybrid Cloud for Storage



• AWS is pushing for ”hybrid cloud”

        • Part of your infrastructure is on the cloud

        • Part of your infrastructure is on-premises


• This can be due to

        • Long cloud migrations

        • Security requirements

        • Compliance requirements

        • IT strategy


• S3 is a proprietary storage technology (unlike EFS / NFS), so how do you expose the S3 data on-premises?

• AWS Storage Gateway!





------------------------------------------------------ AWS Storage Cloud Native Options


-- Block

        - Amazon EBS

        - EC2 Instance Store

-- File        

        - Amazon EFS

        - Amazon FSx

-- Object

        - Amazon S3

        - Amazon Glacier





------------------------------------------------------ AWS Storage Gateway


- AWS Storage Gateway stores volume, snapshot, and tape data in the AWS region in which the gateway is activated


-- it works as hybrid cloud service 

-- the data that u have in S3, ebs,fsx,glacier  move these data into the on-premises through the "storage gateway", it it support only "S3, ebs,fsx,glacier" 

-- Storage Gateway provides a standard set of storage protocols such as iSCSI(Internet Small Computer Systems Interface ), SMB, and NFS, which allow you to use AWS storage without rewriting your existing applications.

-- ur lap is also called on-premises 

-- it is not possible to mount storage service(s3,efs,glacier,FSx) to ur on-premises but by use of storage gateway u can do this 




• Bridge between on-premises data and cloud data

• Use cases:

        • disaster recovery

        • backup & restore

        • tiered storage

        • on-premises cache & low-latency files access


• Types of Storage Gateway:

        • S3 File Gateway

        • FSx File Gateway

        • Volume Gateway

        • Tape Gateway





------------------------------------------------------ Amazon S3 File Gateway


• Configured S3 buckets are accessible using the NFS and SMB protocol

• Most recently used data is cached in the file gateway

• Supports S3 Standard,S3 StandardIA,S3 OneZoneA,S3 IntelligentTiering

• Transition to S3 Glacier using a Lifecycle Policy

• Bucket access using IAM roles for each File Gateway

• SMB Protocol has integration with Active Directory (AD) for user authentication




Application Server <-------(NFS or SMB)---->s3 filegateway <------(HTTPS)------> (AWS cloud )s3 storage class ----> (lifecycle policy) --> S3 glacier






------------------------------------------------------ Amazon FSx File Gateway



• Native access to Amazon FSx for Windows File Server

• Local cache for frequently accessed data

• Windows native compatibility (SMB, NTFS, Active Directory...)

• Useful for group file shares and home directories





------------------------------------------------------ Volume Gateway



• Block storage using iSCSI protocol backed by S3

• Backed by EBS snapshots which can help restore on-premises volumes!

• Cached volumes: low latency access to most recent data

• Stored volumes: entire dataset is on premise, scheduled backups to S3






------------------------------------------------------ Tape Gateway



• Some companies have backup processes using physical tapes (!)

• With Tape Gateway, companies use the same processes but, in the cloud

• VirtualTape Library (VTL) backed by Amazon S3 and Glacier

• Back up data using existing tape-based processes (and iSCSI interface)

• Works with leading backup software vendors







------------------------------------------------------ Storage Gateway – Hardware appliance




-- as you can see in all these diagrams from before, the gateway has to be installed on your corporate data center, it has to run within your corporate data center. But sometimes you do not have virtual servers to run this additional gateway.

-- So an option for you is to leverage hardware from AWS. So it's called Storage Gateway Hardware Appliance.


• Using Storage Gateway means you need on-premises virtualization

• Otherwise, you can use a Storage Gateway Hardware Appliance

• You can buy it on amazon.com

• Works with File Gateway,Volume Gateway,Tape Gateway

• Has the required CPU, memory, network, SSD cache resources

• Helpful for daily NFS backups in small data centers






------------------------------------------------------ Storage Gateway – Hands ON 





-- we do not have on-premises ,so consider ur lap as on-premises and VM as Ec2 and install Storage gateway appliance(agent) on on-premises

-- this is chargable 

-- go to soage gateway and create s3 file gateway 

-- Platform options = ec2 

-- customize ur settings 

-- c.o launch instance , automatically AMI is take here 

-- select Instance type is = m5.xlarge

-- add additional volume of 150 GiB of gp2 

-- c.o checkbox and nxt 

-- copy IP of instance and give in nxt step as a connection 

-- now on-premises and file gate way now connected 

-- activate and configure 

-- now ceate s3 private bucket 

-- now file share we have to create , if u open gateway u will have options like what to creat now we have files so create file share 

-- all the data come to the file share and this data stored in AWS S3 

-- c.o customize configuration , in step 3 u can give accesss client to all (0.0.0.0/0)

-- open file share u will find "mount point"

-- launch t2.micro instance for testing 

-- connect nstance 

-- sudo -s 

-- yum install -y nfs-utils

-- mkdir filesystem

-- go to file share and copy mount point for linux 

-- mount -t nfs -o nolock,hard 172.31.1.243:/store-gate [MountPath]/

-- cd foledername

-- now indirectly iam in S3 only if i put any data and stored this data in S3 

-- do df -h n see s3 bucket is there so u r doing mount 

-- create some files 

-- check in S3 ur file will store in S3 automatically 

-- the file u created all go through Storage gateway appliance(agent) --> storefile gateway--> S3 


-- delete all resource 

-- delete additional volume also it has 150 GIB 






------------------------------------------------------ Storage Gateway – SysOps



• File Gateway is POSIX compliant (Linux file system)

        • POSIX metadata ownership, permissions, and timestamps stored in the object’s metadata in S3

• Reboot Storage Gateway VM: (e.g., maintenance)

        • File Gateway: simply restart the Storage GatewayVM

        • Volume and Tape Gateway:

                • Stop Storage Gateway Service (AWS Console,VM local Console, Storage Gateway API)

                • Reboot the Storage GatewayVM

                • Start Storage Gateway Service (AWS Console,VM local Console, Storage Gateway API)





------------------------------------------------------ Storage Gateway – Activations


• Two ways to get Activation Key:

        • Using the Gateway VMCLI

        • Make a web request to the GatewayVM (Port 80)


• Troubleshooting Activation Failures:

        • Make sure the GatewayVM has port 80 opened

        • Check that the Gateway VM has the correct time and synchronizing its time automatically to a Network Time Protocol (NTP) server






------------------------------------------------------ Storage Gateway – Volume Gateway Cache


• Cached mode: only the most recent data is stored

• Looking at cache efficiency

        • Look at the CacheHitPercent metric (you want it to be high)

        • Look at the CachePercentUsed (you don’t want it to be too high)


• Create a larger cache disk

        • Use the cached volume to clone a new volume of a larger size

        • Select the new disk as the cached volume









========================================================  Amazon CloudFront  ========================================================






• Content Delivery Network (CDN)

• Improves read performance, content is cached at the edge

• Improves users experience

• 216 Point of Presence globally (edge locations)

• DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall


-- it is global servie 

-- Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.

-- Amazon CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers.

-- CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.

-- Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity.

-- we can put it in front of our Auto Scaling group and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won't need to scale as much).

--  here u have s3 bucket which have static website in it 

-- if ur user in U.S , the user connect to our website through edge Location not directly to our website , if he cnnect direct from our region the latency will be high 

-- we need to setup CloudFront it create distributions and origin is = S3 bucket 

-- from edge Location to website the data is stored through the CDN , it is super speed managed by the AWS Network

-- once u create CF , it give nasty URL 







-------------------------- Amazon CloudFront capabilities on routing, security, and high availability.





1 Amazon CloudFront High availability : 

    - Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover

EXP : 

-- You can set up Amazon CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.

-- To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.


2 Amazon CloudFront Routing : 

   - Amazon CloudFront can route to multiple origins based on the content type

EXP : 

-- You can configure a single Amazon CloudFront web distribution to serve different types of requests from multiple origins.

-- For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a Amazon CloudFront web distribution.


3 Amazon CloudFront security :

  - Use field level encryption in Amazon CloudFront to protect sensitive data for specific content

EXP :

-- Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack.

-- This encryption ensures that only applications that need the data—and have the credentials to decrypt it—are able to do so.

-- To use field-level encryption, when you configure your Amazon CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them.

-- You can encrypt up to 10 data fields in a request. (You can’t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.)








------------------------------------------------------ CloudFront – Origins




1 S3 bucket

  • For distributing files and caching them at the edge

  • Enhanced security with CloudFront Origin Access Control (OAC) 

  • OAC is replacing Origin Access Identity(OAI)

  • CloudFront can be used as an ingress (to upload files to S3)



2 Custom Origin (HTTP)

  • Application Load Balancer

  • EC2 instance

  • S3 website (must first enable the bucket as a static S3 website)

  • Any HTTP backend you want






------------------------------------------------------ CloudFront hands ON 




-- open S3 and create private Bucket 

-- as u create private bucket , no one can access ur url through the S3 

-- Only access through by Cloud-Front directly coz, it is privtae bucket and we did not enable Static hosting also 

-- go to CF in console 

-- create ditrubtion on CF 

-- Origina Domain = load balancer / S3 -- these are the places wher u can host ur applications 

-- OAC --> Create control settings --> do not change any n create 

-- it is created access from S3 

-- Compress objects automatically : CloudFront can automatically compress certain files that it receives from the origin before delivering them to the viewer. CloudFront compresses files only when the viewer supports it, as specified in the Accept-Encoding header in the viewer request.


-- Default root object - optional = index.html  -----> must and should u have to give this , otherwise u won’t get o/p 


----- once u create distrubtion , the S3 bucket policy wil gwt generated copy that policy and paste in bucket policy 

-- now ur appn is getting deployed all over the world 

-- through the CF url customers will able to connect ur webiste through the edge locations 

-- once u change the content of ur website and do uploud again n if u do refresh u won't get new content 

-- u have too do "invalidate the Cache" 

-- go to CF and create invalidation for /index.html , it will get latest file from the S3 and give latest content to customers 

-- By default, CloudFront caches files in edge locations for 24 hours. 







-----------------------------------  CloudFront vs S3 Cross Region Replication


CloudFront:

  - Global Edge network
  - Files are cached for a TTL (maybe a day)
  - Great for static content that must be available everywhere

S3 Cross Region Replication:

  - Must be setup for each region you want replication to happen
  - Files are updated in near real-time
  - Read only 
  - Great for dynamic content that needs to be available at low-latency in few regions






----------------------------------- CloudFront – ALB or EC2 as an origin




-- EC2 : EC2 Instances Must be Public

user <--> edge locatin <--> Allow Public IP of Edge Locations <--> EC2 Instances Must be Public

-- load balancer

user <--> edge locatin(it has public IP's) <--> Allow Public IP of Edge Locations <--> Application Load Balancer Must be Public <-->Allow Security Group of Load Balance<-->EC2 Instances
Can be Private .






------------------------------------------------------ CloudFront Geo Restriction



• You can restrict who can access your distribution

   • Allowlist: Allow your users to access your content only if they're in one of the countries on a list of approved countries.
   • Blocklist: Prevent your users from accessing your content if they're in one of the countries on a list of banned countries.

• The “country” is determined using a 3rd party Geo-IP database

• Use case: Copyright Laws to control access to content




------------------------------------------------------ CloudFront Access Logs



• Logs every request made to CloudFront into a logging S3 bucket



------------------------------------------------------ CloudFront Reports


• It’s possible to generate reports on:

        • Cache Statistics Report

        • Popular Objects Report

        • Top Referrers Report

        • Usage Reports

        • Viewers Report



• These reports are based on the data from the Access Logs.


-- you don't need to enable Access Logs to be sent into S3 to have these reports being generated.






------------------------------------------------------ CloudFront troubleshooting


• CloudFront caches HTTP 4xx and 5xx status codes returned by S3 ( or the origin server)

• 4xx error code indicates that user doesn’t have access to the underlying bucket (403) or the object user is requesting is not found (404)

• 5xx error codes indicates Gateway issues





------------------------------------------------------ CloudFront Caching



• Cache based on

        • Headers

        • Session Cookies

        • Query String Parameters


• The cache lives at each CloudFront Edge Location

• You want to maximize the cache hit rate to minimize requests on the origin

• Control the TTL (0 seconds to 1 year), can be set by the origin using the Cache- Control header, Expires header...

• You can invalidate part of the cache using the CreateInvalidation API






------------------------------------------------------ CloudFront Cache Behaviour for Headers


-- so when the client passes an HTTP request to your CloudFront distribution, it passes headers with it.



GET/image/cat.jpg HTTP/1.1
Host: pics.mywebsite.com
User-Agent: Mozilla/5.0 (Mac OS X 10_15_2....) Date: Tue, 28 Jan 2020 17:01:57 GMT Authorization: SAPISIDHASH fdd00ecee39fe.... Keep-Alive: 300
Accept-Ranges: bytes



-- So you can configure CloudFront in three ways,


1 Forward all headers to your origin 

        • => no caching, every request to origin ,so effectively you're not using CloudFront for caching if you forward all headers.

        • =>TTL must be set to 0 , because you're not doing any caching in CloudFront.


2 Forward a whitelist of headers

        • caching based on values in all the specified headers

        - If you forward a whitelist of headers, that means you only forward some headers as part of this request, then the caching is going to be based on all the values in the specified headers.




3 (None) == Forward only the default headers

        • no caching based on request headers

        • Best caching performance








------------------------------------------------------ CloudFront Origin Headers vs Cache Behavior



• Origin Custom Headers:

        • Origin-level setting

        • Set a constant header / header value for all requests to origin


• Behavior setting:

        • Cache-related settings

        • Contains the whitelist of headers to forward





------------------------------------------------------ CloudFront Caching TTL


• “Cache-Control: max-age” is preferred to “Expires” header

• If the origin always sends back the header Cache-Control , then you can set the TTL to be controlled only by that header

• In case you want to set min/max boundaries, you choose “customize” for the Object Caching setting

• In case the Cache-Control header is missing, it will default to “default value”





------------------------------------------------------ CloudFront Cache Behaviour for Cookies


Cookies is a specific request header




GET/image/cat.jpg HTTP/1.1
...
Cookie: username=John Doe; location=uk;
lang = eng; user_id = 12342 ...





1       Default: do not process the cookies 
                
                • Caching is not based on cookies
                • Cookies are not forwarded


2    Forward a whitelist of cookies

        • caching based on values in all the specified cookies


3    Forward all cookies

        • Worst caching performance






------------------------------------------------------ CloudFront Cache Behaviour for Query Strings



GET /image/cat.jpg?border=red&size=large HTTP/1.1



1  Default: do not process the query strings

        • Caching is not based on query strings

        • Parameters are not forwarded


2  Forward a whitelist of query strings

        • Caching based on the parameter whitelist

3  Forward all query strings

        • Caching based on all parameters








------------------------------------------------------ CloudFront – Increasing Cache Ratio


• Monitor the CloudWatch metric CacheHitRate

• Specify how long to cache your objects: Cache-Control max-age header

• Specify none or the minimally required headers

• Specify none or the minimally required cookies

• Specify none or the minimally required query string parameters

• Separate static and dynamic distributions (two origins)






------------------------------------------------------ CloudFront with ALB sticky sessions



-- how to use CloudFront with your ALB when you have enabled sticky sessions.

-- So, say you have an Application Load Balancer and a Target Group, and you've enabled sticky sessions so that you want the same request from the same user to go to the same backend EC2 instances.

-- So, you're set up CloudFront with an Edge Location and you want the two to work together.

        - So, the solution is

        • Must forward / whitelist the cookie that controls the session affinity to the origin to allow the session affinity to work

        • Set a TTL to a value lesser than when the authentication cookie expires









======================================================== Databases in AWS for SysOps ========================================================




• RDS stands for Relational Database Service

• It’s a managed DB service for DB use SQL as a query language.

• It allows you to create databases in the cloud that are managed by AWS

        • Postgres
        • MySQL
        • MariaDB
        • Oracle
        • Microsoft SQL Server
        • IBM DB2
        • Aurora (AWS Proprietary database)




------------------------------------------------------ Advantages over using RDS VS deploy database on ec2 

-- RDS is managed Service
-- Automated provisioning , Os patching 
-- continuously backups and restore to specific timestamp (Point in Time Restore)!
-- monitoring dashboards
-- read replicas for improving read performance 
-- Multi AZ setup for disaster recovery
-- storage backed by EBS(gp2 or io1)

-- BUT you can’t SSH into your instances






------------------------------------------------------ RDS - storage Auto Scaling 


-- helps u to increase storage on ur RDS DB instance dynamically

-- when RDS detects u are running out of free database storage , it scales automatically 

IMP : u have to set Maximum storage threshold 

-- automatically modify if storage is 

A : free storage is < 10% of allocated storage
B : low storage atleast before 5 min 
C : 6 hrs have passed since last modification 

-- Useful for applications with unpredictable workloads

-- Supports all RDS database engines







------------------------------------------------------ RDS Read Replicas for read scalability


• Up to 15 Read Replicas  Within AZ, Cross AZ or Cross Region 

• Replication is ASYNC, so reads are eventually consistent ,between the main RDS database instance and the two Read Replicas.

• Replicas can be promoted to their own DB

• Applications must update the connection string leverage ead replicas




------------------------------------------------------ RDS Read Replicas – Use Cases



• You have a production database that is taking on normal load

• You want to run a reporting application to run some analytics

• You create a Read Replica to run the new workload there

• The production application is unaffected

• Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE)





------------------------------------------------------ RDS Read Replicas – Network Cost



• In AWS there’s a network cost when data goes from one AZ to another

• For RDS Read Replicas within the same region, you don’t pay that fee






------------------------------------------------------ RDS Multi AZ (Disaster Recovery)


• SYNC replication

• One DNS name – automatic app failover to standby

• Increase availability

• Failover in case of loss of AZ, loss of network, instance or storage failure

• No manual intervention in apps

• Not used for scaling

• Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)




IMP NOTE : The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)

-- u need to pay Double amount when u have enabled Multi-AZ 

-- if something happens to ur main DB server , the failover happens here , the requests will send to backend server 

-- endpoint won't change even in failover time , One DNS name – automatic app failover to standby

-- DB operations will not have failover(for eg :some one delete table it wont get failover , tese are responsible by us )  , anything realted to network, servers etc will have fail over 

-- RDS DB instances can be reserved 

-- Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby

-- Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason







------------------------------------------------------ RDS – From Single-AZ to Multi-AZ


• Zero downtime operation (no need to stop the DB)

• Just click on “modify” for the database

• The following happens internally:

        • A snapshot is taken

        • A new DB is restored from the snapshot in a new AZ

        • Synchronization is established between the two databases




workflow = Master Db instance --> snapshot is taken --> restore from the snapshot and create new db in new AZ --> standby Db will created , both db instances are in synchronous Replication




------------------------------------------------------ RDS Multi AZ – Failover Conditions



• The primary DB instance


        • Failed
        • OS is undergoing software patches
        • Unreachable due to loss of network connectivity 
        • Modified (e.g., DB instance type changed)
        • Busy and unresponsive
        • Underlying storage failure








-------------------------------------------------------------- Amazon RDS maintenance 




-- Occasionally, AWS performs maintenance to the hardware, operating system (OS), or database engine version for a DB instance or cluster.

 1 Hardware maintenance :

  -- Before Amazon RDS schedules maintenance, you receive an email notification about the scheduled maintenance windows. This includes the time of the maintenance and the Availability Zones that are affected. 

  -- During hardware maintenance, Single-AZ deployments are unavailable for a few minutes. 

  -- For Multi-AZ deployments with an affected Availability Zone, your deployment is unavailable for the time it takes the instance to fail over, usually about 60 seconds. 

  -- If maintenance affects only the secondary Availability Zone, then there's no failover or downtime.


2 OS maintenance

  -- To postpone scheduled OS maintenance, adjust your preferred maintenance window. Or, you can choose Defer upgrade from the Actions dropdown menu in the Amazon RDS console. 

  -- To minimize downtime, modify the Amazon RDS DB instance to a Multi-AZ deployment. For Multi-AZ deployments, OS maintenance applies to the secondary instance. 

  -- The instance fails over, and then the primary instance updates. The downtime is during failover. 
  
  Note: If you upgrade to a Multi-AZ deployment, then you incur higher costs. To determine your costs, use the AWS Pricing Calculator.


3  DB engine maintenance

  -- When you upgrade your DB instance's database engine in a Multi-AZ deployment, maintenance occurs on the primary and replica instance at the same time. 

  -- This is also true for non-Amazon Aurora instances in Multi-AZ deployments. Throughout the maintenance window, both the primary and secondary DB instances in the Multi-AZ deployment are unavailable. 

  -- This operation causes downtime until the upgrade is complete. The duration of the downtime varies based on the size of your DB instance.

    NOTE : If the instance is a read replica, then the database engine version upgrade occurs independently from the source instance. By default, maintenance occurs first on the primary instance and then on the replica. 

  
  -- Upgrades to the database engine level require downtime. Even if your RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances upgrade at the same time.

  -- This causes downtime until the upgrade completes, and the duration of the downtime varies based on the size of your DB instance. For more information, 

    NOTE : If you upgrade a SQL Server DB instance in a Multi-AZ deployment, then both the primary and standby instances are upgraded.

          - Amazon RDS performs rolling upgrades, so the outage is only for the duration of a failover.








-------------------------------------------------------------- RDS PROXY 





-------- Lambda by default


        • By default, your Lambda function is launched outside your own VPC (in an AWS-owned VPC)

        • Therefore, it cannot access resources in your VPC (RDS, ElastiCache, internal ELB...)



-------- Lambda in VPC

        • You must define the VPC ID, the Subnets and the Security Groups

        • Lambda will create an ENI (Elastic Network Interface) in your subnets

        • AWSLambdaVPCAccessExecutionRole





-------- RDS Proxy for AWS Lambda


        • When using Lambda functions with RDS, it opens and maintains a database connection

        • This can result in a “TooManyConnections” exception

        • With RDS Proxy, you no longer need code that handles cleaning up idle connections and managing connection pools

        • Supports IAM authentication or DB authentication, auto-scaling

        • The Lambda function must have connectivity to the Proxy (public proxy => public Lambda, private proxy => Lambda in VPC)








-------------------------------------------------------------- DB Parameter Groups



• You can configure the DB engine using Parameter Groups

• Dynamic parameters are applied immediately

• Static parameters are applied after instance reboot

• You can modify parameter group associated with a DB (must reboot)

• See documentation for list of parameters for a DB technology


• Must-know parameter:

        • PostgreSQL / SQL Server: rds.force_ssl=1 => force SSL connections

        • MySQL / MariaDB: require_secure_transport=1 => force SSL connections





https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Concepts.General.SSL.html







-------------------------------------------------------------- RDS Backup vs. Snapshots



                Backups                                                                 Snapshots

• Backups are “continuous” and allow point in time recovery                     • Snapshots takes IO operations and can stop the database from seconds to minutes

• Backups happen during maintenance windows                                     • Snapshots taken on Multi AZ DB don’t impact the master – just the standby

• When you delete a DB instance, you can retain automated backups               • Snapshots are incremental after the first snapshot (which is full)

• Backups have a retention period you set between 0 and 35 days                 • You can copy & share DB Snapshots across accounts 

• To disable backups, set retention period to 0                                 • Manual Snapshots don’t expire

                                                                                • You can take a “final snapshot” when you delete your DB






-- Restoring from Automated Backups or DB Snapshots creates a new DB Instance







-------------------------------------------------------------- RDS Snapshots Sharing


-- u can't share backups 


• Manual snapshots: can be shared with AWS accounts

• Automated snapshots: can’t be shared, copy first

• You can only share unencrypted snapshots and snapshots encrypted with a customer managed key

• If you share an encrypted snapshots, you must also share any customer managed keys used to encrypt them






-------------------------------------------------------------- RDS Events & Event Subscriptions



• RDS keeps record of events related to:

        • DB instances

        • Snapshots

        • Parameter groups, security groups ...


• Example: DB state changed from pending to running


• RDS Event Subscriptions

        • Subscribe to events to be notified when an event occurs using SNS

        • Specify the Event Source (instances, SGs, ...) and the Event Category (creation, failover, ...)


• RDS delivers events to EventBridge







-------------------------------------------------------------- RDS Database Log Files




DB Instance ---> Store Logs: -----------> CW Logs ------> CW Alarm --------(Alert)------> Amazon SNS ---------(notification)-------> DB Admin

                - General
                - Audit
                - Error
                - Slow Query




-- see events and subscription in rds console 






-------------------------------------------------------------- RDS with CloudWatch


• CloudWatch metrics associated with RDS (gathered from the hypervisor):

        • DatabaseConnections

        • SwapUsage

        • ReadIOPS / WriteIOPS

        • ReadLatency / WriteLatency

        • ReadThroughPut / WriteThroughPut

        • DiskQueueDepth

        • FreeStorageSpace


• Enhanced Monitoring (gathered from an agent on the DB instance)

        • Useful when you need to see how different processes or threads use the CPU

        • Access to over 50 new CPU, memory, file system, and disk I/O metrics




--> rds database --> modify --> monitoring --> u will find enhnace monitoring option 








-------------------------------------------------------------- RDS Performance Insights



• Visualize your database performance and analyze any issues that affect it

• With the Performance Insights dashboard, you can visualize the database load and filter the load:


        • By Waits => find the resource that is the bottleneck (CPU, IO, lock, etc...) 

        • By SQL statements => find the SQL statement that is the problem

        • By Hosts => find the server that is using the most our DB

        • By Users => find the user that is using the most our DB



• DBLoad = the number of active sessions for the DB engine

• You can view the SQL queries that are putting load on your database



--  Performance Insights is not supported right now on db.t2 instance classes, 






-------------------------------------------------------------- Amazon Aurora



-- Aurora is a proprietary technology from AWS (not open sourced)

-- Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database)

-- Aurora is “AWS cloud optimized” and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS

-- Aurora storage automatically grows in increments of 10GB, up to 128 TB.

-- Aurora can have up to 15 replicas and the replication process is faster than MySQL (sub 10 ms replica lag)

-- Failover in Aurora is instantaneous (happens immediately, without any delay). It’s HA (High Availability) native.

-- Aurora costs more than RDS (20% more) – but is more efficient

-- There are no standby instances in Aurora. Aurora performs an automatic failover to a read replica when a problem is detected.


-- Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target

   - If the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails over to a new primary instance in one of two ways:
     
      1 By promoting an existing Aurora Replica to the new primary instance

      2 By creating a new primary instance






-------------------------------------------------------------- Aurora High Availability and Read Scaling



----- 6 copies of your data across 3 AZ:


- 4 copies out of 6 needed for writes
- 3 copies out of 6 need for reads
- Self healing with peer-to-peer replication
- Storage is striped across 100s of volumes

-- One Aurora Instance takes writes (master)

-- Automated failover for master in less than 30 seconds

-- Master + up to 15 Aurora Read Replicas serve reads

-- Support for Cross Region Replication







-- Consider this scenario - the primary instance of an Amazon Aurora cluster is unavailable because of an outage that has affected an entire AZ. The primary instance and all the reader instances are in the same AZ.

As a SysOps Administrator, what action will you take to get the database online?

ANS : You must manually create one or more new DB instances in another AZ

EXP :

        - Suppose that the primary instance in your cluster is unavailable because of an outage that affects an entire AZ. In this case, the way to bring a new primary instance online depends on whether your cluster uses a multi-AZ configuration. 

        - If the cluster contains any reader instances in other AZs, Aurora uses the failover mechanism to promote one of those reader instances to be the new primary instance.

        - If your provisioned cluster only contains a single DB instance, or if the primary instance and all reader instances are in the same AZ, you must manually create one or more new DB instances in another AZ.







--------------------------------------------------------------  difference b/w Aurora read replicas and MySQL Replicas 




1 Aurora read replicas :

Features 

- Number of replicas                         =  Up to 15
- Replication type                           =  Asynchronous (milliseconds)
- Performance impact on primary              =  Low
- Replica location                           =  In-region
- Act as failover target                     =  Yes (no data loss)
- Automated failover                         =  Yes
- Support for user-defined replication delay =  NO
- Support for different data or schema vs. primary = No 


- When you create a second, third, and so on DB instance in an Aurora-provisioned DB cluster, Aurora automatically sets up replication from the writer DB instance to all the other DB instances. 

- These other DB instances are read-only and are known as Aurora Replicas.

Aurora Replicas have two main purposes. 

    - You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. 
    
    - Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer.



2 MySQL Read Replicas


Features 

- Number of replicas                         =  Up to 5
- Replication type                           =  Asynchronous (seconds)
- Performance impact on primary              =  High
- Replica location                           =  cross-region
- Act as failover target                     =  Yes (potentially minutes of data loss)
- Automated failover                         =  No
- Support for user-defined replication delay =  NO
- Support for different data or schema vs. primary = Yes







-------------------------------------------------------------- Aurora DB Cluster




-- client has WEP and REP 

-- we have Writer endPoint (WEP), which is used  when ever the master fails ,still the client able to communicate and it is automatically redirected to right instance 

-- we have Reader endPoint (REP), it is difficult to connect which replica to the client , to avoid this confusion we have REP , it hs same features of WRP , it connects with load balancer and it connects automatically to all read replicas

NOTE : Load balancing happens at the connections level , not the statement level




-- u can't disable Aurora DB Cluster Backups 




-------------------------------------------------------------- Features of Aurora



- Automatic fail-over

- Backup and Recovery

- Isolation and security

- Industry compliance

- Push-button scaling

- Automated Patching with Zero Downtime

- Advanced Monitoring

- Routine Maintenance

- Backtrack: restore data at any point of time without using backups








-------------------------------------------------------------- Backups, Backtracking & Restores in Aurora




• Automatic Backups

        • Retention period 1-35 days (can’t be disabled)

        • PITR, restore your DB cluster within 5 minutes of the current time

        • Restore to a new DB cluster


• Aurora Backtracking

        • Rewind the DB cluster back and forth in time (up to 72 hours)

        • Doesn’t create a new DB cluster (in-place restore)

        • Supports Aurora MySQL only


• Aurora Database Cloning

        • Creates a new DB cluster that uses the same DB cluster volume as the original cluster

        • Uses copy-on-write protocol (use the original/single copy of the data and allocate storage only when changes made to the data)

        • Example: create a test environment using your production data







-------------------------------------------------------------- RDS & Aurora Security



• At-rest encryption:

        • Database master & replicas encryption using AWS KMS – must be defined as launch time

        • If the master is not encrypted, the read replicas cannot be encrypted

        • To encrypt an un-encrypted database, go through a DB snapshot & restore as encrypted


• In-flightencryption: TLS-ready by default,use the AWS TLS root certificates client-side

• IAM Authentication: IAM roles to connect to your database (instead of username/pw)

• Security Groups: Control Network access to your RDS / Aurora DB

• No SSH available except on RDS Custom

• Audit Logs can be enabled and sent to CloudWatch Logs for longer retention





-------------------------------------------------------------- Aurora for SysOps



• You can associate a priority tier (0-15) on each Read Replica

        • Controls the failover priority

        • RDS will promote the Read Replica with the highest priority (lowest tier)

        • If replicas have the same priority, RDS promotes the largest in size

        • If replicas have the same priority and size, RDS promotes arbitrary replica (randomly)


• You can migrate an RDS MySQL snapshot to Aurora MySQL Cluster






-------------------------------------------------------------- Aurora: CloudWatch metrics



• AuroraReplicaLag: amount of lag when replicating updates from the primary instance

• AuroraReplicaLagMaximum: max. amount of lag across all DB instances in the cluster

• AuroraReplicaLagMinimum: min. amount of lag across all DB instances in the cluster

• If replica lag is high, that means the users will have a different experience based on which replica they get the data from (due to eventual consistency)

• DatabaseConnections: current number of connections to a DB instance

• InsertLatency: average duration of insert operations






-------------------------------------------------------------- Amazon ElastiCache Overview




• The same way RDS is to get managed Relational Databases...

• ElastiCache is to get managed Redis or Memcached

• Caches are in-memory databases with really high performance, low latency

• Helps reduce load off of databases for read intensive workloads

• Helps make your application stateless

• AWS takes care of OS maintenance / patching, optimizations, setup, configuration, monitoring, failure recovery and backups

• Using ElastiCache involves heavy application code changes




-- it is developer resource mainly

-- it is regional 

-- it is also data base only 

-- Amazon ElastiCache can be used as a distributed in-memory cache for session management.

-- In-memory data base caching Service , high performance, low latency

-- Helps reduce load off of databases for read intensive workloads

-- Cache : all frquently accessed data is stored at this place 

-- it is used for Read Purpose 

-- it can handled Session data(cookies) 

-- it supports Encryption ( IN-TRANSIT and DATA at rest) 

-- Helps make your application stateless

-- Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Session stores can be set up using both Memcached or Redis for ElastiCache.






-------------------------------------------------------------- ElastiCache Solution Architecture - DB Cache




• Applications queries ElastiCache, if not available, get from RDS and store in ElastiCache.

• Helps relieve load in RDS

• Cache must have an invalidation strategy to make sure only the most current data is used in there.




-------------------------------------------------------------- ElastiCache Solution Architecture – User Session Store


• User logs into any of the application

• The application writes the session data into ElastiCache

• The user hits another instance of our application

• The instance retrieves the data and the user is already logged in





-------------------- it supports two Engines 



1 Redis : permannent

-- it hs 2 types of modes 

-- it is like NoSQL database 

1  Cluster Mode Enabled  

-- Cluster = Collection of Shards

-- Shard = Collection of Nodes/Server

-- Each Shard has 6 nodes = 1 Primary Node , 5 Replica Node

-- u can have 500 Shards per cluster 


2  Cluster Mode disabled

-- it has ony one shard = 1 primary and 5 replias 




- it supports HA, failover and backups

- Data is Persistent 

- Multi AZ with Auto-Failover

- Read Replicas to scale reads and have high availability

- Data Durability using AOF(Append Only File) persistence

- Backup and restore features

- Supports Sets and Sorted Sets

- Replication

- Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.

- In addition to strings, Redis supports lists, sets, sorted sets, hashes, bit arrays, and hyperlog logs.

- Applications can use these more advanced data structures to support a variety of use cases. 

- For example, you can use Redis Sorted Sets to easily implement a game leaderboard that keeps a list of players sorted by their rank.





2 MemcacheD --temporary 

- it not supports HA, failover and backups

- Data not Persistent 

- Multi-node for partitioning of data (sharding)

- No high availability (replication)

- No backup and restore

- Multi-threaded architecture

- sharding(pieces)

- Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached.






-------------------------------------------------------------- ElastiCache – Redis vs Memcached



        REDIS                                                                                    MEMCACHED

• Multi AZ with Auto-Failover                                                    • Multi-node for partitioning of data (sharding)

• Read Replicas to scale reads and have high availability                        • No high availability (replication)

• Data Durability using AOF persistence                                          • Non persistent

• Backup and restore features                                                    • No backup and restore

• Supports Sets and Sorted Sets                                                  • Multi-threaded architecture






------------------------------ Choosing between Redis and Memcached


-- Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine.

--  Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Understand your requirements and what each engine offers to decide which solution better meets your needs.


                                                               Memcached                         Redis

      
Sub-millisecond latency	                                         Yes                               Yes

Developer ease of use                                            Yes                               Yes      

Data partitioning	                                               Yes                               Yes   

Support for a broad set of programming languages	               Yes                               Yes 

Advanced data structures	                                        -                                Yes

Multithreaded architecture	                                     yes                                -

Snapshots	                                                        -                                Yes

Replication	                                                      -                                Yes

Transactions                                                      -                                Yes

Pub/Sub	                                                          -                                Yes

Lua scripting	                                                    -                                Yes

Geospatial support	                                              -                                Yes
       
   


EXP :

Sub-millisecond latency
  
  - Both Redis and Memcached support sub-millisecond response times. By storing data in-memory they can read data more quickly than disk based databases.

Developer ease of use

  - Both Redis and Memcached are syntactically easy to use and require a minimal amount of code to integrate into your application.

Data partitioning
  
   - Both Redis and Memcached allow you to distribute your data among multiple nodes. This allows you to scale out to better handle more data when demand grows.

Support for a broad set of programming languages
 
   - Both Redis and Memcached have many open-source clients available for developers. Supported languages include Java, Python, PHP, C, C++, C#, JavaScript, Node.js, Ruby, Go and many others.

Advanced data structures
 
   - In addition to strings, Redis supports lists, sets, sorted sets, hashes, bit arrays, and hyperloglogs. Applications can use these more advanced data structures to support a variety of use cases. For example, you can use Redis Sorted Sets to easily implement a game leaderboard that keeps a list of players sorted by their rank.

Multithreaded architecture

   - Since Memcached is multithreaded, it can make use of multiple processing cores. This means that you can handle more operations by scaling up compute capacity.

Snapshots

   - With Redis you can keep your data on disk with a point in time snapshot which can be used for archiving or recovery.

Replication
 
   - Redis lets you create multiple replicas of a Redis primary. This allows you to scale database reads and to have highly available clusters.

Transactions

   - Redis supports transactions which let you execute a group of commands as an isolated and atomic operation.

Pub/Sub

   - Redis supports Pub/Sub messaging with pattern matching which you can use for high performance chat rooms, real-time comment streams, social media feeds, and server intercommunication.

Lua scripting
 
   - Redis allows you to execute transactional Lua scripts. Scripts can help you boost performance and simplify your application.

Geospatial support

   - Redis has purpose-built commands for working with real-time geospatial data at scale. You can perform operations like finding the distance between two elements (for example people or places) and finding all elements within a given distance of a point.









-------------------------------------------------------------- ElastiCache Replication: Cluster Mode Disabled



• One primary node, up to 5 replicas

• Asynchronous Replication

• The primary node is used for read/write

• The other nodes are read-only

• One shard, all nodes have all the data

• Guard against data loss if node failure

• Multi-AZ enabled by default for failover

• Helpful to scale read performance





-------------------------------------------------------------- Redis Scaling – Cluster Mode Disabled



• Horizontal:

        • Scale out/in by adding/removing read replicas (max. 5 replicas)


• Vertical:

        • Scale up/down to larger/smaller node type

        • ElastiCache will internally create a new node group, then data replication and DNS update
 




 -------------------------------------------------------------- ElastiCache Replication: Cluster Mode Enabled



 • Data is partitioned across shards (helpful to scale writes)

 • Each shard has a primary and up to 5 replica nodes (same concept as before)

 • Multi-AZ capability

 • Up to 500 nodes per cluster:

        • 500 shards with single master

        • 250 shards with 1 master and 1 replica

        • 83 shards with one master and 5 replicas





-------------------------------------------------------------- ElastiCache for Redis – Auto Scaling



• Automatically increase/decrease the desired shards or replicas

• Supports both Target Tracking and Scheduled Scaling Policies

• Works only for Redis with Cluster Mode Enabled






-------------------------------------------------------------- ElastiCache – Redis Connection Endpoints


• Standalone Node

        • One endpoint for read and write operations


• Cluster Mode Disabled Cluster

        • Primary Endpoint – for all write operations

        • Reader Endpoint – evenly split read operations across all read replicas

        • Node Endpoint – for read operations


• Cluster Mode Enabled Cluster

        • Configuration Endpoint – for all read/write operations that support Cluster Mode Enabled commands

        • Node Endpoint – for read operations






-------------------------------------------------------------- ElastiCache – Redis for SysOps



------------------- Redis Scaling – Cluster Mode Enabled





• Horizontal:

        • Scale out/in by adding/removing read replicas (max. 5 replicas)


• Vertical:

        • Scale up/down to larger/smaller node type

        • ElastiCache will internally create a new node group, then data replication and DNS update
 





------------------- Redis Scaling – Cluster Mode Enabled (complicated)



• Two Modes:

        • Online Scaling: continue serving requests during the scaling process (no downtime, some degradation in performance)

        • Offline Scaling: unable to serve requests during the scaling process (backup and restore).Additional configurations supported (change node type,upgrade engine version, ...)


• Horizontal: (Resharding and Shard Rebalancing)

        • Resharding: scale out/in by adding/removing shards

        • Shard Rebalancing: equally distribute the keyspaces among the shards as possible

        • Supports Online and Offline Scaling


• Vertical: (change read/write capacity)

        • Scale up/down to larger/smaller node type

        • Supports Online Scaling







-------------------------------------------------------------- Redis Metrics to Monitor



• Evictions: the number of non-expired items the cache evicted to allow space for new writes (memory is overfilled). 

Solution:

        • Choose an eviction policy to evict expired items (e.g., evict least recently used (LRU) items)

        • Scale up to larger node type (more memory) or scale out by adding more nodes


• CPUUtilization: monitor CPU utilization for the entire host

        • Solution: scale up to larger node type (more memory) or scale out by adding more nodes


• SwapUsage: should not exceed 50 MB

        • Solution: verify that you have configured enough reserved memory



• CurrentConnections: the number of concurrent and active connections

        • Solution: investigate application behavior to address the issue


• DatabaseMemoryUsagePercentage: the percentage of memory utilization

• NetworkBytesIn/Out & NetworkPacketsIn/Out

• ReplicationBytes: the volume of data being replicated

• ReplicationLag: how far behind the replica is from the primary node







-------------------------------------------------------------- ElastiCache – MemcacheD for SysOps



--------------- Memcached Scaling


• Memcached Clusters can have 1-40 nodes (soft limit)

• Horizontal:

        • Add/remove nodes from the cluster

        • Auto-discovery allow your app to find nodes


• Vertical:

        • Scale up/down to larger/smaller node type

        • Scale up process:

                • Create a new cluster with the new node type

                • Update your application to use the new cluster’s endpoints

                • Delete the old cluster

        • Memcached clusters/nodes start out empty        





--------------- Memcached Auto Discovery


• Typically you need to manually connect to individual cache nodes in the cluster using its DNS endpoints

• Auto Discovery automatically identifies all of the nodes

• All the cache nodes in the cluster maintain a list of metadata about all other nodes

• This is seamless from a client perspective




--------------- Memcached Metrics to Monitor



• Evictions: the number of non-expired items the cache evicted to allow space for new writes (memory is overfilled). Solution:

        • Choose an eviction policy to evict expired items (e.g., LRU items)

        • Scale up to larger node type (more memory) or scale out by adding more nodes


• CPUUtilization

        • Solution: scale up to larger node type or scale out by adding more nodes


• SwapUsage: should not exceed 50 MB

• CurrConnections: the number of concurrent and active connections

        • Solution: investigate application behavior to address the issue

• FreeableMemory: amount of free memory on the host        








=============================================================== AWS Monitoring, Audit & Performance ===============================================================






-------------------------------------------------------------- AWS CloudWatch Metrics




• CloudWatch provides metrics for every services in AWS

• Metric is a variable to monitor (CPUUtilization, NetworkIn...)

• Metrics belong to namespaces

• Dimension is an attribute of a metric (instance id, environment, etc...).

• Up to 30 dimensions per metric

• Metrics have timestamps

• Can create CloudWatch dashboards of metrics





-- CloudWatch is all about Alarms,Events and Logs

-- it is regional only 

-- CW is used to monitor performance of all as resource 

-- to monitor the resource , CW need Host level metrics also known as default metrics 

     1 CPU 

     2 Network 

     3 Disk --- volume 

     4 Status Check 

-- memory not comes under HLM 

-- memory is custom metrics 

-- 2 types of moitoring 

     1 Basic and 
     2 Detailed Moitoiring 

-- Basic is free and it will take every 5 min data

-- deatiled monitoiring : every 1 min data points , billable


-- u can create alaram in CW 

-- alarms can do actions like (terminate , reboot ,stop , recover ) 

-- Alarm has 3 States : 

     1 In Alarm : > 90 

     2 OK : < 90

     3 Insufficient : ec2 stopped due to some reasons



-- we also have concept called " Composite Alarms"

- CW alarms are single metric

- Composite Alarms are monitoring the states of multiple alarms 

 eg: AND or OR conditions 




IMP : In CloudWatch Logs, Log Retention Policy defined at ........................... level.


ANS : Log Groups 




A systems administration team is configuring Amazon EC2 metrics that are sent to Amazon CloudWatch for monitoring purposes. The team is looking for a metric that can help them identify the processing power required to run an application on the selected instance.

Which of the below metric should be used for this requirement?


ANS : CPUUtilization metric should be used to identify the processing power required 

EXP : 

        - CPUUtilization specifies the percentage of allocated EC2 compute units that are currently in use on the instance. 

        - This metric identifies the processing power required to run an application on a selected instance. This metric is expressed in Percent.

        - Depending on the instance type, tools in your operating system can show a lower percentage than CloudWatch when the instance is not allocated a full processor core.










---------------------------------------- some  terminologies to understand for the Cloudwatch 


1  NameSpaces : Containers for monitoring data , it is a way to keep things seperate 

      -- NameSpace has got a name , it can be anythng as long as it stays within the rule set 

      -- All aws data goes to aws namespace ---> AWS/Service

      -- Namespace contains related metric 


2  Metric : it is a collection of related Data Points , in the time ordered structures 

      Eg: cpu usage network IN/OUT 


3  Data Point : let us say we have metric called CPU Utilization , everytime any server measures its utilization and send it into cloudwatch that goes into the CPU utilization metrics and each one of those measures so everytime the server reports the cpu that measure is called "Data Point" 


     --it has 2 components 

     1 timestamp 

     2 value 


Note : CPU utilization metric could contain data from many servers ,so how do we seperate data for this? so use " Dimesions"


4  Dimensions : these are Name Value Pairs that seperate data point for different things or perspective withn the same metric 


      -- while sending data points to cloud watch ,AWS also send in , these two 

      A  Name = InstanceID , Value =I-xxxx

      B  name = InstanceType , value =t2.micro.......


5  ALARM : CW also allows to take actions based on metrics which is done using Alarms 

      -- two states 

      A  OK --> Everything is working fine 

      B  ALARM --> Something bad has happened 

      C insufficient Data ---> instance has stopped or not exists




REF : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html










---------------------------------------- EC2 Detailed monitoring 


• EC2 instance metrics have metrics “every 5 minutes”

• With detailed monitoring (for a cost), you get data “every 1 minute”

• Use detailed monitoring if you want to scale faster for your ASG! , it gives you some benefits for your ASG, if you want to scale out and in faster.

• The AWS Free Tier allows us to have 10 detailed monitoring metrics


• Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)








---------------------------------------- CloudWatch Custom Metrics


• Possibility to define and send your own custom metrics to CloudWatch

• Example: memory (RAM) usage, disk space, number of logged in users ...

• Use API call "PutMetricData"

• Ability to use dimensions (attributes) to segment metrics

     • Instance.id
     • Environment.name

IMP • Metric resolution (StorageResolution API parameter – two possible value):

      • Standard: 1 minute (60 seconds)
      • High Resolution: 1/5/10/30 second(s) – Higher cost


• Important: Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly)

     - Something good to know is that with custom metrics, when you push a metric in the past or in the future, this works as well.

     - So this is a very important exam point.

     - So if you are pushing a metric up to two weeks in the past or two hours in the future, you're not going to get an error from CloudWatch,

     - This is going to accept your metric as is. And so that means you need to make sure that your EC2 instance time is currently configured. If you want the metrics to be synchronized with the actual time from AWS.

    - See Timestamp in documentation


-- for more custom metrics examples 

REF : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html





----------------------------------------  LAB for custom metrics (Memory)


Expore topic : till now we only see Host level metrics , now find how to get metrics for memory 

- By default, we cannot monitor Memory metrics on EC2 Instances.

- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-scripts-intro.html

- use this link for the documentation 

or u can do in another process

- create ec2 instance 

- crate one role and attach policies 

-  attach policies cloudwatchfullacces and AmazonSSMFullAccess

-  why SSM? 

ANs: i have to store particular value(json document which is used to fetch the memory utilixation from aws ec2 and send it to AWS cloudwatch ) in ssm

-  create a parameter in the system manager(system manager --> application --> parameter) with the name (u can give any name) 

eg :  /alarm/AWS-CWAgentLinConfig 

- go system manager -->paramter store --> give name --> in the value place copy json script 

{
	"metrics": {
		"append_dimensions": {
			"InstanceId": "${aws:InstanceId}"
		},
		"metrics_collected": {
			"mem": {
				"measurement": [
					"mem_used_percent"
				],
				"metrics_collection_interval": 60
			}
		}
	}
}


-- ceate ec2 and attach role to ec2 and create with userdata with cloudwatch agent to install

userdata: 

#!/bin/bash
wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip
unzip AmazonCloudWatchAgent.zip
sudo ./install.sh
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:/alarm/AWS-CWAgentLinConfig -s


-- check whether clouwtch agent is instaled or not by using 

-- sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status

-- if it is running it is success , after one minute

-- now go to CLoudwatch and --> all metrics --> CW Agent --> give instacne id and chck logs of memory








---------------------------------------- CloudWatch Dashboards



• Great way to setup custom dashboards for quick access to key metrics and alarms

• Dashboards are global

• Dashboards can include graphs from different AWS accounts and regions

• You can change the time zone & time range of the dashboards

• You can setup automatic refresh (10s, 1m, 2m, 5m, 15m)

• Dashboards can be shared with people who don’t have an AWS account (public, email address, 3rd party SSO provider through Amazon Cognito)

• Pricing:

        • 3 dashboards (up to 50 metrics) for free 

        • $3/dashboard/month afterwards





---------------------------------------- CloudWatch Dashboards hands on 



-- go to CW --> dashboard --> create dashboard --> 

- you can add many different things. You can add a line, a stacked area, a number, some texts, some logs table, alarm status, and so on

- u can selelct any metric as u want to push your custom Dashboard 

- metrics can come from different regions in the same CloudWatch dashboard, which is quite handy.

- once you done , you can save and you can also share the Dashboards





  
---------------------------------------- CloudWatch Logs


-- So CloudWatch Logs is the perfect place to store your application logs in AWS.

-- And to do so, you must first define log groups.

• Log groups: arbitrary name, usually representing an application

• Log stream: instances within application / log files / containers

• Can define log expiration policies (never expire, 1 day to 10 years...)

• CloudWatch Logs can send logs to:

     • Amazon S3 (exports)
     • Kinesis Data Streams
     • Kinesis Data Firehose
     • AWS Lambda
     • OpenSearch

• Logs are encrypted by default

• you Can setup KMS-based encryption with your own keys






---------------------------------- what types of logs can go into CloudWatch Logs? (CloudWatch Logs - Sources)


• we can send the logs using the SDK, CloudWatch Logs Agent, CloudWatch Unified Agent

• Elastic Beanstalk: which is used to collect logs from the application directly into CloudWatch.

• ECS: ECS will send the logs directly from the containers into CloudWatch.

• AWS Lambda: collection from function logs , will send logs from the functions themselves.

• VPC Flow Logs: will send logs specific to your VPC metadata network traffic.

• API Gateway : will send all the requests made to the API Gateway into CloudWatch Logs.

• CloudTrail : you can send the logs directly based on the filter.

• Route53: will log all the DNS queries made to its service.






---------------------------------------- what if you wanted to query the logs in CloudWatch Logs? (CloudWatch Logs Insights)


-- So, it's a querying capability within CloudWatch Logs which allows you to write your query.

-- You specify the timeframe you want to apply your query to and then automatically you're going to get a result as a visualization.

-- This visualization can also be exported either as a result or added to a dashboard for being able to return it whenever you want.

• Search and analyze log data stored in CloudWatch Logs

• Example: find a specific IP inside a log, count occurrences of “ERROR” in your logs...

      - So there are lots of simple queries provided as part of the console for CloudWatch Logs Insights. 

      - For example, you can find the most 25 most recent events, or you can have a look at how many events had exceptions or errors in your logs, or you can look for a specific IP and so on ..
 

• Provides a purpose-built query language

      • Automatically discovers fields from AWS services and JSON log events

      • Fetch desired event fields, filter based on conditions, calculate aggregate statistics, sort events, limit number of events...

      • Can save queries and add them to CloudWatch Dashboards


• Can query multiple Log Groups in different AWS accounts

• It’s a query engine, not a real-time engine








---------------------------------------- CloudWatch Logs – S3 Export


-- CloudWatch Logs can be exported into several destinations.


1 CloudWatch Logs – S3 Export

     • Log data can take up to 12 hours to become available for export

        - So this is for a batch export to send all your logs into Amazon S3, and this export can take up to 12 hours to be completed.

     • The API call to initiate this export is called CreateExportTask.

     • So because this is a batch export, this is Not near-real time or real-time... use Logs Subscriptions instead







---------------------------------------- CloudWatch Logs Subscriptions


• Get a real-time log events from CloudWatch Logs for processing and analysis

• Send to Kinesis Data Streams, Kinesis Data Firehose, or Lambda

• Subscription Filter – filter which logs are events delivered to your destination

      - So, the subscription filter can send data into Kinesis Data Streams. This would be a great choice if you wanted to use, for example, the integration with Kinesis Data Firehose, Kinesis Data Analytics, or Amazon EC2, or Lambda.

      - You can also directly send it into Kinesis Data Firehose. From there, you can send it in near real-time fashion into Amazon S3 or the OpenSearch Service,

      - So you can write your own custom Lambda function, or you can use a managed Lambda function that is sending data in real-time into the OpenSearch Service.






---------------------------------------- CloudWatch Logs Aggregation Multi-Account & Multi Region


-- thanks to these subscription filters, it is possible for you to aggregate data from different CloudWatch Logs into different accounts and different regions into a common destination such as the Kinesis Data Stream in one specific accounts. And then Kinesis Data Firehose. And then in near real-time into Amazon S3.

-- So that is very possible, and that is a way for you to perform log aggregation.



---------------------------------------- Cross-Account Subscription


• Cross-Account Subscription – send log events to resources in a different AWS account (KDS, KDF)

-- So, let's say you have a sender account and the recipient accounts.

-- So you create a CloudWatch Log subscription filter in sender account , and then this gets sent into a subscription destination, which is a virtual representant of the Kinesis Data Stream in the recipient accounts.

-- Then you attach a destination access policy to allow the first account to actually send data into this destination.

-- Then you create an IAM role in the recipient account, which has the permission to send records into your Kinesis Data Stream, and you make sure that this role can be assumed by the first account.

-- when you have all these things in place, then it is possible for you to send data from CloudWatch Logs in one account into a destination in another account.





----------------------------------------  Live Tail


-- So first let's create a log group

-- go to log group --> create log stream called demostream 

-- go to demostream  , we have option called tailing  , do tailing 

-- select log group as u want and select Select log streams , apply filter 

-- So that means that as events are being posted into Cloudwatch logs, they are going to appear here in our Live Tail,

-- now go to demostream  --> actions --> create log event (Hello world)

-- now check in live tail , it will be there 

-- So this is quite a nice way because if you have log streaming very fast, it can all appear here. And then from this we can get more information around when this happened, the group and so on.

-- So it's just a very cool, very easy feature to debug your CloudWatch logs. And from a pricing perspective,

-- you only get a few hours a day, so maybe one hour a day of free usage of Live Tail. So please make sure to cancel and close your lifetail session so that you don't have any cost, but you have one hour of free every day.







---------------------------------------- CloudWatch Alarms


• Alarms are used to trigger notifications for any metric

• Various options (sampling, %, max, min, etc...)

• Alarm States:

    • OK :  means that it's not triggered.

    • INSUFFICIENT_DATA  : means that there's not enough data for the alarm to determine a state.

    • ALARM : which is that your threshold has been breached and therefore a notification will be sent.


• Period:

    • Length of time in seconds to evaluate the metric

    • High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec




Question : A CloudWatch Alarm set on a High-Resolution Custom Metric can be triggered as often as ......................

ANS : 10 sec

EXP : If you set an alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds.







---------------------------------------- CloudWatch Alarm Targets 


-- alarms have three main targets.


1 • Stop,Terminate, Reboot, or Recover an EC2 Instance

2 • Trigger Auto Scaling Action

3 • Send notification to SNS (from which you can do pretty much anything)






---------------------------------------- CloudWatch Alarms – Composite Alarms


• CloudWatch Alarms are on a single metric

-- but then if you wanted to have multiple metrics, use Composite Alarms

• Composite Alarms are monitoring the states of multiple other alarms

-- So the composite alarm is the action of combining all these other alarms together.

• you can use AND and OR conditions to be able to be very flexible in terms of the condition you're checking for.

• Helpful to reduce “alarm noise” by creating complex composite alarms

   - it's very helpful to reduce alarm noise because you can create complex composite alarms and saying, 
   
   - for example, if the CPU is high and the network is high, then don't alert me because I only wanna know when the CPU is high and the network is low, this kind of things.






---------------------------------------- EC2 Instance Recovery


• Status Check:

     • Instance status = check the EC2 VM

     • System status = check the underlying hardware


• Recovery: Same Private, Public, Elastic IP, metadata, placement group







---------------------------------------- CloudWatch Alarm: good to know


• Alarms can be created based on CloudWatch Logs Metrics Filters

• To test alarms and notifications, set the alarm state to Alarm using CLI

    aws cloudwatch set-alarm-state --alarm-name "myalarm" --state-value ALARM --state-reason "testing purposes"







-- A junior developer working on configuring CloudWatch alarms is unable to figure out why a particular CloudWatch Alarm is constantly in the ALARM state.

As a SysOps Administrator, which of these options would you suggest as a fix for the issue?


ANS : Alarms continue to evaluate metrics against the configured threshold, even after they have already triggered. You can adjust the alarm threshold if you do not want it to be in ALARM state

EXP : 

        - Alarms continue to evaluate metrics against your chosen threshold, even after they have already triggered. This allows you to view its current up-to-date state at any time.

        - You may notice that one of your alarms stays in the ALARM state for a long time. 

        - If your metric value is still in breach of your threshold, the alarm will remain in the ALARM state until it no longer breaches the threshold. 

        - This is normal behavior. If you want your alarm to treat this new level as OK, you can adjust the alarm threshold accordingly.






---------------------------------------- CloudWatch Alarms Hands On


-- create once ec2 instance , it will take 5 minutes to get some metrics 

-- go to cw and go to all alaram --> select metric -->choose ec2 --> paste instance is --> choose CPUUtilization -->  CPUUtilization > 90 --> Additional configuration Datapoints to alarm = 3 of 3 --> choose ec2 action as u want --> create alaram

-- So now this alarm obviously does have insufficient data, so we need to wait 15 minutes for it 

-- we could go into the EC2 instance and launch a way to get the CPU very high for 15 minutes, but this would be a very, very long, 

-- or we can use the API call name, Set alarm state, to really see what would happen if this alarm went into the breach phase.

-- open cloushell , go to goole and search for cloudwatch alarm state , enter this below cmd as ur names 

   -  aws cloudwatch set-alarm-state --alarm-name <value> --state-value <value> --state-reason <value>

   eg : aws cloudwatch set-alarm-state --alarm-name terminateec2 --state-value ALARM --state-reason "Testing"


-- paste this in cli , it will turn to in alarm state , then as per rule it will going to terminate the ec2 instance 







---------------------------------------- CloudWatch Synthetics Canary


• Configurable script that monitor your APIs, URLs, Websites, ...

• Reproduce what your customers do programmatically to find issues before customers are impacted

• Checks the availability and latency of your endpoints and can store load time data and screenshots of the UI

• Integration with CloudWatch Alarms

• Scripts written in Node.js or Python

• Programmatic access to a headless Google Chrome browser

• Can run once or on a regular schedule








---------------------------------------- CloudWatch Synthetics Canary Blueprints


• Heartbeat Monitor – load URL, store screenshot and an HTTP archive file

• API Canary – test basic read and write functions of REST APIs

• Broken Link Checker – check all links inside the URL that you are testing

• Visual Monitoring – compare a screenshot taken during a canary run with a baseline screenshot

• Canary Recorder – used with CloudWatch Synthetics Recorder (record your actions on a website and automatically generates a script for that)

• GUI Workflow Builder – verifies that actions can be taken on your webpage (e.g., test a webpage with a login form)





---------------------------------------- CloudWatch Synthetics Canary Hands On


-- cloudwatch ---> application signals or Application logs ----> Synthetics Canaries 

-- create canary with any application link (amazon,google.....) --> use hearbeat canary --> remaing all are default --> create canary and wait for 2-3 minutes 

-- this will gives the info about the application , it is working fine or not 








----------------------------------------------------- Amazon EventBridge (formerly CloudWatch Events)





-- Amazon EventBridge is a serverless service that uses events to connect application components together, making it easier for developers to build scalable event-driven applications.

-- Event Bridge is an Event Bus that helps in integrating different AWS Services , and custom applications and SAAS Applications 

-- Earlier we have Cloud watch events , the only negative point is that , it did not support SAAS applications and custom applications outside from the AWS 

-- EventBridge was build for the same purpose , u can create effortless event-driven architectures

Eg 1 : u want to receive SNS notification every time production EC2 instance are terminated , this will be done by EventBridge

Eg 2 : automated deployment using code pipelines at 11PM everyday

Eg 3 : if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


-- these all things will done by the event Bridge and u can schedule the events also 

-- It is fully managed service 

-- pay what u use model 


Different Parts in EventBridge:


Working flow -----


Event Producer --> Event --> Amazon EventBridge event Bus --> Rule ---> AWS Lambda , AWS Kinesis Data Firehose, Amazon Simple Notification Service


--  Event producer : AWS Service / Custom applications / Third Party SaaS providers,

--  From the producers the event will generated , this will transform to the bus and it move to rules , in rules there are multiple rules and this rules will send to multiple targets 

-- A single rule may have multiple targets and max target is 5 ,all targets will process in the event in parallel


Important terms in EventBridge :

1 Event  : An Event indicated a change in an environment 

2 Rule   : A rule matches incoming events and routes them to targets for processing  

3 Target : target application of ur rule. A target process events 

Targets can include ec2 , lambda functions , kinesis streams , Ecs tasks,  , the target receive events in JSON format 

4 Event Bus : An event bus receives events. When u create a rule , u associate it with a specific event bus and rules is matched only to events received by that event bus 

- rule can not be create standAlone , it should have it's parent as EventBus

Components of EventBridge :

1 EventBridge Rule : A rule matches incoming events and sends them to targets for processing.

2 EventBridge Pipes : A pipe connects an event source to a target with optional filtering and enrichment.

3 EventBridge Schedule : A schedule invokes a target one-time or at regular intervals defined by a cron or rate expression.

4 EventBridge Schema registry : Schema registries collect and organize schemas.


EventBus : Usecases

if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


In the above example , the working flow will be like 


User deletes his subscription --> custom event is trigged --> rights custom event would be pass into the EventBus --> rule matching --> then it reaches to the targets , once the rule is matched  here then it sends to targets --> 1 the targets are Feedback service 2 Schedule call service 3 offers service 





stepehe note

-- formerly known as CloudWatch Events

• Schedule: Cron jobs (scheduled scripts)

    - Schedule Every hour ----------> Lambda (Trigger script on Lambda function)


-- but not just a schedule like every hour, it can also react to an event pattern. 

• Event Pattern: Event rules to react to a service doing something

    - So there are event rules that can react to a service doing something. For example, you can react to the event of IAM root user sign in in the console. 
    
    - So when that happens, maybe you want to send a message into an SNS topic and receive an email notification, so that if anyone is using the root account, then you will receive an email,

    - which may be a good security feature for your accounts.


          IAM Root User Sign in Event (EVENT)-------------> SNS Topic with Email Notification



• you have different destinations like Trigger Lambda functions, send SQS/SNS messages...


• Event buses can be accessed by other AWS accounts using Resource-based Policies

• You can archive events (all/filter) sent to an event bus (indefinitely or set period)

• Ability to replay archived events ,which is super handy for debugging, super handy for troubleshooting, and for fixing production as well.







----------------------------------------------------- Amazon EventBridge – Schema Registry


-- EventBridge receives a lot of events from different places and so therefore, you need to understand what the events are going to look like and remember, these events are in this Json  format we just saw.

• EventBridge can analyze the events in your bus and infer the schema

• The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus

• Schema can be versioned





----------------------------------------------------- Amazon EventBridge – Resource-based Policy


• Manage permissions for a specific Event Bus

• Example: allow/deny events from another AWS account or AWS region

• Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region







-- A developer has created rules for different events on Amazon EventBridge with AWS Lambda function as a target. The developer has also created an IAM Role with the necessary permissions and associated it with the rule. The rule however is failing, and on initial analysis, it is clear that the IAM Role associated with the rule is not being used when calling the Lambda function.

What could have gone wrong with the configuration and how can you fix the issue?


ANS : For Lambda functions configured as a target to EventBridge, you need to provide resource-based policy. IAM Roles will not work 

EXP : 

        - IAM roles for rules are only used for events related to Kinesis Streams. For Lambda functions and Amazon SNS topics, you need to provide resource-based permissions.

        - When a rule is triggered in EventBridge, all the targets associated with the rule are invoked. Invocation means invoking the AWS Lambda functions, publishing to the Amazon SNS topics, and relaying the event to the Kinesis streams.

        - In order to be able to make API calls against the resources you own, EventBridge needs the appropriate permissions. 

        - For Lambda, Amazon SNS, Amazon SQS, and Amazon CloudWatch Logs resources, EventBridge relies on resource-based policies. For Kinesis streams, EventBridge relies on IAM roles.










----------------------------------------------------- Amazon EventBridge  hands ON 




-- open CW --> Events (leftside)--> create new event-bus --> enabled Archives and Schema discovery

        - So under Event buses, we have access here to the default event bus. And this event bus is the one that is created by default in your account. And you can already start defining rules on it.


-- go to rules --> create rule --> default (event bus) --> Rule with an event pattern --> Event source = AWS events or EventBridge partner events --> sample event = ec2 instance change notification --> Event pattern = AWS service = ec2 --> ec2 instance change notification --> specific state --> sns --> create 



{
  "version": "0",
  "id": "651d5f8b-947c-4c0f-acb7-5ac4e41a1b8a",
  "detail-type": "EC2 Instance State-change Notification",
  "source": "aws.ec2",
  "account": "123456789012",
  "time": "2015-11-11T21:33:19Z",
  "region": "us-east-1",
  "resources": ["arn:aws:ec2:us-east-1:123456789012:instance/i-abcd3333"],
  "detail": {
    "instance-id": "i-abcd3333",
    "state": "stopped"
  }
}



-- So now you could try it out and start to launch an EC2 instance and then terminate it or stop it and make sure you are subscribed into your SNS topic with your email, and then you would receive an email whenever, well, an instance is stopped or terminated,

-- create ec2 instance --> and stopped it u will receive an notification 





----------------------------------------------------- Amazon EventBridge Content Filtering hands ON  




-- filtering events can be very helpful in EventBridge because you may want to only react to specific kind of events and not everything that is provided within a specific rule.

-- filter types:


        - Prefix matching

        - Prefix matching while ignoring case

        - Suffix matching .. etc 

        https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-pattern-operators.html



-- go to rules --> create rule --> default (event bus) --> Rule with an event pattern --> Sample event = s3 object created --> aws service = s3 --> event type = all events 



{
  "version": "0",
  "id": "17793124-05d4-b198-2fde-7ededc63b103",
  "detail-type": "Object Created",
  "source": "aws.s3",
  "account": "123456789012",
  "time": "2021-11-12T00:00:00Z",
  "region": "ca-central-1",
  "resources": ["arn:aws:s3:::example-bucket"],
  "detail": {
    "version": "0",
    "bucket": {
      "name": "example-bucket"
    },
    "object": {
      "key": "example-key",
      "size": 5,
      "etag": "b1946ac92492d2347c6235b4d2611184",
      "version-id": "IYV3p45BT0ac8hjHg1houSdS1a.Mro8e",
      "sequencer": "00617F08299329D189"
    },
    "request-id": "N4N7GDK58NMKJ12R",
    "requester": "123456789012",
    "source-ip-address": "1.2.3.4",
    "reason": "PutObject"
  }
}




-- in event type u did not find the exact option to select so --> edit pattern --> from the above sample code you can paste what you want exactly


eg: 


{
  "source": ["aws.s3"],
  "detail-type": "Object Created"
}

        - this is for object created in s3 



eg 2 : when you want to filter only on specific bucket 


{
  "source": ["aws.s3"],
  "detail-type": "Object Created",
   "detail": {
    "version": "0",
    "bucket": {
      "name": "example-bucket"
    }
   }
}



eg 3 


-- But let's say for example, you wanted to match for a prefix of buckets, not just my bucket, but anything that starts with mybuckets.

-- select prefix matching and click on insert , you can also find this from abovve link documentation 



{
  "source": ["aws.s3"],
  "detail-type": "Object Created",
   "detail": {
    "version": "0",
    "bucket": {    
      "name" : [ { "prefix": "example-bucket" } ]
    }
   }
}


-- by this , I want you to match the buckets, whose name has the prefixed, mybuckets. And this is very helpful because now we match many more events.



eg 4 


suffix matching 


{
  "source": ["aws.s3"],
  "detail-type": "Object Created",
   "detail": {
    "version": "0",
    "bucket": {    
      "name" : [ { "prefix": "example-bucket" } ]
    },
     "object": {
       "key": [ { "suffix": ".png" } ]
     }
   }
}



-- like this you have create so many filter thanks to this event pattern, only events matching these conditions will be matched and sent to my targets.









----------------------------------------------------- Amazon EventBridge Inpput Transformation Hands on 




        - this is what the input event looks like. So we have an id,

{
  "version": "0",
  "id": "7bf73129-1428-4cd3-a780-95db273d1602",
  "detail-type": "EC2 Instance State-change Notification",
  "source": "aws.ec2",
  "account": "123456789012",
  "time": "2015-11-11T21:29:54Z",
  "region": "us-east-1",
  "resources": ["arn:aws:ec2:us-east-1:123456789012:instance/i-abcd1111"],
  "detail": {
    "instance-id": "i-abcd1111",
    "state": "pending"
  }
}


-- So we have an id, we have a detail type, we have a source, an account, a timestamp, we have a region, we have resource which is the instance ID that it supplies to the full ARN. 

-- this is a JSON document and it may look fine, but maybe a downstream application wants to have a specific event type and doesn't want to use this, but instead wants to use a simplified version of it

-- where maybe the instance ID is not under detail but it's just one level up. For whatever reason you may want to transform this.





-- we want to have an input, but we want to transform it


-- go to rules --> create rule --> default (event bus) --> Rule with an event pattern --> Event source = AWS events or EventBridge partner events --> sample event = ec2 instance change notification --> Event pattern = AWS service = ec2 --> ec2 instance change notification --> target --> CW --> give name aws/events/ ....    \

     Additional settings --> Configure target input = input tranformer --> click on configure input transformer --> pop up will come sample event = ec2 instance change notification --> Target input transformer --> give some inputs to fetch value from the above sample event \




     {
    "instance": "$.detail.instance-id",
    "state": "$.detail.state",
    "timestamp": "$.time",
    "resource" : "$.resources[0]"
    }


-- in Template section give like this 

        <instance> was launched at <timestamp> with <state>

-- click on generate o/p u will get like this 

        i-abcd1111 was launched at 2015-11-11T21:29:54Z with pending



-- So everything is working great, but to send stuff into CloudWatch log groups, you need to actually include a JSON document.

-- So we'll include this JSON document, and it's going to be a very simple one.

{
    "timestamp": <timestamp>,
    "message" :  "<instance> was launched at <timestamp> in <state> and arn <resource>."

}



-- genarate o/p 


{
  "timestamp": 2015 - 11 - 11 T21: 29: 54 Z,
  "message": i - abcd1111 was launched at 2015 - 11 - 11 T21: 29: 54 Z with pending and arn: aws: ec2: us - east - 1: 123456789012: instance / i - abcd1111

}


-- now confirm and create rule 


-- now lets create one ec2instance , So these instances being launched is going to actually generate some EventBridge data. So EventBridge will send it to CloudWatch log.

-- go to CW --> go to loggroup created by eventBridge --> u will see the logs 


EG 

i-0e2f24a0496bb84a2 was launched at 2024-10-11T11:53:30Z in running and arn arn:aws:ec2:ap-south-1:298132369629:instance/i-0e2f24a0496bb84a2.


-- So our integration is perfectly working.

-- We have taken an event from EventBridge, we've transformed it, thanks to input transformers. And now it's in CloudWatch logs.









======================================= practicals for event bridge 


Example 1 

1  Schedule AWS Lambda Functions Using EventBridge


-- lambda scheduling use cases 

- Automated backups at EOD of ur applications 

- backend cleaning (including logs and temp files )

- consolidated reports after business hours , so lambda can trigger  Athena queries and can run any database queries and send to business stack holders through SNS 


- to schedule events u have two options 

1 to schedule  at fixed rate -- for every 1 minute 

2 Cron job (10 * * * * )

-- open console 

-- open lambda --> python 3.9 --> create function 

-- write python code to invoke lambda function

import json
from datetime import datetime

def lambda_handler(event, context):
    # TODO implement
    currentTime = datetime.now()
    print("Time at which Lambda invoked" + str(currentTime))


-- give empty JSON to test {}

-- now go to Amazon eventBridge in console 

-- c.o create rule --> schedule --> continue to create rule --> schedule that run at regular intervals--> select 1 minute --> select lambda function --> create rule 

-- u can check in CloudWatch logs , for invoking is there or not 

--  the logs are generated 


-- if u want to monitor ur invocations --> Metrics --> all Metrics --> query --> choose AWS/events --> Myrule name 

- metric name = COUNT(invocation)

- filter by = Rulename = rule name that u have created in lambda function to test the function 

- choose number in right corner to see the no. of invocations 


-------------------------------------------

Example 2 : EventBridge with SNS 


-- AS we know that EventBridge will use for 2 process 

1 event-event process : when the event has occurred according to our rule then it will trigger to the targets 

2 schedule process : do schedule 

-- in the above example we have seen Schedule event

-- now event to event 

- for example the ec2 is stopped then it will send alert to the subscribers on SNS Topic 

-- open SNS in console --> create one topic --> standard --> create topic --> open topic -->create subscription --> add protocol email or phone number 

-- when ever the instance is getting stooped then I would like send an alert to all the subscribers 

-- this is called event-event process 

-- create one ec2 instance 

-- create rule in event bridge --> name --> rule with an event pattern --> in event pattern = select ec2 --> select event type as u want --> in Target1 choose AWS Service = SNS topic select SNS topic --> create rule 

-- do stop the instance 

-- u will get notification once the instance is get stopped 


--------------- now for schedule event 

-- create another rule , that is based on the time to stop the instance 

-- select schedule pattern option --> A schedule that runs at a regular rate, such as every 10 minutes. --> target = terminateinstanceAPI call --> u can also add another target to get notification 
 

-- the instance will get terminated after the time that u have specified 


-- this is simple basic example for schedule pattern 









----------------------------------------------------- Service Quotas CloudWatch Alarms



• Notify you when you’re close to a service quota value threshold

• Create CloudWatch Alarms on the Service Quotas console

• Example:Lambda concurrent executions

        - then if you go over 900, and the limit is 1000 by default, then you should trigger a CloudWatch Alarm. And this will tell us as Administrators that we should probably request a quota increase for the Lambda Concurrency Execution,

• Helps you know if you need to request a quota increase or shutdown resources before limit is reached




----------------------------------------------------- Alternative:Trusted Advisor + CW Alarms


• Limited number of Service Limits checks in Trusted Advisor (~50)

• Trusted Advisor publishes its check results to CloudWatch

• You can create CloudWatch Alarms on service quota usage (Service Limits)






----------------------------------------------------- Service Quotas CloudWatch Alarms Hands ON 



-- open service quor=ta in console and create alarm as you want to 





----------------------------------------------------- AWS CloudTrail




• Provides governance, compliance and audit for your AWS Account

• CloudTrail is enabled by default!

• Get an history of events / API calls made within your AWS Account by:

        • Console
        • SDK
        • CLI
        • AWS Services


• Can put logs from CloudTrail into CloudWatch Logs or S3

• A trail can be applied to All Regions (default) or a single Region.

• If a resource is deleted in AWS, investigate CloudTrail first!






---------------------------------------------------------------- CloudTrail Events





1 • Management Events:

     • Operations that are performed on resources in your AWS account

     • Examples:

          • Configuring security (IAM AttachRolePolicy)

          • Configuring rules for routing data (Amazon EC2 CreateSubnet)

          • Setting up logging (AWS CloudTrail CreateTrail)

          • By default, trails are configured to log management events.

          • Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)



2 • Data Events:

      • By default, data events are not logged (because high volume operations)

      - So what are Data Events?

      • Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject):as you can see, these can be happening a lot on an S3 bucket. so this is why they're not logged by default and u have the option can separate Read and Write Events

      - So a Read Event will be a GetObject whereas a wright Event would be a DeleteObject or a PutObject.

      - Another kind of event you can have in a CloudTrail are AWS Lambda function execution activities.

      • AWS Lambda function execution activity (the Invoke API)

          - So whenever someone uses the Invoke API so you can get insights about how many times your Lambda functions are being evoked.

          - this could be really high volumes, if your Lambda functions are executed a lot.




3  CloudTrail Insights (u have to pay)


-- So when we have so many Management Events across all types of services and so many APIs happening very quickly in your accounts, 

-- it can be quite difficult to understand what looks odd, what looks unusual and what doesn't.

-- so this is where CloudTrail Insights comes in.


-- So with CloudTrail Insights and you have to enable it and you have to pay for it, it will analyze your events and try to detect unusual activity in your accounts.

• Enable CloudTrail Insights to detect unusual activity in your account:

    EG : 
    
    • inaccurate resource provisioning

    • hitting service limits

    • Bursts of AWS IAM actions

    • Gaps in periodic maintenance activity


• CloudTrail Insights analyzes normal management events to create a baseline

• And then continuously analyzes write events to detect unusual patterns

    • Anomalies appear in the CloudTrail console
    • Event is sent to Amazon S3
    • An EventBridge event is generated (for automation needs)









---------------------------------------------------------------- CloudTrail Events Retention


• Events are stored for 90 days in CloudTrail and then afterwards they're deleted,

• To keep events beyond this period, log them to S3 and use Athena



---------------------------------------------------------------- LAB 

-- open cloud trail --> event history u can able to see all the events which u have done in the aws account through the root user or normal user 

-- for lab , lets create a new cloud trail 

-- create new s3 bucket , all the API events that will send to s3 and stored in this bucket 

-- Log file SSE-KMS encryption  and Log file validation = uncheck no need for this demo 

-- management events --> create trail that's it 

-- wait for 5 min atleast to see the current time and date iin the trail 

-- after 5 min go n check in the s3 bucket --> it will create log files 

-- in the mean while create one simple ec2 instance for example purpose\

-- check in the history it will shows u the instance running in the trail

-- this is how u will get all the API calls that U have made in ur AWS Account 







------------------------------------------------------- Amazon EventBridge – Intercept API Calls


-- A very important cloud trail integration you need to know about is the one with Amazon EventBridge to intercept any API calls.

-- So let's say you wanted to receive an SNS notification, anytime a user would delete a table in DynamoDB by using the DeleteTable API Call.

-- So what happens that whenever we do an API call in AWS, as you know, the API call itself is going to be logged in CloudTrail. That's for any API call.

-- But also  these all these API calls will end up as events as well in Amazon EventBridge.

-- so we can look for that very specific delete table API call, and create a rule out of it. And this rule will have a destination the destination being Amazon SNS and therefore, we can create alerts.



 User -----------(DeleteTable API Call)------> DynamoDB -----------(Log API call)-----> CloudTrail (any API call) -----------(event)------> Amazon EventBridge -------(alert) ------------> SNS





-------------- few more examples on how you can integrate Amazon Eventbridge and CloudTrail.


1 

   -- For example, say, you wanted to be notified whenever a user was assuming a role in your accounts.

   -- So the AssumeRole is an API in the IAM service and therefore, is going to be logged by CloudTrail.

   -- And then using EventBridge integration, we can trigger a message into an SNS topic.


   User ------ (AssumeRole) ---------> IAM Role --------(API Call logs) ----------> CloudTrail --------(event)---------> EventBridge -------> SNS





2 

    -- Similarly, we can also intercept API calls that, for example, change the Security Group inbound rules. So the Security Group call is called AuthorizeSecurityGroupIngress, and it's an EC2 API call.

    -- So these are going to be logged again by CloudTrail and then they will appear in EventBridge and then we can trigger a notification in SNS.



    User -------------(AuthorizeSecurityGroupIngress)--------> EC2(Security Group) ------------- (API Call logs) -------------> CloudTrail ------------(event)------> EventBridge ---------> SNS









------------------------------------------------------- CloudTrail – Log File Integrity Validation (CloudTrail for SysOps)



-- So when you do API calls within AWS, they're going to be logged by CloudTrail and you can have these logs being sent into Amazon S3, okay, every one hour. 

-- But, you can also create what's called a digest file.

• Digest Files:

        • References the log files for the last hour and contains a hash of each

        • Stored in the same s3 bucket as log files (Different folder)

• Helps you determine whether a log file was modified/deleted after CloudTrail delivered it

• Hashing using SHA-256, Digital Signing using SHA- 256 with RSA

• Protect the S3 bucket using bucket policy, versioning, MFA Delete protection, encryption, object lock

• Protect CloudTrail using IAM






------------------------------------------------------- CloudTrail – Integration with EventBridge




• Used to react to any API call being made in your account

• CloudTrail is not “real-time”:

        • Delivers an event within 15 minutes of an API call

        • Delivers log files to an S3 bucket every 5 minutes


-- So this is not a real-time automation on top of API calls, but this is for you a way to get some kind of integration on top of any API calls made within CloudTrail when it is delivered into EventBridge.





------------------------------------------------------- CloudTrail – OrganizationsTrails



• A trail that will log all events for all AWS accounts in an AWS Organization

• Log events for management and member accounts

• Trail with the same name will be created in every AWS account (IAM permissions)

• Member accounts can’t remove or modify the organization trail (view only)









------------------------------------------------------- AWS Config



• Helps with auditing and recording compliance of your AWS resources

• Helps record configurations and changes over time

• Questions that can be solved by AWS Config:

        • Is there unrestricted SSH access to my security groups?

        • Do my buckets have any public access?

        • How has my ALB configuration changed over time?


• You can receive alerts (SNS notifications) for any changes

• AWS Config is a per-region service

• Can be aggregated across regions and accounts

• Possibility of storing the configuration data into S3 (analyzed by Athena)





------------------------------------------------------- Config Rules



• Can use AWS managed config rules (over 75)

• Can make custom config rules (must be defined in AWS Lambda)

        • Ex: evaluate if each EBS disk is of type gp2

        • Ex: evaluate if each EC2 instance is t2.micro

• Rules can be evaluated / triggered:

        • For each config change

        • And / or: at regular time intervals


• AWS Config Rules does not prevent actions from happening (no deny)


• Pricing: no free tier, $0.003 per configuration item recorded per region, $0.001 per config rule evaluation per region





------------------------------------------------------- Config Rules – Remediations




-- Now, although you cannot deny any action from happening from within the config, you can do remediations of your non-compliant resources using an SSM Automation Documents.

-- So the idea is, for example, you are monitoring whether or not your IAM access keys have expired.

        - For example, they are older than 90 days. In which case you want to mark them as non-compliant.

        - So this will not prevent them from not being compliant, but you can trigger whenever a resource is not compliant, a remediation action.


• Use AWS-Managed Automation Documents or create custom Automation Documents

        • Tip: you can create custom Automation Documents that invokes Lambda function


• You can set Remediation Retries if the resource is still non-compliant after auto-remediation







-- A media company stores all their articles on Amazon S3 buckets. As a security measure, they have server access logging enabled for all the buckets. The company is looking at a solution that can regularly check if logging is enabled for all the existing buckets and for any new ones they create. If the solution can also automate the remedy, it will be a perfect fit for their requirement.

Which of the following would you suggest to address the given use-case?

ANS :  Use AWS Config rules to check whether or not an S3 bucket has logging enabled, and carry out the necessary remediation if needed


EXP : 

      - AWS Config keeps track of the configuration of your AWS resources and their relationships to other resources. It can also evaluate those AWS resources for compliance. 

      - This service uses rules that can be configured to evaluate AWS resources against desired configurations.

      - For example, there are AWS Config rules that check whether or not your Amazon S3 buckets have logging enabled or your IAM users have an MFA device enabled.

      - AWS Config rules use AWS Lambda functions to perform the compliance evaluations, and the Lambda functions return the compliance status of the evaluated resources as compliant or noncompliant. 

      - The non-compliant resources are remediated using the remediation action associated with the AWS Config rule. 

      - With the Auto-Remediation feature of AWS Config rules, the remediation action can be executed automatically when a resource is found non-compliant.

      - AWS Config Auto Remediation feature has auto remediate feature for any non-compliant S3 buckets using the following AWS Config rules:

            - s3-bucket-logging-enabled s3-bucket-server-side-encryption-enabled s3-bucket-public-read-prohibited s3-bucket-public-write-prohibited

      - These AWS Config rules act as controls to prevent any non-compliant S3 activities.

      






------------------------------------------------------- Config Rules – Notifications



• Use EventBridge to trigger notifications when AWS resources are non- compliant

• Ability to send configuration changes and compliance state notifications to SNS (all events – use SNS Filtering or filter at client-side)







------------------------------------------------------- Config  hands On

-- Customize AWS Config to record configuration changes for all supported resource types, or for only the supported resource types that are relevant to you. Globally recorded resources (RDS global clusters and IAM users, groups, roles, and customer managed policies) may be recorded in more than this Region. You are charged based on the number of configuration items recorded. 

-- open AWS config in AWS 

-- this is not free 

-- Recording method = All resource types with customizable overrides

-- choose role created by aws ( u no need to create aws role , aws will create for you)

-- and also bucket will create for you 

-- in rules search for the SSH and select this ssh this will check Checks whether security groups that are in use disallow unrestricted incoming SSH traffic.

-- create config , now config going to monitor the entire AWS environment 

--  changes that made in AWS will send to s3 

-- wait for 1-3 min to get details 

-- u will see the noncomplianct resources on dashboard

-- for me there are 4 noncompliance records are there so , open resources(manage resource) and delete ssh connection in Security groups 

-- now wait 1-2 min and check , now i do not have any noncompliance resources 

-- u can also check resources in left side , u can check time line of the resource when n how the changes happen 

-- it is best for auditing 



---------------------------------- IMP : once u delete rules in console , it won’t delete and u will get charge , so to delete this 

-- create one instance and connect instance install aws cli and login with ur keys 

-- follow these steps 

1 Turn off Recording for that region using the console

2 Delete the Rule by going to actions, delete rule

3 Use the AWS CLI and delete the default recording by

aws configservice delete-configuration-recorder --configuration-recorder-name default --region <region-name>

4 Delete the service linked role created for AWS Config  ( search for AWSConfigService role in roles)


-- Refresh the Config home page to make it appear fresh.

-- If necessary delete the config bucket and its objects.








------------------------------------------------------- AWS Config – Aggregators



-- Well, say you are managing multiple accounts and multiple regions within these accounts. So we have Account A, Account B, and they're called "source accounts", why?

        - Because they all have a deployment of AWS Config.

        - But you want to aggregate all this information into a central AWS account. And this account is called an aggregator account, and this is only in the aggregator account that you're going to create an aggregator


• The aggregator is created in one central aggregator account , they're not created in each individual source accounts.

• Aggregates rules, resources, etc... across multiple accounts & regions

• If using AWS Organizations, no need for individual Authorization , You just create your aggregator in your management account in Organization, and the authorizations will happen automatically.

        -- But if you're not using AWS Organizations then you would need to create an authorization in an account A to say "Hey, I authorize the AWS accounts aggregator "to collect data,"

        -- and then you would create a second authorization, obviously in Account B, to do the exact same thing.

        -- Once these authorizations are in place, and this is only, again, if you're not using AWS Organizations, then the aggregator will be able to collect data, to pull data from these targeted accounts and aggregate them.



• Rules are created in each individual source AWS account

• Can deploy rules to multiple target accounts using CloudFormation StackSets






------------------------------------------------------- CloudWatch vs CloudTrail vs Config





• CloudWatch 

  • Performance monitoring (metrics, CPU, network, etc...) & dashboards
  • Events & Alerting
  • Log Aggregation & Analysis

• CloudTrail
  • Record API calls made within your Account by everyone 
  • Can define trails for specific resources
  • Global Service

• Config

 • Record configuration changes
 • Evaluate resources against compliance rules
 • Get timeline of changes and compliance





------------------------------------ For an Elastic Load Balancer


• CloudWatch:

        • Monitoring Incoming connections metric

        • Visualize error codes as a % over time

        • Make a dashboard to get an idea of your load balancer performance


• Config:

        • Track security group rules for the Load Balancer

        • Track configuration changes for the Load Balancer

        • Ensure an SSL certificate is always assigned to the Load Balancer (compliance)


• CloudTrail:

        • Track who made any changes to the Load Balancer with API calls








======================================================== AWS Account Management ========================================================








------------------------------------------------------- AWS Health Dashboard - Service History



• Shows all regions, all services health

• Shows historical information for each day

• Has an RSS feed you can subscribe to

• Previously called AWS Service Health Dashboard





------------------------------------------------------- AWS Health Dashboard – Your Account


• Previously called AWS Personal Health Dashboard (PHD)

• AWS Account Health Dashboard provides aler ts and remediation guidance when AWS is experiencing events that may impact you.

• While the Service Health Dashboard displays the general status of AWS services,Account Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.

• The dashboard displays relevant and timely information to help you manage events in progress and provides proactive notification to help you plan for scheduled activities.

• Can aggregate data from an entire AWS Organization

• Global service

• Shows how AWS outages directly impact you & your AWS resources

• Alert, remediation, proactive, scheduled activities



-- click on Bell symbol in console --> see all health events 






------------------------------------------------------- Health Event Notifications



• Use EventBridge to react to changes for AWS Health events in your AWS account

• Example: receive email notifications when EC2 instances in your AWS account are scheduled for updates

• This is possible for Account events (resources that are affected in your account) and Public Events (Regional availability of a service)

• Use cases: send notifications, capture event information, take corrective action...





------------------------------------------------------- AWS Organizations




---- Organizational Units (OU) , 

-- In organzations u can create seperate units for different lify cycles for eg : Bussiness unit , environmental lifecycle , project based 

AWS Organizations :

 • Global service

 • Allows to manage multiple AWS accounts

 • The main account is the management account

 • Other accounts are member accounts

 • Member accounts can only be part of one organization -- IMP for Exam

 • Consolidated Billing across all accounts - single payment method

 • Pricing benefits from aggregated usage (volume discount for EC2, S3...) 

 • Shared reserved instances and Savings Plans discounts across accounts

        - So if a reserved instance is unused in one account, another account can benefit from it and therefor the discounts apply across the entire organization, which is really good for cost savings.

 • API is available to automate AWS account creation






 --------------------- Advantages

 • Multi Account vs One Account Multi VPC

 • Use tagging standards for billing purposes

 • Enable CloudTrail on all accounts, send logs to central S3 account

 • Send CloudWatch Logs to central logging account

 • Establish Cross Account Roles for Admin purposes





------------------ Security:  Service Control Policy (SCP)

• IAM policies applied to OU or Accounts to restrict Users and Roles

• They do not apply to the management account (full admin power)

• Must have an explicit allow (does not allow anything by default – like IAM)

--  Service control policy (SCP) is one type of policy that you can use to manage your organization.

--  SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.

-- SCPs are available only in an organization that has all features enabled. 

-- SCPs aren't available if your organization has enabled only the consolidated billing features.

-- Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform. 







------------------------------------------------------- AWS Organizations – Reserved Instances (for SysOps)




• For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account.

• This means that all accounts in the organization can receive the hourly cost benefit of Reserved Instances that are purchased by any other account.

• The payer account (master account) of an organization can turn off Reserved Instance (RI) discount and Savings Plans discount sharing for any accounts in that organization, including the payer account

• This means that RIs and Savings Plans discounts aren't shared between any accounts that have sharing turned off.

• To share an RI or Savings Plans discount with an account, both accounts must have sharing turned on.





------------------------------------------------------- AWS Organizations – IAM Policies



• Use aws:PrincipalOrgID condition key in your resource-based policies to restrict access to IAM principals from accounts in an AWS Organization

        - to allow access from any, IAM principles from all the accounts in your organization.

        - So, say you have an organization with a management account and two member accounts with different IAM users.

        - If you set up an extra bucket and you're using the bucket policy, the aws:PrincipalOrgID condition, then you're giving access automatically to any user or any roles within your organization to this extra buckets.

        - So, it's a good way for you to not have to specify every individual accounts, but instead reference only the leading organization of your accounts.






------------------------------------------------------- AWS Organizations – Tag Policies





https://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/


• Helps you standardize tags across resources in an AWS Organization

• Ensure consistent tags, audit tagged resources, maintain proper resources categorization, ...

• You define tag keys and their allowed values

• Helps with AWS Cost Allocation Tags and Attribute-based Access Control

• Prevent any non-compliant tagging operations on specified services and resources (has no effect on resources without tags)

• Generate a report that lists all tagged/non-compliant resources

• Use CloudWatch Events to monitor non-compliant tags




ExampleInc-CostAllocation.json

The following is an example of a tag policy that reports on Cost Allocation tags:


{
  "tags": {
    "example-inc:cost-allocation:ApplicationId": {
      "tag_key": {
        "@@assign": "example-inc:cost-allocation:ApplicationId"
      },
      "tag_value": {
        "@@assign": [
          "DataLakeX",
          "RetailSiteX"
        ]
      }
    },
    "example-inc:cost-allocation:BusinessUnitId": {
      "tag_key": {
        "@@assign": "example-inc:cost-allocation:BusinessUnitId"
      },
      "tag_value": {
        "@@assign": [
          "Architecture",
          "DevOps",
          "FinanceDataLakeX"
        ]
      }
    },
    "example-inc:cost-allocation:CostCenter": {
      "tag_key": {
        "@@assign": "example-inc:cost-allocation:CostCenter"
      },
      "tag_value": {
        "@@assign": [
          "123-*"
        ]
      }
    }
  }
}




2 ExampleInc-DisasterRecovery.json



The following is an example of a tag policy that reports on Disaster Recovery tags:


{
    "tags": {
        "example-inc:disaster-recovery:rpo": {
            "tag_key": {
                "@@assign": "example-inc:disaster-recovery:rpo"
            },
            "tag_value": {
                "@@assign": [
                    "6h",
                    "24h"
                ]
            }
        }        
    }
}








------------------------------------------------------- AWS Control Tower



• Easy way to set up and govern a secure and compliant multi-account AWS environment based on best practices

• Benefits:

        • Automate the set up of your environment in a few clicks

        • Automate ongoing policy management using guardrails

        • Detect policy violations and remediate them

        • Monitor compliance through an interactive dashboard


• AWS Control Tower runs on top of AWS Organizations:

        • It automatically sets up AWS Organizations to organize accounts and implement SCPs (Service Control Policies)






------------------------------------------------------- AWS Service Catalog



• Users that are new to AWS have too many options, and may create stacks that are not compliant / in line with the rest of the organization

• Some users just want a quick self-service portal to launch a set of authorized products pre-defined by admins

• Includes: virtual machines, databases, storage options, etc...

• Enter AWS Service Catalog!




------------------------------------------------------- Service Catalog diagram



ADMIN TASKS 

        - So your admins are going to create products, and products are a short name for CloudFormation templates.

        - Then we build a portfolio, which is a collection of products, (a collection of CloudFormation templates.)

        - then we define controls such as IAM Permissions, for who can access this portfolio.


USER TASKS 

        - Now your users on the other side, they will have a product list available to them, and they will see that list, thanks to the IAM Permissions we have defined from before.

        - if they're happy with the product, they will launch it, and then the product will be provisioned, which is ready to use, properly configured, and properly typed.

        - So the idea is that here, we allow our users to pick and choose from a list of CloudFormation templates, organizing portfolios, and launch them safely.

        - So our users may not have access to AWS at all, but only to the Service Catalog, and they can launch full stacks, without directly accessing CloudFormation, only by using the CloudFormation templates authorized in the Service Catalog.





------------------------------------------------------- AWS Service Catalog – Sharing Catalogs



• Share portfolios with individual AWS accounts or AWS Organizations

• Sharing options: 2 options 

        1 • Share a reference of the portfolio, then import the shared portfolio in the recipient account (stays in-sync with the original portfolio)

        2 • Deploy a copy of the portfolio into the recipient account (must re-deploy any updates)


• Ability to add products from the imported portfolio to the local portfolio




------------------------------------------------------- AWS Service Catalog – TagOptions Library



• Easily manage tags on provisioned products

• TagOption:

        • Key-value pair managed in AWS Service Catalog

        • Used to create an AWS Tag

        - For example, we can associate the TagOption environment with the key and value prod,We associate it with portfolio A,

        - that means at any time we launch a stack from a product in portfolio A, then it will inherit the tags, key environment and value prod, 

        


• Can be associated with Portfolios and Products

• Use cases: proper resources tagging, defined allowed tags, ...

• Can be shared with other AWS accounts and AWS Organizations






------------------------------------------------------- AWS Service Catalog Hands ON 


-- open  AWS Service Catalog --> Administration --> product list --> create product --> Choose CF --> owner = give name of urs --> upload any CF file --> create product 

-- Now, you need to add this product into a portfolio.

-- go to portfolio --> owner name --> create 

-- open  portfolio --> we can add products --> add product that we have created

-- so now we want to provide access to this portfolio.

-- go to access tab in portfolio --> grant access --> So let's use IAM principle, and you have to choose which groups or which users should have access to our portfolio.--> add as your wish 

-- now login with the users wich u have provide access --> go to service catalog --> products --> able to see the CF stacks to launch --> launch product 

-- it will create CF template , eventhough user do not have access to CF , they can launch CF templates from users side 






------------------------------------------------------- AWS Billing Alarms


• Billing data metric is stored in CloudWatch us-east-1

• Billing data are for overall worldwide AWS costs

• It’s for actual cost, not for project costs





------------------------------------------------------- AWS Billing Alarms Hands ON 



-- you need to go on the left-hand side into Alarms and then Billing. So this only appears in us-east-1.

 --> go to billing section --> Preferences and Settings = Billing Preferences -->  Alert preferences = select and give email 

 -- create alarm --> select billing as metric --> byservice --> Total Estimated Charge--> select metric --> give threshlod value --> 

 -- this is how you can create alarms for aws billing




------------------------------------------------------- Cost Explorer


 • Visualize, understand, and manage your AWS costs and usage over time

 • Create custom reports that analyze cost and usage data.

 • Analyze your data at a high level: total costs and usage across all accounts

 • Or Monthly, hourly, resource level granularity

 • Choose an optimal Savings Plan (to lower prices on your bill)

 • Forecast usage up to 12 months based on previous usage





------------------------------------------------------- AWS Budgets



• Create budget and send alarms when costs exceeds the budget

• 4 types of budgets: Usage, Cost, Reservation, Savings Plans

• For Reserved Instances (RI)

        • Track utilization

        • Supports EC2, ElastiCache, RDS, Redshift


• Up to 5 SNS notifications per budget

• Can filter by: Service, Linked Account,Tag, Purchase Option, Instance Type, Region, Availability Zone, API Operation, etc...

• Same options as AWS Cost Explorer!

• 2 budgets are free, then $0.02/day/budget



-- see in console 



------------------------------------------------------- Cost Allocation Tags


• Use cost allocation tags to track your AWS costs on a detailed level

• AWS generated tags

        • Automatically applied to the resource you create

        • Starts with Prefix aws: (e.g. aws: createdBy)


• User-defined tags

        • Defined by the user

        • Starts with Prefix user:



-- see in console         





------------------------------------------------------- Cost and Usage Reports



• Dive deeper into your AWS costs and usage

• The AWS Cost & Usage Report contains the most comprehensive set of AWS cost and usage data available

• Includes additional metadata about AWS services, pricing, and reservations (e.g., Amazon EC2 Reserved Instances (RIs))

• The AWS Cost & Usage Report lists AWS usage for each:

        • service category used by an account

        • in hourly or daily line items

        • any tags that you have activated for cost allocation purposes


• Can be configured for daily exports to S3

• Can be integrated with Athena, Redshift or QuickSight


-- see in console 







------------------------------------------------------- AWS Compute Optimizer



• Reduce costs and improve performance by recommending optimal AWS resources for your workloads

• Helps you choose optimal configurations and right- size your workloads (over/under provisioned)

• Uses Machine Learning to analyze your resources’ configurations and their utilization CloudWatch metrics

• Supported resources

        • EC2 instances

        • EC2AutoScalingGroups

        • EBS volumes

        • Lambda functions


• Lower your costs by up to 25%

• Recommendations can be exported to S3








======================================================= Disaster Recovery ================================================================================





------------------------------------------------------- AWS DataSync


• Move large amount of data to and from

        • On-premises / other cloud to AWS (NFS, SMB, HDFS, S3 API...) – needs agent

        • AWS to AWS (different storage services) – no agent needed


• Can synchronize to:

        • Amazon S3 (any storage classes – including Glacier)

        • Amazon EFS

        • Amazon FSx (Windows, Lustre, NetApp, OpenZFS...)


• Replication tasks can be scheduled hourly, daily, weekly

• File permissions and metadata are preserved (NFS POSIX, SMB...)

• One agent task can use 10 Gbps, can setup a bandwidth limit



-- AWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the AWS DataSync API and Console, and Amazon CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. 

-- AWS DataSync performs data integrity verification both during the transfer and at the end of the transfer.









------------------------------------------------------- diff b/w storage gateway and data sync 



1 Description :

DataSync : AWS DataSync is an online data transfer service that simplifies, automates, and accelerates the process of copying large amounts of data to and from AWS storage services over the Internet or over AWS Direct Connect.

Storage Gateway : AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage by linking it to S3. Storage Gateway provides 3 types of storage interfaces for your on-premises applications: file, volume, and tape


2  How it Work ?

DataSync : Uses an agent which is a virtual machine (VM) that is owned by the user and is used to read or write data from your storage systems. You can activate the agent from the Management Console. The agent will then read from a source location, and sync your data to Amazon S3, Amazon EFS, or Amazon Fsx for Windows File Server.

Storage Gateway : Uses a Storage Gateway Appliance – a VM from Amazon – which is installed and hosted on your data center. After the setup, you can use the AWS console to provision your storage options: File Gateway, Cached Volumes, or Stored Volumes, in which data will be saved to Amazon S3.
- You can also purchase the hardware appliance to facilitate the transfer instead of installing the VM

3  Protocols 

DataSync :  AWS DataSync can copy data between Network File Systems (NFS), SMB file servers or self-managed object storages. It can also move data between your on-premises storage and AWS Snowcone, Amazon S3, Amazon EFS, or Amazon FSx,

Storage Gateway : File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as NFS and SMB.
- Volume Gateway stores your data locally in the gateway and syncs them to Amazon S3. It also allows you to take point-in-time copies of your volumes with EBS snapshots which you can restore and mount to your appliance as iSCSI device. 
- Tape Gateway data is immediately stored in Amazon S3 and can be archived to Amazon S3 Glacier or Amazon S3 Glacier Deep Archive.

4  Pricing 

DataSync : You are charged standard request, storage, and data transfer rates to read from and write to AWS services, such as Amazon S3, Amazon EFS, AmazonFSx for Windows File Server, and AWS KMS.

Storage Gateway :  You are charged based on the type and amount of storage you use, the requests you make, and the amount of data transferred out of AWS.


combination : You can use a combination of DataSync and File Gateway to minimize your on-premises’ operational costs while seamlessly connecting on-premises applications to your cloud storage. AWS DataSync enables you to automate and accelerate online data transfers to AWS storage services. File Gateway then provides your on-premises applications with low latency access to the migrated data.







------------------------------------------------------- AWS Transfer Family





-- Fully managed Service for file transer securely 

-- A fully-managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol

-- Supported Protocols
• AWS Transfer for FTP (File Transfer Protocol (FTP))
• AWS Transfer for FTPS (File Transfer Protocol over SSL (FTPS)) 
• AWS Transfer for SFTP (Secure File Transfer Protocol (SFTP))

-- Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)

-- Pay per provisioned endpoint per hour + data transfers in GB

-- Usage: sharing files, public datasets, CRM, ERP, ...

-- data can be transferred in and out of s3 buckets and EFS 

-- generally we are uploaing our files in S3 through the console  but we are doing through the http , 

-- now w r going to transfer files securely from ur laptop 

-- for this we have to install software called "FileZilla" which is used to transfer fis very securley 

-- b/w ur laptop and s3 or EFS we have to create One "Transfer Server" , this server allows us to do  SFTP,FTP,FTPS, we can choose any one , and we hav to create one user in Transfer server 

-- in FileZilla u can login as with user name 

-- once u create server it will give u endpoint 








-------------------------- Hands ON 

-- create public bucket 

-- create one bucket policy through policy generator 

eg :   

{
  "Id": "Policy1704630072890",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1704629976563",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket",
        "s3:PutObject"
      ],
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::transfer-dem",
                       "arn:aws:s3:::transfer-dem/*" ],
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "49.205.119.80/32"
        }
      },
      "Principal": "*"
    }
  ]
}

-- transfer-dem = name 

-- now create transfer server 

-- open aws transfer family in console 

-- create server , selete SFTP --> Srvice managed --> select as per ur requriemnets 

--now add user in server 

-- select server c.o add user --> create one role in IAM --> truste entity = ec2 --> use case = Transfer (imp) --> give s3 full access --> create role 

-- go to puttyGen n create SSH keys(mouseover)  and save private key with .ppk 

-- now download Filezilla 

-- open filezilla --File(top) --> site manager --> new site --> copy end point of server --> selet SFTP n paste endpoint of server and give port number as 22--> logon type : key file --> give ur private file u downoaded -- connect 


-- once u coonect u jst drag do upload ur files u want and go check in s3 in console --- getting files 

-- this is how u can transfer files throug SFTP from ur lap to s3 or EFS 








------------------------------------------------------- AWS Backup





• Fully managed service

• Centrally manage and automate backups across AWS services

• No need to create custom scripts and manual processes

• Supported services:
 • Amazon EC2 / Amazon EBS
 • Amazon S3
 • Amazon RDS (all DBs engines) / Amazon Aurora / Amazon DynamoDB 
 • Amazon DocumentDB / Amazon Neptune
 • Amazon EFS / Amazon FSx (Lustre & Windows File Server)
 • AWS Storage Gateway (Volume Gateway)   

 -- EFS , EC2 , DynamoDB = Service level backups , 
    for eg we always take backups of EFS files 

 -- RDS, EBS , AWS Storage gateway , amazon aurora = service level snapshots , 
    for eg we always take snapshots of EBS volumes 



• Supports cross-region backups

• Supports cross-account backups

• Supports PITR for supported services

• On-Demand and Scheduled backups

• Tag-based backup policies

• You create backup policies known as Backup Plans
 
   • Backup frequency (every 12 hours, daily, weekly, monthly, cron expression)
   • Backup window
   • Transition to Cold Storage (Never, Days,Weeks, Months,Years)
   • Retention Period (Always, Days,Weeks, Months,Years)


AWS Backup ----------Create Backup Plan (frequency, retention policy) -------> Assign AWS service -------Automatically backed up to-----> S3


------- Backup Plan : it allows you to configure when to take the backups , u can define frequency of taking backup and also retention policy 

------- backup vault ; -- it is like a container which stores all ur backups 



------------------------------------------------------- AWS Backup Vault Lock


• Enforce a WORM (Write Once Read Many) state for all the backups that you store in your AWS Backup Vault

• Additional layer of defense to protect your backups against:

   • Inadvertent or malicious delete operations
   • Updates that shorten or alter retention Periods

• Even the root user cannot delete backups when enabled


----------------- new features 

Announcing New Features :

-- Support for EBS snapshot archive  Explore long-term, lower-cost archive storage lifecycle options for EBS snapshots

-- Restore testing  Periodically validate recovery readiness and gain insights into restore times

-- Jobs dashboard  Monitor backup health through visual representations of aggregated backup, copy, and restore metrics



------------------------------------------------------- LAB 

-- create one instance with tags

-- now go to aws backup --> settings --> configure resources select only ec2 and deselect remaining

-- now create on-demand backup

-- backup --> dashboard --> create on-demand backup 

-- select ec2 and instance id

-- give retention period 

-- create new backup vault , with default keys , and give tags

-- IAM role  = default 

-- create backup , wait for few minutes to create 

-- once ur jobs is created 

-- go to backup vault --> inside ur vault --> one recovery point will get assigned, u can retrieve ec2 from this recovery point 

-- instead of creating on-demand backup , we can also create backup plans and configure frequency and rentention period time 

-- go to backup plan --> Build a new plan 

-- Backup window  = give time according to ur time zone 

-- do not choose region for destination 

--  now go to backup plans --> assign resources  with default role 

-- in the tags <key> equals <ur value>

-- assign resources 

-- now go to ur backup valut --> select recovery point and see the backup type = manual 

--- now do restoring 

-- first terminate ec2 instance 

-- now go to backup --> backup vault --> recovery point --> restore 

-- it will create one restore job 

-- once it get completed , go n check the ec2 will get back , restore is successfull

-- this is how we can restore thr resource through aws backup , it is manual backup 

-- after sometime it will automatically starts one job for automated backup that we have created 

-- do check ur restore jobs for every 1 hr for automated restore jobs 

-- this is automated backups 

-- clear all resources that u have created 









======================================================= Security & Compliance =======================================================





------------------------------------------------------- AWS Shared Responsibility Model



• AWS responsibility - Security of the Cloud

        • Protecting infrastructure (hardware, software, facilities, and networking) that runs all the AWS services

        • Managed services like S3, DynamoDB, RDS, etc.


• Customer responsibility - Security in the Cloud

        • For EC2 instance, customer is responsible for management of the guest OS (including security patches and updates), firewall & network configuration, IAM

        • Encrypting application data


• Shared controls:

        • Patch Management, Configuration Management, Awareness & Training





------------------------------------------------------- Example, for RDS



• AWS responsibility:

        • Manage the underlying EC2 instance, disable SSH access

        • Automated DB patching

        • Automated OS patching

        • Audit the underlying instance and disks & guarantee it functions


• Your responsibility:

        • Check the ports / IP / security group inbound rules in DB’s SG

        • In-database user creation and permissions

        • Creating a database with or without public access

        • Ensure parameter groups or DB is configured to only allow SSL connections

        • Database encryption setting





Example, for S3 


• AWS responsibility:

        • Guarantee you get unlimited storage

        • Guarantee you get encryption

        • Ensure separation of the data between different customers

        • Ensure AWS employees can’t access your data


• Your responsibility:

        • Bucket configuration

        • Bucket policy / public setting

        • IAM user and roles

        • Enabling encryption




https://aws.amazon.com/compliance/shared-responsibility-model/







------------------------------------------------------- What’s a DDOS* Attack?




• DDoS: Distributed Denial of Service – many requests at the same time





------------------------------------------------------- DDoS Protection on AWS



• AWS Shield Standard: protects against DDoS attack for your website and applications, for all customers at no additional costs

• AWS Shield Advanced: 24/7 premium DDoS protection

• AWS WAF: Filter specific requests based on rules

• CloudFront and Route 53:

        • Availability protection using global edge network

        • Combined with AWS Shield, provides attack mitigation at the edge


• Be ready to scale – leverage AWS Auto Scaling




Sample Reference Architecture  : https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/






-------------------------------------------------------  AWS Shield



• DDoS: Distributed Denial of Service – many requests at the same time

1  AWS Shield Standard:

  • Free service that is activated for every AWS customer
  • Provides protection from attacks such as SYN("synchronize")/UDP Floods, Reflection attacks and other layer 3/layer 4 attacks


2  AWS Shield Advanced:

  • Optional DDoS mitigation service ($3,000 per month per organization)

  • Protect against more sophisticated attack on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53

  • 24/7 access to AWS DDoS response team (DRP)
 
  • Protect against higher fees during usage spikes due to DDoS 

  • Shield Advanced automatic application layer DDoS mitigation automatically creates, evaluates and deploys AWS WAF rules to mitigate layer 7 attacks







------------------------------------------------------- AWS WAF – Web Application Firewall




-- AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources.

-- AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.


EPV : If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more "IP match conditions." An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.



• Protects your web applications from common web exploits (Layer 7)

• Layer 7 is HTTP (vs Layer 4 is TCP/UDP)

• Deploy on

 • Application Load Balancer
 • API Gateway
 • CloudFront
 • AppSync GraphQL API
 • Cognito User Pool 

• Define Web ACL (Web Access Control List) Rules:

 • IP Set: up to 10,000 IP addresses – use multiple Rules for more IPs
 • HTTP headers, HTTP body, or URI strings Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
 • Size constraints, geo-match (block countries)
 • Rate-based rules (to count occurrences of events) – for DDoS protection

• Web ACL are Regional except for CloudFront

• A rule group is a reusable set of rules that you can add to a web ACL






---------------- WAF – Fixed IP while using WAF with a Load Balancer



• WAF does not support the Network Load Balancer (Layer 4)

• We can use Global Accelerator for fixed IP and WAF on the ALB


Users <---------> Global Accelerator (fixed IP) <-----------> ALB <----------------> EC2 instances 
                                                               |
                                                               |   (attached)
                                                               |
                                                              AWS WAF  , it has WebACL

                                       IMP NOTE : WebACL must be in the same AWS Region as ALB


EPV : 

AWS WAF - How it Works?:

-- To block specific countries, you can create a AWS WAF geo match statement listing the countries that you want to block, 

-- to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. 



EPV : Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.

ANS : Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule






------------------------------------------------------- AWS Firewall Manager





• Manage rules in all accounts of an AWS Organization

• Security policy: common set of security rules
 
• WAF rules (Application Load Balancer, API Gateways, CloudFront)
• AWS Shield Advanced (ALB, CLB, NLB, Elastic IP, CloudFront)
• Security Groups for EC2, Application Load Balancer and ENI resources in VPC 
• AWS Network Firewall (VPC Level)
• Amazon Route 53 Resolver DNS Firewall
• Policies are created at the region level

• Rules are applied to new resources as they are created (good for compliance) across all and future accounts in your Organization







------------------------------------------------------- WAF vs. Firewall Manager vs. Shield



• WAF, Shield and Firewall Manager are used together for comprehensive protection

• Define your Web ACL rules in WAF

• For granular protection of your resources, WAF alone is the correct choice

• If you want to use AWS WAF across accounts, accelerate WAF configuration, automate the protection of new resources, use Firewall Manager with AWS WAF

• Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the Shield Response Team (SRT) and advanced reporting.

• If you’re prone to frequent DDoS attacks, consider purchasing Shield Advanced





------------------------------------------------------- Penetration Testing on AWS Cloud


• AWS customers are welcome to carry out security assessments or penetration tests against their AWS infrastructure without prior approval for 8 services:


        • Amazon EC2 instances, NAT Gateways, and Elastic Load Balancers 
        • Amazon RDS
        • Amazon CloudFront
        • Amazon Aurora
        • Amazon API Gateways
        • AWS Lambda and Lambda Edge functions 
        • Amazon Lightsail resources
        • Amazon Elastic Beanstalk environments


• Prohibited Activities

        • DNS zone walking via Amazon Route 53 Hosted Zones

        • Denial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS

        • Port flooding

        • Protocol flooding

        • Request flooding (login request flooding, API request flooding)


• For any other simulated events, contact aws-security-simulated- event@amazon.com

• Read more: https://aws.amazon.com/security/penetration-testing/






------------------------------------------------------- Amazon Inspector



• Automated Security Assessments

it onlt evaluates 

1 For EC2 instances

  • Leveraging the AWS System Manager (SSM) agent
  • Analyze against unintended network accessibility
  • Analyze the running OS against known vulnerabilities


2 For Container Images push to Amazon ECR

  • Assessment of Container Images as they are pushed

3  For Lambda Functions

  • Identifies software vulnerabilities in function code and package dependencies

  • Assessment of functions as they are deployed



• Reporting & integration with AWS Security Hub 

• Send findings to Amazon Event Bridge





------------------------------ What does Amazon Inspector evaluate?


• Remember : only for EC2 instances, Container Images & Lambda functions

• Continuous scanning of the infrastructure, only when needed

• Package vulnerabilities (EC2, ECR & Lambda) – database of CVE

• Network reachability (EC2)

• A risk score is associated with all vulnerabilities for prioritization




 
 ------------------------------------------------------- Amazon Inspector Hands ON 




 -- open in console --> activate Inspector 

 -- now create ec2 instance linux 2023 

 -- as we launch this instance it is going to be scanned by the Inspector service.

 -- check in Environment coverage in Inspector console , u can see 0/1 instance 

 -- go to Account management --> instances --> u can see that not scanning instance list coz Unmanaged EC2 instance --> 

  -- click on Unmanaged EC2 instance , follow these instructions --> give instanceid --> give role blank --> execute   ( if you get error then go for nxt step )

 -- go to ssm --> quicksetup --> Host Management --> Targets = current region --> all instances --> create 



 -- now scanning happening , you can see all vulnerabilities if any happens for lambda and ecr and ec2 

 -- go to general setting in console --> deactivates the inspector 








------------------------------------------------------- Logging in AWS for security and compliance


• To help compliance requirements,AWS provides many service-specific security and audit logs

• Service Logs include:

        • CloudTrail trails - trace all API calls

        • Config Rules - for config & compliance over time

        • CloudWatch Logs - for full data retention

        • VPC Flow Logs - IP traffic within your VPC

        • ELB Access Logs - metadata of requests made to your load balancers

        • CloudFront Logs - web distribution access logs

        • WAF Logs - full logging of all requests analyzed by the service


• Logs can be analyzed using AWS Athena if they’re stored in S3

• You should encrypt logs in S3, control access using IAM & Bucket Policies, MFA

• Move Logs to Glacier for cost savings

• Read whitepaper if interested at: https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitep aper.pdf







------------------------------------------------------- Amazon GuardDuty




• Intelligent Threat discovery to protect your AWS Account , (malicious activities)

• Uses Machine Learning algorithms, anomaly detection, 3rd party data

• One click to enable (30 days trial), no need to install software

• Input data includes:

  • CloudTrail Events Logs – unusual API calls, unauthorized deployments

     • CloudTrailManagementEvents–createVPCsubnet,createtrail,... 
     • CloudTrailS3DataEvents–getobject,listobjects,deleteobject,...

  • VPC Flow Logs – unusual internal traffic, unusual IP address

  • DNS Logs – compromised EC2 instances sending encoded data within DNS queries 

  • Optional Features – EKS Audit Logs, RDS & Aurora, EBS, Lambda, S3 Data Events...

• Can setup EventBridge rules to be notified in case of findings

• EventBridge rules can target AWS Lambda or SNS

• Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)



VPC Flow Logs / CloudTrail Logs / DNS Logs (AWS DNS) 

optional features = / S3 Logs / EBS Volumes / RDS & Aurora Login Activity / Lambda Network Activity / EKS Audit Logs & Runtime Monitoring ------ > GuardDuty ---> EventBridge --> SNS / Lambda








------------------------------------------------------- AWS Macie




• Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.

• Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)

S3 Buckets-------(analyze)------> Macie Discover Sensitive Data (PII)-------(notify)------->Amazon EventBridge--------(integrations)---....







------------------------------------------------------- Trusted Advisor 



• No need to install anything – high level AWS account assessment

-- It's going to check for a few things and advise you on them.

        - So the checks can be, for example, 
        
        - do you have EBS Public Snapshots? 
        
        - Or do you have RDS Public Snapshots? Or are you using the root accounts for your accounts? 
        
        - So all these things are checked by Trusted Advisor


• Analyze your AWS accounts and provides recommendation on 6 categories:

 • Cost optimization 
 • Performance
 • Security
 • Fault tolerance
 • Service limits
 • Operational Excellence


• Business & Enterprise Support plan

   • Full Set of Checks
   • Programmatic Access using AWS Support API

 



A video streaming app uses Amazon Kinesis Data Streams for streaming data. The systems administration team needs to be informed of the shard capacity when it is reaching its limits.

How will you configure this requirement?


ANS : Monitor Trusted Advisor service check results with Amazon CloudWatch Events

EXP : 

        - AWS Trusted Advisor checks for service usage that is more than 80% of the service limit.


        https://docs.aws.amazon.com/awssupport/latest/user/service-limits.html#kinesis-shards-per-region







-------------------------------------------------------  Encryption 101 





-- do google for the pictures of encryption for better understanding 


1 Encryption in flight (TLS / SSL)

   • Data is encrypted before sending and decrypted after receiving

   • TLS certificates help with encryption (HTTPS)

   • Encryption in flight ensures no MITM (man in the middle attack) can happen


         client                                                                                            server 

   Client ---(username : admin )------> TLS encryption ----(hbchdubchdubvchutsvhsbcv)------->     TLS Decryption------(username : admin )----Https website           



2  Server-side encryption at rest

  • Data is encrypted after being received by the server

  • Data is decrypted before being sent

  • It is stored in an encrypted form thanks to a key (usually a data key)

  • The encryption / decryption keys must be managed somewhere, and the server must have access to it

  eg : AWS s3 


3  Client-side encryption 

  • Data is encrypted by the client and never decrypted by the server

  • Data will be decrypted by a receiving client

  • The server should not be able to decrypt the data

  • Could leverage Envelope Encryption

 



 ------------------------------------------------------- AWS KMS (Key Management Service)





• Anytime you hear “encryption” for an AWS service, it’s most likely KMS

• AWS manages encryption keys for us

• Fully integrated with IAM for authorization

• Easy way to control access to your data

• Able to audit KMS Key usage using CloudTrail

• Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM...)

• Never ever store your secrets in plaintext, especially in your code!

  • KMS Key Encryption also available through API calls (SDK, CLI)
  • Encrypted secrets can be stored in the code / environment variables






--------------------------------------- KMS KeysTypes :




• KMS Keys is the new name of KMS Customer Master Key

 • Symmetric (AES-256 keys)
        
        • Single encryption key that is used to Encrypt and Decrypt
        • AWS services that are integrated with KMS use Symmetric CMKs
        • You never get access to the KMS Key unencrypted (must call KMS API to use)

• Asymmetric (RSA & ECC key pairs)

        • Public (Encrypt) and Private Key (Decrypt) pair
        • Used for Encrypt/Decrypt, or Sign/Verify operations
        • The public key is downloadable, but you can’t access the Private Key unencrypted
        • Use case: encryption outside of AWS by users who can’t call the KMS API





--------------------------------------- • Types of KMS Keys:


  • AWS Owned Keys (free): SSE-S3, SSE-SQS, SSE-DDB (default key)

  • AWS Managed Key: free (aws/service-name, example: aws/rds or aws/ebs) 

  • Customer managed keys created in KMS: $1 / month

  • Customer managed keys imported (must be symmetric key): $1 / month

  • + pay for API call to KMS ($0.03 / 10000 calls)



• Automatic Key rotation:

  • AWS-managed KMS Key: automatic every 1 year

  • Customer-managed KMS Key: (must be enabled) automatic every 1 year

  • Imported KMS Key: only manual rotation possible using alias







--------------------------------------- Copying Snapshots across regions 



-- So KMS keys are scoped per region.


-- That means that if we have an EBS volume encrypted with KMS key in a region, for example, eu-west-2,

-- then if you want to copy that to a different region we have to do several steps.

-- First of all, we have to take a snapshot of this EBS volume. And if we take a snapshot from an encrypted snapshot then this snapshot itself will also be encrypted with the same KMS key.

-- Then, to copy the snapshot to another region, we need to re-encrypt the snapshot using a different KMS key. And this is something AWS will do for you.

-- But the same KMS key cannot live in two regions. So now we have an EBS snapshot. It's encrypted with KMS with a different key and it lives in another region.

-- Now we restore the snapshot into its own EBS volume with KMS,  and it's KMS key B into the region ap-southeast-2.







--------------------------------------- KMS Key Policies



• Control access to KMS keys, “similar” to S3 bucket policies

• Difference: you cannot control access without them

• Default KMS Key Policy:

  • Created if you don’t provide a specific KMS Key Policy
  • Complete access to the key to the root user = entire AWS account

• Custom KMS Key Policy:

  • Define users, roles that can access the KMS key
  • Define who can administer the key
  • Useful for cross-account access of your KMS key







---------------------------------------------------------- Copying Snapshots across accounts


1. Create a Snapshot, encrypted with your own KMS Key (Customer Managed Key)

2. Attach a KMS Key Policy to authorize cross-account access

3. Share the encrypted snapshot with the target accounts.

4. (in target) Create a copy of the Snapshot, encrypt it with a CMK in your account

5. Create a volume from the snapshot







------------------------- KMS Multi-Region Keys



• Identical KMS keys in different AWS Regions that can be used interchangeably

• Multi-Region keys have the same key ID, key material, automatic rotation...

• Encrypt in one Region and decrypt in other Regions

• No need to re-encrypt or making cross-Region API calls

• KMS Multi-Region are NOT global (Primary + Replicas)

• Each Multi-Region key is managed independently

• Use cases: global client-side encryption, encryption on Global DynamoDB, Global Aurora









---------------------------------------------------------- KMS Hands ON



-- open KMS in console --> customer - managed Keys --->  Symmetric ----> Encrypt and decrypt ---> KMS - recommended ----> Single-region key --> alias = give anyname ---> nxt ---> nxt ---> create 

-- now do some data encrypt and decrypt 

-- open cloudshell create one folder like kms 

-- cd kms --> create onefile "ExampleSecretFile.txt" and enter some secrets in that file , eg: supersecret

-- u have to cmnds 



# 1) encryption
aws kms encrypt --key-id alias/tutorial --plaintext fileb://ExampleSecretFile.txt --output text --query CiphertextBlob  --region ap-south-1 > ExampleSecretFileEncrypted.base64

# base64 decode for Linux or Mac OS 
cat ExampleSecretFileEncrypted.base64 | base64 --decode > ExampleSecretFileEncrypted

# base64 decode for Windows
certutil -decode .\ExampleSecretFileEncrypted.base64 .\ExampleSecretFileEncrypted


# 2) decryption

aws kms decrypt --ciphertext-blob fileb://ExampleSecretFileEncrypted   --output text --query Plaintext > ExampleFileDecrypted.base64  --region ap-south-1

# base64 decode for Linux or Mac OS 
cat ExampleFileDecrypted.base64 | base64 --decode > ExampleFileDecrypted.txt


# base64 decode for Windows
certutil -decode .\ExampleFileDecrypted.base64 .\ExampleFileDecrypted.txt



-- give cmnds without any gaps at the starting point 

--  now perform 1st cmnd , this will create one file , ExampleSecretFileEncrypted.base64 , do cat this , this represents my encrypted file.

        aws kms encrypt --key-id alias/tutorial --plaintext fileb://ExampleSecretFile.txt --output text --query CiphertextBlob  --region ap-south-1 > ExampleSecretFileEncrypted.base64


-- now decode 

       cat ExampleSecretFileEncrypted.base64 | base64 --decode > ExampleSecretFileEncrypted

     - this will create "ExampleSecretFileEncrypted " this file 

     - So this is the kind of secret file that you would share with someone.


-- now decrypt the file 

      aws kms decrypt --ciphertext-blob fileb://ExampleSecretFileEncrypted   --output text --query Plaintext > ExampleFileDecrypted.base64  --region ap-south-1


      - this will create "ExampleFileDecrypted.base64" 

      - cat ExampleFileDecrypted.base64


-- now decode 

      cat ExampleFileDecrypted.base64 | base64 --decode > ExampleFileDecrypted.txt


      - now do cat ExampleFileDecrypted.txt , u will see the secret 










---------------------------------------------------------- KMS Automatic Key Rotation



• AWS-managed KMS Keys: automatically rotated every 1 year

• For Customer-Managed Symmetric KMS Key

        • Automatic key rotation is optionally enabled

        • Customize Rotation Period between 90 and 2560 days (default: 365 days)

        • Previous key is kept active so you can decrypt old data

        • New Key has the same KMS Key ID (only the backing key is changed)






---------------------------------------------------------- KMS On-Demand Key Rotation



• For Customer-Managed Symmetric KMS Key (not AWS managed CMK)

• Does NOT require Automatic Key Rotation to be enabled

• Does NOT change existing Automatic Rotation schedules        

• Limit to how many times you can trigger an on-demand key rotation




---------------------------------------------------------- KMS Manual Key Rotation (Customer-Managed Symmetric KMS Key & Imports)



• When you want to rotate key (example: every month)

• New Key has a different KMS Key ID

• Keep the previous key active so you can decrypt old data

• Better to use aliases in this case (to hide the change of key for the application)

• Good solution to rotate KMS Key that are not eligible for automatic rotation (like asymmetric KMS Key)





---------------------------------------------------------- KMS Alias Updating



• Better to use aliases in this case (to hide the change of key for the application)





---------------------------------------------------------- Changing The KMS Key For An Encr ypted EBS Volume  (KMS for SysOps)



• You can’t change the encryption keys used by an EBS volume , if u want to do so , then 

• Create an EBS snapshot and create a new EBS volume and specify the new KMS key





---------------------------------------------------------- Sharing KMS Encrypted RDS DB Snapshots



• You can share RDS DB snapshots encrypted with KMS CMK with other accounts, but must first share the KMS CMK with the target account using Key Policy




---------------------------------------------------------- KMS Key Deletion Considerations



• Schedule CMK for deletion with a waiting period of 7 to 30 days

        • CMK’s status is “Pending deletion” during the waiting period


• During the CMK’s deletion waiting period:

        • The CMK can’t be used for cryptographic operations (e.g., can’t decrypt KMS- encrypted objects in S3 – SSE-KMS)

        • The key is not rotated even if planned


• You can cancel the key deletion during the waiting period

• Consider disabling your key instead of deleting it if you’re not sure!





---------------------------------------------------------- KMS Key Deletion – CloudWatch Alarm


• Use CloudTrail, CloudWatch Logs, CloudWatch Alarms and SNS to be notified when someone tries to use a CMK that’s ”Pending deletion” in a cryptographic operation (Encrypt, Decrypt, ...)








---------------------------------------------------------- CloudHSM


• KMS => AWS manages the software for encryption , and will have control over the encryption keys.

• CloudHSM => AWS provisions encryption hardware , AWS will provision some encryption hardware.

              - It's called an HSM device, so a dedicated hardware which is a hardware security module.

• Dedicated Hardware (HSM = Hardware Security Module)

• You manage your own encryption keys entirely (not AWS)


- The HSM device is going to be set up within the cloud of AWS,

• HSM device is tamper resistant, FIPS 140-2 Level 3 compliance

• Supports both symmetric and asymmetric encryption (SSL/TLS keys)

• No free tier available

• Must use the CloudHSM Client Software

• Redshift supports CloudHSM for database encryption and key management

• Good option to use with SSE-C encryption


- IAM permissions:

       • CRUD an HSM Cluster

- CloudHSM Software:

       • Manage the Keys
       • Manage the Users





---------------------------------------------------------- CloudHSM High Availability


• CloudHSM clusters are spread across Multi AZ (HA)

• Great for availability and durability
 



---------------------------------------------------------- CloudHSM – Integration with AWS Services


• Through integration with AWS KMS

• Configure KMS Custom Key Store with CloudHSM

• Example: EBS, S3, RDS ...

        
       





---------------------------------------------------------- AWS Artifact (not really a service)




• Portal that provides customers with on-demand access to AWS compliance documentation and AWS agreements

• Artifact Reports - Allows you to download AWS security and compliance documents from third-party auditors, like AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports

• Artifact Agreements - Allows you to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA) or the Health Insurance Portability and Accountability Act (HIPAA) for an individual account or in your organization

• Can be used to support internal audit or compliance




---------------------------------------------------------- AWS Certificate Manager (ACM)



• Easily provision, manage, and deploy TLS Certificates

• Provide in-flight encryption for websites (HTTPS)

• Supports both public and private TLS certificates

• Free of charge for public TLS certificates

• Automatic TLS certificate renewal

• Integrations with (load TLS certificates on)

  • Elastic Load Balancers(CLB,ALB,NLB)
  • CloudFront Distributions 
  • APIs on API Gateway

• Cannot use ACM with EC2 (can’t be extracted)







-------------------------- ACM – Requesting Public Certificates




1 List domain names to be included in the certificate

   • Fully Qualified Domain Name (FQDN): corp.example.com
   • WildcardDomain:*.example.com


2 Select Validation Method: DNS Validation or Email validation

   • DNS Validation is preferred for automation purposes
   • Email validation will send emails to contact addresses in the WHOIS database
   • DNS Validation will leverage a CNAME record to DNS config (ex: Route 53)

3 It will take a few hours to get verified

4 The Public Certificate will be enrolled for automatic renewal

   • ACM automatically renews ACM-generated certificates 60 days before expiry






------------------------- ACM – Importing Public Certificates


• Option to generate the certificate outside of ACM and then import it

• No automatic renewal, must import a new certificate before expiry

• ACM sends daily expiration events starting 45 days prior to expiration

    • The # of days can be configured
    • Events are appearing in EventBridge

• AWS Config has a managed rule named acm-certificate-expiration-check to check for expiring certificates (configurable number of days) ---- IMP for exam 





---------------------------------------------------------- API Gateway - Endpoint Types


• Edge-Optimized (default): For global clients

        • Requests are routed through the CloudFront Edge locations (improves latency)

        • The API Gateway still lives in only one region


• Regional:

        • For clients within the same region

        • Could manually combine with CloudFront (more control over the caching strategies and the distribution)


• Private:


        • Can only be accessed from your VPC using an interface VPC endpoint (ENI)

        • Use a resource policy to define access




---------------------------------------------------------- ACM – Integration with API Gateway


• Create a Custom Domain Name in API Gateway

• Edge-Optimized (default): For global clients

        • Requests are routed through the CloudFront Edge locations (improves latency)

        • The API Gateway still lives in only one region

        • TheTLS Certificate must be in the same region as CloudFront, in us-east-1

        • Then setup CNAME or (better)A - Alias record in Route53

• Regional:

        • For clients within the same region

        • The TLS Certificate must be imported on APIGateway,in the same region as the API Stage

        • Then setup CNAME or (better)A - Alias record in Route53







---------------------------------------------------------- SSM Parameter Store





• Secure storage for configuration and secrets

• Optional Seamless Encryption using KMS

• Serverless, scalable, durable, easy SDK

• Version tracking of configurations / secrets

• Security through IAM

• Notifications with Amazon EventBridge

• Integration with CloudFormation





---------------------------------------------------------- SSM Parameter Store Hierarchy


• /my-department/
    • my-app/
        • dev/
            • db-url
            • db-password

        • prod/
            • db-url
            • db-password

• other-app/



- You also have the opportunity to access Secrets of Secrets Manager through the Parameter Store by using this reference right here.

        • /aws/reference/secretsmanager/secret_ID_in_Secrets_Manager


- there are something called Public Parameters that are issued by AWS that you can use.

        • /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 (public)
           







---------------------------------------------------------- SSM Parameter Standard and advanced parameter tiers




Standard :

- Total number of parameters allowed (per AWS account and Region) :  10,000

- Maximum size of a parameter value                               :  4 KB

- Parameter policies available                                    :  No

- Cost                                                            :  No additional charge

- Storage Pricing                                                 :  Free






Advanced :

- Total number of parameters allowed (per AWS account and Region) :  100,000

- Maximum size of a parameter value                               :  8 KB

- Parameter policies available                                    : yes

- Cost                                                            :  charges Apply

- Storage Pricing                                                 :  $0.05 per advanced parameter per month






---------------------------------------------------------- Parameters Policies (for advanced parameters)



• Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as passwords

• Can assign multiple policies at a time




1 Expiration (to delete a parameter) : 

{
    "Type": "Expiration",
    "Version": "1.0",
    "Attributes": {
        "Timestamp": "2018-12-02T21:34:33.000Z"
    }
}





2 ExpirationNotification (EventBridge)



{
    "Type": "ExpirationNotification",
    "Version": "1.0",
    "Attributes": {
        "Before": "15",
        "Unit": "Days"
    }
}


-- So in this example, 15 days before the parameter expires we'll receive notification in EventBridge which gives us enough time to actually update it

-- and make sure the parameter is not getting deleted because of the TTL.





3 NoChangeNotification (EventBridge)


{
    "Type": "NoChangeNotification",
    "Version": "1.0",
    "Attributes": {
        "After": "20",
        "Unit": "Days"
    }
}


-- maybe sometimes you wanna make sure the parameters change once in a while.

-- So you can have a no change notification in EventBridge so that if a parameter has not been updated for 20 days, then you will be notified as well.








---------------------------------------------------------- SSM Parameter Store Hands ON CLI



-- open ssm in console --> choose parameter store on left side panel ---> give path to store value (/my-app/dev/db-url) ---> string --> value = dev.database.subbu.com:3306 --> create parameter

-- dev.database.subbu.com:3306 = u can give any value 

-- now do create dev password
  
-- give path to store value (/my-app/dev/db-password) ---> securestring --> value = give password here --> KMS Key ID = i am using my own key (eg: tutorial) -----> create parameter       

-- now do create for prod environment also same like as Dev

EG : /my-app/prod/db-url , /my-app/prod/db-password


-- So we are going to use this CLI to get the parameters.

-- open cloudshell

       aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password


-- for the password it's a SecureString, and here is the value of it, which is an encrypted value.

-- So for this, you basically need to decrypt it.

-- for this you have a special parameter and it's called with-decryption,

-- so this will check whether or not I have the KMS permission to decrypt this secret that was encrypted with the KMS tutorial key.

         aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password --with-decryption


-- now observe changes 


  aws ssm get-parameters-by-path --path /my-app/dev     =  u will get all parameters from specific path if u want 

   aws ssm get-parameters-by-path --path /my-app/ --recursive  --with-decryption     = u will get all parameters under /my-app/ 









---------------------------------------------------------- SSM Parameter Store Hands ON with LAMBDA 


-- create one function with py 3.8 runtime 




import json

import boto3

ssm = boto3.client('ssm', region_name="ap-south-1")

def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"])
    print(db_password)
    return "Worked!"



-- now go to configuration ---> permissons --->  create inline policy --> system manager ---> give all access --> all resources --> nxt 

-- now do refresht he lambda page 

-- if u get errror , after adding permissions , then wait for 5 min 

-- now u will get this

-- u can see 'SecureString' is encrypted here 

-- So what we'd like to do is now decrypt it,

-- so in code for password decrypt , add 

        db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"], WithDecryption = True)


-- now do test , u will get (AccessDeniedException) error 

-- because we're not allowed to use the customer master key and decrypt our secrets.

-- So it turns out that because having given KMS access to my IAM role we're not allowed to decrypt the secrets,

-- so this is a good proof that even though I have access to this database password, because it's encrypted and I don't have access to KMS I'm not able to decrypt it,

-- and so that DB password is really safe and secure.

-- to fix this add permissions 

     permissons --> create inline policy --> kms --> add all permissions --> all resources--> create 


-- now do test , u will get decrypted values 


-- now to access through the Environment Variables 

-- create  Environment Variable --> DEV_OR_PROD	 = dev

-- add this in code 



import json

import boto3
import os 

ssm = boto3.client('ssm', region_name="ap-south-1")
dev_or_prod = os.environ['DEV_OR_PROD']


def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-password"], WithDecryption = True)
    print(db_password)
    return "Worked!"




-- if u test this u will get dev values 

--  go to env variable , change to prod (DEV_OR_PROD	 = prod )

--  do test again , u will prod values 

        









---------------------------------------------------------- AWS Secrets Manager ------------------------



• Newer service, meant for storing secrets

• Capability to force rotation of secrets every X days

• Automate generation of secrets on rotation (uses Lambda)

• Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)

• Secrets are encrypted using KMS

• Mostly meant for RDS integration






---------------------------------------------------------- AWS Secrets Manager – Multi-Region Secrets


• Replicate Secrets across multiple AWS Regions

• Secrets Manager keeps read replicas in sync with the primary Secret

• Ability to promote a read replica Secret to a standalone Secret

• Use cases: multi-region apps, disaster recovery strategies, multi-region DB...






---------------------------------------------------------- AWS Secrets Manager Hands ON


-- open secret manager --> store a new secret ---> Other type of secret ---> give key and values ----> select ur KMS key or default ----> Secret name = prod/my-key-api --> create 





---------------------------------------------------------- Secrets Manager – Monitoring



• CloudTrail captures API calls to the Secrets Manager API

• CloudTrail captures other related events that might have a security or compliance impact on your AWS account or might help you troubleshoot operational problems.

• CloudTrail records these events as non-API service events.

        • RotationStarted event

        • RotationSucceeded event

        • RotationFailed event

        • RotationAbandoned event – a manual change to a secret instead of automated rotation

        • StartSecretVersionDelete event

        • CancelSecretVersionDelete event

        • EndSecretVersionDelete event



• Combine with CloudWatch Logs and CloudWatch alarms for automations










---------------------------------------------------------- SSM Parameter Store vs Secrets Manager




• Secrets Manager ($$$): 

       • Automatic rotation of secrets with AWS Lambda

       • Lambda function is provided for RDS, Redshift, DocumentDB

       • KMS encryption is mandatory

       • Can integration with CloudFormation


• SSM Parameter Store ($):

       • Simple API

       • No secret rotation (can enable rotation using Lambda triggered by EventBridge)

       • KMS encryption is optional

       • Can integration with CloudFormation

       • Can pull a Secrets Manager secret using the SSM Parameter Store API









============================================================== Identity ==============================================================





---------------------------------------------------------- IAM Security Tools


• IAM Credentials Report (account-level)

        • a report that lists all your account's users and the status of their various credentials

        - go to IAM Dashboard --> credential Report


• IAM Access Advisor (user-level)

        • Access advisor shows the service permissions granted to a user and when those services were last accessed.

        • You can use this information to revise your policies.

        - Go to IAM Dashboard --> users --> select users u want --> access Advisor (LAst Accessed)






---------------------------------------------------------- IAM Access Analyzer



• Find out which resources are shared externally


        • S3 Buckets
        • IAM Roles
        • KMS Keys
        • Lambda Functions and Layers 
        • SQS queues
        • Secrets Manager Secrets


• Define Zone of Trust = AWS Account or AWS Organization

• Access outside zone of trusts => findings




---------------------------------------------------------- What’s Identity Federation?



• Federation lets users outside of AWS to assume temporary role for accessing AWS resources.

• These users assume identity provided access role.

• Federation assumes a form of 3rd party authentication


        • Microsoft Active Directory (~= SAML) 
        • Single Sign On
        • Open ID
        • Cognito
        • LDAP


• Using federation, you don’t need to create IAM users (user management is outside of AWS)






---------------------------------------------------------- SAML Federation For Enterprises



• To integrate Active Directory / ADFS with AWS (or any SAML 2.0)

• Provides access to AWS Console or CLI (through temporary creds)

• No need to create an IAM user for each of your employees


https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html

https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html




if you don't have a SAML 2.0 way of identifying your users then you need to use custom identity broker.






---------------------------------------------------------- Custom Identity Broker Application For Enterprises


• Use only if identity provider is not compatible with SAML 2.0

• The identity broker must determine the appropriate IAM policy



https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_ common-scenarios_federated-users.html






---------------------------------------------------------- AWS Cognito - Federated Identity Pools For Public Applications




• Goal:

        • Provide direct access to AWS Resources from the Client Side


• How:

        • Log in to federated identity provider – or remain anonymous

        • Get temporary AWS credentials back from the Federated Identity Pool

        • These credentials come with a pre-defined IAM policy stating their permissions


• Example:

        • provide (temporary) access to write to S3 bucket using Facebook Login


• Note:

        • Web Identity Federation is an alternative to using Cognito but AWS recommends against it






---------------------------------------------------------- AWS STS – Security Token Service



• Allows to grant limited and temporary access to AWS resources.

• Token is valid for up to one hour (must be refreshed)

• AssumeRole

        • Within your own account: for enhanced security

        • Cross Account Access: assume role in target account to perform actions there


• AssumeRoleWithSAML

        • return credentials for users logged with SAML


• AssumeRoleWithWebIdentity

        • return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible...)

        • AWS recommends against using this, and using Cognito instead


• GetSessionToken

        • for MFA, from a user or AWS account root user




• GetFederationToken: obtain temporary creds for a federated user

• GetCallerIdentity: return details about the IAM user or role used in the API call

• DecodeAuthorizationMessage: decode error message when an AWS API is denied





----------------------------------------------------- Using STS to Assume a Role



• Define an IAM Role within your account or cross-account

• Define which principals can access this IAM Role and we authorize everything with IAM policies.

• Use AWS STS (Security Token Service) to retrieve credentials and impersonate the IAM Role you have access to (AssumeRole API)

• Temporary credentials can be valid between 15 minutes to 1 hour






----------------------------------------------------- STS with MFA


• Use GetSessionToken from STS

• Appropriate IAM policy using IAM Conditions

• aws:MultiFactorAuthPresent:true

        - For example, this role only allows us to stop instances or terminate instances only if we have MFA on,

        - so MultiFactorAuthPresent:true. And this is how we would use it.

• Reminder, GetSessionToken returns:

      • Access ID
      • Secret Key
      • SessionToken
      • Expiration date






----------------------------------------------------- Cross-Account Access with STS



-- let's say we have a Dev account and a Prod account and we have an S3 bucket in Prod that we want to access from IAM users in a group in Dev.

-- So the first thing we have to do is to create an IAM role in the production account And this IAM role is going to have access to the S3 buckets.

-- Next, we grant in the IAM group permissions, to assume the UpdateApp IAM role from the other accounts.

-- So we're going to be able to assume this role and then we're going to get back STS Role Credentials.

-- And then these credentials will allow us to access the bucket in the target accounts.

-- So this IAM role, that we define in the production accounts, must also of course, authorize the development accounts to assume it.

-- so if you have an IAM group of testers that are not authorized to assume this role then they cannot access your production account.









============================================================================== Amazon Cognito ==================================================




• Give users an identity to interact with our web or mobile application

-- So these users usually sit outside of our AWS account, hence the name Cognito, because it gives an identity to users that we don't know about yet.

-- So we have two kind of sub-services within Cognito.


1 • Cognito User Pools:

      • Sign in functionality for app users

      • Integrate with API Gateway & Application Load Balancer


2 • Cognito Identity Pools (Federated Identity):

      • Provide AWS temporarily credentials to users so they can access some AWS resources directly

      • Integrate with Cognito User Pools as an identity provider




• Cognito vs IAM: “hundreds of users”, ”mobile users”, “authenticate with SAML” 

       - So if you are asking yourself don't we already have users in IAM?

       - But Cognito is gonna be for your web and mobile application users, which sits outside of AWS.

       - So look for the keywords such as hundreds of users or mobile users, or authentication with whatever mechanisms such as SAML and so on.






----------------------------------------------------------------- Cognito User Pools (CUP) – User Features


• Create a serverless database of user for your web & mobile apps

• Simple login: Username (or email) / password combination

• Password reset

• Email & Phone Number Verification

• Multi-factor authentication (MFA)

• Federated Identities: users from Facebook, Google, SAML...

• Login sends back a JSON Web Token (JWT)

      - when your users log in with the Cognito User Pool, what they get back from the API is a JWT, so a JSON Web Token.






----------------------------------------------------------------- Cognito User Pools (CUP) - Integrations


• CUP integrates with API Gateway and Application Load Balancer





--------------------------------------------------------------- CUP LAB 






 Cognito User Pools (CUP) 

 -- create user pool 

 -- open console and create user pool

 -- Cognito user pool sign-in options --> email 

 --  step 2 Required attributes --> add any one for user to register --> name 

 -- step 3 --> Send email with Cognito 

 -- step 5 --> name of userpool --> enable hosted UI  --> give domain name any thing eg demo-user-pool-cognito

 -- Initial app client --> public client name --> Allowed callback URL = localhost or anything like www.flipkart.com

 -- once u created usewr pool --> open user pool ,u can see users and groups 

 -- create user --> set password , once u create user pool 

 --  go to google and search for amazon cognito ui --> view signup page in documentation 

 https://<your domain>/oauth2/authorize?response_type=code&client_id=<your app client id>&redirect_uri=<your callback url>

 -- go to user pools --> app integration -->  change ur details in the above link 

 
https://demo-user-pool-66.auth.ap-south-1.amazoncognito.com/oauth2/authorize?response_type=code&client_id=juroqdd4a95dac05aom4dvadj&redirect_uri=https://www.amazon.com

-- u will get domain name  in APP integration section

-- once the url is ready , paste the link in the browser 

-- this will pop-up sign in page , this is called hosted UI , enter ur details 

-- once u enter it will ask u to change password 

-- once u enter details , u will redirected to the link which u have given in call back url 

--  so delete cookies in ur browser and try to login again 

-- now try to sign up as new user and see how it works 

-- u can also do MFA for another layer of security 






----------------------------------------------------------------- Cognito User Pools – Lambda Triggers



• CUP can invoke a Lambda function synchronously on these triggers:


1 Authentication Events : 


        - Pre Authentication Lambda Trigger : Custom validation to accept or deny the sign-in request

        - Post Authentication Lambda Trigger : Event logging for custom analytics

        - Pre Token Generation Lambda Trigger : Augment or suppress token claims


2 Sign-Up

        - Pre Sign-up Lambda Trigger : Custom validation to accept or deny the sign-up request

        - Post Confirmation Lambda Trigger : Custom welcome messages or event logging for custom analytics

        - Migrate User Lambda Trigger : Migrate a user from an existing user directory to user pools


3 Messages : 

        - Custom Message Lambda Trigger : Advanced customization and localization of messages


4 Token Creation : 

        - Pre Token Generation Lambda Trigger : Add or remove attributes in Id tokens







----------------------------------------------------------------- Cognito User Pools – Hosted Authentication UI


• Cognito has a hosted authentication UI that you can add to your app to handle sign-up and sign-in workflows

• Using the hosted UI, you have a foundation for integration with social logins, OIDC or SAML

• Can customize with a custom logo and custom CSS





----------------------------------------------------------------- CUP – Hosted UI Custom Domain


• For custom domains, you must create an ACM certificate in "us-east-1"

         - So the trick you need to know is that if you're using custom domains with Cognito User Pools, regardless of where it is created, 
         
         - you must create an certificate for using HTTPS. That certificate must be in ACM, 


• The custom domain must be defined in the “App Integration” section

         - because this is going to be a configuration for all app claims. This is a general configuration.






----------------------------------------------------------------- CUP – Adaptive Authentication



• Block sign-ins or require MFA if the login appears suspicious

• Cognito examines each sign-in attempt and generates a risk score (low, medium, high) for how likely the sign-in request is to be from a malicious attacker

• Users are prompted for a second MFA only when risk is detected

• Risk score is based on different factors such as if the user has used the same device, location, or IP address

• Checks for compromised credentials, account takeover protection, and phone and email verification

• Integration with CloudWatch Logs (sign-in attempts, risk score, failed challenges...)





----------------------------------------------------------------- Decoding a ID Token; JWT – JSON Web Token


• CUP issues JWT tokens (Base64 encoded): 

   • Header
   • Payload
   • Signature
   - so Base64 is allowing you to transmit that over the network easily,   


• The signature must be verified to ensure the JWT can be trusted

• Libraries can help you verify the validity of JWT tokens issued by Cognito User Pools

• The Payload will contain the user information (sub UUID, given_name, email, phone_number, attributes...)

• From the sub UUID, you can retrieve all users details from Cognito / OIDC



<header>.
{
   "sub":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "device_key": "aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "cognito:groups":[
      "testgroup"
   ],
   "iss":"https://cognito-idp.us-west-2.amazonaws.com/us-west-2_example",
   "version":2,
   "client_id":"xxxxxxxxxxxxexample",
   "origin_jti":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "event_id":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "token_use":"access",
   "scope":"phone openid profile resourceserver.1/appclient2 email",
   "auth_time":1676313851,
   "exp":1676317451,
   "iat":1676313851,
   "jti":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "username":"my-test-user"
}
.<token signature>





----------------------------------------------------------------- Application Load Balancer – Authenticate Users



• Your Application Load Balancer can securely authenticate users

    • Offload the work of authenticating users to your load balancer

    • Your applications can focus on their business logic


-- So you can authenticate in multiple ways for your application load balancer.    



• Authenticate users through:

        1 Identity Provider (IdP): OpenID Connect (OIDC) compliant

        2 Cognito User Pools:

              • SocialIdPs,such as Amazon,Facebook,or Google

              • Corporate identities using SAML, LDAP, or MicrosoftAD


-- So the first one, first option is to integrate directly without Cognito user pools              


• Must use an HTTPS listener to set authenticate-oidc & authenticate-cognito rules

• OnUnauthenticatedRequest – authenticate (default), deny, allow






----------------------------------------------------------------- Application Load Balancer – Cognito Auth.







           1. GET /api/data                                                3. GET /api/data
Users ---------------------------------> Application Load Balancer ------------------------------->  Amazon ECS

                                                   |
                                                   |
                                                   |
                          2. authenticate          |            HTTPS listener Action: authenticate-cognito
                                                   |
                                                   |
                                                   |

                                              Amazon Cognito




-- let's say we have an application load balancer connected to Amazon ECS and we want to implement a login through Amazon Cognito

-- the users will do GET/api/data. Then the ALB is going to be set up with HTTPS with the action authenticate-cognito.

-- So Cognito will do its thing and authenticate the user, 

-- and then the payload and the request is passed on to Amazon ECS with the added information of the user that was doing the request from Cognito.







----------------------------------------------------------------- ALB – Auth through Cognito User Pools


• Create Cognito User Pool, Client and Domain

• Make sure an ID token is returned

• Add the social or Corporate IdP if needed

• Several URL redirections are necessary

• Allow your Cognito User Pool Domain on your IdP app's callback URL.  for example 

      • https://domain- prefix.auth.region.amazoncognito.com/saml2/ idpresponse

      • https://user-pool-domain/oauth2/idpresponse





----------------------------------------------------------------- Application Load Balancer – OIDC Auth.


see fig 






----------------------------------------------------------------- ALB – Auth.Through an Identity Provider (IdP) That is OpenID Connect (OIDC) Compliant




• Configure a Client ID & Client Secret

• Allow redirect from OIDC to your Application Load Balancer DNS name (AWS provided) and CNAME (DNS Alias of your app)

• https://DNS/oauth2/idpresponse

• https://CNAME/oauth2/idpresponse





----------------------------------------------------------------- Cognito Identity Pools (Federated Identities) -------------



• Get identities for “users” so they obtain temporary AWS credentials

       - So, our users are outside of our AWS environment, and there can be a web application users, or mobile users, and they want access to stuff within our AWS environments.

       - So for example, they want to have access to DynamoDB table, or an S3 bucket. And so to access these things, they need temporary AWS credentials.

       - So we cannot create normal IAM users for these users because there are too many of them, it doesn't scale, and we don't trust them.

       - So instead, we're going to give these users access to AWS through a Cognito Identity Pool.


-- So this identity pool can allow your users to log in through a trusted third party.       

• Your identity pool (e.g identity source) can include:

      • Public Providers (Login with Amazon, Facebook, Google, Apple)

      • Users in an Amazon Cognito user pool

      • OpenID Connect Providers & SAML Identity Providers

      • Developer Authenticated Identities (custom login server)

      • Cognito Identity Pools allow for unauthenticated (guest) access



• Users can then access AWS services directly or through API Gateway

      - So, once the users have obtained these AWS credentials, then they can access the AWS services directly,

      - with an API call using an SDK, or through the API gateway.

      • The IAM policies applied to the credentials are defined in Cognito

      • They can be customized based on the user_id for fine grained control







-------------------------- web identity federation and token management 

-- when ever u are login into website some websites will ask u to login through the google , or facebook or gmail etc this is called Identity federation 

-- u can do in sign-in experience page in user pool 

-- u can create ur own log in page using Amplify , and other SDk's


----------------------- LAB , we are login through the SAML 2.0

-- now demo 

--- open IAM Identity center

--  choose Organization

-- Confirm your identity source = Identity Center directory  , if u want to change u can like Active direcory , etc ..

-- now create user in IICenter , IAM users and IIC users are different , copy details of user 

-- now open cognito --> select add user directories to ur app in drop down list --> create userpool --> choose Federated identity providers --> email --> SAML --> no MFA , this is not applicable for the federation users --> nxt --> Send email with Cognito --> skip Step 4 for now(we will do after creating SAML application in IIC) --> give username for pool --> domain name --> client name --> give callback URL --> advanced app client settings --> OAuth 2.0 grant type = choose only Implicit grant --> create userpool

-- now copy the user pool id , we will use them in SAML creation in IIC 

-- now create SAML 2.O application in IIC 

-- go to IIC --> choose application on left side --> add application --> I have an application I want to set up --> SAMl 2.O --> copy link of "IAM Identity Center SAML metadata file" --> Application ACS URL = <cognito domain url>/saml2/idpresponse (eg : https://myuserpool123.auth.ap-south-1.amazoncognito.com/saml2/idpresponse)--> Application SAML audience = urn:amazon:cognito:sp:<cognito pool id> 

-- once u submit --> under actions --> edit attribute mapping --> 

${user:subject}    format = persistent 

-- add new attribute 

email     ${user:email}    format = basic 

-- save changes 

-- now choose assign users --> select user that we have created --> assign user

-- let's configure IAM identity center as a SAML identity provider n cognito user pool 

-- go to cognito in console --> inside user pool --> sign-in experience --> Federated identity provider sign-in =  add Federated identity provider = SAML --> name of ur SAML --> Metadata document source = Enter metadata document endpoint URL (paste url that u have copied while creating SAML , eg : https://portal.sso.ap-south-1.amazonaws.com/saml/metadata/Mjk4MTMyMzY5NjI5X2lucy0zNzMxZjY2ODg5OTY3NjBh) --> saml attribute provide email--> add identity provider 

-- now u must add this identity provider to userpool app client , to do this 

-- go to cognito in console --> inside user pool --> app integration --> open client --> in hosted UI choose edit --> choose identity provider = SAML name that u have created --> save changes 

-- now do test 

-- go to cognito in console --> inside user pool --> app integration --> open client --> view hosted UI --> here u are do lofin with SAML give credentials and login 

-- it is same like when u want to sign up with some other website it will ask u to sign in with fackbook , google , twitter etc..... same we have done here 

-- by this we have successfully configured IIC as identity provider of ur userpool 







---------------------- Cognito Identity Pools(Federated Identity) which gives temporary access to AWS resources  LAB 


-- use identity as cognito userpools

-- go to identity pools in cognito 

-- create identity --> Authenticated access --> amzn cognito user pool --> create new role ( give any name for ur role )--> user pool details --> Role settings = Use default authenticated role --> Claim mapping = Use default mappings --> name of ur identity pool --> create 

-- make sure u have not blocked pop up for ur browsers

-- go to role created and add s3 full accesss for the demo purpose

-- u can add these policies if u get any error later ( only add when u get error)


1 . cognito policy = name of policy 


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "s3:*",
                "cognito-identity:GetCredentialsForIdentity"
            ],
            "Resource": "*"
        }
    ]
}




2 cognito-s3-permissions (optional coz we have added s3 full access in above step )

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:ListAllMyBuckets",
            "Resource": "*"
        }
    ]
}




-- Trust Relationship 


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "",
            "Effect": "Allow",
            "Principal": {
                "Federated": "cognito-identity.amazonaws.com"
            },
            "Action": [
                "sts:AssumeRoleWithWebIdentity",
                "sts:TagSession"
            ],
            "Condition": {
                "StringEquals": {
                    "cognito-identity.amazonaws.com:aud": "<IdentityPoolId>"
                },
                "ForAnyValue:StringLike": {
                    "cognito-identity.amazonaws.com:amr": "authenticated"
                }
            }
        }
    ]
}





-- now go to userpool ---> client information ---> edit --> 

-- OAuth grant types 
  
      - Authorization code grant
      -  - Implicit grant

-- OpenID Connect scopes

      - aws.cognito.signin.user.admin

      - email
      - openid
      - phone
      - profile

-- now go to postman and add the following info 

-- do not add anyurl in the url tab just leave as blank with GET method


     - type         = Oauth 2.o
     - Token Name   = AccessToken
     - Callback URL = www.amazon.com
     - Auth URL      = https://browser-pool.auth.ap-south-1.amazoncognito.com/oauth2/authorize
     - Access Token URL = https://browser-pool.auth.ap-south-1.amazoncognito.com/oauth2/token
     - Client ID        = get from userpool appclient id
     - Client Secret    =  get from userpool appclient id
     - Scope            =  openid profile
     - Refresh Token URL  = u will get from userpool (Token signing key URL)



 -- add headers :

              Key                 Value
           CONTENT-TYPE            application/x-amz-json-1.1
           X-AMZ-TARGET            com.amazonaws.cognito.identity.model.AWSCognitoIdentityService.GetId

-- https://browser-pool.auth.ap-south-1.amazoncognito.com - this is domain name of ur userpool 

-- u have to add  /oauth2/authorize( Auth URL ) and /oauth2/token (Access Token URL)

-- click on "get new access token " and login using credentials u will get tokens 

-- copy id_token and store it some where 

-- copy access token and go to online jwt token decode in google and paste and check the info

-- go to postman and type = Post and give identity url in the url section eg : https://cognito-identity.ap-south-1.amazonaws.com/  (based on region u can change)

-- add headers like above step 

-- now go to body --> raw --> Text




{
   "IdentityPoolId":"ap-south-1:9750241a-7acb-42b3-919d-54bac1f1fb44",
   "Logins": {
       "cognito-idp.ap-south-1.amazonaws.com/ap-south-1_SH87tNLwB": ""
    }
}




-- IdentityPoolId - u will get from identity pools

-- cognito-idp.ap-south-1.amazonaws.com  ( replace ur region)

-- /ap-south-1_SH87tNLwB -- u will get this from Identity pools --> UserAccess --> Identity provider 

-- click on post , the identityId will create and also do check in console also (identity pools --> identity browser )

-- once u do all correct u will get one o/p like this 


{
    "IdentityId": "ap-south-1:456a12f0-dda6-cf0a-f6fa-705b538d8a59"
}



-- go to aws cli and use below cmnds

-- replace the values below cmnd (https://repost.aws/knowledge-center/cognito-identity-pool-grant-user-access)

         aws cognito-identity get-credentials-for-identity --identity-id region:abcdef --logins cognito-idp.example_region.amazonaws.com/example_cognito_user_pool_id = example_cognito_user_id_token


-- eg it will look like this 




aws cognito-identity get-credentials-for-identity --identity-id us-east-1:456a12f0-dd26-cbc6-4360-7a3c189b8725 --logins cognito-idp.us-east-1.amazonaws.com/us-east-1_dh67H1QNF=eyJraWQiOiJrY3pmTTkwd0ZYd3JGUG0weWtqNTdUaVRuTnJ1Z3R1NmpiMm9NRTBvN2FjPSIsImFsZyI6IlJTMjU2In0.eyJhdF9oYXNoIjoiRFJoLVNneWtRQ1BuaURxa0IwZlA1dyIsInN1YiI6ImE0ZDhmNGE4LTkwNTEtNzAxNy04ZDljLTU1NGEzMGM2M2JhNiIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV9kaDY3SDFRTkYiLCJjb2duaXRvOnVzZXJuYW1lIjoic2F0aHlhIiwib3JpZ2luX2p0aSI6IjRlY2I2NjZjLTUwNWQtNDZkYS1iZWQ3LWM4MDQ5Mzk3YWJkMyIsImF1ZCI6Imsya2pzMmppZWUxMXY2aGRwMG9iOXRxazEiLCJldmVudF9pZCI6IjdiMzg3ODVjLWExNmYtNDIzMS1hODU5LTI3YjM4OTk1Y2NmZiIsInRva2VuX3VzZSI6ImlkIiwiYXV0aF90aW1lIjoxNzI1MTk1MTk5LCJleHAiOjE3MjUxOTg3OTksImlhdCI6MTcyNTE5NTE5OSwianRpIjoiMTI0MjkwYzAtNmM4OS00NTMyLTgwNTgtYWJkYTIyMTY3ZGUxIiwiZW1haWwiOiJzYXRoeWFkaGFybWE2NjY2QGdtYWlsLmNvbSJ9.JZS1u5Q0JxS0cECF9Djla5rNEN4dY4U6mVWXpaRomdpXgykfkZ0AtoO9WZLsXl37yLACRMNK0HQJyB4QahMWVgHoWfwtPCz6QkwRFKBVMbPOytxaXs0xyuNEMJh1-KP64JeC-anG8i3zCJTLWh9ziCTad4hBgkfopyAovQ5bGgHKDo7lpENGObpoq-eNSpU4etKClcAVPGdjcZS50N61sCjP7Nc1iuV98d9CbP2ViZNadbatHxv8yc9I7mmwBFrYbX8xefI4uNKz5im_AvsbUnXjFEk_tyoDtB-VgogXPcFnYOVffDKx2xz7szq0HRfqshLPWA1lN5Q5l0sxHclaKg



-- u will temporary credentials 


-- once u got temporary credentials

-- go to postman --> method = GET --> type = aws signature 

-- in the URL section 

         https://cf-templates-12zxw0rmzpafi-us-east-1.s3.us-east-1.amazonaws.com/


-- by using this url u can retrieve buckets in the specific region 

 -- s3 endpoint will change according to region 

 --  cf-templates-12zxw0rmzpafi-us-east-1    = bucket name present in us-east-1 region    

 --  give credentials and service name is s3 and once u enter u will get buckets and it's objects , u can also leverage permissions (limited to guests)











----------------------------------------------------------------- Cognito Identity Pools – IAM Roles


• Default IAM roles for authenticated and guest users

       - So that means the guest users will have one IAM role, and the other will have another one's IAM role.


• Define rules to choose the role for each user based on the user’s ID       

• You can partition your users’ access using policy variables

        - hen we can customize the IAM policy, thanks to using policy variables, and 
        
        - this will be allowing us to make sure that the users will only get access to what they need in DynamoDB or Amazon S3, 


• IAM credentials are obtained by Cognito Identity Pools through STS

• The roles must have a “trust” policy of Cognito Identity Pools








----------------------------------------------------------------- Cognito Identity Pools – Guest User example






{
  "Version": "2012-10-17",
  "Statement": [
    {
    
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:region:accountID:map/ExampleMap"
    }
  ]
}



-- So we want to give our guest users access to AWS, so we want to create an IAM policy,

-- that would allow any guest user to do a get object on a bucket for my picture, 

-- So this would give us, for the guest users, access to AWS with a very simple, and obviously restricted, IAM policy.






----------------------------------------------------------------- Cognito Identity Pools – Policy variable on S3



{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ListYourObjects",
      "Effect": "Allow",
      "Action": "s3:ListBucket",
      "Resource": [
        "arn:aws:s3:::bucket-name"
      ],
      "Condition": {
        "StringLike": {
          "s3:prefix": [
            "cognito/application-name/${cognito-identity.amazonaws.com:sub}/*"
          ]
        }
      }
    },
    {
      "Sid": "ReadWriteDeleteYourObjects",
      "Effect": "Allow",
      "Action": [
        "s3:DeleteObject",
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-name/cognito/application-name/${cognito-identity.amazonaws.com:sub}/*"
      ]
    }
  ]
}



-- for the authenticated users, you can define a policy variable on Amazon S3,

-- so our users are connected, but we want to make sure that they only have access to a prefix in your S3 bucket,






----------------------------------------------------------------- Cognito Identity Pools – DynamoDB





-- This example shows how you might create an identity-based policy that allows item-level access to the MyTable DynamoDB table based on an Amazon Cognito identity pool user ID. This policy grants the permissions necessary to complete this action programmatically from the AWS API or AWS CLI.


-- To use this policy, you must structure your DynamoDB table so the Amazon Cognito identity pool user ID is the partition key. 




{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "dynamodb:DeleteItem",
                "dynamodb:GetItem",
                "dynamodb:PutItem",
                "dynamodb:Query",
                "dynamodb:UpdateItem"
            ],
            "Resource": ["arn:aws:dynamodb:*:*:table/MyTable"],
            "Condition": {
                "ForAllValues:StringEquals": {
                    "dynamodb:LeadingKeys": ["${cognito-identity.amazonaws.com:sub}"]
                }
            }
        }
    ]
}







----------------------------------------------------------------- Cognito User Pools vs Identity Pools vs Cognito Sync



• Cognito User Pools (for authentication = identity verification)

    • Database of users for your web and mobile application

    • Allows to federate logins through Public Social, OIDC, SAML...

    • Can customize the hosted UI for authentication (including the logo)

    • Has triggers with AWS Lambda during the authentication flow

    • Adapt the sign-in experience to different risk levels (MFA, adaptive authentication, etc...)


• Cognito Identity Pools (for authorization = access control) 

    • Obtain AWS credentials for your users

    • Users can login through Public Social, OIDC, SAML & Cognito User Pools

    • Users can be unauthenticated (guests)

    • Users are mapped to IAM roles & policies, can leverage policy variables


• CUP + CIP = authentication + authorization    




-- Cognito Sync

       - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data.

       - You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. 

       - The client libraries cache data locally so your app can read and write data regardless of device connectivity status

       - When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.










==================================================================== Amazon Route 53 ====================================================================





-- Domian registration 

-- DNS routing 

-- Health checks

-- Routng policies 

----------------------------------------------------------------- DNS ?



-- Domain Name System which translates the human friendly hostnames into the machine IP addresses

eg : www.google.com => 172.217.18.36

-- DNS is the backbone of the Internet

-- DNS uses hierarchical naming structure 

- .com
- example.com
- www.example.com
- api.example.com

--- DNS Terminologies

-- Domain Registrar : Amazon Route 53, GoDaddy, ...

-- DNS Records: A, AAAA, CNAME(Canonical Name), NS, ...

-- Zone File: contains DNS records

-- Name Server: resolves DNS queries (Authoritative or Non-Authoritative)

-- Top Level Domain (TLD): .com, .us, .in, .gov, .org, ...

-- Second Level Domain (SLD): amazon.com, google.com, ...

eg :  http://api.www.example.co  --> it is known as URL 

in this URL 

- .com  --> TOP levle domain
- example.com --> Second Level Domain (SLD)
- www.example.com ---> Sub Domain
- api.example.com ---> FQDN (Fully Qualified Domain Name)
- http ----> protocol








----------------------------------------------------------------- Amazon Route 53


• A highly available, scalable, fully managed and Authoritative DNS

        • Authoritative = the customer (you) can update the DNS records


• Route 53 is also a Domain Registrar

• Ability to check the health of your resources

• The only AWS service which provides 100% availability SLA

• Why Route 53? 53 is a reference to the traditional DNS port






----------------------------------------------------------------- Route 53 – Records



• How you want to route traffic for a domain

• Each record contains:

        • Domain/subdomain Name – e.g., example.com

        • Record Type – e.g., A or AAAA

        • Value – e.g., 12.34.56.78

        • Routing Policy – how Route 53 responds to queries

        • TTL – amount of time the record cached at DNS Resolvers


• Route 53 supports the following DNS record types:

        • (must know)A /AAAA / CNAME / NS

        • (advanced)CAA/DS/MX/NAPTR/PTR/SOA/TXT/SPF/SRV






----------------------------------------------------------------- Route 53 – RecordTypes




• A – maps a hostname to IPv4

• AAAA – maps a hostname to IPv6

• CNAME – maps a hostname to another hostname

        • The target is a domain name which must have an A or AAAA record

        • Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)


• Example: you can’t create for example.com, but you can create for www.example.com

• NS – Name Servers for the Hosted Zone

        • Control how traffic is routed for a domain






-- we have A Record , AAAA Record , Cname recor , Alias Record  , MX Record 

-- Most common use is A record + Alias Record 

1 A Record : URL to IPv4


http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records it contains subbu.com --> Ec2 instance IP 

URL --> ipv4


2  AAAA Reacord : URL to IPv6


3 Alias Record : URL to Any Resource :

http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records(subbu.com --> ec2 ip or elb nasty url)

- url to ANY Resource 

-- Maps a hostname to an AWS resource

-- An extension to DNS functionality

-- Automatically recognizes changes in the resource’s IP addresses

-- Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com

-- Alias Record is always of type A/AAAA for AWS resources (IPv4 / IPv6)

-- You can’t set the TTL 



----------- Route 53 – Alias Records Targets

-- Elastic Load Balancers
-- CloudFront Distributions
-- API Gateway
-- Elastic Beanstalk environments
-- S3 Websites
-- VPC Interface Endpoints
-- Global Accelerator accelerator
-- Route 53 record in the same hosted zone Elastic Load Balancer



----- You cannot set an ALIAS record for an EC2 DNS name

ANS : You actually can create a CNAME (or even A) record without an Elastic IP. But every time your EC2 instance is restarted and so moved to another host system the IP address (and your external hostname) of your instance will change.

-- If you can live with this and accept the fact that during the TTL of your DNS record your instance is not reachable you can use the external IP and create an A record with your subdomain. 
-- But as DNS is not very fast in distributing changes (even with a low TTL you can't make sure every resolver handles the TTL correctly) you don't want to do such things most of the times. 
-- This is why AWS provides the Elastic IP - so your IP address which is in the DNS record never changes but the routing behind this IP address is changed by AWS if you reassign it to another instance (or you reboot your host). 
-- This routing change is only inside the AWS data centers and so it is quite fast (within a few seconds) and your instance is reachable again for all users.




4 CNAME Record : URL to URL : 

http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records (subbu.com-->ELB Nasty URL)

URL to URL 

--- here 

-- http://subbbu.com  ----- Main Domain / Naked domain / Zone Apex Record 

-- admin.gopi.com , web.subbu.com ----- called Sub-domains 


-- CNAME Records are bilable , where as Alias Rec are free 

-- for Naked domain/Main domain we can not use CNAME , instead use Alias 

-- for sub-domains u can u can use CNAME

-- the target is a domain name which must have an A or AAAA record

-- Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)

-- Example: you can’t create for example.com, but you can create for www.example.com

-- Note: Always choose Alias over CNAME 





EPV : A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com.As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?

ANS : Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com

EXP : 

-- You can create an alias record at the top node of a DNS namespace, also known as the zone apex, however, you cannot create a CNAME record for the top node of the DNS namespace. 

-- So, if you register the DNS name covid19survey.com, the zone apex is covid19survey.com. You can't create a CNAME record for covid19survey.com, but you can create an alias record for covid19survey.com that routes traffic to www.covid19survey.com.




IMP NOTE : 

-- You should also note that Amazon Route 53 doesn't charge for alias queries to AWS resources but Route 53 does charge for CNAME queries.

-- Additionally, an alias record can only redirect queries to selected AWS resources such as Amazon S3 buckets, Amazon CloudFront distributions, and another record in the same Amazon Route 53 hosted zone;

-- however a CNAME record can redirect DNS queries to any DNS record. So, you can create a CNAME record that redirects queries from app.covid19survey.com to app.covid19survey.net.







----------------------------------------------------------------- Route 53 – Hosted Zones



• A container for records that define how to route traffic to a domain and its subdomains



-- in R53 , we should create "Hosted Zones"(container of Recoreds) first 

-- Hosted Zone (Domain name = subbu.com ) are same 

• You pay $0.50 per month per hosted zone



-- 2 types of H.Z 


public : contains records that specify how to route traffic on the Internet (public domain names)

private : contain records that specify how you route traffic within one or more VPCs (private domain names)
        
        -- You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs.


IMP : For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:

enableDnsHostnames

enableDnsSupport




EPV (private zone) : A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved?

ANS : Enable DNS hostnames and DNS resolution for private hosted zones

-- DNS hostnames and DNS resolution are required settings for private hosted zones. DNS queries for private hosted zones can be resolved by the Amazon-provided VPC DNS server only.




------------------- 1 DNS hostnames: 

-- For non-default virtual private clouds that aren't created using the Amazon VPC wizard, this option is disabled by default.

-- If you create a private hosted zone for a domain and create records in the zone without enabling DNS hostnames, private hosted zones aren't enabled. To use a private hosted zone, this option must be enabled.




------------------- 2 DNS resolution: 

-- Private hosted zones accept DNS queries only from a VPC DNS server.

-- The IP address of the VPC DNS server is the reserved IP address at the base of the VPC IPv4 network range plus two.

--  Enabling DNS resolution allows you to use the VPC DNS server as a Resolver for performing DNS resolution. Keep this option disabled if you're using a custom DNS server in the DHCP Options set, and you're not using a private hosted zone.




EPV : The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this.

Which of the following settings of the VPC need to be enabled? (Select two)


ANS : enableDnsHostnames and enableDnsSupport

EXP : For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:

enableDnsHostnames

enableDnsSupport






----------------------------------------------------------------- Create First record 



-- go to R53 in console --> create public hosted zone if you do have any previously --> create sample record like test --> give random ip for demo like 11.22.33.44

-- now do test this record --> go to cloudshell (coz we do not have 11.22.33.44 in realtime )--> sudo yum install -y bind-utils --> it will install nslookup and dig 

-- now do  

        nslookup test.subbu.com

        - here test = record name that u have created and subbu.com is ur hosted zone name 

-- do 

        dig test.subbu.com

        - u will get more details 






----------------------------------------------------------------- Route 53 – Records TTL (TimeTo Live)


-- TTL (time to live), is the amount of time, in seconds, that you want DNS recursive resolvers to cache information about a record. 

-- If you specify a longer value (for example, 172800 seconds, or two days), you reduce the number of calls that DNS recursive resolvers must make to Amazon Route 53 to get the latest information for the record.

-- This has the effect of reducing latency and reducing your bill for Route 53 service.

-- However, if you specify a longer value for TTL, it takes longer for changes to the record (for example, a new IP address) to take effect because recursive resolvers use the values in their cache for longer periods before they ask Route 53 for the latest information.

--  If you're changing settings for a domain or subdomain that's already in use, AWS recommends that you initially specify a shorter value, such as 300 seconds, and increase the value after you confirm that the new settings are correct.



-- High TTL – e.g., 24 hr

        - Less traffic on Route 53
        - Possibly outdated records

-- Low TTL – e.g., 60 sec.

        - More traffic on Route 53 ($$)
        - Records are outdated for less time 
        - Easy to change records


-- Except for Alias records, TTL is mandatory for each DNS record






----------------------------------------------------------------- Route 53 – Records TTL (TimeTo Live) Hands on 




-- create 2 instances with the userdata from different az or regions 

-- create record test.subbu.com and give any ip of 2 ec2 and set ttl as u want 

-- copy test.subbu.com and paste in chrome , so for eg if you give ttl = 120 sec then it will cache 120 sec 

-- now do change the IP give IP of 2nd ec2 and update record and go to chrome and refresh still u will get response from the 1st ip coz it get cached for 120 sec .

-- so after 120 sec it will give response from the updated IP ,

-- this is how TTL works 






----------------------------------------------------------------- CNAME vs Alias



• AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname:

        • lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com


• CNAME:

        • Points a hostname to any other hostname. (app.mydomain.com => blabla.anything.com)

        • ONLY FOR NON ROOT DOMAIN (aka. something.mydomain.com)


• Alias:

        • Points a hostname to an AWS Resource (app.mydomain.com => blabla.amazonaws.com)

        • Works for ROOT DOMAIN and NON ROOT DOMAIN (aka mydomain.com)

        • Free of charge

        • Native health check




----------------------------------------------------------------- Route 53 – Alias Records



• Maps a hostname to an AWS resource

• An extension to DNS functionality

• Automatically recognizes changes in the resource’s IP addresses

• Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com

• Alias Record is always of type A/AAAA for AWS resources (IPv4 / IPv6)

• You can’t set the TTL It is set automatically by Route 53.




----------------------------------------------------------------- Route 53 – Alias Records Targets



• Elastic Load Balancers

• CloudFront Distributions

• API Gateway

• Elastic Beanstalk environments

• S3 Websites

• VPC Interface Endpoints

• Global Accelerator accelerator

• Route 53 record in the same hosted zone



• You cannot set an ALIAS record for an EC2 DNS name



----- You cannot set an ALIAS record for an EC2 DNS name

ANS : You actually can create a CNAME (or even A) record without an Elastic IP. But every time your EC2 instance is restarted and so moved to another host system the IP address (and your external hostname) of your instance will change.

-- If you can live with this and accept the fact that during the TTL of your DNS record your instance is not reachable you can use the external IP and create an A record with your subdomain. 
-- But as DNS is not very fast in distributing changes (even with a low TTL you can't make sure every resolver handles the TTL correctly) you don't want to do such things most of the times. 
-- This is why AWS provides the Elastic IP - so your IP address which is in the DNS record never changes but the routing behind this IP address is changed by AWS if you reassign it to another instance (or you reboot your host). 
-- This routing change is only inside the AWS data centers and so it is quite fast (within a few seconds) and your instance is reachable again for all users.







----------------------------------------------------------------- Route 53 – Routing Policies  (for hands on check in course for each policy)



-- Define how Route 53 responds to DNS queries

-- Don’t get confused by the word “Routing”
   - It’s not the same as Load balancer routing which routes the traffic
   - DNS does not route any traffic, it only responds to the DNS queries

-- Route 53 Supports the following Routing Policies

        • Simple
        • Weighted
        • Failover
        • Latency based
        • Geolocation
        • Multi-Value Answer
        • Geoproximity (using Route 53 Traffic Flow feature)






1               Simple Routing Policy

        -- when u search for soemthing it wil goto R53 

        http://subbbu.com --> r53 --> hosted ones --> Records --> ELB DNS name 

        -- here we are doing simple routing so it is called "simple Routing Policy"

        -- it does not have Health Checks 

        -- if any down happens of websites u won't get any response from website 

        • Typically, route traffic to a single resource

        • Can specify multiple values in the same record

        • If multiple values are returned, a random one is chosen by the client

        • When Alias enabled, specify only one AWS resource

        



2               FailOver Routing Policy : (Active-passive)


        -- u have to create 2 records for this policy 

        eg : 

        subbu.com --> ELB DNS Name(Mumbai) -- primary record 

        subbu.com --> ELB DNS Name(ireland) -- Secondary record 


        if one record get down , the request will automaticall go to second record 

        -- for this u have to setup 2 sites in different regions for high availability if downtime of websites happens

        -- if u have maintainnece page ,u go for S3 , u can redirect to S3 ur request  

        -- it has health checks



3                  GeoLocation Routing Policy;


        -- for eg u have cutomers all over the world if they are in china , japan , india, austria etc

        -- here if some one is from japan , search for http://subbu.com but he wants in Japanese language 

        -- so we have application in each region , we do not have other options , so in each region we do have  appn in region seperately , in this case CFront is not work 

        -- if user search for something from japan , he will get reply from japan only 

        -- how to do? 

        we have to create records for each regions eg : if u have 5 regions then create 5 records 

        http://subbu.com--> Mumbai ELB/IP
        http://subbu.com--> sydney ELB/IP
        http://subbu.com--> us ELB/IP
        http://subbu.com--> ALSKA ELB/IP
        http://subbu.com--> japan ELB/IP


        -- R53 will identify user's/request location automatically and redirect to the correct record 

        -- Different from Latency-based!

        -- This routing is based on user location

        -- Specify location by Continent, Country or by US State (if there’s overlapping, most precise location selected)

        -- Should create a “Default” record (in case there’s no match on location)

        • Use cases: website localization, restrict content distribution, load balancing, ...

        -- Can be associated with Health Checks




EPV : 1) Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights

- Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from.

- For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights.

2) Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution

- You can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution.

- When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located.

- If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following: Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries.





4                Latency Routing Policy 



                -- u have different Regions , u have appn in every region seperately

                -- if he made any request it does not matter about the regions 

                -- which regions give low latency from there resonxe will get 

                -- Latency is based on traffic between users and AWS Regions

                -- Can be associated with Health Checks (has a failover capability)

                • Redirect to the resource that has the least latency close to us

                • Super helpful when latency for users is a priority



5       Mutli-Value Routing Policy 

                --  Same as Simple routing Polcy but it has Health Checks 

                -- share the traffic based on the traffic 

                -- Use when routing traffic to multiple resources

                -- Route 53 return multiple values/resources

                -- Can be associated with Health Checks (return only values for healthy resources)

                -- Up to 8 healthy records are returned for each Multi-Value query

                -- Multi-Value is not a substitute for having an ELB



6      Weighted Routing Policy 

                -- Control the % of the requests that go to each specific resource

                -- Assign each record a relative weight:

                𝑡𝑟𝑎𝑓𝑓𝑖𝑐(%) = weight for a specific resource / sum of all the weights for all records

                -- DNS records must have the same name and type

                -- Can be associated with Health Checks

                -- Use cases: load balancing between regions, testing new application versions...

                -- Assign a weight of 0 to a record to stop sending traffic to a resource

                -- If all records have weight of 0, then all records will be returned equally




7      Geoproximity (using Route 53 Traffic Flow feature)


        -- Route traffic to your resources based on the geographic location of users and resources

        -- Ability to shift more traffic to resources based on the defined bias

        -- To change the size of the geographic region, specify bias values:

                - To expand (1 to 99) – more traffic to the resource
                - To shrink (-1 to -99) – less traffic to the resource

        -- Resources can be:

                - AWS resources (specify AWS region)
                - Non-AWS resources (specify Latitude and Longitude)

        IMP -- You must use Route 53 Traffic Flow to use this feature 




----- Route 53 – Traffic flow



• Simplify the process of creating and maintaining records in large and complex configurations

• Visual editor to manage complex routing decision trees

• Configurations can be saved as Traffic Flow Policy
  
   • Can be applied to different Route 53 Hosted Zones (different domain names)

   • Supports versioning










-------------------------- Route 53 – Health Checks




• HTTP Health Checks are only for public resources

• Health Check => Automated DNS Failover:

   - Health checks that monitor an endpoint (application, server, other AWS resource)

   - Health checks that monitor other health checks (Calculated Health Checks)

   - Health checks that monitor CloudWatch Alarms (full control !!) – e.g., throttles of DynamoDB, alarms on RDS, custom metrics, ... (helpful for private resources)


• Health Checks are integrated with CW metrics





----------------------------------- Health Checks – Monitor an Endpoint



• About 15 global health checkers will check the endpoint health

   • Healthy/Unhealthy Threshold – 3(default)

   • Interval – 30 sec (can set to 10 sec – higher cost)

   • Supportedprotocol:HTTP, HTTPS and TCP

   • If > 18% of health checkers report the endpoint is healthy, Route 53 considers it Healthy. Otherwise, it’s Unhealthy

   • Ability to choose which locations you want Route 53 to use

• Health Checks pass only when the endpoint responds with the 2xx and 3xx status codes

• Health Checks can be setup to pass / fail based on the text in the first 5120 bytes of the response

• Configure you router/firewall to allow incoming requests from Route 53 Health Checkers







----------------------------------- Route 53 – Calculated Health Checks




• Combine the results of multiple Health Checks into a single Health Check

• You can use OR, AND, or NOT

• Can monitor up to 256 Child Health Checks

• Specify how many of the health checks need to pass to make the parent pass

• Usage: perform maintenance to your website without causing all health checks to fail






-------------------------- Health Checks – Private Hosted Zone



-- Route 53 health checkers are outside the VPC

-- They can’t access private endpoints (private VPC or on-premises resource)

IMP -- You can create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm itself

        - So the idea is that we're going to monitor the health of our EC2 instance in a private subnet with a CloudWatch Metric.

        - And then if the metric is breached, we're going to create a CloudWatch Alarm on it. And when the alarm goes into the alarm state,

        - then the health checker is going to be automatically unhealthy and therefore will have created exactly what we want, which is a health check on a private resource,



-- subbu.com -> elb gives nasty url --> ec2(it has appn)

-- whenever u type subbu.com in browser -> R53--> Hoste Zone --> ELB--> Nasty Url--> Ec2 Instance 



--- whenevr u creat "Public Hosted Zone " 2 records will create automatically 

1 NS(Name Server) Record = Pool of servers ( ~ it give 4 Servers) ,it identifies the server(website) 

2 SOA Record : Admin for Hosted Zones ( it has IP address) 


-- NS and SOA are default records , automatically created and managed by AWS 

-- u can purchase domains in 2 ways 



1 R53 

2  3rd party (go daddy) 


-- if u purchase in R53 ,it wil create hoste zones automaticlaly and  NS and SOA records created automatically 


-- whenever u purchase domain 3rd party domain registery , u should connect t AWS ,'coz ur request is here going to GO-daddy so u should connct to AWS 

-- HOW to conect ? 


-- once u create hosted zones in R53 ,the NS records wil crated (4 servers) , these servers u should updated in th Go-daddy , once u updated requests are redirected to AWS 


-- it has some latency 


-- NS and SOA records cannot be deleted 








-------------------------------------- healthcheck Hands ON 



check in course 






----------------------------------- Routing Policies – IP-based Routing





-- Routing is based on clients’ IP addresses

-- You provide a list of CIDRs for your clients and the corresponding endpoints/locations (user-IP-to-endpoint mappings)

-- Use cases: Optimize performance, reduce network costs

-- Example: route end users from a particular ISP to a specific endpoint






-----------------------------------  Routing Policies – Multi-Value




--  Same as Simple routing Polcy but it has Health Checks 

-- share the traffic based on the traffic 

-- Use when routing traffic to multiple resources

-- Route 53 return multiple values/resources

-- Can be associated with Health Checks (return only values for healthy resources)

-- Up to 8 healthy records are returned for each Multi-Value query

-- Multi-Value is not a substitute for having an ELB





----------------------------------- Domain Registar vs. DNS Service


• You buy or register your domain name with a Domain Registrar typically by paying annual charges (e.g., GoDaddy, Amazon Registrar Inc., ...)

• The Domain Registrar usually provides you with a DNS service to manage your DNS records

• But you can use another DNS service to manage your DNS records

• Example: purchase the domain from GoDaddy and use Route 53 to manage your DNS records




----------------------------------- 3rd Party Registrar with Amazon Route 53



• If you buy your domain on a 3rd par ty registrar, you can still use Route 53 as the DNS Service provider

        1. Create a Hosted Zone in Route 53

        2. Update NS Records on 3rd party website to use Route 53 Name Servers

• Domain Registrar != DNS Service

• But every Domain Registrar usually comes with some DNS features






----------------------------------- S3 Website with Route 53



For acme.example.com:

        • Create an S3 bucket with the same name as the target record (acme.example.com)

        • Enable S3 website on the bucket (and enable S3 bucket public settings)

        • Create a Route 53 Alias record to the S3 website endpoint or type A – IPv4 address


Note: • This only works for HTTP traffic (for HTTPS, use CloudFront) 








----------------------------------------------------------------- Route 53 – Hybrid DNS



• By default, Route 53 Resolver automatically answers DNS queries for:

        • Local domain names for EC2 instances

        • Records in Private Hosted Zones

        • Records in public Name Servers


-- Now, if you want to resolve DNS queries between your VPC and other private networks, your own networks that has other DNS resolvers, then you're going to need a hybrid DNS.


• Hybrid DNS – resolving DNS queries between VPC (Route 53 Resolver) and your networks (other DNS Resolvers)

• Networks can be:

        • VPC itself / Peered VPC

        • On-premises Network (connected through Direct Connect or AWS VPN)






----------------------------------------------------------------- Route 53 – Resolver Endpoints 




• Inbound Endpoint

        • So inbound endpoints are allowing DNS Resolvers on your network can forward DNS queries to Route 53 Resolver

        • Allows your DNS Resolvers to resolve domain names for AWS resources (e.g., EC2 instances) and records in Route 53 Private Hosted Zones


• Outbound Endpoint

        • Route 53 Resolver conditionally forwards DNS queries to your DNS Resolvers

        • Use Resolver Rules to forward DNS queries to your DNS Resolvers


• Associated with one or more VPCs in the same AWS Region

• Create in two AZs for high availability

• Each Endpoint supports 10,000 queries per second per IP address





----------------------------------------------------------------- Route 53 – Resolver Rules


• Control which DNS queries are forwarded to DNS Resolvers on your network

• Conditional Forwarding Rules (Forwarding Rules)

        • Forward DNS queries for a specified domain and all its subdomains to target IP addresses

• System Rules

        • Selectively overriding the behavior defined in Forwarding Rules (e.g., don’t forward DNS queries for a subdomain acme.example.com)


• Auto-defined System Rules

        • Defines how DNS queries for selected domains are resolved (e.g., AWS internal domain names, Privated Hosted Zones)


• If multiple rules matched, Route 53 Resolver chooses the most specific match

• Resolver Rules can be shared across accounts using AWS RAM

        • Manage them centrally in one account

        • Send DNS queries from multiple VPC to the target IP defined in the rule









----------------------------------------------------------------- Route53 Hands ON 




-- to do this buy domain in R53 registry or do purchase from any other 3rd party services 

-- i Bought from NameCheap 

-- go to your domain --> manage domain --> custom domain in drop down menu

-- now create one ec2 instance with the user data 

-- make sure you are using linux 2 version 

#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html


-- follow below step only if ur getting apache test page , otherwise continue with next steps

IMP NOTE : If u r getting apache test page I instead of ur content) no need to worry , just follow below things -- Geek Dairy

-- connect Linux machine and login as super user sudo su

-- Method 1

-- removing/renaming Welcome Page

mv /etc/httpd/conf.d/welcome.conf /etc/httpd/conf.d/welcome.conf_backup

-- Make sure that Apache is restarted (as root) with the command:

systemctl restart httpd


-- Method 2

-- allow Indexes in /etc/httpd/conf.d/welcome.conf

-- Without an index at the DocumentRoot, the default Apache Welcome page will display unless /etc/httpd/conf.d/welcome.conf is modified to allow Indexes. Edit /etc/httpd/conf.d/welcome.conf to allow Indexes.

-- Comment the Options line (add a # mark) in /etc/httpd/conf.d/welcome.conf as shown below:

vi /etc/httpd/conf.d/welcome.conf
<LocationMatch "^/+$">
#   Options -Indexes
    ErrorDocument 403 /error/noindex.html
</LocationMatch>

              or 

you can enable Indexes by changing the – to a +

vi /etc/httpd/conf.d/welcome.conf
<LocationMatch "^/+$">
    Options +Indexes
    ErrorDocument 403 /error/noindex.html
</LocationMatch>

-- systemctl restart httpd

-- thats it !


-- now do create another ec2 with same userdata in 1b availability zone

-- make sure you have to give HTTP in SG 

-- now create one Load Balancer (application load balancer) 

-- it is not good to give nasty url of LB so do make over for this link 

-- go to R53 service 

-- You have to create one hosted zone in R53 

-- if You purchase domain in AWS itself automatically it will create Hosted zone for you 

-- if u buy some other places u have to create ur hosted zones 

-- create hosted zone, the name is  ( it should be same ur domain name)

-- mine is  subbucloud.lat

-- create hosted zone with public type 

-- once u created hosted zone it will create 2 Records automatically Ns and SOA

-- NS have 4 servers , u should give these servers in ur custom domain (3rd party) 

-- update ur Name servers in ur custom domain 

-- now create records to do make up of ur url 

-- here u can go with subdomian or with naked (main) Url 


1 simple routing policy

-- type sample is sub-domain and record type is A

-- choose Alias Always on , if u do not on u have to give ur IP but we have LB so choose Alias

-- in route traffic section 

select   alias to application and classic load balancer 

select AZ , LB and Routing policy 

-- create record


2  FailOver Routing Policy

-- FailOver it means in simple words if ur website is facing down period in one Region then , automatically the Load balancer will shift the request into the another another region 

-- in Genaral we have deployed our application in 2 regions for high availability , if one region goes down automatically the request shift to another region through the load balancer 

-- for this we have 2 primary site and secondary site , if primary site is not working then it will redirect to secondary site 

-- we know how to create load balancers in different regions but now we will try with the maintainance page which is stored in S3 

-- create one html files add some content related to the maintainance and do store in the s3 bucket (IMP: ur record name and bucket name should be same  then only it works )

-- make sure ur bucket name is ur domain for easy understanding purpose (ACL is enabled for bucket)

-- upload html file and make file has Public and enable static weh hosting ( in index document u have to give ur index.html extension like maintainace.html)

-- create records for load balancer and s3 

-- for s3 record u have to slect end point as s3 website endpoint in dropdown below 

-- check ur url is working 

-- now stop the primary site and observe the changes 

-- if ur primary site is not working then it will shift to secondary site so , here u will get mainatainanace page , through the health checks the LOad balancer know that primary site is down so , it will shift traffic to the nxt site

-- getting maintainance page successfully 


-- u can also do same for geo location policy









===================================================================== Amazon VPC (Virtual Private Cloud) =====================================================================



IMP Points :

-- Regardless of the type of subnet, the internal IPv4 address range of the subnet is always private

-- When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block

-- By default, all subnets can route between each other, whether they are private or public




--------------------Steps to create our own VPC

1 crate VPC

2 Create IG and attachto the VPC 

3 Create Public and private Subnets

4 Create NAT Gateway(in public subnet) 

5 Create public RT --> all traffic is routed to IG , public subnet is associated 

6 create Private RT --> All Traffic is routed to NAT,Private Subnet is Associated 

7 Create a new Security Group, Allow RDP/SSH 

8 launch ec2 in Public subnet (Bastion Server) and another in Private Subnet 

9 try to connecting to private server through Bastion server 


NOTE : IG and NAT(Managed by AWS) Gateways are "Services not Servers" 




--- from ur laptap , u can directly connect to public internnet, but u can not connect private subnet directly 

-- to connect private subnet from internet(from latap , not from company)  , u have to launch one Ec2 in Public Subnet which is called "Baston server / Jump server" ( it is only for learnig purpose not real time scenario) 

-- first login into baston server and connect to private subnet 

-- in real scenario ur company is private network , u are using VPN Connection from company to AWS , so u no need to login into baston server , u can directly acces private servers 

-- if u want to connect client private subnets u first need to login into client's Baston server 






Question : one person came to u and ask , i have ec2 instnce in private subnet and i do not want internet access to my private subnet what u do? but he wants to Access AWS services from private subnet

SOln:

-- By default 3 sunets are there , these are allocated to each AZ , all these are public subnets only ( u rable to login into ec2) that's why it is public subnets 

-- we do not have NAT for Default VPC


but he wants to Access AWS services from private subnet ? 

-- in AWS We have VPC ENDPOINTS : it is use to Access only AWS services without NAT Gateway








----------------------------------------------------------------- Understanding CIDR – IPv4





-- Classless Inter-Domain Routing – a method for allocating IP addresses

• Used in Security Groups rules and AWS networking in general


• They help to define an IP address range:

        • We’ve seen WW.XX.YY.ZZ/32 => one IP

        • We’ve seen 0.0.0.0/0 => all IPs

        • But we can define:192.168.0.0/26 =>192.168.0.0 – 192.168.0.63 (64 IP addresses)



• A CIDR consists of two components

        • Base IP

                • Represents an IP contained in the range (XX.XX.XX.XX)

                • Example:10.0.0.0,192.168.0.0,...


• Subnet Mask

        • Defines how many bits can change in the IP

        • Example:/0,/24,/32

        • Can take two forms:

                • /8 <---> 255.0.0.0

                • /16 <---> 255.255.0.0

                • /24 <---> 255.255.255.0

                • /32 <---> 255.255.255.255



-- private ip series starts with eg: 172.98.90.2/16


--  172.98.90.2 = Base IP 

-- /16 = SubnetMask

-- eg: u can create 3 subnets in VPC

192.168.1.0/24 = s1  -----> it it is routing to IG , so it is called Public subnet (1a AZ)

192.168.2.0/24 = s2 ------> it it is routing to NAT , so it is called private subnet (1B)

192.168.3.0/24 = s3 ------> it it is routing to VPCENDPOINTS , so it is called private subnet (1c)


-- for private purpose it starts with 192, 10, 172 

-- rest all are for public IP's

-- 1 subnet should be in one AZ at the same Time 

--- 1 AZ can have multiple subnets  





----------------------------------------------------------------- Understanding CIDR – Subnet Mask




• The Subnet Mask basically allows part of the underlying IP to get additional next values from the base IP

Quick Remeber 

        - /32 - no octet will change 

        - /24 - last octet will change 

        - /16 - last 2 octet will change 

        - /8 - last 3 octet will change 

        - /0 - all change 





------------------------------------- what is SubnetMask :this refers to how many IP's that u get inside subnet

-- we have formula 

-------------------------------   2^32-n


for eg /24 then 


2^32-24 == 2^8 = 256 IP's u wil get , u can launch 256 ec2 u can launch 


NOTE : u have to remove 5 IP's from each subnet maks , here u will get 251 


- 5 IP's are reserved for each subnets 

.0 = Network purpose 

.1 =  VPC Router

.2 = amazon -provided DNS 

.3 = Future purpose 

.255 = BroadCasting 


/32 = only one IP





• When in doubt, use this website https://www.ipaddressguide.com/cidr


----------------------------------------------------------------- Public vs. Private IP (IPv4)




• The Internet Assigned Numbers Authority (IANA) established certain blocks of IPv4 addresses for the use of private (LAN) and public (Internet) addresses

• Private IP can only allow certain values:

  • 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)----> big networks

  • 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)-------> AWS default VPC in that range

  • 192.168.0.0 – 192.168.255.255 (192.168.0.0/16) ------->  e.g., home networks


• All the rest of the IP addresses on the Internet are Public





---------------------------------- VPC in AWS – IPv4



• You can have multiple VPCs in an AWS region (max. 5 per region – soft limit)

• Max. CIDR per VPC is 5, for each CIDR:

 • Min. size is /28 (16 IP addresses)

 • Max. size is /16 (65536 IP addresses)

• Because VPC is private, only the Private IPv4 ranges are allowed:

  • 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)
  • 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)
  • 192.168.0.0 – 192.168.255.255 (192.168.0.0/16)    


• Your VPC CIDR should NOT overlap with your other networks (e.g., corporate)





------------------------------------------- steps to create VPC with CIDR

1  crate VPCwith CIDR (192.168.0.0/16)


2  Create IGW and attach to the VPC 

3  Create Public subnet(192.168.1.0/24)

4  Create Private subnet(192.168.2.0/24)

5  create NAT Gateway (in public subnet, and also we have to provide EIP for NAT) 

6  Create Public RT ---> al traffic is routed to IGW ,public subnet is associated 

7  create Private RT --> all traffic is routed to NAT, rivate Subnet is Associated 

8  Create a new Security Group , allow RDP/SSH 

9 Launch EC2 instance in public Subnet (Bastion Server) and Another in private Subnet 

10 Connect to BAstion first and then into the private server 








------------------------------------------- Default VPC Walkthrough


• All new AWS accounts have a default VPC

• New EC2 instances are launched into the default VPC if no subnet is specified

• Default VPC has Internet connectivity and all EC2 instances inside it have public IPv4 addresses

• We also get a public and a private IPv4 DNS names






-------------------------------------------  VPC – Subnet (IPv4)




• AWS reserves 5 IP addresses (first 4 & last 1) in each subnet

• These 5 IP addresses are not available for use and can’t be assigned to an EC2 instance

• Example: if CIDR block 10.0.0.0/24, then reserved IP addresses are:

   • 10.0.0.0 – Network Address
   • 10.0.0.1 – reserved by AWS for the VPC router
   • 10.0.0.2 – reserved by AWS for mapping to Amazon-provided DNS
   • 10.0.0.3 – reserved by AWS for future use
   • 10.0.0.255 – Network Broadcast Address.AWS does not support broadcast in a VPC, therefore the address is reserved


• Exam Tip, if you need 29 IP addresses for EC2 instances:
  • You can’t choose a subnet of size /27 (32 IP addresses, 32 – 5 = 27 < 29)
  • You need to choose a subnet of size /26 (64 IP addresses, 64 – 5 = 59 > 29)

the number decreasing from 32 , 31, 30 ,29 -------> the more IP's we get , so choose according to ur requriments








-------------------------------------  Internet Gateway : it provide internet to the VPC , creat IG and attach to VPC




• Allows resources (e.g., EC2 instances) in a VPC connect to the Internet

• It scales horizontally and is highly available and redundant

• Must be created separately from a VPC

• OneVPC can only be attached to one IGW and vice versa

• Internet Gateways on their own do not allow Internet access... 

• Route tables must also be edited!

-- An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.

-- An Internet Gateway serves two purposes: 

   1 to provide a target in your VPC route tables for internet-routable traffic and 

   2 to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses.

-- Additionally, an Internet Gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic.



EPV : While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway.

Which conditions should be met for internet connectivity to be established? (Select two)

ANS : 

1 The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic

EXP : The network access control list (network ACL) that is associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity.

2 The route table in the instance’s subnet should have a route to an Internet Gateway

EXP : A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.






---------------------------------------------------------- Bastion Hosts



• We can use a Bastion Host to SSH into our private EC2 instances

• The bastion is in the public subnet which is then connected to all other private subnets

• Bastion Host security group must allow inbound from the internet on port 22 from restricted CIDR, for example the public CIDR of your corporation

• Security Group of the EC2 Instances must allow the Security Group of the Bastion Host, or the private IP of the Bastion host






---------------------------------------------------------- Bastion Hosts Hands ON 


-- check hands on of VPC ENDPOINTS 






---------------------------------------------------------- NAT Instance (outdated, but still at the exam)



• NAT = Network Address Translation

• Allows EC2 instances in private subnets to connect to the Internet

• Must be launched in a public subnet

• Must disable EC2 setting: Source / destination Check

        - The server now says, hey, I know how to reply to you because the source is me and the destination is the public IP of your NAT instance,

        - and then the NAT instance will reply back the traffic to the EC2 instance, it's smart enough to know which one to go to, and say, hey, the source is the public server and the destination is your private IP.

        - this is why the source and destination check must be disabled on the EC2 instance for your NAT instance.

• Must have Elastic IP attached to it

• RouteTables must be configured to route traffic from private subnets to the NAT Instance



----------- NAT Instance – Comments



• Pre-configured Amazon Linux AMI is available

  • Reached the end of standard support on December 31, 2020

• Not highly available / resilient setup out of the box

  • You need to create an ASG in multi-AZ + resilient user-data script

• Internet traffic bandwidth depends on EC2 instance type

• You must manage Security Groups & rules:

• Inbound:
  • Allow HTTP / HTTPS traffic coming from Private Subnets

  • Allow SSH from your home network (access is provided through Internet Gateway)

• Outbound:
  • Allow HTTP / HTTPS traffic to the Internet







---------------------------------------------------------- NAT Instance Hands ON 




-- create ur own VPC with CIDR 192.168.0.0/16

Tenancy defines how EC2 instances are distributed across physical hardware and affects pricing. There are three tenancy options available: Shared ( default ) — Multiple AWS accounts may share the same physical hardware. Dedicated Instance ( dedicated ) — Your instance runs on single-tenant hardware.

-- when u create vpc , automatically one main route table will created 

-- create 2 subnets (public and private)

-- by default these 2 subnets will route to the main route table 

-- create internet gateway and attach to the vpc

-- create RT public and private 

-- now go to public RT and add Route--> edit routes --> add route --> 0.0.0.0/0--> IGW  and also associate public subnet for public RT 

-- now go to private RT and add Route-->  associate private subnet for private RT 

-- now create NAT instance , ( Must be launched in a public subnet)

-- open ec2 --> browse more AMI's --> search for nat --> select amzn-ami-vpc-nat-2018.03.0.20221018.0-x86_64-ebs linux based instance 

-- create one new SG  allow SSH , All ICMP-IPv4 

-- edit vpc select ur vpc --> select Public subnet -->  Auto-assign public IP = enable --> select SG --> create instance 

-- now go to RT --> private RT --> routes --> edit routes --> add ur NAT instance --> save changes 

-- now turn off source and destination check for the NAT insatnce 

Each EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives. However, a NAT instance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable source/destination checks on the NAT instance.

-- Choose instance --> Actions--> Networking,-->Change source/destination check.

-- now launch instance on  private subnet , select ur defalut SG , disable public ip

-- now we have NAT instance and private instance , now check how will EC2 instances in private subnets to connect to the Internet

-- connect NAT instance , if ur ubable to connect , do connect through the putty

-- now try to connect private server

-- --  to conect private server , u just copy SSH Client id and paste in public istance 

eg : ssh -i "terraform-key.pem" ec2-user@192.168.2.31


-- create one file to store .pem value in the public instance 

-- vi terraform-key.pem and paste content of .pem file 

-- we do ot have read permissions for this , so create prmisions 

     chmod 400 terraform-key.pem

-- now try to connect --> Yes

-- once u connect succesfully ,it will connect to the private server

-- now we are in private server , u  can do install anything here 






---------------------------------------------------------- NAT Gateway





----------- Basics of NAT Gateway

-- A NAT gateway supports the following protocols: TCP, UDP, and ICMP.

-- The job of NAT Gateway is provide internet to the Private Subnet and convert Private IP to Public IP 

-- The NAT Gateway Should put in Public subnet 

• AWS-managed NAT, higher bandwidth, high availability, no administration

• Pay per hour for usage and bandwidth

• NAT GW is created in a specific Availability Zone, uses an Elastic IP

• Can’t be used by EC2 instance in the same subnet (only from other subnets)

• Requires an IGW (Private Subnet => NATGW => IGW)

• 5 Gbps of bandwidth with automatic scaling up to 100 Gbps

• No Security Groups to manage / required

-- A NAT gateway supports the following protocols: TCP, UDP, and ICMP.

-- You cannot associate a security group with a NAT gateway.

-- You can use a network access control list (network ACL) to control the traffic to and from the subnet in which the NAT gateway is located.

-- A NAT gateway can support up to 55,000 simultaneous connections to each unique destination.

-- You cannot associate a security group with a NAT gateway. You can associate security groups with your instances to control inbound and outbound traffic.

-- You can use a network ACL to control the traffic to and from the subnet for your NAT gateway. NAT gateways use ports 1024–65535. 

-- You can't route traffic to a NAT gateway through a VPC peering connection.

-- You can't route traffic through a NAT Gateway when traffic arrives over a hybrid connection (Site to Site VPN or Direct Connect) through a Virtual Private Gateway. 

-- You can route traffic through a NAT Gateway when traffic arrives over a hybrid connection (Site to Site VPN or Direct Connect) through a transit gateway.






---------------------------------------------------------- NAT Gateway with High Availability




• NAT Gateway is resilient within a single Availability Zone

• Must create multiple NAT Gateways in multiple AZs for fault-tolerance

• There is no cross-AZ failover needed because if an AZ goes down it doesn't need NAT
(if an AZ goes down, all ec2 in that AZ will also be down, No point connecting ec2 in one AZ with NAT GW in other AZ)



-- Public subnet : it is exposd to the internet , public subnet raffic is roouted to Internet Gateway 

-- Private subnet : it is not exposd to the internet , private subnet raffic is roouted to NAT(NEtwork Addres Translator) Gateway 


EPV : A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture.


ANS : For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433

EXP : The above rules make sure that web servers are listening for traffic on all sources on the HTTPS protocol on port 443. The web servers only allow outbound traffic to MSSQL servers in Security Group B on port 1433.

For security group B: Add an inbound rule that allows traffic only from security group A on port 1433

EXP : The above rule makes sure that the MSSQL servers only accept traffic from web servers in security group A on port 1433.





---------------------------------------------------------- NAT Gateway vs. NAT Instance





                                       NAT Gateway                                                               NAT Gateway

Availability                      Highly available within AZ (create in another AZ)                            Use a script to manage failover between instances

Bandwidth                         Up to 100 Gbps                                                                Depends on EC2 instance type

Maintenance                       Managed by AWS                                                                Managed by you (e.g., software, OS patches, ...)

Cost                              Per hour & amount of data transferred                                         Per hour, EC2 instance type and size, + network $

Public IPv4                       YES                                                                                   YES

Private IPv4                      YES                                                                                   YES

Security Groups                   NO                                                                                    YES

Use as Bastion Host?              NO                                                                                    YES
     




More at: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html







---------------------------------------------------------- DNS Resolution in VPC


• DNS Resolution (enableDnsSupport)

        • Decides if DNS resolution from Route 53 Resolver server is supported for the VPC

        • True (default): it queries the Amazon Provider DNS Server at 169.254.169.253 or the reserved IP address at the base of the VPC IPv4 network range plus two (.2)



• DNS Hostnames (enableDnsHostnames)

        • By default,

                • True => default VPC

                • False => newly created VPCs

        • Won’t do anything unless enableDnsSupport = true

        • If True, assigns public hostname to EC2 instance if it has a public IPv4   




-- go to vpc created by you --> right click --> enable the above 2 options 






---------------------------------------------------------- Security Groups & NACLs




---------- NACL 





-- it is another layer of security to the ec2 

-- if u want tight the security go for NACL

-- like SG , NACL has inbound and outbound rules


-- NACL will hit first then move to SG

-- in VPC they aare multiple Subnets

-- 1 subnet is associated to 1 AZ

-- 1 subnet can't be in multiple AZ at the same time 

-- 1 AZ can have mltiple subnets

-- 1 NACL can have multiple subnets

-- NACL is subnet level and SG is ec2 level 

-- in NACL u can deny but in S.G u cant 
           
-- inbound n outbound rules 

- default NACL is there 

-- NACL will hit first 

-- by default , inbound rules are allowed

-- u can deny on NACL and allow also

-- NACL is subnet level

-- if u create any new NACL, inbound rules,outbound rules are DENY

-- NACL are STATELESS

-- if u allow any inbound rule, u need to allow on ob rule also


• NACL are like a firewall which control traffic from and to subnets

• One NACL per subnet, new subnets are assigned the Default NACL



IMP ------------ 


• You define NACL Rules:

    • Rules have a number (1-32766), higher precedence with a lower number
    • First rule match will drive the decision
    • Example: if you define #100 ALLOW 10.0.0.10/32 and #200 DENY 10.0.0.10/32, the IP address will be allowed because 100 has a higher precedence over 200
    • The last rule is an asterisk (*) and denies a request in case of no rule match
    • AWS recommends adding rules by increment of 100

    • Newly created NACLs will deny everything

    • NACL are a great way of blocking a specific IP address at the subnet level


------------------------- Default NACL

• Accepts everything inbound/outbound with the subnets it’s associated with

• Do NOT modify the Default NACL, instead create custom NACLs


------------------------------ LAB NACL 

-- create one instance with the user data

#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html


-- make sure u have allowed inbound rules for SSH and HTTP

-- now go to NACL --> add HTTP rule in inbound rules and make it as DENY , give rule number less than 100 , so now try in browser it will gives u error , coz here rule number came into picture , less the rule number more the priority

-- if u give rule number > 100 it will allow coz, u have given rule number > 100 , so small number has high priorities

-- now go to outbound rules , change from allow to deny 

-- now try in browser , it won’t give u value coz , in NACL it is stateless , u have to give both inbound and outbound rules , if u deny any one it won’t work 



NOTE : in SG , it is stateful , no need to give inbound and outbound rules , even if u did not give outbound rules it will give o/p



-- When you create a custom Network ACL and associate it with a subnet, by default, this custom Network ACL denies all inbound and outbound traffic until you add rules. 

-- A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.

-- Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).

-- The client that initiates the request chooses the ephemeral port range. The range varies depending on the client's operating system. Requests originating from Elastic Load Balancing use ports 1024-65535. List of ephemeral port ranges:

      - Many Linux kernels (including the Amazon Linux kernel) use ports 32768-61000.

      - Requests originating from Elastic Load Balancing use ports 1024-65535.

      - Windows operating systems through Windows Server 2003 use ports 1025-5000.

      - Windows Server 2008 and later versions use ports 49152-65535.

      - A NAT gateway uses ports 1024-65535.

      - AWS Lambda functions use ports 1024-65535.




------------------------------------ Security Group vs. NACLs

        Security Group                                                                                     NACL

Operates at the instance level                                                         Operates at the subnet level

Supports allow rules only                                                              Supports allow rules and deny rules

Stateful: return traffic is automatically allowed, regardless of any rules             Stateless: return traffic must be explicitly allowed by rules (think of ephemeral ports)

All rules are evaluated before deciding whether to allow traffic                       Rules are evaluated in order (lowest to highest) when deciding whether to allow traffic, first match wins

Applies to an EC2 instance when specified by someone                                   Automatically applies to all EC2 instances in the subnet that it’s associated with






---------------------------------------------------------- Ephemeral Ports



• For any two endpoints to establish a connection, they must use ports

• Clients connect to a defined port, and expect a response on an ephemeral port

• Different Operating Systems use different port ranges, examples:

        • IANA & MSWindows10 ----> 49152–65535

        • Many Linux Kernels -----> 32768 – 60999





---------------------------------------------------------- VPC – Reachability Analyzer



• A network diagnostics tool that troubleshoots network connectivity between two endpoints in your VPC(s)

• It builds a model of the network configuration, then checks the reachability based on these configurations (it doesn’t send packets)

• When the destination is

        • Reachable – it produces hop-by-hop details of the virtual network path

        • Not reachable – it identifies the blocking component(s) (e.g., configuration issues in SGs, NACLs, Route Tables, ...)


• Use cases: troubleshoot connectivity issues, ensure network configuration is as intended, ...




---------------------------------------------------------- VPC – Reachability Analyzer Hands ON 



-- create 2 instances 

-- open Reachability Analyzer --> create --> give source = instance 1st ec2 and dest = 2nd instance --> port = 443 for demo --> TCP protocal

-- start analyzer and see changes 


 



 ---------------------------------------------------------- VPC Peering



-- by deault 2 vpc can't talk each other, but if it is required we can use "VPC Peering"

-- You can't route traffic to a NAT gateway through a VPC peering connection.

• Privately connect two VPCs using AWS’ network

• Make them behave as if they were in the same network

• Must not have overlapping CIDRs

• VPC Peering connection is NOT transitive (must be established for each VPC that need to communicate with one another)

• You must update route tables in each VPC’s subnets to ensure EC2 instances can communicate with each other

• You can create VPC Peering connection between VPCs in different AWS accounts/regions

• You can reference a security group in a peeredVPC (works cross accounts – same region)




-- A highly critical financial services application is being moved to AWS Cloud from the on-premises data center. The application uses a fleet of Amazon EC2 instances provisioned in different geographical areas. The Chief Technology Officer (CTO) of the company needs to understand the communication network used between instances at various locations when they interact using public IP addresses.

ANS :  

        - When two instances communicate using public IP addresses, the following three scenarios are possible: 

                1. Traffic between two EC2 instances in the same AWS Region stays within the AWS network, even when it goes over public IP addresses. 

                2 Traffic between EC2 instances in different AWS Regions stays within the AWS network if there is an Inter-Region VPC Peering connection between the VPCs where the two instances reside.

                3 Traffic between EC2 instances in different AWS Regions where there is no Inter-Region VPC Peering connection between the VPCs where these instances reside, is not guaranteed to stay within the AWS network.







 ---------------------------------------------------------- VPC Peering Hands ON 




-- create peering can be done in same region , same account and different account also 

-- create 2 vpc's , one in mumbai and one in ieland regions 

-- Two VPC CIDR should be UNIQUE 

-- u can connect to ur servers n vpc which is another region through the VPC Peering 

-- create 2 vpc in different region 

-- follow above steps for creating all requriments IGW ,create public nad private subnets, NAT , Public and private RT's ssociate route tables and Subnets and SG and 2 instances  for vpc 

-- do the same things in other region CIDR 192.169.0.0/16, u can do in same region or u can do in another account also ( no need to create public instance(baston)) , create private subnet only 

-- You must update route tables in each VPC’s subnets to ensure EC2 instances can communicate with each other

-- once u do all setup 

--  go to mumbai region connect BAston server first ,sudo -s

-- create one file to store terraform-key.pem value , vi terraform-key.pem

-- copy SSH Client link of private server 

-- chmod 400 terraform-key.pem

-- now u are able to connect private server from the baston server 

------- now do sudo -s

-- now frm mumbai u should get connect to the ireland 

-- go and copy SSH CLient of ireland private instance 


-- now go to mumbai location and open peering connection , once u created Peering conection 

-- go to VPC and check in peering and do accept 

-- once u accept , open private RT and dd route of VPC trafic of ireland (192.169.0.0/16) 

-- do same i ireland private RT (192.168.0.0/16) 


-- now go to SG of mumbai --> add 192.169.0.0/16 in inbound rules 

-- do vice versa in ireland SG 192.168.0.0/16

-- once u do u can able to login to irland vpc private server 




-------- if u want to create vpc for ur company 

u have to create 

1 Virtual Private Gateway 

2 Customer Gateway 

3 Site-to-Site Vpn --> ownlaod configuration n give it to the comonay network admin to setup VPN Connction from Company to AWS 







---------------------------------------------------------- VPC Endpoints (AWS PrivateLink)






• Every AWS service is publicly exposed (public URL)

• VPC Endpoints (powered by AWS PrivateLink) allows you to connect to AWS services using a private network instead of using the public Internet

• They’re redundant and scale horizontally

• They remove the need of IGW, NATGW, ... to access AWS Services

• In case of issues:

  • Check DNS Setting Resolution in your VPC 
  • CheckRouteTables



------------- Types of Endpoints


1 Interface Endpoints (powered by PrivateLink)

  • Provisions an ENI (private IP address) as an entry point (must attach a Security Group)
  • Supports most AWS services
  • $ per hour + $ per GB of data processed
  - dynamodb does not support for interface endpoints

2 Gateway Endpoints

  • Provisions a gateway and must be used as a target in a route table (does not use security groups)
  • Supports both S3 and DynamoDB only 
  • Free


------------------------------------------- Gateway or Interface Endpoint for S3?

• Gateway is most likely going to be preferred all the time at the exam

• Cost: free for Gateway, $ for interface endpoint

IMP : • Interface Endpoint is preferred access is required from on- premises (Site to Site VPN or Direct Connect), a different VPC or a different region



--------------------------------------- LAB for endpoints



-- create VPC with CIDR 192.168.0.0/16

-- One IGW attach to only one VPC at the same time 

-- create IGW , and attach to vpc that u have created 

-- create public subnet , give subnet CIDR as 192.168.1.0/24

-- create private subnet , give subnet CIDR as 192.168.2.0/24

-- now Create NAT Gateway, it should be in the public subnet only 

-- create RT , once new VPC created a Default RT created automatically 

-- create RT public and private 

-- now go to public RT and add Route--> edit routes --> add route --> 0.0.0.0/0--> IGW  and also associate public subnet for public RT 

-- now go to private RT and add Route--> edit routes --> add route --> 0.0.0.0/0--> NAT GAteway  and also associate private subnet for private RT 

-- now create ur own SG's with ur VPC

-- launch 1 instance , in public subnet 

-- launch 1 instance , in private  subnet and disable Public 

-- now connect to public subnet instance , sudo -s

--    try connect to private server 

--  to conect private server , u just copy SSH Client id and paste in public istance 

eg : ssh -i "terraform-key.pem" ec2-user@192.168.2.31


-- create one file to store .pem value in the public instance 

-- vi terraform-key.pem and paste content of .pem file 

-- we do ot have read permissions for this , so create prmisions 

     chmod 400 terraform-key.pem

-- now try to connect --> Yes

-- once u connect succesfully ,it will connect to the private server 

previusly it was root@ip-192-168-1-129 ec2-user 

-- once u connect to private server u will redirected to the private IP of private server like    
[ec2-user@ip-192-168-2-31 ~]$ 


--------------------------------------------------- ENDPOINTS

-- now u are in private server 

-- do sudo -s

-- this private server in the private subnet and Associated to Private Route Table , in Private RT all traffic is routed to NAT Gateway, so it has internet access 

-- test whether it has internet or not 

--  yum install -y git ,it installd so , it has internet

-- but i do not wnat internet access , so go to Private RT and delete entry for NAT 

-- not able to login 

--   i want only access to AWS service from this linux machine without internet access 

-- so first install AWS CLI 

-- we r in private server so enable NAT access to download AWS CLI in Private RT 

-- curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

-- unzip awscliv2.zip

sudo ./aws/install

-- follow above  steps to install 

-- now i do not want to give keys in the ec2 instance itself 'coz it is not safe to give keys here so , attach role to this insatce 

-- IAM -->roles --> Aws service (TE) --> user EC2 --> give full access of which service u want to access , (s3) --> create role 

-- attach role to the private server 

-- go to private server and try to access bucket 

-- create bucket in linux 

      aws s3 mb s3://vpc-bucket --region ap-south-1

-- u can able to create bucket it has NAT mens full internet access , check in s3 bucket is created 


-----------now i do not want intrnet Acces , so delete NAT entry in Private RT 

-- now we do not have internet Access but i want to acces the AWS service like s3 

-- create endpoint in the console 

-- ENDPOINTS are 3 typrs 

1 Interface-Endpoint : it uses ENI(elastic network interface) , it has private link 

2 Gateway Load Balancer :  it uses ENI(elastic network interface) , it has private link 

3 Gateway Endpoints : recommended , it works with Routing tables 

-- use Gateway Endpoint 

-- search for s3  and use Gateway Endpoints --> select VPC and attach RT to private Routing Tables 

-- EndPOINT use AWS inernal network to acces the services 

-- go n check in Private RT it will create one oute for ENDPOINT ( pl-78a54011)

-- pl = prefix list = group of network ranges  

-- we have created ENDPINTS , now try to Access 

-- now try to create bucket 

aws s3 mb s3://vpc-bucket19876 --region ap-south-1

-- u will able to create without NAT and internet 

-- aws s3 ls 

-- it works only to connect for AWS Services only 








---------------------------------------------------------- VPC Flow Logs



• Capture information about IP traffic going into your interfaces:

 • VPC Flow Logs
 • Subnet Flow Logs
 • Elastic Network Interface (ENI) Flow Logs

• Helps to monitor & troubleshoot connectivity issues

• Flow logs data can go to S3, CloudWatch Logs, and Kinesis Data Firehose

• Captures network information from AWS managed interfaces too: ELB, RDS, ElastiCache, Redshift,WorkSpaces, NATGW,Transit Gateway...

• Helps to monitor & troubleshoot connectivity issues. Example: 

   • Subnets to internet
   • Subnets to subnets
   • Internet to subnets




---------------------------------------------------------- VPC Flow Logs – Troubleshoot SG & NACL issues



-- So let's have a look at the typical incoming request for your NACL and your subnet. So remember, your NACL's are stateless and your security groups are stateful.


Incoming Requests:

        • Inbound REJECT => NACL or SG

        • InboundACCEPT,OutboundREJECT=> NACL

Outgoing Requests:

        • Outbound REJECT => NACL or SG

        • OutboundACCEPT,InboundREJECT=> NACL





---------------------------------------------------------- VPC Flow Logs Hands ON 




-- go to clouwatch and create log group 

-- CloudWatch --> logs --> loggroups --> create log group to store the logs 

-- create VPC with CIDR 192.168.0.0/16

-- create IGW , and attach to vpc that u have created

-- one RT will created when u create VPC , go to route table --> edit routes --> add internet gateway 

-- now create subnet for VPC , jst one for practice purpose 

-- as we know the vpc flowlogs can be created at 
  
   • VPC Flow Logs
   • Subnet Flow Logs
   • Elastic Network Interface (ENI) Flow Logs

-- in this demo we are creating at VPC level 

-- go back to VPC --> actions --> create flowlog for ACCEPT for this demo 

-- create one role for this demo 

-- create ec2 instance with user data  with in the subnet that u have created 

policy 

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogGroups",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    }
  ]
}   

-- trust policy

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "vpc-flow-logs.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
} 

-- attach role to the instance 

-- allow http and SSH for ur SG 

-- now go to CW and check logs 

-- it will create some logs , these are flow logs and has all the information , it looks like 

eg : 

2 298132369629 eni-05988048dea221b5a 3.5.211.132 192.168.1.77 443 36382 6 741 6075864 1708930056 1708930116 ACCEPT OK

2 298132369629 eni-05988048dea221b5a 192.168.1.77 3.5.211.132 36382 443 6 96 5957 1708930056 1708930116 ACCEPT OK


VPC Flow Logs Syntax

2 298132369629 eni-05988048dea221b5a 3.5.211.132 192.168.1.77 443 36382 6 741 6075864 1708930056 1708930116 ACCEPT OK


explanation : 

2                     =  version 
298132369629          =  account-id
eni-05988048dea221b5a =  interface-id
3.5.211.132           =  srcaddr
192.168.1.77          =  dstaddr
443                   =  srcport 
36382                 =  dstport
6                     =  protocol
741                   =  packets
6075864               =  bytes
1708930056            =  start
1708930116            =  end
ACCEPT                =  action
OK                    =  log-status


• srcaddr & dstaddr – help identify problematic IP

• srcaddr & dstaddr – help identify problematic IP

• Action – success or failure of the request due to Security Group / NACL

• Can be used for analytics on usage patterns, or malicious behavior

• Query VPC flow logs using Athena on S3 or CloudWatch Logs Insights


• Flow Logs examples: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs- records-examples.html






---------------------------------------------------------- AWS Site-to-Site VPN



-- So the idea is that, now we have a VPC but we may have also a structure within the corporate data center, and we wanna connect AWS to our corporate data center using a private connection.

-- So for this, we'll have a customer gateway on the corporation side, and a VPN gateway on the VPC side. And we're going to establish, through the public internet, a private site-to-site VPN connection.

-- So it's a VPN connection, so it's encrypted. It goes over the public internet, though. And using this, we have linked effectively the network of our VPC to the network of our corporate data center.

-- A VPN connection refers to the connection between your VPC and your own on-premises network. Site-to-Site VPN supports Internet Protocol security (IPsec) VPN connections.

-- By default, instances that you launch into an Amazon VPC can't communicate with your own (remote/ on-premises) network.

-- VPN connection: A secure connection between your on-premises equipment and your VPCs.

-- VPN tunnel: An encrypted link where data can pass from the customer network to or from AWS.

-- Customer gateway: An AWS resource which provides information to AWS about your customer gateway device.




• Virtual Private Gateway (VGW)

        • VPN concentrator on the AWS side of the VPN connection

        • VGW is created and attached to the VPC from which you want to create the Site-to-Site VPN connection

        • Possibility to customize the ASN (Autonomous System Number)


• Customer Gateway (CGW)

        • Software application or physical device on customer side of the VPN connection

        • https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html#DevicesTested








---------------------------------------------------------- So, how do we set up the customer gateway device that is on premises?



• Customer Gateway Device (On-premises)

        • What IP address to use?

                • Public Internet-routable IP address for your Customer Gateway device

                        - Well, if your customer gateway is public, there is a public internet-routable IP address for your customer gateway device. Then you would use this one,between your VGW and your CGW using the public IP of the customer gateway.

                • If it’s behind a NAT device that’s enabled for NAT traversal (NAT-T), use the public IP address of the NAT device

                        - But it's possible for your customer gateway to also be private and have a private IP. 
                        
                        - In which case, then it is very common for it to be behind a NAT device that has NAT-T enabled. And then, that NAT device has a public IP, in which case, the IP address you should use for the CGW is the public IP of the NAT device.

                        - And then, the site-to-site VPN connection can be established.

• Impor tant step: enable Route Propagation for theVirtual Private Gateway in the route table that is associated with your subnets

        - even though this is set up, this site-to-site VPN connection will not work until you enable route propagation in your VPC within your subnets. And then, when this is done, then the connectivity will work.


• If you need to ping your EC2 instances from on-premises, make sure you add the ICMP protocol on the inbound of your security groups








--------------------------------------- The following are the key concepts for Site-to-Site VPN:




1 VPN connection: A secure connection between your on-premises equipment and your VPCs.

2 VPN tunnel: An encrypted link where data can pass from the customer network to or from AWS.

     Each VPN connection includes two VPN tunnels which you can simultaneously use for high availability.

3 Customer gateway: An AWS resource which provides information to AWS about your customer gateway device.

4 Customer gateway device: A physical device or software application on your side of the Site-to-Site VPN connection.

5 Target gateway: A generic term for the VPN endpoint on the Amazon side of the Site-to-Site VPN connection.

6 Virtual private gateway: A virtual private gateway is the VPN endpoint on the Amazon side of your Site-to-Site VPN connection that can be attached to a single VPC.

7 Transit gateway: A transit hub that can be used to interconnect multiple VPCs and on-premises networks, and as a VPN endpoint for the Amazon side of the Site-to-Site VPN connection.





--------------------------------------- Site-to-Site VPN limitations





A Site-to-Site VPN connection has the following limitations.

   -- IPv6 traffic is not supported for VPN connections on a virtual private gateway.

   -- An AWS VPN connection does not support Path MTU Discovery.


--- What is IPSec VPN?

An IPSec VPN is a VPN software that uses the IPSec protocol to create encrypted tunnels on the internet. It provides end-to-end encryption, which means data is scrambled at the computer and unscrambled at the receiving server.


-- What is the meaning of IPSec?

IPSEC stands for IP Security. It is an Internet Engineering Task Force (IETF) standard suite of protocols between 2 communication points across the IP network that provide data authentication, integrity, and confidentiality.



-- we can connect the services only which are present in the VPC only 







---------------------------------------------------------- AWS Site-to-Site VPN Hands ON 




STEP 1 (AWS SIDE)

--  for this we do not have on-premises , so use another region as ur on-premises

-- first go to mumbai region --> create vpc with 

-- create one subnet with 10.1.0.0/24

-- now create IGW and attach to the VPC 

-- now create RT and add subnet association and IGW 

STEP 2 : (Customer Side)

-- now go to singapore region , go to vpc and create vpc with 10.2.0.0/16

-- create subnet with 10.2.0.0/24

-- now create IGW and attach to the VPC 

-- now create RT and add subnet association and IGW 

STEP 3 : 

-- now in singapore region , create one ec2 instance (amazon linux 2 , kernal version 5.10)

-- create one new SG , wich allows SSH , ALL TCP and ALL ICMP - IPv4

-- launch instance 


STEP 4 

-- now go to mubai region 

-- VPC --> Virtual private network (VPN) --> create Virtual private Gateway and attach to vpc

-- now create customer gate way , in the palce of IP address (copy IPV4 of singapore ec2 address and paste it), no need to give Certificate 

-- now create site to site vpn connection 

-- Create VPN connection --> Target gateway type = VPG --> Customer gateway =Customer gateway ID --> Routing options = static --> Static IP prefixes = subnet prefic of singapore region (10.2.0.0/16)

-- it will take 4-5 min to create 

-- now open Route table in mumbai region --> route Propagation , it will show u VPW automatically --> edit --> enable --save 

-- now go to site to site vpn connection --> download configuration --> vendor = generic --> download 

-- it will contails all the details which is necessary to the vpn connection 


STEP 5 


-- now go to singapore region 

-- connect ec2 instance 

Installation of Openswan 

openswan : Openswan is an IPsec implementation for Linux. It has support for most of the extensions (RFC + IETF drafts) related to IPsec, including IKEv2, X.509 Digital Certificates, NAT Traversal, and many others.



1 sudo su 

2 yum install openswan -y

3 vim /etc/ipsec.conf
   In /etc/ipsec.conf uncomment following line if not already uncommented:
                " include /etc/ipsec.d/*.conf "

4 vim /etc/sysctl.conf --> add below lines in that file 
   net.ipv4.ip_forward = 1
   net.ipv4.conf.all.accept_redirects = 0
   net.ipv4.conf.all.send_redirects = 0

5 service network restart

6 vim /etc/ipsec.d/aws-vpn.conf

   conn Tunnel1
        authby=secret
        auto=start
        left=%defaultroute
        leftid=Customer end Gateway VPN public IP
        right=AWS Virtual private gateway ID- public IP
        type=tunnel
        ikelifetime=8h
        keylife=1h
        phase2alg=aes128-sha1;modp1024
        ike=aes128-sha1;modp1024
        keyingtries=%forever
        keyexchange=ike
        leftsubnet=Customer end VPN CIDR
        rightsubnet=AWS end VPN CIDR
        dpddelay=10
        dpdtimeout=30
        dpdaction=restart_by_peer
-- here update the values of left , right ids and left and right subnets  , from the download configuration file copy n paste values 
-- in configuration file 
#3: Tunnel Interface Configuration
Outside IP Addresses: = public ip 
inside  IP Addresses: = private ip

now do copy Outside IP Addresses of customer gateway and paste in leftid

now do copy Outside IP Addresses of Virtual Private Gateway	 and paste in right 

also update left and right subnet ids of ur vpc 

left subnet=Customer end VPN CIDR (on-premises )
 rightsubnet = AWS end VPN CIDR (AWS Side)


 conn Tunnel1
        authby=secret
        auto=start
        left=%defaultroute
        leftid=52.221.225.18
        right=3.6.30.187
        type=tunnel
        ikelifetime=8h
        keylife=1h
        phase2alg=aes128-sha1;modp1024
        ike=aes128-sha1;modp1024
        keyingtries=%forever
        keyexchange=ike
        leftsubnet=10.2.0.0/16
        rightsubnet=10.1.0.0/16
        dpddelay=10
        dpdtimeout=30
        dpdaction=restart_by_peer

paste these values in the /etc/ipsec.d/aws-vpn.conf 


7 vim /etc/ipsec.d/aws-vpn.secrets
customer_public_ip aws_vgw_public_ip: PSK "shared secret" -- this is format n update the IP'S

-- copy Outside IP Addresses of customer gateway and paste in customer_public_ip
-- copy Outside IP Addresses of Virtual Private Gateway	 and paste in aws_vgw_public_ip

eg : customer_public_ip aws_vgw_public_ip: PSK "shared secret" 
     52.221.225.18 3.6.30.187: PSK "ochAgBIM5pI3Rtzd1mwUKxW7ioiS0twD"

-- "shared secret"  = IPSec Tunnel #1 --> Pre-Shared Key  

8 Commands to enable/start ipsec service

   chkconfig ipsec on
   service ipsec start
   service ipsec status




-- now go to mumbai region  --> site to site vpn --> tunnel --> tunnel1 is UP 


Step 6 

-- go to mumbai region --> create ec2 instance with ur vpc configuration

-- in Sg add ICMP ipv4 

-- copy private IP of the instance 

--  go to sinagpore instance connect --> ping <private IP of mumbai instance>

u will get responce 


--------------- done --------------------------







---------------------------------------------------------- AWS VPN CloudHub



• Provide secure communication between multiple sites, if you have multiple VPN connections

• Low-cost hub-and-spoke model for primary or secondary network connectivity between different locations (VPN only)

  -- the hub and spoke model provides a means of distribution that relies on a central location (the hub) and a number of spokes leading out from that hub.


• It’s a VPN connection so it goes over the public Internet

• To set it up,connect multiple VPN connections on the same VGW, setup dynamic routing and configure route tables


-- If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. 

-- This enables your remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. 

-- The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. 

-- This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.






---------------------------------------------------------- Direct Connect (DX)



IMP to know , they are 5 parts in aws to work on hybrid networking 

1 Direct connect 

2 site 2 site vpn connection

3 client vpn 

4 open vpn (3rd party)

5 bastion host


- apart this DX , remaining are wotk through the internet 

- only DX works with Intranet , which means direct connection to ur server and aws network through the cables(wired connection), all the cost will be paid by customer , it is one time set up




• Provides a dedicated private connection from a remote network to your VPC

• Dedicated connection must be setup between your DC and AWS Direct Connect locations

• You need to setup a Virtual Private Gateway on your VPC

• Access public resources (S3) and private (EC2) on same connection

• Use Cases:

   • Increase bandwidth throughput - working with large data sets – lower cost

   • More consistent network experience - applications using real-time data feeds 

   • Hybrid Environments (on prem + cloud)

• Supports both IPv4 and IPv6

-- AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS. 

-- Using AWS Direct Connect, data that would have previously been transported over the internet is delivered through a private network connection between your on-premises data center and AWS.

-- Data transfer pricing over AWS Direct Connect is lower than data transfer pricing over the internet,





------- Direct Connect Gateway

• If you want to setup a Direct Connect to one or more VPC in many different regions (same account), you must use a Direct Connect Gateway





---------------------------------------------------------- Direct Connect – Connection Types



• Dedicated Connections: 1Gbps,10 Gbps and 100 Gbps capacity

        • Physical ethernet port dedicated to a customer

        • Request made to AWS first, then completed by AWS Direct Connect Partners

• Hosted Connections: 50Mbps, 500 Mbps, to 10 Gbps

        • Connection requests are made via AWS Direct Connect Partners

        • Capacity can be added or removed on demand

        • 1, 2, 5, 10 Gbps available at select AWS Direct Connect Partners


• Lead times are often longer than 1 month to establish a new connection






---------------------------------------------------------- Direct Connect – Encryption


• Data in transit is not encrypted but is private

• AWS Direct Connect + VPN provides an IPsec-encrypted private connection

• Good for an extra level of security, but slightly more complex to put in place


-- we can connect the all aws services through the direct connection , eg dynamoDB , s3 , ec2 etc....... but in site 2 site , we can connect services only in the vpc 





---------------------------------------------------------- Direct Connect - Resiliency



High Resiliency for Critical Workloads

        - One connection at multiple locations


Maximum Resiliency for Critical Workloads

        - Maximum resilience is achieved by separate connections terminating on separate devices in more than one location.
 







EPV : To establish a private connection between your virtual private cloud (VPC) and the Amazon EFS(elastic file system) API, you can create an interface VPC endpoint. You can also access the interface VPC endpoint from on-premises environments or other VPCs using AWS VPN, AWS Direct Connect, or VPC peering.

-- AWS Direct Connect provides three types of virtual interfaces: public, private, and transit.

1 Public virtual interface

-- To connect to AWS resources that are reachable by a public IP address such as an Amazon Simple Storage Service (Amazon S3) bucket or AWS public endpoints, use a public virtual interface.

-- With a public virtual interface, you can:

   A Connect to all AWS public IP addresses globally.

   B Create public virtual interfaces in any Direct Connect location to receive Amazon’s global IP routes.

   C Access publicly routable Amazon services in any AWS Region (except the AWS China Region).


2 Private virtual interface

-- To connect to your resources hosted in an Amazon Virtual Private Cloud (Amazon VPC) using their private IP addresses, use a private virtual interface. With a private virtual interface, you can:

   A Connect VPC resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances or load balancers on your private IP address or endpoint.

   B Connect a private virtual interface to a Direct Connect gateway. Then, associate the Direct Connect gateway with one or more virtual private gateways in any AWS Region (except the AWS China Region).

   C Connect to multiple Amazon VPCs in any AWS Region (except the AWS China Region), because a virtual private gateway is associated with a single VPC.

Note: For a private virtual interface, AWS advertises the VPC CIDR only over the Border Gateway Protocol (BGP) neighbor. AWS can't advertise or suppress specific subnet blocks in the Amazon VPC for a private virtual interface.


3 Transit virtual interface

-- To connect to your resources hosted in an Amazon VPC (using their private IP addresses) through a transit gateway, use a transit virtual interface. With a transit virtual interface, you can:

A Connect multiple Amazon VPCs in the same or different AWS account using Direct Connect.

B Associate up to three transit gateways in any AWS Region when you use a transit virtual interface to connect to a Direct Connect gateway.

C Attach Amazon VPCs in the same AWS Region to the transit gateway. Then, access multiple VPCs in different AWS accounts in the same AWS Region using a transit virtual interface.


Note: For transit virtual interface, AWS advertises only routes that you specify in the allowed prefixes list on the Direct Connect gateway. For a list of all AWS Regions that offer Direct Connect support for AWS Transit Gateway, see AWS Transit Gateway support.










 ---------------------------------------------------------- Site-to-Site VPN connection as a backup



 -- So the idea is that you have your corporate data center and it's connected to your VPC using Direct Connect. And that's your primary connection and it's expensive, 
 
 -- but maybe sometimes you will have an issue with your Direct Connect connection, and of course, you do not want to have no internet connection into your VPC.

 -- Therefore, you can use a Direct Connect as a secondary connection, but that would be quite expensive. 
 
 -- Or you can set up a site to site VPN connection as a backup connection and set it up so that if the primary connection fails, then it kicks in, and now you are connected through the public internet using site to site VPN, 
 
 -- which can be a bit more reliable because the internet, public internet, may always be accessible, 



 • In case Direct Connect fails, you can set up a backup Direct Connect connection (expensive), or a Site-to-Site VPN connection



    Corporate DC <-------(Direct Connect Primary Connection)------------> vpc
                 <-------(Site-to-Site VPN Backup Connection)------------>






---------------------------------------------------------- Exposing services in yourVPC to otherVPC   (AWS Private Link - VPC ENDPOINT Service)


-- how we can expose services in our VPC to other VPCs.


• Option 1: make it public

        • Goes through the public www

        • Tough to manage access

        - but it is not very optimal. It's not reliable because it goes through the public internet. I mean, there's definitely some better options.


• Option 2:VPC peering

        • Must create many peering relations

        • Opens the whole network

        - you need to have one VPC connected to multiple customers VPCs you need to set up peering connections between all these VPCs together

        - Whereas all you wanted was just to externalize one application service. So as you can expect, there is a better way of doing it. And this way is called AWS PrivateLink





---------------------------------------------------------- AWS PrivateLink (VPC Endpoint Services)



• Most secure & scalable way to expose a service to 1000s ofVPC (own or other accounts)

• Does not require VPC peering, internet gateway, NAT, route tables...

• Requires a network load balancer (Service VPC) and ENI (Customer VPC) or GWLB

• If the NLB is in multiple AZ, and the ENIs in multiple AZ, the solution is fault tolerant!

        - to establish a connection between the NLB and the ENI there was no need to establish VPC peering before. And that's really, really, really powerful as a concept.



-- go to endpoint services in console and create 






---------------------------------------------------------- EC2-Classic & AWS ClassicLink (deprecated)


• EC2-Classic: instances run in a single network shared with other customers

• Amazon VPC: your instances run logically isolated to your AWS account


• ClassicLink allows you to link EC2-Classic instances to a VPC in your account

        • Must associate a security group

        • Enables communication using private IPv4 addresses

        • Removes the need to make use of public IPv4 addresses or Elastic IP addresses


• Likely to be distractors at the exam







---------------------------------------------------------- Transit Gateway



• For having transitive peering between thousands of VPC and on-premises, hub-and-spoke (star) connection

• Regional resource, can work cross-region

• Share cross-account using Resource Access Manager (RAM)

-- What is AWS Resource Access Manager?

Ans : AWS Resource Access Manager (AWS RAM) helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs) in AWS Organizations, and with IAM roles and IAM users for supported resource types. You can use AWS RAM to share resources with other AWS accounts. This eliminates the need to provision and manage resources in every account. When you share a resource with another account, that account is granted access to the resource and any policies and permissions in that account apply to the shared resource.

• You can peer Transit Gateways across regions

• RouteTables:limit which VPC can talk with other VPC

• Works with Direct Connect Gateway,VPN connections

• Supports IP Multicast (not supported by any other AWS ser vice)

-- You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. 

-- RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own.

-- You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps:

Step 1 : create a Resource Share, 

Step 2 : specify resources,

Step 3 : specify accounts. 

-- RAM is available to you at no additional charge





---------------------------------------------------------- Transit Gateway: Site-to-Site VPN ECMP



• ECMP = Equal cost multi-path routing

• Routing strategy to allow to forward a packet over multiple best path

• Use case: create multiple Site- to-Site VPN connections to increase the bandwidth of your connection to AWS




---------------------------------------------------------- Transit Gateway – Share Direct Connect between multiple accounts


-- you can share your direct connect connection between multiple accounts, again, using the transit gateway. How do we do this?

-- Well, we're going to establish a direct connect connection between your corporate data center and a direct connect location,

-- and then we're going to set up a transit gateway into both VPCs in two different accounts, this is something we can do with the transit gateway.

-- then, we connect the direct connect location into direct connect gateway and connect that gateway into the transit gateway.

-- And what this just allowed us to do is to share a direct connect connection between multiple accounts and multiple VPC, which is very handy thanks to the transit gateway.






---------------------------------------------------------- VPC – Traffic Mirroring


• Allows you to capture and inspect network traffic in your VPC

• Route the traffic to security appliances that you manage

• Capture the traffic
   
    • From (Source) – ENIs
    • To (Targets) – an ENI or a Network Load Balancer

• Capture all packets or capture the packets of your interest (optionally, truncate packets)

• Source and Target can be in the same VPC or different VPCs (VPC Peering)

• Use cases: content inspection, threat monitoring, troubleshooting, ...





---------------------------------------------------------- What is IPv6?




• IPv4 designed to provide 4.3 Billion addresses (they’ll be exhausted soon), it is 32 bit

• IPv6 is the successor of IPv4

• IPv6 is designed to provide 3.4 × 10^38, unique IP addresses ,  128 bit

• Every IPv6 address is public and Internet-routable (no private range)

• Format  x.x.x.x.x.x.x.x (x is hexadecimal, range can be from 0000 to ffff)

• Examples:
   
    • 2001:db8:3333:4444:5555:6666:7777:8888
    • 2001:db8:3333:4444:cccc:dddd:eeee:ffff
    • :: = all 8 segments are zero
    • 2001:db8:: = the last 6 segments are zero
    • ::1234:5678 = the first 6 segments are zero
    • 2001:db8::1234:5678 = the middle 4 segments are zero




---------------------------------------------------------- IPv6 in VPC



• IPv4 cannot be disabled for your VPC and subnets

• You can enable IPv6 (they’re public IP addresses) to operate in dual-stack mode

• Your EC2 instances will get at least a private internal IPv4 and a public IPv6

• They can communicate using either IPv4 or IPv6 to the internet through an Internet Gateway




---------------------------------------------------------- IPv6 Troubleshooting



• IPv4 cannot be disabled for your VPC and subnets

• So, if you cannot launch an EC2 instance in your subnet
  
    • It’s not because it cannot acquire an IPv6 (the space is very large)
    • It’s because there are no available IPv4 in your subnet

• Solution: create a new IPv4 CIDR in your subnet




---------------------------------------------------------- Egress-only Internet Gateway



• Used for IPv6 only

• (similar to a NAT Gateway but for IPv6)

• Allows instances in your VPC outbound connections over IPv6 while preventing the internet to initiate an IPv6 connection to your instances

• You must update the Route Tables



---------------------------------------------------------- VPC Section Summary 



• CIDR – IP Range

• VPC – Virtual Private Cloud => we define a list of IPv4 & IPv6 CIDR

• Subnets – tied to an AZ, we define a CIDR

• Internet Gateway – at the VPC level, provide IPv4 & IPv6 Internet Access

• Route Tables – must be edited to add routes from subnets to the IGW,VPC Peering Connections,VPC Endpoints, ...

• Bastion Host – public EC2 instance to SSH into, that has SSH connectivity to EC2 instances in private subnets

• NAT Instances – gives Internet access to EC2 instances in private subnets. Old, must be setup in a public subnet, disable Source / Destination check flag

• NAT Gateway – managed by AWS, provides scalable Internet access to private EC2 instances, when the target is an IPv4 address

• NACL – stateless, subnet rules for inbound and outbound, don’t forget Ephemeral Ports

• Security Groups – stateful, operate at the EC2 instance level

• VPC Peering – connect two VPCs with non overlapping CIDR, non-transitive

• VPC Endpoints – provide private access to AWS Services (S3, DynamoDB, CloudFormation, SSM) within a VPC

• VPC Flow Logs – can be setup at the VPC / Subnet / ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, analyze using Athena or CloudWatch Logs Insights

• Site-to-Site VPN – setup a Customer Gateway on DC, a Virtual Private Gateway on VPC, and site-to-site VPN over public Internet

• AWS VPN CloudHub – hub-and-spoke VPN model to connect your sites

• Direct Connect – setup a Virtual Private Gateway on VPC, and establish a direct private connection to an AWS Direct Connect Location

• Direct Connect Gateway – setup a Direct Connect to many VPCs in different AWS regions

• AWS PrivateLink / VPC Endpoint Services:

    • Connect services privately from your service VPC to customers VPC

    • Doesn’t need VPC Peering, public Internet, NAT Gateway, Route Tables

    • Must be used with Network Load Balancer & ENI

• ClassicLink – connect EC2-Classic EC2 instances privately to your VPC

• Transit Gateway – transitive peering connections for VPC,VPN & DX

• Traffic Mirroring – copy network traffic from ENIs for further analysis

• Egress-only Internet Gateway – like a NAT Gateway, but for IPv6 targets







---------------------------------------------------------- Networking Costs in AWS per GB - Simplified




• Use Private IP instead of Public IP for good savings and better network performance

• Use same AZ for maximum savings (at the cost of high availability)


IMP : 

1  Free for traffic in same availability zone in same region 

2  Free if using private IP in same availability zone in same region 

3  $0.01 if Using private IP b/w two different availability zones in same region

4  $0.02 if using Public IP / Elastic IP b/w two different availability zones in same region

5  $0.02 Inter-region b/w two different regions 






---------------------------------------------------------- Minimizing egress traffic network cost




• Egress traffic: outbound traffic (from AWS to outside)

• Ingress traffic: inbound traffic - from outside to AWS (typically free)

• Try to keep as much internet traffic within AWS to minimize costs

• Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network



-- Egress cost is high when application in corporate data center 

-- Egress cost is minimized when application in AWS only 




---------------------------------------------------------- S3 Data Transfer Pricing – Analysis for USA


• S3 ingress: free

• S3 to Internet: $0.09 per GB

• S3TransferAcceleration:

        • Faster transfer times(50 to 500% better)

        • Additional cost on top of DataTransfer Pricing: +$0.04 to $0.08 per GB

• S3 to CloudFront: $0.00 per GB

• CloudFront to Internet: $0.085 per GB (slightly cheaper than S3)

        • Caching capability(lower latency)

        • Reduce costs associated with S3 Requests Pricing (7x cheaper with CloudFront)

• S3 Cross Region Replication: $0.02 per GB        





---------------------------------------------------------- Pricing: NAT Gateway vs Gateway VPC Endpoint


-- So we have a VPC in the region and we have two private subnets with two different types of EC2 instances. And they both want to access data into an Amazon S3 bucket.


option 1 

        -- So one way to do so is to use the public internet and so to do so, we set up a public subnets, 

        -- which has a NAT Gateway. For the subnet to be public, we need to have a route into an Internet Gateway. 

        -- And so we're going to establish a route using the route table. And then we have a direct connectivity

        -- from the EC2 instance through the NAT Gateway and through the Internet Gateway into the internet. 
        
        -- Then from the internet, we access the data in your Amazon S3 buckets. And so the cost associated with this is $0.045 per hour for your NAT Gateway,

        -- $0.045 NAT Gateway / hour

        -- $0.045 NAT Gateway data processed / GB

        -- $0.09 Data transfer out to S3 (cross-region)

        -- $0.00 Data transfer out to S3 (same-region)


option 2 :

        - But if we were to set up a VPC Endpoints,to access our data into our Amazon S3 bucket privately, then we set up a different route table.

        - So in this case, to access the VPC Endpoint we just set up a route to it. And so the data flows directly from the private subnets into the VPC Endpoints and the S3 buckets.

        - And we have no cost for using Gateway Endpoint and we pay 1 cent per gigabytes of data transferred in and out of your S3 bucket for the same region.

                - No cost for using Gateway Endpoint. $0.01 Data transfer in/out (same- region)

                - So this can be a significantly lower cost to use a VPC Endpoint instead of your NAT Gateway.






---------------------------------------------------------- Network Protection on AWS



• To protect network on AWS, we’ve seen

        • Network Access Control Lists (NACLs)

        • Amazon VPC security groups

        • AWS WAF (protect against malicious requests)

        • AWS Shield & AWS Shield Advanced

        • AWS Firewall Manager (to manage them across accounts)


• But what if we want to protect in a sophisticated way our entire VPC?





--------------------- AWS Network Firewall


• Protect your entire Amazon VPC

• From Layer 3 to Layer 7 protection

• Any direction, you can inspect

  • VPC to VPC traffic

  • Outbound to internet

  • Inbound from internet

  • To/from DirectConnect & Site-to-SiteVPN

• Internally, the AWS Network Firewall uses the AWS Gateway Load Balancer

• Rules can be centrally managed cross- account by AWS Firewall Manager to apply to many VPCs internet




---------------------------- Network Firewall – Fine Grained Controls 


• Supports 1000s of rules

  • IP & port - example: 10,000s of IPs filtering
  • Protocol – example: block the SMB protocol for outbound communications
  • Stateful domain list rule groups: only allow outbound traffic to *.mycorp.com or third-party software repo
  • General pattern matching using regex

• Traffic filtering: Allow,drop,or alert for the traffic that matches the rules

• Active flow inspection to protect against network threats with intrusion - prevention capabilities (like Gateway Load Balancer, but all managed by AWS)

• Send logs of rule matches to Amazon S3, CloudWatch Logs, Kinesis Data Firehose








---------------------------------------------------------- Other Services


1 X-ray 

-- when you do debugging production ,  the good old way:

     • Test locally
     • Add log statements everywhere 
     • Re-deploy in production , and from the logs try to figure out what is breaking, what is happening.

-- It's really painful. It's not best practices.

• Log formats differ across applications using CloudWatch and analytics is hard.

• Debugging: monolith “easy”, distributed services “hard”

• No common views of your entire architecture!


-- here comes x-ray 

-- So, X-ray is going to give you a visual analysis of your application.

-- We'll see that basically as a client doing a request to our application we will see how many of these requests fail or don't fail. And then, from the application we'll see what it does.

-- So, it will call other IPs, it will call SNS it will call a DynamoDB Table. And so, as you can see, we'll be able to trace exactly, visually what happens when we talk to our EC2 instance.








-------------------------------------------------------------------- AWS X-Ray advantages


• Troubleshooting performance (bottlenecks)

• You can Understand dependencies in a microservice architecture, because you can visually see what is happening and how all your microservices interact with one another.

• Pinpoint service issues

• Review request behavior

• Find errors and exceptions 

•  We can answer questions Are we meeting time SLA? , in terms of latency or time to process a request?

• Where I am throttled? we can understand which service really slows down, throttles us.

• Identify users that are impacted by our errors 


 


2 AWS Amplify - web and mobile applications

         • A set of tools and services that helps you develop and deploy scalable full stack web and mobile applications

         • Authentication,Storage,API(REST,GraphQL),CI/CD,PubSub,Analytics,AI/MLPredictions, Monitoring, ...

         • Connect your source code from GitHub,AWS CodeCommit,Bitbucket,GitLab,or upload directly



