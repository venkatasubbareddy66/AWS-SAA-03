

======================================================== AWS CLOUDFORMATION =========================================================



-- CloudFormation Linter - use this extension in vs code 

-- open terminal and enter these cmnds (python need to installed before doing this)

pip3 install cfn-lint
pip3 install pydot






======================================================== 1 Cloud Formation Introdution ======================================================== 


-- use us-east-1 region for cloudformation

-- we're going to create a simple EC2 instance.

-- We're going to add a security group to it.

-- create one yaml file 


intro.yaml


Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0ebfd941bbafe70c6
      InstanceType: t2.micro



-- upload this file in CF 

-- this template will get create ec2 instance for you 

-- now update the template with the following code 





Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0742b4e673072066f
      InstanceType: t2.micro
      SecurityGroups:
        - !Ref SSHSecurityGroup
        - !Ref ServerSecurityGroup

  # an elastic IP for our instance
  MyEIP:
    Type: AWS::EC2::EIP
    Properties:
      InstanceId: !Ref MyInstance

  # our EC2 security group
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  # our second EC2 security group
  ServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: allow connections from specified CIDR ranges
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 192.168.1.1/32




-- Changeset preview   ----> to know the Changes

-- this will create new instance and terminate old instance , So the idea here is that CloudFormation has determined that to apply the updates. It needed to create a new EC2 instance and terminate the old one.

-- now observe the changes and delete the stack once u done







-- Multiple teams of an e-commerce company use the same AWS CloudFormation template to create stacks of resources needed by them. For the next deployment, the teams need to update the stacks and have been testing the changes through change sets. However, the teams suddenly realized that all their change sets have been lost. Unable to figure out the error they have approached you.

As a SysOps Administrator, how will you identify the error and suggest a way to fix the issue?

ANS : A change set was successfully executed and this resulted in rest of the change sets being deleted by CloudFormation

EXP : 

    - Change sets allow you to preview how proposed changes to a stack might impact your existing resources, for example, whether your changes will delete or replace any critical resources,

    - AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. 

    - You can create and manage change sets using the AWS CloudFormation console, AWS CLI, or AWS CloudFormation API.

    - After you execute a change, AWS CloudFormation removes all change sets that are associated with the stack because they aren't applicable to the updated stack.





https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html










======================================================== 2 Cloud Formation Getting Started ======================================================== 





-- if you have code in json , if you want to convert from json to yaml , u can try on this website 


https://www.json2yaml.com/ 





----------------------------------- create s3 bucket using CF




-- try to create bucket using cfn


Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties: {}



-- upload this code and it will create bucket for you 

-- here  Properties: {}  , so we are providing any Properties so open and close brackets , we wont get any error

-- there's a bucket named string, if you do not specify it, then you will get a random bucket name.






----------------------------------- Update the s3 bucket using CF 



-- So before we update our stack, let's talk about the CloudFormation update behavior.

-- So CloudFormation updates resources based on differences between what you submit and the stack's current template.

-- So we'll look at where there's change and do the necessary updates.

-- The method of how it does it's updates depends on the property you have changed for a specific resource.


---------------------- IMP to know



-- you have three kinds of changes that can happen in CloudFormation.

      1 There is an updates with no interruption :
            
            - That means that it doesn't disrupt the resources operations and it doesn't change the physical ID of your resources.

            - For example, if you are updating the IAM instance profile of an EC2 instance, this is something you can do in the console without stopping  your EC2 instance and so can you while using CloudFormation.
    
      2 Update with some interruption :

            - for example, an EC2 instance being stopped. Well, if we change the instance type of our EC2 instance from a t2.micro to t2.large. It requires it to be stopped and then restarted,

            - so this is why it's called update with some interruption.

      3 replacements :

            - Replacements means that you have to recreate a new resource with a new physical ID, because the properties you have changed do not match with the current stack states.   

            - so therefor, the new resource will be created, the references will be changed from other resources to the new resource,and then the older resources will be deleted.

            - For example, if you change the availability zone of an RDS database instance, then the previous one will have to be deleted and the new one will have to be created.



-- we'll see more about updates and how we can maybe save some of these objects or prevent them from being replaced.


-- we're going to update a S3 bucket. And we'll consider two types of updates.

-- We'll consider the updates with no interruption by adding some Access Control.

-- And we'll also see a replacement updates by updating the name of the S3 buckets.

-- So let's have a look to see how CloudFormation reacts in both these cases.


https://repost.aws/knowledge-center/cloudformation-objectownership-acl-error    - REF 





AWSTemplateFormatVersion: "2010-09-09"

Resources:
  PortalBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: LogDeliveryWrite
      VersioningConfiguration:
        Status: Enabled
      WebsiteConfiguration:
        IndexDocument: 'index.html'
        ErrorDocument: 'error.html'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      OwnershipControls:
        Rules:
          - ObjectOwnership: ObjectWriter


-- This allows you to activate ACLs on the bucket.


-- now let's update and upload this templates and see what happens.

-- now change the bucket name the update requires a replacement. so make sure that your S3 bucket has zero objects.

-- this one will be deleted and a new one will be created.




Resources:
  PortalBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: LogDeliveryWrite
      VersioningConfiguration:
        Status: Enabled
      WebsiteConfiguration:
        IndexDocument: 'index.html'
        ErrorDocument: 'error.html'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      OwnershipControls:
        Rules:
          - ObjectOwnership: ObjectWriter
      BucketName: "insert-some-random-string-here-123452433"





-- upload this code and see in CF console

-- in Changeset preview , u can see that replacement = true 

-- there is my S3 bucket that is going to create a new physical resource,

-- once that S3 bucket is created, the other one that we have from right here will be deleted.

-- delete the stack , make sure no objects in s3 bucket









----------------------------------- CloudFormation template options



-- So there's a lot of options you can do when you upload a CloudFormation template.

    - Tags : 
    
         when we specify a tag in CloudFormation these tags will be passed on to every single resource created by CloudFormation which is quite handy.

    - Permissions: 
     
         Permissions is for us to be able to specify an IAM role to create all the resources within CloudFormation.

    - Notification options : 
    
         Notification options is to receive all these events around how your CloudFormation stack is doing to an SNS topic.

    - Timeout :
    
         is to say how long you're willing to wait before declaring that the CloudFormation templates, that creation has failed.
     
    - Roll back on failure:

         Roll back on failure is allowing you to not being able to delete your resources, if in case they're not being created correctly

    - Rollback configuration:

         Rollback configuration which is allowing you to set up a time or a specific Cloudwatch alarm 

    - Termination protection:

         Termination protection is a way for you to disable the deletion of your CloudFormation template accidentally.     





----------------------------------- CloudFormation – Building Blocks


• Template’s Components

        • AWSTemplateFormatVersion – identifies the capabilities of the template “2010-09-09” • Description – comments about the template

        • Resources (MANDATORY) – your AWS resources declared in the template

        • Parameters – the dynamic inputs for your template

        • Mappings – the static variables for your template

        • Outputs – references to what has been created

        • Conditionals – list of conditions to perform resource creation
    
 




----------------------------------- Deploying CloudFormationTemplates



• Manual way

    • Editing templates in Application Composer or code editor
    • Using the console to input parameters, etc...
    • We’ll mostly do this way in the course for learning purposes


• Automated way

    • Editing templates in a YAML file
    • Using the AWS CLI (Command Line Interface) to deploy the templates, or using a Continuous Delivery (CD) tool
    • Recommended way when you fully want to automate your flow








======================================================== CloudFormation – Parameters ======================================================== 




• Parameters are a way to provide inputs to your AWS CloudFormation template

• They’re important to know about if:

     • You want to reuse your templates across the company

     • Some inputs can not be determined ahead of time


• Parameters are extremely powerful, controlled, and can prevent errors from happening in your templates, thanks to types







-------------------------- When should you use a Parameter?


• Ask yourself this:

    • Is this CloudFormation resource configuration likely to change in the future? 
    • If so, make it a parameter






-------------------------- CloudFormation – Parameters Settings


• Parameters can be controlled by all these settings:


   • Type:
       
       • String – A literal string
       • Number – An integer or float
       • CommaDelimitedList – An array of literal strings that are separated by commas
       • List<Number> – An array of integers or floats
       • AWS-Specific Parameter (to help catch invalid values – match against existing values in the AWS account)
       • List<AWS-Specific Parameter>
       • SSM Parameter (get parameter value from SSM Parameter store)
       • Description
       • ConstraintDescription (String)
       • Min/MaxLength
       • Min/MaxValue
       • Default
       • AllowedValues (array)
       • AllowedPattern (regex)
       • NoEcho (Boolean)
       • AWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name
       • AWS::EC2::SecurityGroup::Id – A security group ID
       • AWS::EC2::Subnet::Id – A subnet ID
       • AWS::EC2::VPC::Id – A VPC ID
       • List<AWS::EC2::SecurityGroup::Id> – An array of security group IDs
       • List<AWS::EC2::Subnet::Id> – An array of subnet IDs






-------------------------- How to Reference a Parameter?


• The Fn::Ref function can be leveraged to reference parameters

• Parameters can be used anywhere in a template

Except:
  
    - TemplateFormatVersion,
    - description, 
    - transform and 
    - mappings.

• The shorthand for this inYAML is !Ref

• The function can also reference other elements within the template






------------------------------- CloudFormation – Parameters Example

     1 • AllowedValues (array)

Parameters:
 InstanceType:
   Description: Choose an ec2 type
   Type: String
   AllowedValues:
      - t2.micro
      - t3.micro
      - t2.small


Resources:
  Ec2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
       InstanceType: !Ref InstanceType
       ImageId: xxxxxxxxxxxxxx



-- Here , we have a parameter called InstanceType. To choose an EC2 InstanceType of Type: String.

-- But we have defined AllowedValues being t2.micro, t2.small, or t2.medium with a Default being t2.micro. And this parameter is reused in the EC2Instance.

-- So thanks to it, we'll have a dropdown and the user can only select one of these three values, hence giving them choice while giving you control.







     2 NoEcho (Boolean)


Parameters:
 DBPassword:
   Description: db password
   Type: String
   NoEcho: true

Resources:
  MyDBInstance:
   Type: 'AWS::RDS::DBInstance'
    Properties:
      DBInstanceClass: db.t2.microParameters
      AllocatedStorage: 20
      Engine: mysql 
      MasterUsername: administration
      MasterUserpassword: !Ref DBPassword
      DBInstanceIdentifier: MydbInstance


-- So for example, say we want as a parameter to put in the database password, but of course it is a password so we have to keep it secret.

-- So we want to remove it from the logs and so on. So we'll have NoEcho: true , so that the password is not displayed anywhere.









------------------------------- Parameters Hands on




create one yaml file like this with parameters




Parameters:
  SecurityGroupDescription:
    Description: Security Group Description (Simple parameter)
    Type: String

  SecurityGroupPort:
    Description: Simple Description of a Number Parameter, with MinValue and MaxValue
    Type: Number
    MinValue: 1150
    MaxValue: 65535

  InstanceType:
    Description: WebServer EC2 instance type (has default, AllowedValues)
    Type: String
    Default: t2.small
    AllowedValues:
      - t1.micro
      - t2.nano
      - t2.micro
      - t2.small
    ConstraintDescription: must be a valid EC2 instance type.

  DBPwd:
    NoEcho: true
    Description: The database admin account password (won't be echoed)
    Type: String

  KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instances. Linked to AWS Parameter
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: must be the name of an existing EC2 KeyPair.

  SecurityGroupIngressCIDR:
    Description: The IP address range that can be used to communicate to the EC2 instances
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: (\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})/(\d{1,2})
    ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x.

  MyVPC:
    Description: VPC to operate in
    Type: AWS::EC2::VPC::Id

  MySubnetIDs:
    Description: Subnet IDs that is a List of Subnet Id
    Type: "List<AWS::EC2::Subnet::Id>"

  DbSubnetIpBlocks:
    Description: "Comma-delimited list of three CIDR blocks"
    Type: CommaDelimitedList
    Default: "10.0.48.0/24, 10.0.112.0/24, 10.0.176.0/24"

Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      #we reference the InstanceType parameter
      InstanceType: !Ref InstanceType
      KeyName: !Ref KeyName
      ImageId: ami-0742b4e673072066f
      # here we reference an internal CloudFormation resource
      SubnetId: !Ref DbSubnet1

  MySecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Ref SecurityGroupDescription
      SecurityGroupIngress:
        - CidrIp: !Ref SecurityGroupIngressCIDR
          FromPort: !Ref SecurityGroupPort
          ToPort: !Ref SecurityGroupPort
          IpProtocol: tcp
      VpcId: !Ref MyVPC

  DbSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MyVPC
      # the select function allows us to select across a list
      CidrBlock: !Select [0, !Ref DbSubnetIpBlocks]

  DbSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MyVPC
      # the select function allows us to select across a list
      CidrBlock: !Select [1, !Ref DbSubnetIpBlocks]
      
  DbSubnet3:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MyVPC
      # the select function allows us to select across a list
      CidrBlock: !Select [2, !Ref DbSubnetIpBlocks]




Explanation: in above script we have different types of parameters 

- (\d{1,3}) = 1-3 digits 

- we can reference these parameters within our templates. check resource block

- the Ref function works equally well for parameters, as well as references

- deploy this stack and observe how it is Getting created resources





---------------------------------------- ADVANCED SSM Parameter Type



-- So let's go ahead and learn about how to use a parameter that comes from the Systems Manager Parameter Store. So it's called an SSM Parameter type.

-- So the idea is that we want to store the value of the parameter outside of your CloudFormation template in a service called the Parameter Store.

IMP -- So the value of the parameter in our CloudFormation template is going to present the key in the Parameter Store.

-- the CloudFormation will always fetch the latest value of the parameter independents or source or you can not specify a specific version.

-- CloudFormation does not store Secure String values.

-- So if you're using a parameter that is secure, then it's compatible with CloudFormation and the validation can be done on the SSM Parameter keys but you can not do a validation on the value returned by the Parameter Store.





---------------------- Example - Fetch Latest AMI ID's


-- So the idea is that when we are launching a new two instance and want to use, for example, Amazon Linux 2.

-- We know that the AMI ID does change from region to region. We know that the AMI ID does change from region to region.

-- And on top of that, the latest Amazon Linux 2 always gets updated from AWS from time to time.

-- So that means we need to update all of our templates to keep up with the latest AMI ID of the latest updates of Amazon Linux 2

-- So instead there is a Parameter in the Parameter store from AWS which does always return the latest AMI ID for your Amazon Linux 2. Then on top of it, it is going to be changing from region to region.

-- So we moved the templates and you pass in this Parameter and you do a create stack, CloudFormation will fetch the latest AMI ID from the SSM Parameter store, which will be returned and then use within your templates.






---------------------- ADVANCED SSM Parameter Type Hands On


-- create template like this 


Parameters:
  InstanceType:
    Description: WebServer EC2 instance type
    Type: AWS::SSM::Parameter::Value<String>
    Default: /dev/ec2/instanceType
    
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: !Ref InstanceType
      ImageId: !Ref ImageId





-- So the first one is instance type, and we're going to specify the instance type for an EC2 instance.

-- And so the idea is that we don't want to specify the instance type directly as the parameter, but instead, we want to reference the instance type coming from the SSM parameter store

-- open parameter store in console and create parameter with the name /dev/ec2/instanceType , Tier = std , Type = string , Data type = text,value = t2.micro




-- The second parameter we have in our confirmation template is ImageID. And again, it is a type SSM parameter.

-- now and deploy the stack in CF 

-- once u upload the stacj the image id /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2  , automatically fetch latest AMI  from the public parameters store 

-- create stack , ec2 created , so everry time it will fetch latest AMI from the parameter store 

-- now chnage the value of parameter from t2.micro to t2.small , update the stack , 

-- it will take values from parameter store and update the instance type according that value , in this process the ec2 instance will get stopped and updates and sgin Started










======================================================== AWS CLOUDFORMATION RESOURCES, =========================================================




• Resources are the core of your CloudFormation template (MANDATORY)

• They represent the different AWS Components that will be created and configured

• Resources are declared and can reference each other

• AWS figures out creation, updates and deletes of resources for us

• There are over 700 types of resources (!)

• Resource types identifiers are of the form:

       service-provider::service-name::data-type-name

       EG : AWS::EC2::Volume



https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-instance.html  - ref 









------------------------------------------ optional attributes for resource



- DependsOn :

    - this is a very useful one to allow to draw a dependency between two resources.

    - For example, you want to only create an ECS cluster after the underlying Auto Scaling Group is created.

- DeletionPolicy:

    - this is going to allow you to protect resources from being deleted, even if, the CloudFormation stack is deleted.

    - For example, you never want to lose an RDS database that you create through a CloudFormation stack.Then, you would add a DeletionPolicy to it.


- UpdateReplacePolicy:

    - The UpdateReplacePolicy is also similar. The system protects resources from being replaced during a CloudFormation update.

    



---------------------------------------- 1 DependsOn :


-- allows you to make sure that a resource is created after another one because they depend on each other.

• Applied automatically when using !Ref and !GetAtt

• Use with any resource



Resources:
  Ec2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId:
        Fn::FindInMap:
        - RegionMap
        - Ref: AWS::Region
        - AMI
    DependsOn: myDB
  myDB:
    Type: AWS::RDS::DBInstance
    Properties:
      AllocatedStorage: '5'
      DBInstanceClass: db.t2.small
      Engine: MySQL
      EngineVersion: '5.5'
      MasterUsername: MyName
      MasterUserPassword: MyPassword



-- we can see that we have an EC2 instance and an RDS database instance being created. And in this example we're saying that we want the EC2 instance to depend on the database.      

-- so therefore the EC2 instance will only be created once the database itself has been successfully created.




-------------------------------- DependsOn hands On



Parameters:  
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket

  # the EC2 instance will be created after the S3 bucket
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.micro
    DependsOn: MyS3Bucket





-- the EC2 instance will only be created after the S3 bucket. And in case of a deletion, it will be deleted first and then the S3 bucket will be deleted after.

-- uplaod the stack and observe 

-- now delete the stack u can observe that First, my EC2 instance should be deleted , then the S3 bucket will be deleted.







-------------------------------- 2 DeletionPolicy



• Control what happens when the CloudFormation template is deleted or when a resource is removed from a CloudFormation template

• Extra safety measure to preserve and backup resources

• Default DeletionPolicy=Delete

      • Delete won’t work on an S3 bucket if the bucket is not empty



-- So by default, we've seen that when we delete a CloudFormation template, all the resources within are also deleted. That means that the default DeletionPolicy is delete, so you don't have to specify it because it is the default.


EG:1

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
    DeletionPolicy: Delete  





-------------------------------- CloudFormation – DeletionPolicy Retain


• DeletionPolicy=Retain:

      • Specify on resources to preserve in case of CloudFormation deletes

      • Works with any resources




EG :

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  myS3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain



-- So example here, we have a S3, and we know that by default, it would be deleted when I delete my CloudFormation template, 

-- but maybe we actually wanna keep it, keep the data within because we care about the data of this bucket. And so we would specify at the bottom DeletionPolicy retain.

-- And so even if I delete my CloudFormation templates, this S3 Bucket would stay, and this works with any resources.








----------------------------- 3 CloudFormation – DeletionPolicy Snapshot


• DeletionPolicy=Snapshot

• Create one final snapshot before deleting the resource

• Examples of supported resources:

    • EBS Volume, ElastiCache Cluster, ElastiCache ReplicationGroup

    • RDS DBInstance, RDS DBCluster, Redshift Cluster, Neptune DBCluster, DocumentDB DBCluster





----------- LAB 


-- create one yaml file that has DeletionPolicy


Resources:
  MySG:
    Type: AWS::EC2::SecurityGroup
    DeletionPolicy: Retain
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  MyEBS:
    Type: AWS::EC2::Volume
    DeletionPolicy: Snapshot
    Properties:
      AvailabilityZone: us-east-1a
      Size: 1
      VolumeType: gp2



-- in above example , there is a security group and the DeletionPolicy is retained. So that means that if I delete my transformation stack, this security group should stay.

-- And there is an EBS volume, and the DeletionPolicy is a snapshot. So that means that upon deleting the stack, the volume should go away, but a snapshot should be created first.

-- let's see how it is working 

-- upload yaml file and create one stack 

-- once the resource is created , do delete the stack

-- u can observe that MySG = DELETE_SKIPPED , 'coz we have given retain 

-- ebs volume is deleted but the snapshot is already created for u go n check in console 

-- u can delete manually through console , if u want to delete 









------------------------------- 3 UpdateReplacePolicy:



-- this is to control what happens to a resource if you update a property on the resource whose update behavior is Replacement.

-- For example, if you have a database instance and you change the availabilities on property then it's going to be replaced entirely.

-- this can be used with any kind of resource.

1 By default the UpdateReplacePolicy = delete:

         - that means that CloudFormation will delete the old resource and create a new one with a new physical ID and we have observed that behavior in the past,

-- but we can also set Update Replace Policy to retain. In this case, the old resource will be retained and removed from CloudFormation's scope and then a new resource will be created.         

2 UpdateReplacePolicy = Retain:

         - keeps the resource

3 UpdateReplacePolicy = Snapshot:

         - the snapshot itself that is created, as of this update, is not within CloudFormation's scope.

         - It exists, but it will not appear as a resource within your CloudFormation template.






------------------------------- what is the difference between a Deletion Policy and an Update Replace Policy?



-- Well, they go actually hand-in-hand together.

-- So the Update Replace Policy only applies to resources that are being replaced during a stack update,so this applies to stacks update,

-- while a deletion policy applies to resources deleted when a stack is deleted or when the resource definition is removed and therefore the underlying resource is deleted.

-- So they go hand-in-hand together and you then must use both if you want to protect a resource entirely,





----------------------------- hands On for both




Parameters:
  BucketName:
    Type: String

Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain





-- create stack and it will create s3 bucket 

-- now update the stack and observe

-- then there is no deletion of the previous resource because of the update replace policy

-- so that means that both my buckets exist.

-- but for the management of the buckets under CloudFormation only the latest one is referenced. The previous one is just removed from the scope of CloudFormation.

-- if I delete my stack

-- which also has, by the way, a deletion policy of Retain

-- so both my buckets are going to be retained and the previous one will not be deleted either.

-- So we would need to manually delete them ourself.








----------------------------- CloudFormation – Resources FAQ


• Can I create a dynamic number of resources?

         - For example, you say, hey, I want to create four S3 buckets at a time.

         - Yes, you can by using CloudFormation Macros and Transform


• Is every AWS Service supported?

         - Almost. Only a select few niches are not there yet

         - You can work around that using CloudFormation Custom Resources

         - Usually when there is a new feature of AWS, the CloudFormation update to support the feature comes very, very soon 








======================================================== AWS CLOUDFORMATION MAPPINGS =========================================================



• Mappings are fixed variables within your CloudFormation template

• They’re very handy to differentiate between different environments (dev vs prod), regions (AWS regions), AMI types...

• All the values are hardcoded within the template



EG : 

Mappings: 
  Mapping01: 
    Key01: 
      Name: Value01
    Key02: 
      Name: Value02
    Key03: 
      Name: Value03





RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309


-- So here, based on the region you have, so us-east-1, us-west-1, or eu-west-1, and based on the architecture you're using, for example, HVM64 or HVMG2, this is going to give you a different AMI ID every time.

-- Well, we know that the AMIs are specific based on the region, so of course it makes sense to have a different AMI per region.







------------------------------------------ When would you use Mappings vs. Parameters?



• Mappings are great when you know in advance all the values that can be taken and that they can be deduced from variables such as

    • Region
    • Availability Zone
    • AWS Account
    • Environment (dev vs prod) • etc...


• They allow safer control over the template

• Use parameters when the values are really user specific ,whenever the values are really user specific and you can not predict them ahead of time.







--------------------------------- Accessing Mapping Values (Fn::FindInMap)



• We use Fn::FindInMap to return a named value from a specific key

• !FindInMap [ MapName,TopLevelKey, SecondLevelKey ]


EG :

AWSTemplateFormatVersion: "2010-09-09"
Mappings: 
  RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309
Resources: 
  myEC2Instance: 
    Type: "AWS::EC2::Instance"
    Properties: 
      ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", HVM64]
      InstanceType: m1.small


-- we have an EC2 instance that is using an ImageId. And this ImageId is using the FindInMap function.

-- to use this FindInMap function, we first need to use a map name. So here we have the RegionMap. Then we have the top level key.

-- So what we want to use in here, we have a reference to the pseudo parameter AWS::Region. So if you launch this template in us-east-1, it's going to be us-east-1, and if you launch it to us-west-1, automatically this pseudo parameter is going to resolve to us-west-1.
 
-- And then finally, the type of architecture you want, for example, HVM64. And this works great because, well, AMIs are region specific, and so you want to make sure you have the right AMI for the right region and the right architecture.







---------------------------------------------- Mappings Hands On



-- we'll use the mapping and we'll see how we can combine it with a parameter to change and deduce values within your templates.

-- So we'll do two things with our mappings. We'll change the instance type of our EC2 instance based on the, on the environment that we're in. And we'll also change the AMI of our EC2 instance based on the region we are in.

-- create yaml file 





Parameters:
  EnvironmentName:
    Description: Environment Name
    Type: String
    AllowedValues: [development, production]
    ConstraintDescription: must be development or production

Mappings:
  AWSRegionArch2AMI:
    af-south-1:
      HVM64: ami-06db08e8636583118
    ap-east-1:
      HVM64: ami-0921e2da2f22f9617
    ap-northeast-1:
      HVM64: ami-06098fd00463352b6
    ap-northeast-2:
      HVM64: ami-07464b2b9929898f8
    ap-northeast-3:
      HVM64: ami-0b96303a469fa0678
    ap-south-1:
      HVM64: ami-0bcf5425cdc1d8a85
    ap-southeast-1:
      HVM64: ami-03ca998611da0fe12
    ap-southeast-2:
      HVM64: ami-06202e06492f46177
    ca-central-1:
      HVM64: ami-09934b230a2c41883
    eu-central-1:
      HVM64: ami-0db9040eb3ab74509
    eu-north-1:
      HVM64: ami-02baf2b4223a343e8
    eu-south-1:
      HVM64: ami-081e7f992eee19465
    eu-west-1:
      HVM64: ami-0ffea00000f287d30
    eu-west-2:
      HVM64: ami-0fbec3e0504ee1970
    eu-west-3:
      HVM64: ami-00dd995cb6f0a5219
    me-south-1:
      HVM64: ami-0502022ce8bfa56a9
    sa-east-1:
      HVM64: ami-0c27c96aaa148ba6d
    us-east-1:
      HVM64: ami-0742b4e673072066f
    us-east-2:
      HVM64: ami-05d72852800cbf29e
    us-west-1:
      HVM64: ami-0577b787189839998
    us-west-2:
      HVM64: ami-0518bb0e75d3619ca
  EnvironmentToInstanceType:
    development:
      instanceType: t2.micro
    # we want a bigger instance type in production
    production:
      instanceType: t2.small

Resources:
  EC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: !FindInMap [EnvironmentToInstanceType, !Ref 'EnvironmentName', instanceType]
      # Note we use the pseudo parameter AWS::Region
      ImageId: !FindInMap [AWSRegionArch2AMI, !Ref 'AWS::Region', HVM64]






-- So we have one parameter defined and it's the environment name, it's a string and we allow two values, development and production.

-- now we have the mappings in our CloudFormation template. And this mapping has a first mapping named AWSRegionArch2AMI which is going to map a region  an architecture to an AMI ID.

-- So as we can see, based on the region we're in, we have a different AMI ID.

-- And the second mapping we have defined is an environment to instance type mapping.

-- this one is saying, Hey, if you're in development or production, so these maps, the parameter we have defined right above.

-- So if you're in development then the instance type should be t2 micro. And if you're in production then the instance type should be t2 small.

-- uplaod stack and observe , it will create t2.micro isnatnce coz e are in development 

-- update stack and make into production it will create t2.small








------------------------------- CloudFormation – Pseudo Parameters


• AWS offers us Pseudo Parameters in any CloudFormation template

• These can be used at any time and are enabled by default

• Important pseudo parameters:

    EG ;

    AWS::AccountId

    AWS::Region

    AWS::StackId

    AWS::StackName

    AWS::NotificationARNs

    AWS::NoValue

    AWS::URLSuffix

    AWS::NotificationARNs

    AWS::Partition










======================================================== AWS CLOUDFORMATION OUTPUTS =========================================================



• The Outputs section declares optional outputs values that we can import into other stacks (if you export them first)!

• You can also view the outputs in the AWS Console or in using the AWS CLI

• They’re very useful for example if you define a network CloudFormation, and output the variables such as VPC ID and your Subnet IDs

• It’s the best way to perform some collaboration cross stack, as you let expert handle their own part of the stack




-------------------------- CloudFormation – Outputs Example

• Creating a SSH Security Group as part of one template

• We create an output that references that security group


Outputs:
  StackSSHSecurityGroup:
    Description: The SSH SG
    Value: !Ref MycompanywideSShSG
    Export:
      Name: SSH SecurityGroup



-------------------------- CloudFormation – Outputs Hands On



• Creating a SSH Security Group as part of one template

• We create an output that references that security group


Outputs:
  StackSSHSecurityGroup:
    Description: The SSH SG
    Value: !Ref MycompanywideSShSG
    Export:
      Name: SSH SecurityGroup






-- create one yaml file 



Resources:
  # here we define a SSH security group that will be used in the entire company
  MyCompanyWideSSHSecurityGroup:
    # http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-security-group.html
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
        # we have a lot of rules because it's a perfect security group
        # finance team network
      - CidrIp: 10.0.48.0/24
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22
        # marketing team network
      - CidrIp: 10.0.112.0/24
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22
        # application team support network
      - CidrIp: 10.0.176.0/24
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

Outputs:
  StackSSHSecurityGroup:
    Description: The SSH Security Group for our Company
    Value: !Ref MyCompanyWideSSHSecurityGroup
    Export:
      Name: SSHSecurityGroup




-- deploy stacj in CF and see the SG group will created and also check output section in CF Console we have o/p with name SSHSecurityGroup as export name 

IMP : the outputs that are exported have to be unique within your CloudFormation account on the region, 

-- if you upload the same template again u will get an error









-------------------------- CloudFormation – Outputs Cross-Stack Reference


• We then create a second template that leverages that security group

• For this, we use the Fn::ImportValue function

• You can’t delete the underlying stack until all the references are deleted





-------------------------- CloudFormation – Outputs Cross-Stack Reference - Hands On


create one yaml file 



Parameters:  
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  MySecureInstance:
    # http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-instance.html
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.micro
      SecurityGroups:
        # we reference the output here, using the Fn::ImportValue function
        - !ImportValue SSHSecurityGroup



-- here we are importing the SSHSecurityGroup from the previous stack , note: name should be unique and match correctly

-- create new stack and deploy stack , it will create ec2 instance and that instance will get security group from the cross stack coz we are importing the security group from different stack 

-- now delete first stack , u will get an error coz u have to first delete the underlying infrastructure i.e. delete the recent stack where u import the value then delete the first stack









======================================================== AWS CLOUDFORMATION CONDITIONS =========================================================




• Conditions are used to control the creation of resources or outputs based on a condition

• Conditions can be whatever you want them to be, but common ones are:
   • Environment (dev / test / prod) 
   • AWS Region
   • Any parameter value

• Each condition can reference another condition, parameter value or mapping





-------------------------- How to define a Condition



-- So, you have a conditions block and you create the condition name and then you apply a statement, logical statement.

-- for eg:

  Conditions:
     CreateprodResource: !Equals [!Ref EnvType, prod]
  

- we have the equals function that tests whether or not the parameter environment type is matching the word prod.



• The logical ID is for you to choose. It’s how you name condition

• The intrinsic function (logical) can be any of the following:

   • Fn::And
   • Fn::Equals
   • Fn::If
   • Fn::Not
   • Fn::Or






----------------- How to use a Condition

• Conditions can be applied to resources / outputs / etc...


Resources:
  EC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.micro
      
  MountPoint:
    Type: AWS::EC2::VolumeAttachment
    Condition: CreateProdResources
    Properties:
      InstanceId:
        !Ref EC2Instance
      VolumeId:
        !Ref NewVolume
      Device: /dev/sdh








-------------------------- Intrinsic Functions – Fn::GetAtt



• Attributes are attached to any resources you create and unfortunately not every attribute exists for every resource. for eg we have ref and Get for ec2 instance and not for ec2 volume (check in documentation for more clarity)

-- So the best way to know which attributes exist for your resources is the documentation.





-- The following example template returns the SourceSecurityGroup.OwnerAlias and SourceSecurityGroup.GroupName of the load balancer with the logical name myELB.


EG 

AWSTemplateFormatVersion: 2010-09-09
Resources:
  myELB:
    Type: AWS::ElasticLoadBalancing::LoadBalancer
    Properties:
      AvailabilityZones:
        - eu-west-1a
      Listeners:
        - LoadBalancerPort: '80'
          InstancePort: '80'
          Protocol: HTTP
  myELBIngressGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: ELB ingress group
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          SourceSecurityGroupOwnerId: !GetAtt myELB.SourceSecurityGroup.OwnerAlias
          SourceSecurityGroupName: !GetAtt myELB.SourceSecurityGroup.GroupName



-- in general So anytime we use Ref, we're going to get the reference ID out of it.

-- But if we use GetAtt to get an attribute, we have the option to get more information out of this EC2 instance. So here we have the AvailabilityZone. So for example, we can know in which AZ an instance was launched, for example, us-east-1b. You get the Id again,

-- you could get the PrivateDNSName, the PrivateIp, the PublicDNSName, and the PublicIp. So while the Ref gives you usually a reference to the ID of the resource you have created, the GetAtt allows you to get more out of the resource and you can only get what CloudFormation supports in terms of attributes that are defined in documentation








--------------------------------------- condition Hands On


create yaml file 




Parameters:  
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

  EnvType:
    Description: Environment type.
    Default: test
    Type: String
    AllowedValues:
      - prod
      - test
    ConstraintDescription: must specify prod or test.

Conditions:
  CreateProdResources: !Equals [ !Ref EnvType, prod ]

Resources:
  EC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.micro
      
  MountPoint:
    Type: AWS::EC2::VolumeAttachment
    Condition: CreateProdResources
    Properties:
      InstanceId:
        !Ref EC2Instance
      VolumeId:
        !Ref NewVolume
      Device: /dev/sdh

  NewVolume:
    Type: AWS::EC2::Volume
    Condition: CreateProdResources
    Properties:
      Size: 1
      AvailabilityZone:
        !GetAtt EC2Instance.AvailabilityZone

Outputs:
  VolumeId:
    Condition: CreateProdResources
    Value:
      !Ref NewVolume





-- So it has two parameters. One is "ImageId" to get the latest image ID for Amazon Linux 2, and then we have the "EnvType", environment type.

-- So we have created a Condition right here called "CreateProdResources". And then we are checking for the equality of the parameter "EnvType" to the value "prod".

-- So if we select "prod" for the "EnvType", then this is going to be true. And if we are selecting "test", then this is going to be false.

-- And from that condition, then we'll have the creation or not of specific resources within our template.

-- So the EC2Instance we're creating has two properties, the "ImageId" and the "InstanceType", so we are familiar with this. And there are no Conditions attached to the resource here.

-- But we have a "MountPoint", which is an EC2 VolumeAttachment, to attach an EBS volume onto our EC2Instance.

-- So only in "prod" will we have this VolumeAttachment

-- And if you look at the NewVolume itself, well, it is an EC2 Volume, again that is being conditioned by this Condition named "CreateProdResources".

-- we can see that we use Conditions on resources, but also on outputs.



-- deploy this stack in console with test environment, it will creta eone ec2 as a part of templte 

-- now update and change to prod , we have two new resources , the "MountPoint" and the "NewVolume".

-- So the NewVolume has been created. So this is an EBS Volume, and it is right here, of size 1GB, and it is in use, and it's actually already attached to my EC2Instance.









======================================================== AWS CLOUDFORMATION RULES =========================================================




-- rules are very exciting, because parameters are quite limited in their ability to be validated.

-- parameters are quite limited in their ability to be validated. So we can have some constraints within a single parameter. For example, checking its type, the Min/MaxLength, the Min/MaxValue, AllowedValues or AllowedPatterns.

-- But if we want to do cross validation of all parameters we can use rules.

-- rules are used to perform the parameter validation based on the values of other parameters (cross-parameter validation)

-- For example, we can ensure that all subnets selected within the parameters are within the same VPC,

-- This is something that we cannot do with just parameters alone, we need to use rules for that.

-- So the way it works that's we define our templates our parameters, and our rules.

-- We will create an update in CloudFormation. That CloudFormation will perform a check of the parameters against the rules.And if it's not valid, then CloudFormation will not proceed with the updates or the creates, it will be failed.

-- So defining a rule is not really easy 

-- CloudFormation Rule consists of an optional rule condition and one or more assertion




EG:




Rules:
  ProdInstanceType:
    RuleCondition: !Equals 
      - !Ref Environment
      - prod
    Assertions:
      - Assert:
          !Equals [t2.small, !Ref InstanceType]
        AssertDescription: 'For a production environment, the instance type must be t2.small'

  DevInstanceType:
    RuleCondition: !Equals 
      - !Ref Environment
      - dev
    Assertions:
      # Assert with Or
      # - Assert:
      #     'Fn::Or':
      #       - !Equals [!Ref InstanceType, t2.nano]
      #       - !Equals [!Ref InstanceType, t2.micro]
      # Assert with Contains
      - Assert:
          'Fn::Contains':
            - - t2.nano
              - t2.micro
            - !Ref InstanceType
        AssertDescription: 'For a development environment, the instance type must be t2.nano or t2.micro'
  





1 So each rule consists of a condition. And so to determine whether or not the rule takes effect or assertion, so there's only one per rule.

2 we have an assertion. So what is this rule going to assert?

     - it describes what values are Allowed for a particular parameter . Can contain one or more AssertDescription


-- If you do not define a rule condition then the rule's assertion will take effect with every single create and update operation.     





--------------------------- Rules Example 



-- So for example, if you have an application of a balancer and you configure an SSL listener, so you want to make sure that you have HTTPS outside of your ALB, then you need to provide us ACM certificate.


-- supported functions


Fn::And

Fn::Contains

Fn::EachMemberEquals

Fn::EachMemberIn

Fn::Equals

Fn::If

Fn::Not

Fn::Or

Fn::RefAll

Fn::ValueOf

Fn::ValueOfAll







------------------------------------------- Rules Hands On



-- we're going to create a CloudFormation template that will check the value of an instance type parameter and also the value of the environment parameter and make sure they match together.

-- create one yaml file 





Parameters:
  InstanceType:
    Type: String
    Default: t2.small
    AllowedValues:
      - t2.nano
      - t2.micro
      - t2.small

  Environment:
    Type: String
    Default: dev
    AllowedValues:
      - dev
      - prod
      
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Rules:
  ProdInstanceType:
    RuleCondition: !Equals 
      - !Ref Environment
      - prod
    Assertions:
      - Assert:
          !Equals [t2.small, !Ref InstanceType]
        AssertDescription: 'For a production environment, the instance type must be t2.small'

  DevInstanceType:
    RuleCondition: !Equals 
      - !Ref Environment
      - dev
    Assertions:
      # Assert with Or
      # - Assert:
      #     'Fn::Or':
      #       - !Equals [!Ref InstanceType, t2.nano]
      #       - !Equals [!Ref InstanceType, t2.micro]
      # Assert with Contains
      - Assert:
          'Fn::Contains':
            - - t2.nano
              - t2.micro
            - !Ref InstanceType
        AssertDescription: 'For a development environment, the instance type must be t2.nano or t2.micro'
  

Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: !Ref InstanceType
      ImageId: !Ref ImageId






-- here , we have three parameters. 

      One is instance type, 
      
      the other one is environment, 
      
      and finally, image ID.


-- So the interesting parts goes in the instance types and the environments.      

-- So if we look at instance type, by default, it is a t2 small, and the allowed value is t2 nano, t2 micro and t2 small.

-- And then the environment by default is dev, but we will have value, dev and prod.


IMP :


-- so the logic is that if we are in dev, we want to allow a t2 nano or a t2 micro. And if we are in prod, we only want to allow a t2 small.

-- But this is not something we can do just with parameters.

-- So we have to go and use a rule.

-- So let's create a rule called ProdInstanceType,and the rule has a rule condition to be evaluated or not.

-- So this ProdInstanceType is going to be evaluated only when the environment is equal to prod.

-- this rule is only checking the ProdInstanceType, so only do it when the environment is prod.

-- there's only one assertion, because there's only one minus sign here.(yaml code) And so it's going to assert one thing

-- so it's going to assert one thing which is that t2 small is equal to the instance type.

-- Now similarly, we have a rule for dev instance type.

-- So we're only having this rule run when the environment is dev. And then for the assertions, there are two ways of writing it.

-- So either we use an or function, or we use a contains function.But they both have the same

-- which is saying, hey, the instance type must be either equal to t2 nano or to t2 micro.

-- deploy stack --> choose dev ---> choose t2.small --> create stack , u will get an error cozz' For development environments, the instance type must be t2 nano or t2 micro,as per rules 

-- for same prod also it must allow t2.small only 

-- now deploy stack with dev --> t2.micro it will create successfully









======================================================== AWS CLOUDFORMATION METADATA =========================================================



-- So metadata is just optional and it's for you to include arbitrary YAML to provide details about the templates or resources.

-- The metadata doesn't have an impact around how your CloudFormation template works so far,




---------------------------------- special metadata keys



-- So we have special metadata keys, but some of them are used in AWS and have a special meaning.

1 AWS:CloudFormation:Interface :

      - which allows you to group and order the input parameters when they're displayed in the AWS console.

2 AWS:CloudFormation:Authentication:

      - used to specify Authentication credentials for files or source that you specify in  AWS:CloudFormation:Init.

3 AWS:CloudFormation:Init:

      - Define configuration Tasks for cfn-init , it is the most powerful usage of the metadata






-------------------------------------- AWS:CloudFormation:Interface



-- this allows you to define grouping and ordering of input parameters, when they are displayed in the AWS Console.

-- This is meant when user must input template parameters manually

-- You provide them with grouping and sorting and then they're allowed to put parameters efficiently.

-- For example, if you have a bunch of parameters related to EC2 instances, may be we group them together.(group all the ec2 related parameter together)



-- create one yaml file 





Parameters:
  InstanceType:
    Description: EC2 instance type.
    Type: String
    Default: t2.micro
    AllowedValues:
    - t2.micro
    - t2.small
    - t2.medium
    - m3.medium
    - m3.large
    - m3.xlarge
    - m3.2xlarge
  SubnetID:
    Description: Subnet ID
    Type: AWS::EC2::Subnet::Id
  SecurityGroupID:
    Description: Security Group
    Type: AWS::EC2::SecurityGroup::Id

Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0742b4e673072066f
      InstanceType: !Ref InstanceType
      SecurityGroups:
        - !Ref SecurityGroupID
      SubnetId: !Ref SubnetID

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Network Configuration"
        Parameters:
          - SubnetID
          - SecurityGroupID
      - Label:
          default: "Amazon EC2 Configuration"
        Parameters:
          - InstanceType
    ParameterLabels:
      SubnetID:
        default: "Which subnet should this be deployed to?"





-- So we have a few parameters. We have the InstanceType, which is going to allow us to select an EC2 instance type.

-- We have the SubnetID to choose the EC2 Subnet ID, as well as the SecurityGroupID for our EC2 instance.

-- So we defined some metadata with some CloudFormation::Interface and we are defining ParameterGroups.

        - ParameterGroup is a property of the AWS::CloudFormation::Interface resource that defines a parameter group and the parameters to include in the group.

        - parameter Labels are friendly names that the AWS CloudFormation console displays instead of the logical IDs.


-- So the first one is going to be called Network Configuration and within its we are defining a few parameters which is SubnetID and SecurityGroupID.        

-- deploy the stack and observe the User Interface

-- for Subnet ID, as you can see, there is a question, a new label, which is, Which subnet should this be deployed to? which is a much nicer way of defining your parameters and so on.

-- So this is just to enhance the UI in case you expect users to use the Console a lot to deploy your CloudFormation templates and using this you can make them more user-friendly.








======================================================== AWS CLOUDFORMATION CFN INIT AND USERDATA =========================================================



------------------------------------------ So what is that?



-- Well, many of the CloudFormation templates we have so far are about provisioning computing resources in your cloud. That means EC2 instances, or auto-scaling groups, et cetera, et cetera,

-- you just don't want to just provision the EC2 instances. Sometimes you want these instances to be self configured so that they can perform the job they're supposed to do.

-- so you can fully automate your EC2 instances fleet state with CloudFormation init.




------------------------------------------ User Data in EC2 for CloudFormation


• We can have user data at EC2 instance launch through the console

• Let’s learn how to write the same EC2 user-data script in our CloudFormation template

• The important thing to pass is the entire script through the function Fn::Base64

• Good to know, user data script log is in /var/log/cloud-init-output.log




------------------------------------------ User Data in EC2 for CloudFormation Hands On



create one yaml file 





Parameters:
  KeyName:
    Description: Name of an existing EC2 key pair for SSH access to the EC2 instance.
    Type: AWS::EC2::KeyPair::KeyName

  SSHLocation:
    Description: The IP address range that can be used to SSH to the EC2 instances
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: "(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})"
    ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x.

  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  WebServer:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.micro
      KeyName: !Ref KeyName
      SecurityGroups:
        - !Ref WebServerSecurityGroup
      UserData:
        Fn::Base64: |
           #!/bin/bash
           yum update -y
           amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2
           yum install -y httpd mariadb-server
           systemctl start httpd
           systemctl enable httpd
           usermod -a -G apache ec2-user
           chown -R ec2-user:apache /var/www
           chmod 2775 /var/www
           find /var/www -type d -exec sudo chmod 2775 {} \;
           find /var/www -type f -exec sudo chmod 0664 {} \;
           echo "<?php phpinfo(); ?>" > /var/www/html/phpinfo.php

  WebServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: "Enable HTTP access via port 80 + SSH access"
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 80
        IpProtocol: tcp
        ToPort: 80
      - CidrIp: !Ref SSHLocation
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22




-- So we have key names.This is to provide an SSH key is a certification key which is going to provide a CIDR to allow SSH into the instance 

-- image ID to get the latest AMI directly from the parameter storage of AWS.

-- the important part is user data. So you set it out. It needs to have a base 64 inputs. So we use this function FN based 64 and then this vertical pipe is not an error. It is actually needed.

-- what this vertical pipe is doing is saying that everything underneath is going to be a multiline string. So this represents a multiline string.

-- deploy the stack , This is going to create one as a site security group, and then this is going to create a, these to instance.

-- regardless if the script works or not we are still going to get create successful for user data.


IMP :

-- I copy the IP address and go to it, we see the site cannot be reached. So the EC2UserData that script has not finished running yet the CloudFormation templates give us a create complete.

-- connect to your instance --> sudo su --> cat /var/log/cloud-init-output.log

-- this file represents all the outputs of your EC2UserData scripts.

-- So as we can see in here, everything that was passed as commands are being run in here and you can see the outputs of the EC2UserData scripts, which is quite easy quite easy if you are doing some debugging.






------------------------------------------ The Problems with EC2 User Data


• What if we want to have a very large instance configuration?
 
       - it may not fit in the EC2 user data because you're limited in terms of size to be passed to the scripts.

• What if we want to evolve the state of the EC2 instance without terminating it and creating a new one?

• How do we make EC2 user-data more readable?

• How do we know or signal that our EC2 user-data script completed successfully?



---- All these things are addressed by CloudFormation Init.

• Enter CloudFormation Helper Scripts!

     • Python scripts, that come directly on Amazon Linux AMIs, or can be installed using yum or dnf on non-Amazon Linux AMIs

     • cfn-init, cfn-signal, cfn-get-metadata, cfn-hup


-- cfn-init :

           - it's used to retrieve and interpret the resource metadata, installing packages, creating files, and starting services.

-- cfn-signal :

           - which is a wrapper script to signal with a CreationPolicy or a WaitCondition. That means that the EC2 instance will be able to say, to CloudFormation back whether or not it has been successfully created or not.


-- cfn-get-metadata :

           - is a wrapper script to make it easy to retrieve all metadata defined for a resource or a specific path.


-- cfn-hup:

          - a daemon to check for updates to the metadata section in your CloudFormation templates and execute custom hooks when the changes are detected.


-- Usal Flow :    cfn-init ,then cfn-signal then optionally cfn-hup








------------------------------------------ AWS::CloudFormation::Init 


• A "config" contains the following and is executed in that order

     • Packages: used to download and install pre-packaged apps and components on Linux/Windows (ex. MySQL, PHP,etc...)
     
     • Groups: define user groups

     • Users: define users, and which group they belong to

     • Sources: download files and archives and place them on the EC2 instance

     • Files: create files on the EC2 instance, using inline or can be pulled from a URL

     • Commands: run a series of commands

     • Services: launch a list of sysvinit







EG :



  WebServerHost:
    Type: AWS::EC2::Instance
    Metadata:
      Comment: Install a simple PHP application
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              httpd: []
              php: []
          groups:
            apache: {}
          users:
            "apache":
              groups:
                - "apache"
          sources:
            "/home/ec2-user/aws-cli": "https://github.com/aws/aws-cli/tarball/master"
          files:
            "/tmp/cwlogs/apacheaccess.conf":
              content: !Sub |
                [general]
                state_file= /var/awslogs/agent-state
                [/var/log/httpd/access_log]
                file = /var/log/httpd/access_log
                log_group_name = ${AWS::StackName}
                log_stream_name = {instance_id}/apache.log
                datetime_format = %d/%b/%Y:%H:%M:%S
              mode: '000400'
              owner: apache
              group: apache
            "/var/www/html/index.php":
              content: !Sub |
                <?php
                echo '<h1>AWS CloudFormation sample PHP application for ${AWS::StackName}</h1>';
                ?>
              mode: '000644'
              owner: apache
              group: apache
            "/etc/cfn/cfn-hup.conf":
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
              mode: "000400"
              owner: "root"
              group: "root"
            "/etc/cfn/hooks.d/cfn-auto-reloader.conf":
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.WebServerHost.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource WebServerHost --region ${AWS::Region}
              mode: "000400"
              owner: "root"
              group: "root"
            # Fetch a webpage from a private S3 bucket
            "/var/www/html/webpage.html":
              source: !Sub "https://${MyS3BucketName}.s3.${AWS::Region}.amazonaws.com/webpage.html"
              mode: '000644'
              owner: apache
              group: apache
              authentication: S3AccessCreds
          commands:
            test:
              command: "echo \"$MAGIC\" > test.txt"
              env:
                MAGIC: "I come from the environment!"
              cwd: "~"
          services:
            sysvinit:
              httpd:
                enabled: 'true'
                ensureRunning: 'true'
              postfix:
                enabled: 'false'
                ensureRunning: 'false'
              cfn-hup:
                enable: 'true'
                ensureRunning: 'true'
                files:
                  - "/etc/cfn/cfn-hup.conf"
                  - "/etc/cfn/hooks.d/cfn-auto-reloader.conf"





1 packages:

    - So packages is very handy. It allows you to install packages from the following repositories. So it could be apt, msi, python, rpm, rubygems, and yum.

    - the packages are processed in the following order, rpm, then yum/apt, and then rubygems and python.

    - you can specify a version, or no versions,(empty Array means Latest) if you want to have the latest package installed on your EC2 instance.



2 Group And Users :

     - So if you want to have multiple users and groups with an optional group ID, within your EC2 instance, you can do the following.


3 Sources:

     - So using Sources, we can download whole compress archives from the web and unpack them on the EC2 instance directly.

     - it's very handy if you have a set of standardized scripts for your EC2 instances that you store in Amazon S3

     -  for example, or if you want to download an entire GitHub project directly onto your EC2 instance.

     - So the supported formats for Sources is tar, tar + gzip, tar + bz2, or zip.


4 Files:

     - Files is the main advantage to use CloudFormation init.

     - So files are very powerful because they have, they give you full control over any content you want within your CloudFormation template in your EC2 Instance.

     - they can come in two flavors. Either you pull a file from a specific URL or we can write a file inline form within your template.

     - the additional attributes you're getting for files

          - Source : URL to load file 

          - authentication : name of authentication method to use (used with aws::cloudformation::authentication) - eg: for files that are protected by users/pass

EG :



  files:
            "/tmp/cwlogs/apacheaccess.conf":
              content: !Sub |
                [general]
                state_file= /var/awslogs/agent-state
                [/var/log/httpd/access_log]
                file = /var/log/httpd/access_log
                log_group_name = ${AWS::StackName}
                log_stream_name = {instance_id}/apache.log
                datetime_format = %d/%b/%Y:%H:%M:%S
              mode: '000400'
              owner: apache
              group: apache
            "/var/www/html/index.php":
              content: !Sub |
                <?php
                echo '<h1>AWS CloudFormation sample PHP application for ${AWS::StackName}</h1>';
                ?>
              mode: '000644'
              owner: apache
              group: apache
            "/etc/cfn/cfn-hup.conf":
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
              mode: "000400"
              owner: "root"
              group: "root"
            "/etc/cfn/hooks.d/cfn-auto-reloader.conf":
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.WebServerHost.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource WebServerHost --region ${AWS::Region}
              mode: "000400"
              owner: "root"
              group: "root"
            # Fetch a webpage from a private S3 bucket
            "/var/www/html/webpage.html":
              source: !Sub "https://${MyS3BucketName}.s3.${AWS::Region}.amazonaws.com/webpage.html"
              mode: '000644'
              owner: apache
              group: apache
              authentication: S3AccessCreds
         



-- we have !Sub  function it's going to substitute some variables within my texts right here.

-- for eg ${AWS::StackName} So this is going to be found by the substitutes function because it started with a dollar sign and an open brackets and then is going to replace the log group name by the pseudo variable, AWS StackName .







5 commands : 

     - you can run commands one at a time in the alphabetical order in which they're declared in your CloudFormation templates.

     - you can set a directory from which a command is run as well as environment variables associated with that command. So you're really free to customize your command.

     - For example, you can check the existence of a file and if it doesn't exist, then run the download command.

     - we are using the echo command only if a file doesn't exist.


6 Services :

     - This is a way for you to launch services directly after EC2 instances is launched,( launch a bunch of servers at EC2 instance launch)

     - It ensure services are started when files changed , or packages are updated by cfn-init 








------------------------------------------------ AWS:CloudFormation:Authentication:


-- you can use with files and combining with authentication to retrieve files or sources actually that are going to be more secure.

-- used to specify authentication credentials for files or sources in AWS:CloudFormation:Init

-- 2 types :

      - Basic : used when the source is URL

      - S3 : used when the source is S3


EG 1: EC2 web server authentication





-- This template snippet shows how to get a file from a private S3 bucket within an EC2 instance. The credentials used for authentication are defined in the AWS::CloudFormation::Authentication resource, and referenced by the AWS::CloudFormation::Init resource in the files section.




WebServer: 
  Type: AWS::EC2::Instance
  DependsOn: BucketPolicy
  Metadata: 
    AWS::CloudFormation::Init: 
      config: 
        packages: 
          yum: 
            httpd: []
        files: 
          /var/www/html/index.html: 
            source: !Join
              - ''
              - - 'http://s3.amazonaws.com/'
                - !Ref BucketName
                - '/index.html'
            mode: '000400'
            owner: apache
            group: apache
            authentication: S3AccessCreds
        services: 
          sysvinit: 
            httpd: 
              enabled: 'true'
              ensureRunning: 'true'
    AWS::CloudFormation::Authentication: 
      S3AccessCreds: 
        type: S3
        accessKeyId: !Ref AccessKeyID
        secretKey: !Ref SecretAccessKey
  Properties: 
  EC2 Resource Properties ...





EG : 2 Specifying both basic and S3 authentication




-- The following example template snippet includes both basic and S3 authentication types.

AWS::CloudFormation::Authentication: 
  testBasic: 
    type: basic
    username: !Ref UserName
    password: !Ref Password
    uris: 
      - 'example.com/test'
  testS3: 
    type: S3
    accessKeyId: !Ref AccessKeyID
    secretKey: !Ref SecretAccessKey
    buckets: 
      - !Sub ${BucketName}




EG : 3  IAM roles


The following example shows how to use IAM roles:

     - myRole is an AWS::IAM::Role resource.

     - The Amazon EC2 instance that runs cfn-init is associated with myRole through an instance profile.

     - The example specifies the authentication by using the buckets property, like in Amazon S3 authentication. You can also specify authentication by name.




AWS::CloudFormation::Authentication: 
  rolebased: 
    type: S3
    buckets: 
      - !Sub ${BucketName}
    roleName: !Ref myRole






--------------------------------------------- Hands On


-- go to s3 and create s3 bucket 

-- create one html page like webpage.html 



<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <h1>Hello, I'm a Web Page :). I have been downloaded from S3 bucket.</h1>
</body>
</html>



-- So this is the HTML page that our EC2 instance will access and download from.








--------------------------------------------- cf-init scripts 



-- So cfn-init is a script that helps you read this entire metadata block and we'll apply it to your EC2 instance. (it helps make complex EC2 configurations readable)

-- So, the script will query the CloudFormation service to get init data 

-- so the flow goes like this  

       - CloudFormation will launch an EC2 instance

       - then in the EC2 instance user data we're going to run cfn-init

       - then cfn-init will retrieve the init data from the CloudFormation templates metadata selection

       - then we'll run all the commands, the files, the sources, the packages and so on

       - that we defined in the metadata section.

       - then the logs of the cfn-init script will go to /var/log/cfn-init.log.       





--------------------------------------------- cfn-signal and wait conditions


-- So, once we run cfn-init though, we still haven't told CloudFormation that we had a successful ending of the script

--  so therefore we need to use the cfn-signal scripts to signal to CloudFormation that things went well.

         - So, the cfn-signal must be run right after cfn-init

         - we're going to tell CloudFormation that the resource creation was a success or a failure to really link the bootstrap of your EC2 instance to the success of your CloudFormation stack.

         - So this is what we had from before


-- but now we're going to define a WaitCondition which is going to block the template from successfulness until they receive a signal from cfn-signal.

-- So for this, we can create a CreationPolicy that works for EC2 and ASG, 

-- for example, and we'll defined a Count and you can define a Count greater than one if you need more than one signal.(default count is 1)





--------------------------------------------- cfn-hup



-- So this is used to have your EC2 instance look at the metadata changes in CloudFormation every 15 minutes and apply the metadata configuration again.

-- it relies on two configuration files.

       - /ect/cfn/cfn-hup.conf

       - /ect/cfn/hooks.d/cfn-auto-reloader.conf



-- So the idea is that's CloudFormation will launch retrieve the init data then signal and then thanks to the cfn-hup service,

-- we're going to check the metadata every 15 minutes and we can configure this as well and then in case there is a metadata change within our CloudFormation templates,





--------------------------------------------- cfn-init Hands ON 



-- create a stack like this 






AWSTemplateFormatVersion: '2010-09-09'
Description: AWS CloudFormation Sample Template for CFN Init
Parameters:
  KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instances
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: must be the name of an existing EC2 KeyPair.

  SSHLocation:
    Description: The IP address range that can be used to SSH to the EC2 instances
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: "(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})"
    ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x.

  MyS3BucketName:
    Description: Name of an existing bucket to download files from
    Type: String

  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  WebServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable HTTP access via port 80 and SSH access via port 22
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: !Ref SSHLocation

  WebServerHost:
    Type: AWS::EC2::Instance
    Metadata:
      Comment: Install a simple PHP application
      AWS::CloudFormation::Init:
        config:
          packages:
            yum:
              httpd: []
              php: []
          groups:
            apache: {}
          users:
            "apache":
              groups:
                - "apache"
          sources:
            "/home/ec2-user/aws-cli": "https://github.com/aws/aws-cli/tarball/master"
          files:
            "/tmp/cwlogs/apacheaccess.conf":
              content: !Sub |
                [general]
                state_file= /var/awslogs/agent-state
                [/var/log/httpd/access_log]
                file = /var/log/httpd/access_log
                log_group_name = ${AWS::StackName}
                log_stream_name = {instance_id}/apache.log
                datetime_format = %d/%b/%Y:%H:%M:%S
              mode: '000400'
              owner: apache
              group: apache
            "/var/www/html/index.php":
              content: !Sub |
                <?php
                echo '<h1>AWS CloudFormation sample PHP application for ${AWS::StackName}</h1>';
                ?>
              mode: '000644'
              owner: apache
              group: apache
            "/etc/cfn/cfn-hup.conf":
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
              mode: "000400"
              owner: "root"
              group: "root"
            "/etc/cfn/hooks.d/cfn-auto-reloader.conf":
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.WebServerHost.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource WebServerHost --region ${AWS::Region}
              mode: "000400"
              owner: "root"
              group: "root"
            # Fetch a webpage from a private S3 bucket
            "/var/www/html/webpage.html":
              source: !Sub "https://${MyS3BucketName}.s3.${AWS::Region}.amazonaws.com/webpage.html"
              mode: '000644'
              owner: apache
              group: apache
              authentication: S3AccessCreds
          commands:
            test:
              command: "echo \"$MAGIC\" > test.txt"
              env:
                MAGIC: "I come from the environment!"
              cwd: "~"
          services:
            sysvinit:
              httpd:
                enabled: 'true'
                ensureRunning: 'true'
              postfix:
                enabled: 'false'
                ensureRunning: 'false'
              cfn-hup:
                enable: 'true'
                ensureRunning: 'true'
                files:
                  - "/etc/cfn/cfn-hup.conf"
                  - "/etc/cfn/hooks.d/cfn-auto-reloader.conf"
      AWS::CloudFormation::Authentication:
        # Define S3 access credentials
        S3AccessCreds:
          type: S3
          buckets:
            - !Sub ${MyS3BucketName}
          roleName: !Ref InstanceRole
    CreationPolicy:
      ResourceSignal:
        Timeout: PT15M
        
    Properties:
      ImageId: !Ref ImageId
      KeyName: !Ref KeyName
      InstanceType: t2.micro
      IamInstanceProfile: !Ref InstanceProfile # Reference Instance Profile
      SecurityGroups:
      - !Ref WebServerSecurityGroup
      UserData:
        Fn::Base64:
          !Sub |
            #!/bin/bash -xe
            
            # Get the latest CloudFormation helper scripts
            yum install -y aws-cfn-bootstrap
            
            # Start cfn-init
            /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource WebServerHost --region ${AWS::Region}
            
            # cfn-init completed so signal success or not
            /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource WebServerHost --region ${AWS::Region}
          

  InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: 'sts:AssumeRole'
            Principal:
              Service: ec2.amazonaws.com
            Effect: Allow
            Sid: ''
      Policies:
        - PolicyName: AuthenticatedS3GetObjects
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Action:
                  - 's3:GetObject'
                Resource: !Sub 'arn:aws:s3:::${MyS3BucketName}/*'
                Effect: Allow

  InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
        - !Ref InstanceRole

Outputs:
  InstanceId:
    Description: The instance ID of the web server
    Value:
      !Ref WebServerHost
  WebsiteURL:
    Value:
      !Sub 'http://${WebServerHost.PublicDnsName}'
    Description: URL for newly created LAMP stack
  PublicIP:
    Description: Public IP address of the web server
    Value:
      !GetAtt WebServerHost.PublicIp








-- deploy this stack in cloudformation --> KeyName = give key name that u have created --> MyS3BucketName = created for authentication purpose give that bucket name .

-- remainig are default --> last step we just going to acknowledge that CloudFormation will create an IAM role.So indeed we are creating an IAM role for our EC2 instance so that it can retrieve the file from Amazon S3.

-- create stack and see what happens

-- so my web server security group is creating as well as the instance role and when both of them are created, then my EC2 instance is going to be created.

IMP 

-- so here comes the interesting part. So as we can see now, my web server host is in create in progress.

-- I refresh, now I see that my web server host was in create in progress. And then it was create in progress up until it has received a success signal.

      "Received SUCCESS signal with UniqueId i-0bf609caecc6dddf3"

-- it took about two minutes

-- So CloudFormation did not go into create complete up until the success signal was received.

-- now go to output --> access web url and see o/p and also try /webpage.html  (eg : http://ec2-43-205-115-76.ap-south-1.compute.amazonaws.com/webpage.html)

-- connect ec2 instance  and see logs by doing cat /var/log/cfn-init.log







--------------------------------------------- CreationPolicy



CreationPolicy:
    ResourceSignal:
      Timeout: PT15M
      Count: 5



-- this is something we've seen that allows you to prevent a resource status from rushing the CREATE_COMPLETE stage until CloudFormation receives either 

        - a specified number of success signals or 
        
        - a timeout period exceeded.

-- we use the CFN signal helpers script to signal a resource success or failure.

    - So we have a templates.
    - We create this creation policy.
    - Then the templates is applied into CloudFormation.
    - CloudFormation will create your stack.
    - Maybe you compose of an EC2 instance.
    - And that instance will return back to the success to CloudFormation surveys using the CFN signal script.



-- So supported resources for the creation policy

     - EC2 Instance
     - CloudFormation with condition
     - auto-scaling, auto-scaling group
     - AppStream fleets.


-- EXample : wait until your applications installed and configured on your ec2 instance






--------------------------------------------- cfn-init Troubleshooting




Wait Condition Didn't Receive the Required Number of Signals from an Amazon EC2 Instance

   • Ensure that the AMI you're using has the AWS CloudFormation helper scripts installed. If the AMI doesn't include the helper scripts, you can also download them to your instance

   • Verify that the cfn-init & cfn-signal command was successfully run on the instance. You can view logs, such as /var/log/cloud-init.log or /var/log/cfn-init.log, to help you debug the instance launch

IMP 
   • You can retrieve the logs by logging in to your instance, but you must disable rollback on failure or else AWS CloudFormation deletes the instance after your stack fails to create

   • Verify that the instance has a connection to the Internet. If the instance is in a VPC, the instance should be able to connect to the Internet through a NAT device if it's is in a private subnet or through an Internet gateway if it's in a public subnet

   • to test network connectivity  ,For example, run: curl -I https://aws.amazon.com





--------------------------------------------- User Data vs CloudFormation::Init vs Helper Scripts




-- EC2 User data is an imperative way to provision/bootstrap the EC2 instance using Shell syntax

-- AWS::CloudFormation::Init is a declarative way to provision/bootstrap the EC2 instance using YAML or JSON syntax

-- AWS::CloudFormation::Init is useless if it’s NOT triggered by a script within the EC2 User Data

-- Triggering AWS::CloudFormation::Init inside EC2 User Data is done by using cfn-init or cfn-hup






======================================================== AWS CLOUDFORMATION DRIFT =========================================================




-- So with CloudFormation, we create infrastructure,

--  but it does not prevent users in your account to modify this infrastructure manually, using the console or the CLI.

• But it doesn’t protect you against manual configuration changes

     -so we say that resources have drifted when their configuration that is current in the account is different from the one that was initiated by CloudFormation,

• How do we know if our resources have drifted?

• We can use CloudFormation Drift!

• Detect drift on an entire stack or on individual resources within a stack


EG:

-- So let's go with this workflow,

-- we have a template we uploaded into CloudFormation, it's going to create a stack, and the stack has an SSH security group.

-- Now it turns out that a user goes and modifies the security group rules through the console, and changes it to this new security group.

-- Then, using CloudFormation Drift, we're gonna have a comparison of the old and new resources configuration.

-- then CloudFormation Drift will give us a report around what has drifted or not.

-- we can resolve the stack, or the resource drift by then using Resource Import,

-- not all the resources are supported yet,



Resource that supports Drift Detectition


https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import-supported-resources.html







--------------------------------------------- Drift Hands ON



-- create one yaml file 



Parameters:
  VPCId:
    Description: VPC to create the security group into
    Type: AWS::EC2::VPC::Id
  
Resources:
  SSHSecurityGroup:
    Type: "AWS::EC2::SecurityGroup"
    Properties:
      GroupDescription: Test Drift SSH Security Group
      SecurityGroupIngress:
        - CidrIp: "10.0.0.0/25"
          FromPort: 22
          ToPort: 22
          IpProtocol: tcp
      VpcId: !Ref VPCId

  HTTPSecurityGroup:
    Type: "AWS::EC2::SecurityGroup"
    Properties:
      GroupDescription: Test Drift HTTP Security Group
      SecurityGroupIngress:
        - CidrIp: "0.0.0.0/0"
          FromPort: 80
          ToPort: 80
          IpProtocol: tcp
      VpcId: !Ref VPCId







-- we're going to create multiple security groups. One is going to be an SSHsecurity group with a rule on Port 22.

-- then we're going to have an HTTP security group with a rule on Port 80.

-- deploy stack and select default VPC ID 

-- I'm going to modify the security group settings manually after they are created.

-- go to SSHsecurity make some changes for eg add HTTPS and for HTTP security group delete this SG .

-- we can see it is possible to delete resource manually

-- So it was created by CloudFormation but I can delete it manually still.So there is no protection of resources of CloudFormation within your accounts.

-- So now my resources are deleted and so, and changed they have drifted.

-- therefore what I'm going to do is that I'm going to do stack action and then detect drift.

-- in console right corner upside (Stack Actions)--> do Detect Drift ---> wait for 1 min ---> view drift results --> check Drift status --> select resource anyone --> View drift details --> u can bale to see the changes 









======================================================== AWS CLOUDFORMATION Nested Stacks =========================================================




• Nested stacks are stacks as part of other stacks

• They allow you to isolate repeated patterns / common components in separate stacks and call them from other stacks

• Example:
   
      • Load Balancer configuration that is re-used
  
      • Security Group that is re-used


-- Stack A (root)  , maybe there will be a stack C, as part of it is going to be your nested stack.

-- So, nested stacks are considered to be best practice.

• To update a nested stack, always update the parent (root stack) and then it will apply the updates all the way down to every children.

• Nested stacks can have nested stacks themselves!








--------------------------------------------- Nested Stacks Hands ON



-- create one yaml file 


Eg: nested.yaml




Parameters:
  ApplicationName:
    Description: The application name
    Type: String
  VPCId:
    Description: VPC to create the security group into
    Type: AWS::EC2::VPC::Id
  
Resources:
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Sub Security group for ${ApplicationName}
      SecurityGroupIngress:
        - CidrIp: "10.0.0.0/25"
          FromPort: 22
          ToPort: 22
          IpProtocol: tcp
          Description: SSH for Engineering department
        - CidrIp: "192.168.0.0/25"
          FromPort: 22
          ToPort: 22
          IpProtocol: tcp
          Description: SSH for HR department
      VpcId: !Ref VPCId

Outputs:
  SSHGroupId:
    Value: !Ref SSHSecurityGroup
    Description: Id for the SSH Security Group








create another stack eg: ec2instance.yaml






Parameters:
  VPCId:
    Description: VPC to create the security group and EC2 instance into
    Type: AWS::EC2::VPC::Id

  TemplateURL:
    Description: URL of the nested stack template
    Type: String

  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  SSHSecurityGroupStack:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Ref TemplateURL
      Parameters:
        ApplicationName: !Ref AWS::StackName
        VPCId: !Ref VPCId
      TimeoutInMinutes: 5


  EC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: t2.micro
      # Note we use the pseudo parameter AWS::Region
      ImageId: !Ref ImageId
      SecurityGroupIds:
        - !GetAtt SSHSecurityGroupStack.Outputs.SSHGroupId





-- Type: AWS::CloudFormation::Stack  : we are using nested stacks


-- upload the nested.yaml file in s3 bucket, copy the the object URL of that file

-- now open CF in console and upload ec2instance.yaml and add Template URL that u copied from above step (https://hfgvgdgs.s3.ap-south-1.amazonaws.com/ssh-security-group.yaml) ,So this stack is going to be retrieving the templates here and then applying it as a nested stack.

-- select VPC ID and we have to acknowledge a few capabilities. So number one is that we acknowledge AUTO_EXPAND because we are expanding a CloudFormation stack within the stack.

-- create stack 

-- so if I refresh this, as we can see, and I refresh that on the left hand side, there is a nested stack on the left hand side that is now in CREATE_IN_PROGRESS.

-- check SG for the EC2 instance it came from the Nested Stack






--------------------------------------------- Nested Stack Updates



-- Ensure that updated nested stack are uploaded  onto s3 first

-- then Re-upload the root stack 

-- create updated yaml file 




Parameters:
  ApplicationName:
    Description: The application name
    Type: String
  VPCId:
    Description: VPC to create the security group into
    Type: AWS::EC2::VPC::Id
  
Resources:
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Sub Security group for ${ApplicationName}
      SecurityGroupIngress:
        - CidrIp: "10.0.0.0/25"
          FromPort: 22
          ToPort: 22
          IpProtocol: tcp
          Description: SSH for Engineering department
        - CidrIp: "192.168.0.0/25"
          FromPort: 22
          ToPort: 22
          IpProtocol: tcp
          Description: SSH for HR department
        - CidrIp: "172.16.0.0/31"
          FromPort: 22
          ToPort: 22
          IpProtocol: tcp
          Description: Business Partners
      VpcId: !Ref VPCId

Outputs:
  SSHGroupId:
    Value: !Ref SSHSecurityGroup
    Description: Id for the SSH Security Group





-- in this file we have changed  CidrIp: "172.16.0.0/31" and   Description: Business Partners

-- now reupload this template to the s3 bucket 

-- now go to CF console and choose update and upload ec2instance.yaml file in console and do not change anything 

-- so the SSHSecurityGroupStack is going to be modified. Because we detected that the underlying template has changed, 






--------------------------------------------- Nested Stack Delete


-- to delete nested stack , u have to delete first main stack then automatically it applies delete to nested stack 







--------------------------------------------- Cross Stacks vs. Nested Stacks



• Cross Stacks
    
     • Helpful when stacks have different lifecycles

     • Use Outputs Export and Fn::ImportValue

     • When you need to pass export values to many stacks (VPC Id...)



• Nested Stacks

     • Helpful when components must be re-used

     • Example: re-use how to properly configure an Application Load Balancer

     • The nested stack only is important to the higher-level stack (it’s not shared)







--------------------------------------------- Closing Comments on Nested Stacks     






Exported Stack Output Values vs. Using Nested Stacks:

     - If you have a central resource that is shared between many different other stacks, use Exported Stack Output Values

     - If you need other stacks to be updated right away if a central resource is updated, use Exported Stack Output Values

     - If the resources can be dedicated to one stack only and must be re-usable pieces of code, use Nested Stacks

     - Note that you will need to update each Root stack manually in case of Nested stack updated






nested stacks example at , downlaod s3 template url in google for yaml code 

      https://github.com/aws-samples/ecs-refarch-cloudformation/blob/master/master.yaml





Description: >

  This template deploys a VPC, with a pair of public and private subnets spread 
  across two Availabilty Zones. It deploys an Internet Gateway, with a default 
  route on the public subnets. It deploys a pair of NAT Gateways (one in each AZ), 
  and default routes for them in the private subnets.

  It then deploys a highly available ECS cluster using an AutoScaling Group, with 
  ECS hosts distributed across multiple Availability Zones. 

  Finally, it deploys a pair of example ECS services from containers published in 
  Amazon EC2 Container Registry (Amazon ECR).

  Last Modified: 22nd September 2016
  Author: Paul Maddox <pmaddox@amazon.com>

Resources:
  VPC:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: https://s3.amazonaws.com/ecs-refarch-cloudformation/infrastructure/vpc.yaml
      Parameters:
        EnvironmentName: !Ref AWS::StackName
        VpcCIDR: 10.180.0.0/16
        PublicSubnet1CIDR: 10.180.8.0/21
        PublicSubnet2CIDR: 10.180.16.0/21
        PrivateSubnet1CIDR: 10.180.24.0/21
        PrivateSubnet2CIDR: 10.180.32.0/21

  SecurityGroups:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: https://s3.amazonaws.com/ecs-refarch-cloudformation/infrastructure/security-groups.yaml
      Parameters:
        EnvironmentName: !Ref AWS::StackName
        VPC: !GetAtt VPC.Outputs.VPC

  ALB:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: https://s3.amazonaws.com/ecs-refarch-cloudformation/infrastructure/load-balancers.yaml
      Parameters:
        EnvironmentName: !Ref AWS::StackName
        VPC: !GetAtt VPC.Outputs.VPC
        Subnets: !GetAtt VPC.Outputs.PublicSubnets
        SecurityGroup: !GetAtt SecurityGroups.Outputs.LoadBalancerSecurityGroup

  ECS:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: https://s3.amazonaws.com/ecs-refarch-cloudformation/infrastructure/ecs-cluster.yaml
      Parameters:
        EnvironmentName: !Ref AWS::StackName
        InstanceType: t2.large
        ClusterSize: 4
        VPC: !GetAtt VPC.Outputs.VPC
        SecurityGroup: !GetAtt SecurityGroups.Outputs.ECSHostSecurityGroup
        Subnets: !GetAtt VPC.Outputs.PrivateSubnets

  ProductService:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: https://s3.amazonaws.com/ecs-refarch-cloudformation/services/product-service/service.yaml
      Parameters:
        VPC: !GetAtt VPC.Outputs.VPC
        Cluster: !GetAtt ECS.Outputs.Cluster
        DesiredCount: 2
        Listener: !GetAtt ALB.Outputs.Listener
        Path: /products

  WebsiteService:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: https://s3.amazonaws.com/ecs-refarch-cloudformation/services/website-service/service.yaml
      Parameters:
        VPC: !GetAtt VPC.Outputs.VPC
        Cluster: !GetAtt ECS.Outputs.Cluster
        DesiredCount: 2
        ProductServiceUrl:
          !Join ["/", [!GetAtt ALB.Outputs.LoadBalancerUrl, "products"]]
        Listener: !GetAtt ALB.Outputs.Listener
        Path: /
        ECSServiceAutoScalingRoleARN: !GetAtt ECS.Outputs.ECSServiceAutoScalingRole

  LifecycleHook:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: https://s3.amazonaws.com/ecs-refarch-cloudformation/infrastructure/lifecyclehook.yaml
      Parameters:
        Cluster: !GetAtt ECS.Outputs.Cluster
        ECSAutoScalingGroupName: !GetAtt ECS.Outputs.ECSAutoScalingGroupName

Outputs:
  ProductServiceUrl:
    Description: The URL endpoint for the product service
    Value: !Join ["/", [!GetAtt ALB.Outputs.LoadBalancerUrl, "products"]]

  WebsiteServiceUrl:
    Description: The URL endpoint for the website service
    Value: !Join ["", [!GetAtt ALB.Outputs.LoadBalancerUrl, "/"]]











======================================================== AWS CLOUDFORMATION STACK SETS =========================================================




• Create, update, or delete stacks across multiple accounts and regions with a single operation/template

• Administrator account to create StackSets

• Target accounts to create, update, delete stack instances from StackSets

• When you update a stack set, all associated stack instances are updated throughout all accounts and regions

• Can be applied into all accounts of an AWS organizations




----------------------------------- StackSet operation


-- create StackSet

      - provide template + target accounts/regions


-- Update StackSet
     
      - updates always affects all accounts, You cannot say, "I want to update only part of the stacks as part of my updates."

-- Delete Stack

      - Delete Stack and it's resource from target accounts/regions or
      - you can delete a stack from your StackSet.so the stack will continue to run independently or 
      - you can delete all the stacks from your StackSets.


-- Delete StackSet 

      - you must delete all the stack instances within the StackSet in order to delete the StackSet itself.






----------------------------------- Stackset Deployment options



-- Deployment order

      - So you can give an order of regions where the stacks are deployed 
      
      - and you can say how many operations are performed per region at a time.

-- maximum concurrent accounts      

      - So you can give a max number or percentage that allows you to really control the speed of deployment across all your accounts.


-- Failure tolerance

      - Failure tolerance is for you to understand if you want to have, to trigger a failure if one account or one region fails, then you can specify a tolerance for this.


-- Region concurrencies.      

      - So by default it is sequential. That means one region at a time

      - but you can do parallel region concurrency.


-- Retain stacks.      

      - So when you delete the StackSet, do you want to keep the stacks and the resources running, or do you want to remove everything?









----------------------------------- CloudFormation – StackSets Permission Models



• Self-managed Permissions

        • Create the IAM roles (with established trusted relationship) in both administrator and target accounts

        • Deploy to any target account in which you have permissions to create IAM role




• Service-managed Permissions


      • Deploy to accounts managed by AWS Organizations

      • StackSets create the IAM roles on your behalf (enable trusted access with AWS Organizations)

      • Must enable all features in AWS Organizations

      • Ability to deploy to accounts added to your organization in the future (Automatic Deployments)








----------------------------------- StackSets with AWS Organizations



• Ability to automatically deploy Stack instances to new Accounts in an Organization

• Can delegate StackSets administration to member accounts in AWS Organization

• Trusted access with AWS Organizations must be enabled before delegated administrators can deploy to accounts managed by Organizations





----------------------------------- Stackset Hands On






• We’ll use StackSets to enable AWS Config across AWS regions with a single click

    -  the idea is that you are going to be able to track the resource configuration over time of all your resources within all regions within the same account, which is a great use case.


-- For Hands On We'll have to create IAM roles, add the trust relationships, and then we'll deal with StackSets.    


-- So the first thing you need to do is to create a StackSet administration role,




AWSTemplateFormatVersion: 2010-09-09
Description: Configure the AWSCloudFormationStackSetAdministrationRole to enable use of AWS CloudFormation StackSets.

Resources:
  AdministrationRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: AWSCloudFormationStackSetAdministrationRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: cloudformation.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: AssumeRole-AWSCloudFormationStackSetExecutionRole
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - sts:AssumeRole
                Resource:
                  - "arn:*:iam::*:role/AWSCloudFormationStackSetExecutionRole"



-- this is going to be a role to be deployed in your admin account. And this IAM role is going to be assumed by a CloudFormation.

-- So we're giving the administrator the right to basically assume roles in the target account from which you will be able to deploy StackSets.

-- deploy stack 




2nd step 


-- we have to do is to create the execution role.

-- So this CloudFormation templates must be applied in all the target accounts that you want to use the StackSet feature in.

-- Now we are in a single account type of setting, so we have to deploy this template as well in the same account.

-- But imagine that if you had different target accounts in AWS, then you would deploy these template in all the target accounts.

-- This is an initial setup that you cannot avoid unless you are using AWS organizations.





AWSTemplateFormatVersion: 2010-09-09
Description: Configure the AWSCloudFormationStackSetExecutionRole to enable use of your account as a target account in AWS CloudFormation StackSets.

Parameters:
  AdministratorAccountId:
    Type: String
    Description: AWS Account Id of the administrator account (the account in which StackSets will be created).
    MaxLength: 12
    MinLength: 12

Resources:
  ExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: AWSCloudFormationStackSetExecutionRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              AWS:
                - !Ref AdministratorAccountId
            Action:
              - sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - !Sub arn:${AWS::Partition}:iam::aws:policy/AdministratorAccess




-- So, this template has a parameter, which is the administrator account ID that we'll have to specify.

-- This is necessary to allow this role to be trusting in assuming the parent role, admin role.

-- So this is going to create an IAM role, and this is going to have the right name, and the idea's that we're going to have an assume role policy document to say who can assume this role, and we'll have a reference to the administrator account ID,

-- Deploy the stack --> AdministratorAccountId = ur accountid 





3rd step 









-- Create Stackset , left side in console u have an option like stackset  --> upload the config.yaml file 



config.yaml file 





AWSTemplateFormatVersion: 2010-09-09
Description: Enable AWS Config

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Recorder Configuration
        Parameters:
          - AllSupported
          - IncludeGlobalResourceTypes
          - ResourceTypes
      - Label:
          default: Delivery Channel Configuration
        Parameters:
          - DeliveryChannelName
          - Frequency
      - Label:
          default: Delivery Notifications
        Parameters:
          - TopicArn
          - NotificationEmail
    ParameterLabels:
      AllSupported:
        default: Support all resource types
      IncludeGlobalResourceTypes:
        default: Include global resource types
      ResourceTypes:
        default: List of resource types if not all supported
      DeliveryChannelName:
        default: Configuration delivery channel name
      Frequency:
        default: Snapshot delivery frequency
      TopicArn:
        default: SNS topic name
      NotificationEmail:
        default: Notification Email (optional)

Parameters:
  AllSupported:
    Type: String
    Default: True
    Description: Indicates whether to record all supported resource types.
    AllowedValues:
      - True
      - False

  IncludeGlobalResourceTypes:
    Type: String
    Default: True
    Description: Indicates whether AWS Config records all supported global resource types.
    AllowedValues:
      - True
      - False

  ResourceTypes:
    Type: List<String>
    Description: A list of valid AWS resource types to include in this recording group, such as AWS::EC2::Instance or AWS::CloudTrail::Trail.
    Default: <All>

  DeliveryChannelName:
    Type: String
    Default: <Generated>
    Description: The name of the delivery channel.

  Frequency:
    Type: String
    Default: 24hours
    Description: The frequency with which AWS Config delivers configuration snapshots.
    AllowedValues:
      - 1hour
      - 3hours
      - 6hours
      - 12hours
      - 24hours

  TopicArn:
    Type: String
    Default: <New Topic>
    Description: The Amazon Resource Name (ARN) of the Amazon Simple Notification Service (Amazon SNS) topic that AWS Config delivers notifications to.

  NotificationEmail:
    Type: String
    Default: <None>
    Description: Email address for AWS Config notifications (for new topics).

Conditions:
  IsAllSupported: !Equals
    - !Ref AllSupported
    - True
  IsGeneratedDeliveryChannelName: !Equals
    - !Ref DeliveryChannelName
    - <Generated>
  CreateTopic: !Equals
    - !Ref TopicArn
    - <New Topic>
  CreateSubscription: !And
    - !Condition CreateTopic
    - !Not
      - !Equals
        - !Ref NotificationEmail
        - <None>

Mappings:
  Settings:
    FrequencyMap:
      1hour   : One_Hour
      3hours  : Three_Hours
      6hours  : Six_Hours
      12hours : Twelve_Hours
      24hours : TwentyFour_Hours

Resources:

  ConfigBucket:
    DeletionPolicy: Retain
    Type: AWS::S3::Bucket

  ConfigBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref ConfigBucket
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: AWSConfigBucketPermissionsCheck
            Effect: Allow
            Principal:
              Service:
                - config.amazonaws.com
            Action: s3:GetBucketAcl
            Resource:
              - !Sub "arn:${AWS::Partition}:s3:::${ConfigBucket}"
          - Sid: AWSConfigBucketDelivery
            Effect: Allow
            Principal:
              Service:
                - config.amazonaws.com
            Action: s3:PutObject
            Resource:
              - !Sub "arn:${AWS::Partition}:s3:::${ConfigBucket}/AWSLogs/${AWS::AccountId}/*"

  ConfigTopic:
    Condition: CreateTopic
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub "config-topic-${AWS::AccountId}"
      DisplayName: AWS Config Notification Topic

  ConfigTopicPolicy:
    Condition: CreateTopic
    Type: AWS::SNS::TopicPolicy
    Properties:
      Topics:
        - !Ref ConfigTopic
      PolicyDocument:
        Statement:
          - Sid: AWSConfigSNSPolicy
            Action:
              - sns:Publish
            Effect: Allow
            Resource: !Ref ConfigTopic
            Principal:
              Service:
                - config.amazonaws.com

  EmailNotification:
    Condition: CreateSubscription
    Type: AWS::SNS::Subscription
    Properties:
      Endpoint: !Ref NotificationEmail
      Protocol: email
      TopicArn: !Ref ConfigTopic

  ConfigRecorderRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - config.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWS_ConfigRole"

  ConfigRecorder:
    Type: AWS::Config::ConfigurationRecorder
    DependsOn:
      - ConfigBucketPolicy
    Properties:
      RoleARN: !GetAtt ConfigRecorderRole.Arn
      RecordingGroup:
        AllSupported: !Ref AllSupported
        IncludeGlobalResourceTypes: !Ref IncludeGlobalResourceTypes
        ResourceTypes: !If
          - IsAllSupported
          - !Ref AWS::NoValue
          - !Ref ResourceTypes

  ConfigDeliveryChannel:
    Type: AWS::Config::DeliveryChannel
    DependsOn:
      - ConfigBucketPolicy
    Properties:
      Name: !If
        - IsGeneratedDeliveryChannelName
        - !Ref AWS::NoValue
        - !Ref DeliveryChannelName
      ConfigSnapshotDeliveryProperties:
        DeliveryFrequency: !FindInMap
          - Settings
          - FrequencyMap
          - !Ref Frequency
      S3BucketName: !Ref ConfigBucket
      SnsTopicARN: !If
        - CreateTopic
        - !Ref ConfigTopic
        - !Ref TopicArn







-- deploy stack --> Specify regions = select any two 

-- Having an issue with this =-------- not getting 







----------------------------------- StackSet Drift Detection


• Performs drift detection on the stack associated with each stack instance in the StackSet

• If the current state of a resource in a stack varies from the expected state:

      • The stack considered drifted
      • And the stack instance that the stack associated with considered drifted
      • And the StackSet is considered drifted

• Drift detection identifies unmanaged changes (outside CloudFormation)

• Changes made through CloudFormation to a stack directly (not at the StackSet level), aren’t considered drifted

• You can stop drift detection on a StackSet






----------------------------------- StackSet Delete 


-- To delete this stack set, you first need to delete all the stacks within your stack sets.










======================================================== AWS CLOUDFORMATION CloudFormation Deployment Options =========================================================





----------------------------------------- CloudFormation – ChangeSets



• When you update a stack, you need to know what changes will happen before it applying them for greater confidence

• ChangeSets won’t say if the update will be successful but at least they will tell you what should happen.

• For Nested Stacks, you see the changes across all stacks




----------------------------------------- Stack Creation Failures


-- so if a CloudFormation stack creation fails you will get a rollback complete state.

-- This means that CloudFormation has tried to create some resources but one of these resources has failed to be created and therefore CloudFormation has rolled back everything.

-- to resolve the error there's only one way you need to delete the failed stack and then create a new stack.

-- You cannot update, validate or create a change-set on top of a failed stack.




create one yaml file



Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      # this bucket already exists globally
      BucketName: test




-- we are creating S3 bucket, and we provide a bucket name, and the bucket name is test. Now the test S3 bucket is already taken globally, and so therefore the creation of my S3 buckets should fail.







----------------------------------------- RollBack triggers


-- So it is possible for you to trigger a rollback for whatever reason.

-- whenever you create a stack or you update a stack, you can monitor a CloudWatch Alarm. And if the CloudWatch Alarm goes into an alarm state, then it is going to trigger a rollback.

-- cloudformation monitors the specified CloudWatch alarms During:

       - stack create/update.

       - the monitoring period (By default is zero minutes, but you can have it all the way up to three hours, to monitor)



-- So if any of the alarms in CloudWatch Alarm are in the ALARM state then CloudFormation will rollback the entire stack operations,

-- if you set up a monitoring time but you don't specify any rollback triggers, confirmation will still wait to the specified period before cleaning up all resources for update operations,

-- if you set up a monitoring time of zero minutes, confirmation will still monitor the rollback triggers during the stack create and updates operation.

-- upto 5 cloudwatch Alarms 





----------------------------------------- RollBack triggers Hands ON






create one yaml file 



Parameters:
  TableName:
    Type: String
    Description: The name of our DynamoDB table
  RCU:
    Type: Number
    Default: 5
    Description: How many RCU to provide to the table
    
Resources:
  MyDDBTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Ref TableName
      AttributeDefinitions:
        - 
          AttributeName: "Album"
          AttributeType: "S"
        - 
          AttributeName: "Artist"
          AttributeType: "S"
      KeySchema:
        -
          AttributeName: "Album"
          KeyType: "HASH"
        -
          AttributeName: "Artist"
          KeyType: "RANGE"
      ProvisionedThroughput:
        ReadCapacityUnits: !Ref RCU
        WriteCapacityUnits: 5  





-- this is a template which allows us to provision a DynamoDB table.

-- We have two arguments;

         - the table name and 
         - the RCU.

-- So we're going to, firstly apply these templates and give an RCU of five, and then we're going to create a CloudWatch alarm that will monitor how many RCU they are in my accounts.         

-- then we'll update the templates and go over the limits so that we can have rollback.

-- deploy stack in CF 

-- right now we haven't configured a cloud watch alarm yet because I just want to create this table first, 

-- this way this will generate a CloudWatch metric that I will need it to create my alarm.

-- then we can create the alarm and apply it in our accounts.

-- Now go to cloudwatch Alarm --> create alarms --> Metrics --> USage --> By AWS Resources --> search for DynamoDB and select AccountProvisionedReadCapacityUnits as metric --> threshold value 10 --> create alarm

-- wait for 3-4 min it turn to OK state 

--  So as we can see, my alarm state is okay because some data points have been captured and I can click on the alarm itself and see that currently my RCU is at five and my limit is at 10,

-- now let's go into cloud formation and I'm going to update this stack and we'll use the same current templates but I'm going to change the parameters. And now I want 15 RCU. 

-- cloudformation --> Rollback configuration --> Monitoring time = 10 ---> given arn of CW Alarm ---> update stack

-- so now we are in update complete,

-- Even though my dynamo DB table is in 'update complete' My stack itself is still in 'update in progress'.

-- That's because we are waiting 10 minutes to see if an alarm is not going to be triggered.

-- wait for sometime to get cloudwatch alarm INALARM state

-- now cw alarm has triggered , check in CF Console

-- my dynamo DB table now is updated again. And it's going to be rolled back to its previous good states, which was an RCU of five

-- it automatically delete the stack 







----------------------------------------- Contine Rolling Back an Update 




-- So whenever a rollback happens, maybe sometimes the rollback itself will be a failure and you will be in a state named UPDATES_ROLLBACK_FAILED. In that case, the resource can not return to its original state, causing the rollback to fail.

-- For example, you roll back an old database instance, but it was deleted outside of CloudFormation.

-- So for example, we create a stack, The create is complete, and the RDS database instance is created as part of my stack.

-- Then we update the stack, which is going to create a new RDS database instance, but the update rollback will fail because the new instance will have a failure as well.

-- And it turns out that in the meantime our user has deleted our previous RDS database.


Solution :

       - fix the error manually outside of CloudFormation,and then continue to do the rollback of the stack.

       - we can skip the resources that can not be rolled back successfully. And then the CloudFormation will automatically mark the failed resources as UPDATES_COMPLETE.

       - if you are in the UPDATES_ROLLBACK_FAILED state, this is the only two things you can do.

       - You can not update the stack in this state so you cannot apply an update on top of it.



-- let's see how we can trigger an UPDATE_ROLLBACK_FAILED.






----------------------------------------- Contine Rolling Back an Update Hands ON




create rollback.yaml file



Parameters:
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  ASG:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      AvailabilityZones: 
        - !Select 
          - 0
          - Fn::GetAZs: !Ref 'AWS::Region'
      DesiredCapacity: "1"
      LaunchConfigurationName: !Ref LC
      MaxSize: "1"
      MinSize: "1"
  LC:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties: 
      ImageId: !Ref ImageId
      InstanceType: t2.micro




-- So we're going to specify an image ID, fro EC2

--  we're going to create an AutoScalingGroup,  And we will find an AvailabilityZone to use it.

-- So we'll use the first one from the AZs within the region.

-- deploy this template in CF

-- my resources are created 

-- So now we want to trigger an update and a failure on the updates, which will trigger a rollback.

-- for this create failurerollback.yaml




Parameters:
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  ASG:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      AvailabilityZones: 
        - !Select 
          - 0
          - Fn::GetAZs: !Ref 'AWS::Region'
      DesiredCapacity: "1"
      LaunchConfigurationName: !Ref LC
      MaxSize: "1"
      MinSize: "1"

  LC:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties: 
      ImageId: !Ref ImageId
      InstanceType: t2.nano

  WaitCondition:
    Type: AWS::CloudFormation::WaitCondition
    DependsOn: ASG
    CreationPolicy:
      ResourceSignal:
        Count: 1
        Timeout: PT1M




-- the parameter here is the same for the ImageId. The ASG has the exact same configuration, so we don't change anything here,        

-- but what we're going to change is the LaunchConfiguration.

-- So the LaunchConfiguration now has an InstanceType of t2.nano. Effectively, this will create a new LaunchConfiguration within the CloudFormation,

-- then we also have a WaitCondition, and the WaitCondition is a way for the confirmation template to wait for a signal coming from the CFN signal commands.

-- So what will happen is that this WaitCondition will never receive a signal because nothing in its templates will send a signal to it.

-- therefore, because the WaitCondition will have an error, it will go into a ROLLBACK state.

-- For EG : 

     - unfortunately, an employee wanted to create a new Launch configuration. So went ahead and say NewLaunchConfig, 

     - 



-- Launch configuration will not support from DEC 2022 , so try to use with Launch Temapltes






----------------------------------------- CloudFormation – Stack Policies



• During a CloudFormation Stack update, all update actions are allowed on all resources (default)

• A Stack Policy is a JSON document that defines the update actions that are allowed on specific resources during Stack updates

-- once you have created a stack policy it will prevent stack resources from being unintentionally deleted or updated during a stack update.

• When you set a Stack Policy, all resources in the Stack are protected by default

• Specify an explicit ALLOW for the resources you want to be allowed to be updated


-- to update protected resources,

        - first you need to create a temporary stack policy that will override the old stack policy to allow updates to a protected item.

        - then the override policy will not permanently change this tech policy.

        - once created, while the stack policy cannot be deleted you can edit edit it 

        - for example if you wanted to to allow all of this on all actions on all resources but this type of policy will still be there.


-- Stack policy Example



{
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "Update:*",
            "Principal": "*",
            "Resource": "*"
        },
        {
            "Effect": "Deny",
            "Action": ["Update:Replace", "Update:Delete"],
            "Principal": "*",
            "Resource": "LogicalResourceId/MyDDBTable",
            "Condition": {
                "StringEquals": {
                    "ResourceType": ["AWS::DynamoDB::Table"]
                }
            }
        }
    ]
}




-- Deny updates on MyDDBTable









EG: 1

{
  "Statement" : [
  {
    "Effect" : "Allow",
    "Principal" : "*",
    "Action" : "update:*",
    "Resource" : "*",
   },

  {
    "Effect" : "Deny",
    "Principal" : "*",
    "Action" : "Update:*",
    "Resource" : "LogicalResourceId/ProductionDatabase"
  }
  ]
}


-- So here, we have an example where the first statement is saying "Allow update*" on everything, meaning that everything in your CloudFormation Stack can be updated,

-- second part is saying "Deny update*" on Resource Production Database. That means that whatever is named "Production Database" in your CloudFormation Stack is going to be protected against any kind of updates,



• Protect resources from unintentional updates

• When you set a Stack Policy, all resources in the Stack are protected by default

• Specify an explicit ALLOW for the resources you want to be allowed to be updated






EG: 2



policy 1 



{
  "Statement" : [
    {
      "Effect" : "Deny",
      "Action" : "Update:*",
      "Principal": "*",
      "Resource" : "LogicalResourceId/MyDatabase"
    },
    {
      "Effect" : "Allow",
      "Action" : "Update:*",
      "Principal": "*",
      "Resource" : "*"
    }
  ]
}




policy 2



{
  "Statement" : [
    {
      "Effect" : "Allow",
      "Action" : "Update:*",
      "Principal": "*",
      "NotResource" : "LogicalResourceId/MyDatabase"
    }
  ]
}





Exp: Both the policies deny all update actions on the database with the MyDatabase logical ID. And they allow all update actions on all other stack resources
















----------------------------------------- CloudFormation – Stack Policies Hands ON



SatckPolicy.json 





{
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "Update:*",
            "Principal": "*",
            "Resource": "*"
        },
        {
            "Effect": "Deny",
            "Action": ["Update:Replace", "Update:Delete"],
            "Principal": "*",
            "Resource": "LogicalResourceId/MyDDBTable",
            "Condition": {
                "StringEquals": {
                    "ResourceType": ["AWS::DynamoDB::Table"]
                }
            }
        }
    ]
}










create template.yaml 



Parameters:
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.micro
      
  MyDDBTable:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        - 
          AttributeName: "UserID"
          AttributeType: "S"
      KeySchema:
        -
          AttributeName: "UserID"
          KeyType: "HASH"
      ProvisionedThroughput:
        ReadCapacityUnits: 1
        WriteCapacityUnits: 1





-- So, we have a template and this template will create an EC2:instance of type T2.micro, and also going to create a DynamoDB:table with some keys and RCU and WCU have one.

-- also we're going to provide a stack policy. And this stack policy is going to say,

-- "we allow updates on everything, but we're going to have an explicit deny on a few things."For example, update:replace, and update:delete, is not going to be allowed on my Dynamo DB table if my resource type is of type Dynamo DB table.

-- now upload template.yaml in CF --> stack policy (enter or upload staock policy stackpolicy.json)

-- this will create ec2 and DynamoDB




-- now create updated-template.yaml




Parameters:
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.nano
      
  MyDDBTable:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        - 
          AttributeName: "UserID"
          AttributeType: "S"
        - 
          AttributeName: "Name"
          AttributeType: "S"
      KeySchema:
        -
          AttributeName: "UserID"
          KeyType: "HASH"
        -
          AttributeName: "Name"
          KeyType: "RANGE"
      ProvisionedThroughput:
        ReadCapacityUnits: 1
        WriteCapacityUnits: 1






-- you have a look at template-updated.yaml two things have changed.

      - Number one: my instance type has been changed to a T2 nano and 
      
      - my Dynamo DB table has getting, is getting more attribute definitions in here and in the key schema.


-- changing these attributes under my Dynamo DB table, if you have a look at the documentation, will trigger a replacement of my Dynamo DB table, but thanks to this policy this is going to be denied even if we try to go ahead with it

-- because the update:replace and the update:delete is not allowed on my Dynamo DB table.

-- now CF --> update --> Replace existing template --> upload udapted-template.yaml--> create 

-- the action denied by stack policy is that well updating my Dynamo DB table is not allowed because of the statements at the Policy. 

-- we have protected our Dynamo DB, Dynamo DB table from being deleted, which can be really really helpful in production.










----------------------------------------- CloudFormation – Termination Protection



• To prevent accidental deletes of CloudFormation Stacks, use TerminationProtection

-- this will be applied to any nested stack.

-- So termination protection exists for EC2 instances but also for CloudFormation stacks

-- tighten your IAM Policies (ex: explicit deny on some user groups)
       
      - For example, by having an explicit deny on some user groups that denies the update termination protection API, then you're guaranteed that your users can not delete some stacks.









----------------------------------------- Service Role 




-- So you can create an IAM role that will do all the operations within CloudFormation on your behalf.

-- The idea is that by default if you don't specify an IAM role for the CloudFormation operations, then CloudFormation will use a temporary session that it generates from your user credentials.

-- But sometimes your users do not have access to create the target resources.

-- So instead you can just have a limited set of permissions on your user, such as CloudFormation* and an IAM:PassRole, and then use a service role.

-- for example, that has the S3 create bucket permission to allow to create an S3 bucket that are cleate from CloudFormation.

-- So the use cases for this is that you want to achieve the least privileged principle and you don't want to give the user all the required permissions to create this type of resources.






----------------------------------------- Service Role Hands ON




-- create one yaml file  and policy.json file
 



Parameters:
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: t2.micro
      ImageId: !Ref ImageId



policy.json file



{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "cloudformation:*",
                "s3:*",
                "iam:PassRole"
            ],
            "Resource": "*"
        }
    ]
}



-- create user and add this json policy to the user 

-- deploy template in CF , u will not able to create cox' permissions issue , so then add permission according to that u can can able to privileged access 









----------------------------------------- Quick-Create Links for Stacks



-- So this is a custom URL that is used to launch a CloudFormation stack directly from the console,

EG :


https://eu-central-1.console.aws.amazon.com/cloudformation/home?region=eu-central-1#/stacks/create/review?templateURL=https://s3.eu-central-1.amazonaws.com/cloudformation-templates-eu-central-1/WordPress_Single_Instance.template&stackName=MyWPBlog&param_DBName=mywpblog&param_InstanceType=t2.medium&param_KeyName=DemoKeyPair


-- So the idea is that you want to reduce the number of wizard pages and the amount of user inputs that's required.

-- For example, you can create multiple URLs that specify different values for the same templates

-- then CloudFormation 'll ignore parameters
 
      - that don't exist in the template

      - that are defined with NoEcho property set to true.


https://eu-central-1.console.aws.amazon.com/cloudformation/home?region=eu-central-1#/stacks/create/review?templateURL=https://s3.eu-central-1.amazonaws.com/cloudformation-templates-eu-central-1/WordPress_Single_Instance.template&stackName=MyWPBlog&param_DBName=mywpblog&param_InstanceType=t2.medium&param_KeyName=terraform-key



-- change keypair of urs and copy this link and paste in browser u will get CF console for Quick deployment











======================================================== AWS CLOUDFORMATION Continuous Delivery ========================================================





-- So let's see how we can do continuous delivery with CodePipeline and CloudFormation.

-- So we use CodePipeline which is a managed service to do continuous delivery

-- then CodePipeline is a way to deploy a CloudFormation stacks for you.

-- The idea is that you want to rapidly and reliably make changes to your AWS infrastructure. 

-- And then you can combine CodePipeline with other tools to automatically build  and test changes to your CloudFormation templates before promoting them into production stacks.

-- For Example :

        - So we'll create a workflow that automatically builds a test stack.When we submit a CloudFormation templates into a code repository, and 
        
        - then after CloudFormation has built the test stack then we can test it and then decide maybe with a manual approval to push changes to the production stack.


-- WorkFLow 

       - We're going to have a user push a template and some configuration files into an S3 buckets.

       - This is going to trigger a code pipeline and then the pipeline will create a test stack in CloudFormation, which we create a test stack in which we have to do some testing and then manually approve.

       - When the user does approve the test stack then CloudFormation will go ahead and create a production stack and as well as deleting the test stack.


-- so this is the state we want to get to to have some automated release of our CloudFormation templates, and you can take this at home and then do it within your company.








------------------------------------------------- Hands ON




--create one  word-press-single-instance.yaml 





AWSTemplateFormatVersion: 2010-09-09
Description: >-
  AWS CloudFormation Sample Template WordPress_Single_Instance: WordPress is web
  software you can use to create a beautiful website or blog. This template
  installs WordPress with a local MySQL database for storage. It demonstrates
  using the AWS CloudFormation bootstrap scripts to deploy WordPress.
  **WARNING** This template creates an Amazon EC2 instance. You will be billed
  for the AWS resources used if you create a stack from this template.
Parameters:
  KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instances
    Type: 'AWS::EC2::KeyPair::KeyName'
    ConstraintDescription: must be the name of an existing EC2 KeyPair.
  InstanceType:
    Description: WebServer EC2 instance type
    Type: String
    Default: t2.micro
    AllowedValues:
      - t1.micro
      - t2.nano
      - t2.micro
      - t2.small
      - t2.medium
      - t2.large
      - m1.small
      - m1.medium
      - m1.large
      - m1.xlarge
      - m2.xlarge
      - m2.2xlarge
      - m2.4xlarge
      - m3.medium
      - m3.large
      - m3.xlarge
      - m3.2xlarge
      - m4.large
      - m4.xlarge
      - m4.2xlarge
      - m4.4xlarge
      - m4.10xlarge
      - c1.medium
      - c1.xlarge
      - c3.large
      - c3.xlarge
      - c3.2xlarge
      - c3.4xlarge
      - c3.8xlarge
      - c4.large
      - c4.xlarge
      - c4.2xlarge
      - c4.4xlarge
      - c4.8xlarge
      - g2.2xlarge
      - g2.8xlarge
      - r3.large
      - r3.xlarge
      - r3.2xlarge
      - r3.4xlarge
      - r3.8xlarge
      - i2.xlarge
      - i2.2xlarge
      - i2.4xlarge
      - i2.8xlarge
      - d2.xlarge
      - d2.2xlarge
      - d2.4xlarge
      - d2.8xlarge
      - hs1.8xlarge
      - cr1.8xlarge
      - cc2.8xlarge
    ConstraintDescription: must be a valid EC2 instance type.
  SSHLocation:
    Description: The IP address range that can be used to SSH to the EC2 instances
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: '(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})/(\d{1,2})'
    ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x.
  DBName:
    Default: wordpressdb
    Description: The WordPress database name
    Type: String
    MinLength: '1'
    MaxLength: '64'
    AllowedPattern: '[a-zA-Z][a-zA-Z0-9]*'
    ConstraintDescription: must begin with a letter and contain only alphanumeric characters.
  DBUser:
    NoEcho: 'true'
    Description: The WordPress database admin account username
    Type: String
    MinLength: '1'
    MaxLength: '16'
    AllowedPattern: '[a-zA-Z][a-zA-Z0-9]*'
    ConstraintDescription: must begin with a letter and contain only alphanumeric characters.
  DBPassword:
    NoEcho: 'true'
    Description: The WordPress database admin account password
    Type: String
    MinLength: '8'
    MaxLength: '41'
    AllowedPattern: '[a-zA-Z0-9]*'
    ConstraintDescription: must contain only alphanumeric characters.
  DBRootPassword:
    NoEcho: 'true'
    Description: MySQL root password
    Type: String
    MinLength: '8'
    MaxLength: '41'
    AllowedPattern: '[a-zA-Z0-9]*'
    ConstraintDescription: must contain only alphanumeric characters.
Mappings:
  AWSInstanceType2Arch:
    t1.micro:
      Arch: HVM64
    t2.nano:
      Arch: HVM64
    t2.micro:
      Arch: HVM64
    t2.small:
      Arch: HVM64
    t2.medium:
      Arch: HVM64
    t2.large:
      Arch: HVM64
    m1.small:
      Arch: HVM64
    m1.medium:
      Arch: HVM64
    m1.large:
      Arch: HVM64
    m1.xlarge:
      Arch: HVM64
    m2.xlarge:
      Arch: HVM64
    m2.2xlarge:
      Arch: HVM64
    m2.4xlarge:
      Arch: HVM64
    m3.medium:
      Arch: HVM64
    m3.large:
      Arch: HVM64
    m3.xlarge:
      Arch: HVM64
    m3.2xlarge:
      Arch: HVM64
    m4.large:
      Arch: HVM64
    m4.xlarge:
      Arch: HVM64
    m4.2xlarge:
      Arch: HVM64
    m4.4xlarge:
      Arch: HVM64
    m4.10xlarge:
      Arch: HVM64
    c1.medium:
      Arch: HVM64
    c1.xlarge:
      Arch: HVM64
    c3.large:
      Arch: HVM64
    c3.xlarge:
      Arch: HVM64
    c3.2xlarge:
      Arch: HVM64
    c3.4xlarge:
      Arch: HVM64
    c3.8xlarge:
      Arch: HVM64
    c4.large:
      Arch: HVM64
    c4.xlarge:
      Arch: HVM64
    c4.2xlarge:
      Arch: HVM64
    c4.4xlarge:
      Arch: HVM64
    c4.8xlarge:
      Arch: HVM64
    g2.2xlarge:
      Arch: HVMG2
    g2.8xlarge:
      Arch: HVMG2
    r3.large:
      Arch: HVM64
    r3.xlarge:
      Arch: HVM64
    r3.2xlarge:
      Arch: HVM64
    r3.4xlarge:
      Arch: HVM64
    r3.8xlarge:
      Arch: HVM64
    i2.xlarge:
      Arch: HVM64
    i2.2xlarge:
      Arch: HVM64
    i2.4xlarge:
      Arch: HVM64
    i2.8xlarge:
      Arch: HVM64
    d2.xlarge:
      Arch: HVM64
    d2.2xlarge:
      Arch: HVM64
    d2.4xlarge:
      Arch: HVM64
    d2.8xlarge:
      Arch: HVM64
    hi1.4xlarge:
      Arch: HVM64
    hs1.8xlarge:
      Arch: HVM64
    cr1.8xlarge:
      Arch: HVM64
    cc2.8xlarge:
      Arch: HVM64
  AWSRegionArch2AMI:
    af-south-1:
      HVM64: ami-064cc455f8a1ef504
      HVMG2: NOT_SUPPORTED
    ap-east-1:
      HVM64: ami-f85b1989
      HVMG2: NOT_SUPPORTED
    ap-northeast-1:
      HVM64: ami-0b2c2a754d5b4da22
      HVMG2: ami-09d0e0e099ecabba2
    ap-northeast-2:
      HVM64: ami-0493ab99920f410fc
      HVMG2: NOT_SUPPORTED
    ap-northeast-3:
      HVM64: ami-01344f6f63a4decc1
      HVMG2: NOT_SUPPORTED
    ap-south-1:
      HVM64: ami-03cfb5e1fb4fac428
      HVMG2: ami-0244c1d42815af84a
    ap-southeast-1:
      HVM64: ami-0ba35dc9caf73d1c7
      HVMG2: ami-0e46ce0d6a87dc979
    ap-southeast-2:
      HVM64: ami-0ae99b503e8694028
      HVMG2: ami-0c0ab057a101d8ff2
    ca-central-1:
      HVM64: ami-0803e21a2ec22f953
      HVMG2: NOT_SUPPORTED
    cn-north-1:
      HVM64: ami-07a3f215cc90c889c
      HVMG2: NOT_SUPPORTED
    cn-northwest-1:
      HVM64: ami-0a3b3b10f714a0ff4
      HVMG2: NOT_SUPPORTED
    eu-central-1:
      HVM64: ami-0474863011a7d1541
      HVMG2: ami-0aa1822e3eb913a11
    eu-north-1:
      HVM64: ami-0de4b8910494dba0f
      HVMG2: ami-32d55b4c
    eu-south-1:
      HVM64: ami-08427144fe9ebdef6
      HVMG2: NOT_SUPPORTED
    eu-west-1:
      HVM64: ami-015232c01a82b847b
      HVMG2: ami-0d5299b1c6112c3c7
    eu-west-2:
      HVM64: ami-0765d48d7e15beb93
      HVMG2: NOT_SUPPORTED
    eu-west-3:
      HVM64: ami-0caf07637eda19d9c
      HVMG2: NOT_SUPPORTED
    me-south-1:
      HVM64: ami-0744743d80915b497
      HVMG2: NOT_SUPPORTED
    sa-east-1:
      HVM64: ami-0a52e8a6018e92bb0
      HVMG2: NOT_SUPPORTED
    us-east-1:
      HVM64: ami-032930428bf1abbff
      HVMG2: ami-0aeb704d503081ea6
    us-east-2:
      HVM64: ami-027cab9a7bf0155df
      HVMG2: NOT_SUPPORTED
    us-west-1:
      HVM64: ami-088c153f74339f34c
      HVMG2: ami-0a7fc72dc0e51aa77
    us-west-2:
      HVM64: ami-01fee56b22f308154
      HVMG2: ami-0fe84a5b4563d8f27
Resources:
  WebServerSecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Enable HTTP access via port 80 locked down to the load balancer + SSH
        access
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: !Ref SSHLocation
  WebServer:
    Type: 'AWS::EC2::Instance'
    Metadata:
      'AWS::CloudFormation::Init':
        configSets:
          wordpress_install:
            - install_cfn
            - install_wordpress
            - configure_wordpress
        install_cfn:
          files:
            /etc/cfn/cfn-hup.conf:
              content: !Join 
                - ''
                - - |
                    [main]
                  - stack=
                  - !Ref 'AWS::StackId'
                  - |+

                  - region=
                  - !Ref 'AWS::Region'
                  - |+

              mode: '000400'
              owner: root
              group: root
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Join 
                - ''
                - - |
                    [cfn-auto-reloader-hook]
                  - |
                    triggers=post.update
                  - |
                    path=Resources.WebServer.Metadata.AWS::CloudFormation::Init
                  - 'action=/opt/aws/bin/cfn-init -v '
                  - '         --stack '
                  - !Ref 'AWS::StackName'
                  - '         --resource WebServer '
                  - '         --configsets wordpress_install '
                  - '         --region '
                  - !Ref 'AWS::Region'
                  - |+

              mode: '000400'
              owner: root
              group: root
          services:
            sysvinit:
              cfn-hup:
                enabled: 'true'
                ensureRunning: 'true'
                files:
                  - /etc/cfn/cfn-hup.conf
                  - /etc/cfn/hooks.d/cfn-auto-reloader.conf
        install_wordpress:
          packages:
            yum:
              php73: []
              php73-mysqlnd: []
              mysql57: []
              mysql57-server: []
              mysql57-devel: []
              mysql57-libs: []
              httpd24: []
          sources:
            /var/www/html: 'http://wordpress.org/latest.tar.gz'
          files:
            /tmp/setup.mysql:
              content: !Join 
                - ''
                - - 'CREATE DATABASE '
                  - !Ref DBName
                  - |
                    ;
                  - CREATE USER '
                  - !Ref DBUser
                  - '''@''localhost'' IDENTIFIED BY '''
                  - !Ref DBPassword
                  - |
                    ';
                  - 'GRANT ALL ON '
                  - !Ref DBName
                  - .* TO '
                  - !Ref DBUser
                  - |
                    '@'localhost';
                  - |
                    FLUSH PRIVILEGES;
              mode: '000400'
              owner: root
              group: root
            /tmp/create-wp-config:
              content: !Join 
                - ''
                - - |
                    #!/bin/bash -xe
                  - >
                    cp /var/www/html/wordpress/wp-config-sample.php
                    /var/www/html/wordpress/wp-config.php
                  - sed -i "s/'database_name_here'/'
                  - !Ref DBName
                  - |
                    '/g" wp-config.php
                  - sed -i "s/'username_here'/'
                  - !Ref DBUser
                  - |
                    '/g" wp-config.php
                  - sed -i "s/'password_here'/'
                  - !Ref DBPassword
                  - |
                    '/g" wp-config.php
              mode: '000500'
              owner: root
              group: root
          services:
            sysvinit:
              httpd:
                enabled: 'true'
                ensureRunning: 'true'
              mysqld:
                enabled: 'true'
                ensureRunning: 'true'
        configure_wordpress:
          commands:
            01_set_mysql_root_password:
              command: !Join 
                - ''
                - - mysqladmin -u root password '
                  - !Ref DBRootPassword
                  - ''''
              test: !Join 
                - ''
                - - '$(mysql '
                  - !Ref DBName
                  - ' -u root --password='''
                  - !Ref DBRootPassword
                  - ''' >/dev/null 2>&1 </dev/null); (( $? != 0 ))'
            02_create_database:
              command: !Join 
                - ''
                - - mysql -u root --password='
                  - !Ref DBRootPassword
                  - ''' < /tmp/setup.mysql'
              test: !Join 
                - ''
                - - '$(mysql '
                  - !Ref DBName
                  - ' -u root --password='''
                  - !Ref DBRootPassword
                  - ''' >/dev/null 2>&1 </dev/null); (( $? != 0 ))'
            03_configure_wordpress:
              command: /tmp/create-wp-config
              cwd: /var/www/html/wordpress
    Properties:
      ImageId: !FindInMap 
        - AWSRegionArch2AMI
        - !Ref 'AWS::Region'
        - !FindInMap 
          - AWSInstanceType2Arch
          - !Ref InstanceType
          - Arch
      InstanceType: !Ref InstanceType
      SecurityGroups:
        - !Ref WebServerSecurityGroup
      KeyName: !Ref KeyName
      UserData: !Base64 
        'Fn::Join':
          - ''
          - - |
              #!/bin/bash -xe
            - |
              yum update -y aws-cfn-bootstrap
            - '/opt/aws/bin/cfn-init -v '
            - '         --stack '
            - !Ref 'AWS::StackName'
            - '         --resource WebServer '
            - '         --configsets wordpress_install '
            - '         --region '
            - !Ref 'AWS::Region'
            - |+

            - '/opt/aws/bin/cfn-signal -e $? '
            - '         --stack '
            - !Ref 'AWS::StackName'
            - '         --resource WebServer '
            - '         --region '
            - !Ref 'AWS::Region'
            - |+

    CreationPolicy:
      ResourceSignal:
        Timeout: PT15M
Outputs:
  WebsiteURL:
    Value: !Join 
      - ''
      - - 'http://'
        - !GetAtt 
          - WebServer
          - PublicDnsName
        - /wordpress
    Description: WordPress Website





-- the idea is that with this template we're going to deploy a WordPress instance on a ec2 instance.





-- create one  test-stack-configuration.json



{
  "Parameters" : {
    "DBName" : "TestWordPressDB",
    "DBPassword" : "mysupersecrettestpassword",
    "DBRootPassword" : "mysupersecrettestpassword",
    "DBUser" : "TestDBuser",    
    "KeyName" : "DemoKeyPair"
  }
}


-- change  "KeyName" of urs should be in same region as ur template region 






---- Create one prod-stack-configuration.json 


{
  "Parameters" : {
    "DBName" : "ProdWordPressDB",
    "DBPassword" : "mysupersecretprodpassword",
    "DBRootPassword" : "mysupersecretprodpassword",
    "DBUser" : "ProdDBuser",    
    "KeyName" : "terraform-key"
  }
}



-- put these three files in an Zip and upload to the s3 bucket (enable versioning on the s3 bucket) make sure the bucket is in the same region sa ur template 







----  create wordpress-pipeline.yaml file 



AWSTemplateFormatVersion: "2010-09-09"

Description: >
  AWS CloudFormation Sample Template Continuous Delivery: This template
  builds an AWS CodePipeline pipeline that implements a continuous delivery release
  process for AWS CloudFormation stacks. Submit a CloudFormation source artifact
  to an Amazon S3 location before building the pipeline. The pipeline uses the
  artifact to automatically create stacks and change sets.
  **WARNING** This template creates an Amazon EC2 instance. You will be billed
  for the AWS resources used when you create a stack using this template.

Parameters:
  PipelineName:
    Description: A name for pipeline
    Type: String
  S3Bucket:
    Description: The name of the S3 bucket that contains the source artifact, which must be in the same region as this stack
    Type: String
  SourceS3Key:
    Default: wordpress-single-instance.zip
    Description: The file name of the source artifact, such as myfolder/myartifact.zip
    Type: String
  TemplateFileName:
    Default: wordpress-single-instance.yaml
    Description: The file name of the WordPress template
    Type: String
  TestStackName:
    Default: Test-MyWordPressSite
    Description: A name for the test WordPress stack
    Type: String
  TestStackConfig:
    Default: test-stack-configuration.json
    Description: The configuration file name for the test WordPress stack
    Type: String
  ProdStackName:
    Default: Prod-MyWordPressSite
    Description: A name for the production WordPress stack
    Type: String
  ProdStackConfig:
    Default: prod-stack-configuration.json
    Description: The configuration file name for the production WordPress stack
    Type: String
  ChangeSetName:
    Default: UpdatePreview-MyWordPressSite
    Description: A name for the production WordPress stack change set
    Type: String
  Email:
    Description: The email address where CodePipeline sends pipeline notifications
    Type: String

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "CodePipeline Settings"
        Parameters:
          - PipelineName
          - S3Bucket
          - SourceS3Key
          - Email
      - Label:
          default: "Test Stack Settings"
        Parameters:
          - TestStackName
          - TemplateFileName
          - TestStackConfig
      - Label:
          default: "Production Stack Settings"
        Parameters:
          - ChangeSetName
          - ProdStackName
          - ProdStackConfig

Resources:
  ArtifactStoreBucket:
    Type: AWS::S3::Bucket
    Properties:
      VersioningConfiguration:
        Status: Enabled

  CodePipelineSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      Subscription:
        - Endpoint: !Ref Email
          Protocol: email

  Pipeline:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      ArtifactStore:
        Location: !Ref 'ArtifactStoreBucket'
        Type: S3
      DisableInboundStageTransitions: []
      Name: !Ref 'PipelineName'
      RoleArn: !GetAtt [PipelineRole, Arn]
      Stages:
        - Name: S3Source
          Actions:
            - Name: TemplateSource
              ActionTypeId:
                Category: Source
                Owner: AWS
                Provider: S3
                Version: '1'
              Configuration:
                S3Bucket: !Ref 'S3Bucket'
                S3ObjectKey: !Ref 'SourceS3Key'
              OutputArtifacts:
                - Name: TemplateSource
              RunOrder: 1
        - Name: TestStage
          Actions:
            - Name: CreateStack
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Provider: CloudFormation
                Version: '1'
              InputArtifacts:
                - Name: TemplateSource
              Configuration:
                ActionMode: REPLACE_ON_FAILURE
                RoleArn: !GetAtt [CFNRole, Arn]
                StackName: !Ref TestStackName
                TemplateConfiguration: !Sub "TemplateSource::${TestStackConfig}"
                TemplatePath: !Sub "TemplateSource::${TemplateFileName}"
              RunOrder: 1
            - Name: ApproveTestStack
              ActionTypeId:
                Category: Approval
                Owner: AWS
                Provider: Manual
                Version: '1'
              Configuration:
                NotificationArn: !Ref CodePipelineSNSTopic
                CustomData: !Sub 'Do you want to create a change set against the production stack and delete the ${TestStackName} stack?'
              RunOrder: 2
            - Name: DeleteTestStack
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Provider: CloudFormation
                Version: '1'
              Configuration:
                ActionMode: DELETE_ONLY
                RoleArn: !GetAtt [CFNRole, Arn]
                StackName: !Ref TestStackName
              RunOrder: 3
        - Name: ProdStage
          Actions:
            - Name: CreateChangeSet
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Provider: CloudFormation
                Version: '1'
              InputArtifacts:
                - Name: TemplateSource
              Configuration:
                ActionMode: CHANGE_SET_REPLACE
                RoleArn: !GetAtt [CFNRole, Arn]
                StackName: !Ref ProdStackName
                ChangeSetName: !Ref ChangeSetName
                TemplateConfiguration: !Sub "TemplateSource::${ProdStackConfig}"
                TemplatePath: !Sub "TemplateSource::${TemplateFileName}"
              RunOrder: 1
            - Name: ApproveChangeSet
              ActionTypeId:
                Category: Approval
                Owner: AWS
                Provider: Manual
                Version: '1'
              Configuration:
                NotificationArn: !Ref CodePipelineSNSTopic
                CustomData: !Sub 'A new change set was created for the ${ProdStackName} stack. Do you want to implement the changes?'
              RunOrder: 2
            - Name: ExecuteChangeSet
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Provider: CloudFormation
                Version: '1'
              Configuration:
                ActionMode: CHANGE_SET_EXECUTE
                ChangeSetName: !Ref ChangeSetName
                RoleArn: !GetAtt [CFNRole, Arn]
                StackName: !Ref ProdStackName
              RunOrder: 3
  CFNRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action: ['sts:AssumeRole']
          Effect: Allow
          Principal:
            Service: [cloudformation.amazonaws.com]
        Version: '2012-10-17'
      Path: /
      Policies:
        - PolicyName: CloudFormationRole
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                  - 'ec2:*'
                Effect: Allow
                Resource: '*'
              
  PipelineRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action: ['sts:AssumeRole']
          Effect: Allow
          Principal:
            Service: [codepipeline.amazonaws.com]
        Version: '2012-10-17'
      Path: /
      Policies:
        - PolicyName: CodePipelineAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                - 's3:*'
                - 'cloudformation:CreateStack'
                - 'cloudformation:DescribeStacks'
                - 'cloudformation:DeleteStack'
                - 'cloudformation:UpdateStack'
                - 'cloudformation:CreateChangeSet'
                - 'cloudformation:ExecuteChangeSet'
                - 'cloudformation:DeleteChangeSet'
                - 'cloudformation:DescribeChangeSet'
                - 'cloudformation:SetStackPolicy'
                - 'iam:PassRole'
                - 'sns:Publish'
                Effect: Allow
                Resource: '*'






-- so this is a CloudFormation template that will create a code pipeline complete delivery pipeline.

-- we have a few parameters such 

     - pipeline name

     - S3Bucket 

     - etc... check in yaml code above 







-- now deploy wordpress-pipeline.yaml the stack in CF console 

-- PipelineName = any random name --> S3Bucket = where Zip File is present --> SourceS3Key = name of ur zip file ---> Email = ur mail -> create stack 

-- it will create 5 resource like ArtifactStoreBucket , CFNRole , CodePipelineSNSTopic , Pipeline , PipelineRole

-- go n check mail and subscribe 

-- once u give all the info correct it will create pipeline and create stack for test 

-- we have manual approval steps here and approve that steps to move nxt step 

-- now to delete the stack , delete the prod stack manually and delete the objects in s3 manually then delete the stack 











======================================================== AWS CLOUDFORMATION  Resources - Advanced: Custom Resources, Registry, Modules =========================================================






------------------------------------------------ Custom Resources 




-- So it enable you to write custom provision logic in templates that CloudFormation will run any time you create,update and delete stack.

• Defined in the template using AWS::CloudFormation::CustomResource or Custom::MyCustomResourceTypeName (recommended)

-- So two types of custom resources exist in CloudFormation.

         - Backed by a Lambda function (most common) 

         - an SNS topic


-- Usecase :

       - for example, a new service, and you really need it right now. So you can write your own custom resource(AWS Resource is not covered yet), or

       - if you have an on-premises resource that you want to manage then you could manage it as well as using a Custom Resource.

       - Or if you want to, for example run some logic during your template.

              - For example, running a Lambda function that of it's purpose will serve to empty an S3 bucket before the S3 bucket itself is being deleted,

       - for example, to fetch an AMI id. Although now the Parameter Store is a much better way of doing things or really anything you want.              







------------------------------------------------ How to define a Custom Resource?



• Ser viceToken specifies where CloudFormation sends requests to, such as Lambda ARN or SNS ARN (required & must be in the same region)

• Input data parameters (optional)


 S3CustomResource:
    Type: Custom::S3CustomResource
    Properties:
      ServiceToken: !GetAtt AWSLambdaFunction.Arn
      bucket_name: !Ref SampleS3Bucket    ## Additional parameter here








------------------------------------------------ Custom Resource Request and Response



EG for Request 




{
   "RequestType" : "Create",
   "ResponseURL" : "http://pre-signed-S3-url-for-response",
   "StackId" : "arn:aws:cloudformation:us-west-2:123456789012:stack/mystack/5b918d10-cd98-11ea-90d5-0a9cd3354c10",
   "RequestId" : "unique id for this create request",
   "ResourceType" : "Custom::TestResource",
   "LogicalResourceId" : "MyTestResource",
   "ResourceProperties" : {
      "Name" : "Value",
      "List" : [ "1", "2", "3" ]
   }
}




Response 



{
   "Status" : "SUCCESS",
   "PhysicalResourceId" : "TestResource1",
   "StackId" : "arn:aws:cloudformation:us-west-2:123456789012:stack/mystack/5b918d10-cd98-11ea-90d5-0a9cd3354c10",
   "RequestId" : "unique id for this create request",
   "LogicalResourceId" : "MyTestResource",
   "Data" : {
      "OutputName1" : "Value1",
      "OutputName2" : "Value2",
   }
}







------------------------------------------------ Hands-On: Lambda-backed Custom Resource



• You can’t delete a non-empty S3 bucket

• To delete a non-empty S3 bucket, you must first delete all the objects inside it

• We’ll create a custom resource with AWS Lambda that will be used to empty an S3 bucket before deleting it

- create our first Custom Resource!

-- create lambda-back.yaml 







Parameters:
  S3BucketName:
    Type: String
    Description: "S3 bucket to create"
    AllowedPattern: "[a-zA-Z][a-zA-Z0-9_-]*"

Resources:
  SampleS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref S3BucketName
    DeletionPolicy: Delete

  S3CustomResource:
    Type: Custom::S3CustomResource
    Properties:
      ServiceToken: !GetAtt AWSLambdaFunction.Arn
      bucket_name: !Ref SampleS3Bucket    ## Additional parameter here

  AWSLambdaFunction:
     Type: AWS::Lambda::Function
     Properties:
       Description: "Empty an S3 bucket!"
       FunctionName: !Sub '${AWS::StackName}-${AWS::Region}-lambda'
       Handler: index.handler
       Role: !GetAtt AWSLambdaExecutionRole.Arn
       Timeout: 360
       Runtime: python3.8
       Code:
         ZipFile: |
          import boto3
          import cfnresponse
          ### cfnresponse module help in sending responses to CloudFormation
          ### instead of writing your own code

          def handler(event, context):
              # Get request type
              the_event = event['RequestType']        
              print("The event is: ", str(the_event))

              response_data = {}
              s3 = boto3.client('s3')

              # Retrieve parameters (bucket name)
              bucket_name = event['ResourceProperties']['bucket_name']
              
              try:
                  if the_event == 'Delete':
                      print("Deleting S3 content...")
                      b_operator = boto3.resource('s3')
                      b_operator.Bucket(str(bucket_name)).objects.all().delete()

                  # Everything OK... send the signal back
                  print("Execution succesfull!")
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
              except Exception as e:
                  print("Execution failed...")
                  print(str(e))
                  response_data['Data'] = str(e)
                  cfnresponse.send(event, context, cfnresponse.FAILED, response_data)

  AWSLambdaExecutionRole:
     Type: AWS::IAM::Role
     Properties:
       AssumeRolePolicyDocument:
         Statement:
         - Action:
           - sts:AssumeRole
           Effect: Allow
           Principal:
             Service:
             - lambda.amazonaws.com
         Version: '2012-10-17'
       Path: "/"
       Policies:
       - PolicyDocument:
           Statement:
           - Action:
             - logs:CreateLogGroup
             - logs:CreateLogStream
             - logs:PutLogEvents
             Effect: Allow
             Resource: arn:aws:logs:*:*:*
           Version: '2012-10-17'
         PolicyName: !Sub ${AWS::StackName}-${AWS::Region}-AWSLambda-CW
       - PolicyDocument:
           Statement:
           - Action:
             - s3:PutObject
             - s3:DeleteObject
             - s3:List*
             Effect: Allow
             Resource:
             - !Sub arn:aws:s3:::${SampleS3Bucket}
             - !Sub arn:aws:s3:::${SampleS3Bucket}/*
           Version: '2012-10-17'
         PolicyName: !Sub ${AWS::StackName}-${AWS::Region}-AWSLambda-S3
       RoleName: !Sub ${AWS::StackName}-${AWS::Region}-AWSLambdaExecutionRole






-- We have one parameter of the S3BucketName, so this is the name of the S3 bucket to create.

-- Custom::S3CustomResource - So this is just a random name that we gave it,

-- The service token is going to be referencing either a lambda function or SNS topic, but in this example we're doing the lambda function.

-- deploy this template --> S3BucketName = any name u want to create s3 bucket

-- it created 4 Resources

-- no physical ID for custom resource 

-- now upload the some file to the bucket created by CF now 

-- So now the important stuff is that the Lambda function has some code, and so it's going to be invoked every time we create, update, or delete this text.

-- open lambda function and check the code 

-- this thing called cfnresponse is actually a library that is going to help us sending responses to CloudFormation by leveraging directly this pre-signed URL

     - it create cfnresponse.py file also , the code is 






#  Copyright 2016 Amazon Web Services, Inc. or its affiliates. All Rights Reserved.
#  This file is licensed to you under the AWS Customer Agreement (the "License").
#  You may not use this file except in compliance with the License.
#  A copy of the License is located at http://aws.amazon.com/agreement/ .
#  This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied.
#  See the License for the specific language governing permissions and limitations under the License.

from __future__ import print_function
import urllib3
import json

SUCCESS = "SUCCESS"
FAILED = "FAILED"

http = urllib3.PoolManager()


def send(event, context, responseStatus, responseData, physicalResourceId=None, noEcho=False, reason=None):
    responseUrl = event['ResponseURL']

    print(responseUrl)

    responseBody = {
        'Status' : responseStatus,
        'Reason' : reason or "See the details in CloudWatch Log Stream: {}".format(context.log_stream_name),
        'PhysicalResourceId' : physicalResourceId or context.log_stream_name,
        'StackId' : event['StackId'],
        'RequestId' : event['RequestId'],
        'LogicalResourceId' : event['LogicalResourceId'],
        'NoEcho' : noEcho,
        'Data' : responseData
    }

    json_responseBody = json.dumps(responseBody)

    print("Response body:")
    print(json_responseBody)

    headers = {
        'content-type' : '',
        'content-length' : str(len(json_responseBody))
    }

    try:
        response = http.request('PUT', responseUrl, headers=headers, body=json_responseBody)
        print("Status code:", response.status)


    except Exception as e:

        print("send(..) failed executing http.request(..):", e)     









-- check cloudwatch logs for the Events info 

-- now delete the stack ,the S3 bucket to be deleted properly by CloudFormation.











------------------------------------------------ Hands-On: SNS-backed Custom Resource





-- So this time we're going to publish the the Custom Resource to an SNSTopic and the SNSTopic can do whatever it wants.

-- This SNS topic will publish to an HTTP endpoint, which will handle all custom resource requests

-- so let's create a web server for Hands ON and this web server will be listening to requests coming from SNS.

-- So first we need to create this web server, for this. I'm going to create an EC2 instance with the UserData and then install node js on it.



nodejs.sh (userdata)


#!/bin/bash
# Install NodeJS
curl -sL https://rpm.nodesource.com/setup_14.x | bash -
yum install -y nodejs

# Install Our Application
mkdir /home/ec2-user/webserver && cd /home/ec2-user/webserver 
npm init -y
npm install express body-parser chalk cfn-response
chown -R ec2-user:ec2-user /home/ec2-user/*



https://rpm.nodesource.com/setup_14.x  change this to the current version , check in google or run direct cmnds on ec2 it will give suggestions 





-- add 8080 port number in SG for the Ec2 instance

-- now connect to the instance --> cd webserver --> create one server.js file on server vi server.js 



server.mjs 




import express from 'express';
import bodyParser from 'body-parser';
import chalk from 'chalk';
import cfnresponse from 'cfn-response';

const app = express();
app.use(bodyParser.urlencoded({ extended: true }));
app.use(bodyParser.text());

const port = 8080; // HTTP Server Port
const context = { logStreamName: "logStream", done: () => {} };
const customResourcePhysicalID = 'MySeleniumTest123';

// Handle POST requests
app.post('/', (req, res) => {
  // Get HTTP request body
  const body = JSON.parse(req.body);

  // Subscription confirmation
  if (body.Type == 'SubscriptionConfirmation') {
    console.log(chalk.red('======= SNS Subscription Confirmation received! ======='));
    console.log(chalk.green('Subscription URL:'), body.SubscribeURL, '\n\n');
  } else if (body.Type == 'Notification') { // Notification
    // Get the custom resource request
    const request = JSON.parse(body.Message);

    console.log(chalk.red('======= SNS Notification received! ======='));
    console.log(chalk.green('Request:'), request, '\n\n');

    // Create && Update
    if (request.RequestType == 'Create' || request.RequestType == 'Update') {
      setTimeout(() => {
        cfnresponse.send(request, context, cfnresponse.SUCCESS, {}, customResourcePhysicalID);
      }, 10000); // Time to wait before continue (Create/Update)
    } else { // Delete
      cfnresponse.send(request, context, cfnresponse.SUCCESS, {}, customResourcePhysicalID);
    }
  }

  return res.sendStatus(200);
});

app.listen(port, () => {
  console.log(`HTTP server listening at http://localhost:${port}`);
});





-- then finally, we're going to run this server

-- to do so type  node server.mjs 

-- u will get an URL  http://localhost:8080

-- Next we're going to create an SNSTopic.yaml and it takes two parameters to server, IP and the ports.





Parameters:
  ServerIP:
    Type: String
    Description: Web Server IP

  Port:
    Type: Number
    Default: 8080

Resources:
  CustomResourceSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      Subscription:
        - Endpoint: !Sub "http://${ServerIP}:${Port}"
          Protocol: http

Outputs:
  SNSTopicCustomResource:
    Value: !Ref CustomResourceSNSTopic
    Export:
      Name: SNSTopicCustomResource





-- it's going to create a Custom Resource SNS Topic.

-- deploy the stack --> ServerIP = public ip of server --> create 

-- we're creating an SNSTopic right now. And what should happen is that the SNSTopic should create a subscription.

--  And that subscription should arrive into our web server if everything is working properly.

-- go and check in serer u will see a message like 


======= SNS Subscription Confirmation received! =======
Subscription URL: https://sns.us-east-1.amazonaws.com/?Action=ConfirmSubscription&TopicArn=arn:aws:sns:us-east-1:298132369629:fvfvfvfvfv-CustomResourceSNSTopic-UR7hgfYjo6RG&Token=2336412f37fb687f5d51e6e2425ba1f25831937612e99a6a4dc8bba83e2d9d3ba9cdeb76966911599aa99415b051fa861db58d0e03b396fc3e26d45b7282d8d65379397a5eb1a95341edd0b78e6d06e1791799586df10c1b6471e54631beaac757c791d4e077eaf620f8d9be884f1e2303c37216c64f8ab06320a5bc2183de2aa80e723f7c229e6f6d0a2dffceb15d341cbc71a400f995a35677f4329636ca68 


-- paste this url in the browser u will get access denied , go to sns console and refresh the page it will show confirmed as subscriber 




-- we can go into the last step which is to create our resource backed SNS resource backed Custom Resource, (SNS Custom Resource,)

-- create one sns-backed.yaml 




Resources:
  MySeleniumTest:
    Type: Custom::SeleniumTester
    Properties:
      ServiceToken: !ImportValue SNSTopicCustomResource
      seleniumTester: 'SeleniumTest'
      endpoints: [ "http://mysite.com", "http://myecommercesite.com", "http://search.mysite.com" ]
      frequencyOfTestsPerHour: [ "3", "2", "4", "1" ]




-- deploy this stack 

-- The Custom Resource should send a message into SNS and then the SNS forwards the message into our EC2 instance web server.

-- go and check in server u will receive an message like this 





======= SNS Notification received! =======
Request: {
  RequestType: 'Create',
  ServiceToken: 'arn:aws:sns:us-east-1:298132369629:fvfvfvfvfv-CustomResourceSNSTopic-UR7hgfYjo6RG',
  ResponseURL: 'https://cloudformation-custom-resource-response-useast1.s3.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A298132369629%3Astack/plplplpl/11f22d60-7f2b-11ef-87fb-126da39524d9%7CMySeleniumTest%7Ccbc2ec26-7cad-47c9-b86d-5f0b651cecc2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240930T125406Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=AKIA6L7Q4OWTWZQYQNPY%2F20240930%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b0b7719142b3bdd1bd0cd2558c86e1397c0ab41fa735acd0cdcf423eda435058',
  StackId: 'arn:aws:cloudformation:us-east-1:298132369629:stack/plplplpl/11f22d60-7f2b-11ef-87fb-126da39524d9',
  RequestId: 'cbc2ec26-7cad-47c9-b86d-5f0b651cecc2',
  LogicalResourceId: 'MySeleniumTest',
  ResourceType: 'Custom::SeleniumTester',
  ResourceProperties: {
    ServiceToken: 'arn:aws:sns:us-east-1:298132369629:fvfvfvfvfv-CustomResourceSNSTopic-UR7hgfYjo6RG',
    seleniumTester: 'SeleniumTest',
    endpoints: [
      'http://mysite.com',
      'http://myecommercesite.com',
      'http://search.mysite.com'
    ],
    frequencyOfTestsPerHour: [ '3', '2', '4', '1' ]
  }
} 


Response body:
 {"Status":"SUCCESS","Reason":"See the details in CloudWatch Log Stream: logStream","PhysicalResourceId":"MySeleniumTest123","StackId":"arn:aws:cloudformation:us-east-1:298132369629:stack/plplplpl/11f22d60-7f2b-11ef-87fb-126da39524d9","RequestId":"cbc2ec26-7cad-47c9-b86d-5f0b651cecc2","LogicalResourceId":"MySeleniumTest","Data":{}}
Status code: 200
Status message: OK





-- delete this stack now , u will receive an notification like this 



Request: {
  RequestType: 'Delete',
  ServiceToken: 'arn:aws:sns:us-east-1:298132369629:fvfvfvfvfv-CustomResourceSNSTopic-UR7hgfYjo6RG',
  ResponseURL: 'https://cloudformation-custom-resource-response-useast1.s3.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A298132369629%3Astack/s3bucketdelete/ee121b20-7f2b-11ef-b0ec-1264993986c7%7CMySeleniumTest%7C69200ff0-499d-4fab-907f-357c750de4ea?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240930T130228Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=AKIA6L7Q4OWTWZQYQNPY%2F20240930%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=11d11d0891f80b91aceb516b0e504156d57b166d4196cddf990999d39ccd4986',
  StackId: 'arn:aws:cloudformation:us-east-1:298132369629:stack/s3bucketdelete/ee121b20-7f2b-11ef-b0ec-1264993986c7',
  RequestId: '69200ff0-499d-4fab-907f-357c750de4ea',
  LogicalResourceId: 'MySeleniumTest',
  PhysicalResourceId: 'MySeleniumTest123',
  ResourceType: 'Custom::SeleniumTester',
  ResourceProperties: {
    ServiceToken: 'arn:aws:sns:us-east-1:298132369629:fvfvfvfvfv-CustomResourceSNSTopic-UR7hgfYjo6RG',
    seleniumTester: 'SeleniumTest',
    endpoints: [
      'http://mysite.com',
      'http://myecommercesite.com',
      'http://search.mysite.com'
    ],
    frequencyOfTestsPerHour: [ '3', '2', '4', '1' ]
  }
} 






-- delete all stacks and ec2 also 












------------------------------------------------ Custom Resources Closing Comments



-- cfn-response module help in sending responses to CloudFormation instead of writing your own code (Python / NodeJS)

      - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html

-- Custom resource helper framework for Python simplifies writing custom resources by implementing best practices and abstractions 

      - https://github.com/aws-cloudformation/custom-resource-helper


-- You can create your own resources using Resource Types,

      - https://aws.amazon.com/blogs/mt/managing-resources-using-aws-cloudformation-resource-types/





------------------------------------------------ Wait Condition




-- pause the creation of a stack and wait for a signal before it continues to create the stack.

-- for example, we want to start an application on EC2 instance and make sure that when it was actually fully configured, then we would continue on with the creation of the stack.

-- properties:

     - Count : no.of succcess signals required to continue stack creation (default 1)

     - Timeout : time to wait for the number of  signals (max 43200 sec = 12 hrs )

     - Handle : reference to  Type: AWS::CloudFormation::WaitConditionHandle





--  AWS::CloudFormation::WaitConditionHandle : is a way for you to create a pre-signed URL that was going to be able to receive a success failure and then send that into WaitCondition.

-- So for EC2 and ASG you can use a creation policy






------------------------------------------------ Wait Condition Hands ON



create wait.yaml file 





Parameters:
  KeyName:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instances
  SSHLocation:
    Type: String
    Description: The IP address range that can be used to SSH to the EC2 instances
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: "(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})"
    ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x.
  
  ImageId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  InstanceWaitHandle:
    Type: AWS::CloudFormation::WaitConditionHandle
  
  InstanceWaitCondition:
    Type: AWS::CloudFormation::WaitCondition
    DependsOn: MyEC2Instance
    Properties:
      Handle: !Ref InstanceWaitHandle
      Count: 1
      Timeout: "3600"
  
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      InstanceType: t2.micro
      KeyName: !Ref KeyName
      SecurityGroups:
      - !Ref InstanceSecurityGroup
      UserData:
        Fn::Base64:
          !Sub |
            #!/bin/bash -xe

            # Update to the latest packages
            yum update -y

            # Get the latest CloudFormation helper scripts
            yum install -y aws-cfn-bootstrap

            # Make your configuration here
            date > /tmp/datefile
            cat /tmp/datefile

            sleep 10s

            # Signal the status from instance
            /opt/aws/bin/cfn-signal -e $? --data "This is from the EC2 instance" --reason "Build process complete." '${InstanceWaitHandle}'

  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: !Ref SSHLocation

  MySNSTopic:
    Type: AWS::SNS::Topic
    DependsOn: InstanceWaitCondition
    Properties:
      TopicName: DemoTopic




-- the resource has a WaitConditionHandle, so as you can see is a very simple type of resource       

-- then the WaitCondition itself depends on the EC2 instance. So it will be created after the EC2 instance is created.

-- deploy the stack 

-- the instance weight condition is in progress and we have one hour of time out.

-- So It is going to wait for a signal coming from the EC2 to instance they should take about 10 seconds after it has bootstrapped.

-- it shows you how WaitConditions work. that's it 









------------------------------------------------ CloudFormation – Dynamic References




-- so far all the stuff we have within our CloudFormation templates comes from the CloudFormation templates,

-- but it is possible for you to reference external values that are stored in the SSM parameter store and secrets manager, within direct clear your CloudFormation templates,

• Reference external values stored in Systems Manager Parameter Store and Secrets Manager within CloudFormation templates

-- so this is different from using an SSM parameter store in the parameters, because now these references can be done from within your templates, so at runtime CloudFormation will retrieve the templates,

• CloudFormation retrieves the value of the specified reference during create/update/delete operations

• For example: retrieve RDS DB Instance master password from Secrets Manager

• Supports

      • ssm – for plaintext values stored in SSM Parameter Store
      • ssm-secure – for secure strings stored in SSM Parameter Store
      • secretsmanager – for secret values stored in Secrets Manager


‘{{resolve:service-name:reference-key}}’     -- template 


-- upto 60 dynamic references in a template 


-- so you can retrieve a string or a string list from the SSM parameter store, and if you don't specify a version, then CloudFormation we'll use the latest version of that parameter,


IMP:

-- it does not support retrieving public SSM parameters such as the Amazon Linux 2 AMI,




------------------------------------------------ CloudFormation – Dynamic References for ssm


EG :


Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties: 
       AccessControl:‘{{resolve:ssm:s3AccessControl:2}}’



-- as you can see that's very very different from when we had parameters,

-- so parameters were defined outside of resources and then they were referenced from within the resources,

-- but here we can use this dynamic parameters from within the resources or other places in your templates.






------------------------------------------------ CloudFormation – Dynamic References for SSM Secure

{{resolve:ssm-secure:parameter-name:version}}



EG:


  MyIAMUser:
    Type: AWS::IAM::User
    Properties:
      UserName: 'MyUserName'
      LoginProfile:
        Password: '{{resolve:ssm-secure:IAMUserPassword:10}}'



-- stored in secureString form , so it could be passwords, license keys,etc ...

-- CloudFormation will never store the actual parameter value, so this makes it a very secure solution,

-- Now, these secrets can not be used with every single kind of resources,

    https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html#template-parameters-dynamic-patterns-resources







------------------------------------------------ CloudFormation – Dynamic References for Secrets Manager



{{resolve:secretsmanager:secret-id:secret-string:json-key:version-stage:version-id}}




  MyRDSInstance:
    Type: 'AWS::RDS::DBInstance'
    Properties:
      DBName: MyRDSInstance
      AllocatedStorage: '20'
      DBInstanceClass: db.t2.micro
      Engine: mysql
      MasterUsername: '{{resolve:secretsmanager:MyRDSSecret:SecretString:username}}'
      MasterUserPassword: '{{resolve:secretsmanager:MyRDSSecret:SecretString:password}}'





-- it will retrieve the entire secrets or secret value stored in the secrets manager service

-- for example, database credentials, passwords, third party API keys, 

-- if you want to update your secrets, you must first update the resource containing the secretsmanager dynamic reference, using one of the resource properties,









------------------------------------------------ CloudFormation – Dynamic References Hands ON 





-- create yaml file 





Parameters:
  KeyName:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instances
  ImageId: # Public SSM Paramater
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2

Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref ImageId
      KeyName: !Ref KeyName
      # ssm dynamic reference
      InstanceType: '{{resolve:ssm:/ec2/instanceType:1}}'

  MyIAMUser:
    Type: AWS::IAM::User
    Properties:
      UserName: 'sample-user'
      LoginProfile:
        # ssm-secure dynamic reference (latest version)
        Password: '{{resolve:ssm-secure:/iam/userPassword}}'

  MyDBInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBInstanceClass: db.t2.micro
      Engine: mysql
      AllocatedStorage: "20"
      VPCSecurityGroups:
      - !GetAtt [DBEC2SecurityGroup, GroupId]
      # secretsmanager dynamic reference
      MasterUsername: '{{resolve:secretsmanager:MyRDSSecret:SecretString:username}}'
      MasterUserPassword: '{{resolve:secretsmanager:MyRDSSecret:SecretString:password}}'
  
  DBEC2SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: 'Allow access from anywhwere to database'
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 3306
        ToPort: 3306
        CidrIp: "0.0.0.0/0"





-- the image ID is still used because we reference a public SSM parameter.

-- but public SSM parameter can only work  within the "parameter section" for now, not within the template by using dynamic references.

-- if you scroll down now, and look at this EC2 instance, the image ID is referencing a parameter.

-- but the instance type now, instead of specifying t2.micro manually, we are resolving it from the SSM parameter store

-- now go to parameter store --> create path like this /ec2/instanceType --> std --> string --> value = t2.micro

-- now SSM a confirmation will resolve this parameter from SSM directly.

-- same for sample I am user also create path /iam/userPassword --> SecureString ---> My current account ---> value = give anypassword --> create 

-- this password only lives within the parameter store. As we can see now this is a secure string, and is going to be retrieved at run time by again, our CloudFormation template.

--   MyDBInstance: --> open secret manager --> Other type of secret --> create username and password --> name = same as in code --> create 

-- So now we have these usernames and passwords are going to be resolved directly from the secrets manager, 

-- deploy yaml file 










------------------------------------------------------- AWS CLOUDFORMATION UpdatePolicy 




-- UpdatePolicy is a way for you to specify how CloudFormation will update a few of the following resources.

     - AWS::AutoScaling::AutoScalingGroup(3 update policies)

            - AutoScalingRollingUpdate,
            - AutoScalingReplacingUpdate,
            - AutoScalingScheduledAction.


    -  AWS::ElasticCache::ReplicationGroup

            - it will modify the ReplicationGroup shards by adding or removing shards, rather than replacing resources in ElastiCache.        

    -  AWS::Elasticsearch::Domain

            - also can be upgraded to a new version rather than replacing the resource using an update policy.

    - AWS::Lambda::Alias

            - if you update it, then CloudFormation, using the policy, can perform a code that deploy deployments when the version changes on the alias.            





-- AutoScalingRollingUpdate : 

      - to specify whether CloudFormation update instances that are in an auto scaling group.if you want to update them in batches or all at once.

  UpdatePolicy:
      AutoScalingRollingUpdate:
        MaxBatchSize: 1
        MinInstancesInService: 1
        PauseTime: PT15M
        WaitOnResourceSignals: true





-- AutoScalingReplacingUpdate

       - to specify whether CloudFormation replaces an ASG with a new one, or will replace the instances within the ASG.

       - So this is a Boolean. And in case you want to replace the ASG yourself, then CloudFormation will create a new ASG and retain the old ASG until it has finished bootstrapping the new.

UpdatePolicy:
      AutoScalingRollingUpdate:
         willReplace: Boolean





-- AutoScalingScheduledAction.

      - you specify how CloudFormation handles updates for the group size property.

      - So MinSize, MaxSize and DesiredCapacity,of an ASG that has a scheduled action.



UpdatePolicy:
      AutoScalingScheduledAction:
        IgnoreUnmodifiedGroupSizeProperties: Boolean








------------------------------------------------------- CloudFormation Registry 



-- we've seen custom resources but now we're going to go one step deeper.

-- there's something called the registry,that contains either private or public extensions.

-- So what are extensions?

      - Extensions are artifacts that will augment the functionality of CloudFormation resources and properties.

      -  you can register these extensions into the CloudFormation Registry.

-- So these extensions can be written by anyone such as AWS, Amazon Partners, Marketplace sellers, and the community and we have different extension types.

-- we have different extension types.

     - private extensions : that you create or that are shared with you 

     - Public  extensions : which are provided by AWS,(ex:AWS::DynamoDB::Table)


-- Now to create these extensions we'll be practicing using the CloudFormation CLI.





---------------------- Resource Types 


-- if we look at the resource types for these extensions 

-- it used to model and provision resources in CloudFormation that don't exist.

-- So if you have a custom resource, you can create a custom resource backed by Lambda function but you can also create a resource type.

-- to create a resource type , it will follow a different structure such as "Organization::Service::Resource".

-- resource type package consists of 

       - JSON schema that defines your type

       - handlers to perform the required actions such as create, update, delete, read, and list.



-- Steps to create Resource Types 

      - Model: create and validate schema that serves as the definition of your Resource Types 

      - Develop: Write a handler that defines five core operations(create, read, update, delete and list,)on your  Resource Types  and then you test them locally.

      - Register: you register the resource type in CloudFormation using the CLI. So it can use them in your CloudFormation templates.


-- these handlers are developed in four languages right now.      

     - Python, Java, TypeScript, and Go.







------------------------ Cloudformation CLI 



-- So the CloudFormation CLI will be used to develop and test the third party extensions

      - https://github.com/aws-cloudformation/cloudformation-cli


-- you can register extensions for use  in CloudFormation

-- we said that there are four languages supported to write your own extensions.

      - Python, Java, TypeScript, and Go.








------------------------------------------------------- 3rd Party Resource Type 



-- there are some that can be downloaded and added to your accounts via the CloudFormation Registry from other vendors.


-- EG:
 

      - https://github.com/spotinst/spotinst-aws-cloudformation-registry

      - https://github.com/newrelic/cloudformation-partner-integration

      - https://github.com/fortinet/aws-cloudformation-resource-provider

      - https://github.com/mnalezin/DynatraceInstallerAgent

      - https://github.com/densify-dev/cloudformation-optimization-as-code

      - https://github.com/DataDog/datadog-cloudformation-resources

      - https://github.com/opsgenie/opsgenie-cloudformation-resources








------------------------------------------------------- Custom Resources vs resource Types 




https://aws.amazon.com/blogs/mt/managing-resources-using-aws-cloudformation-resource-types/#:~:text=When%20a%20custom%20resource%20is%20invoked%20by%20CloudFormation%2C%20the%20Lambda,registered%20with%20the%20CloudFormation%20registry.










------------------------------------------------------- Template Modules



https://aws.amazon.com/blogs/mt/introducing-aws-cloudformation-modules/



-- So modules are used to package resources and their configuration for reuse across different stack templates.

-- these are different from nested stacks. So when use a nested stack, you are actually creating a sub stack as part of your templates.

-- But modules is more about code replacements 

-- usecases:

         - keep resource configurations aligned with best practices,

         - use code written by experts.

-- But with modules you are replacing code within your CloudFormation templates.  Whereas with nested stacks you are creating different stacks as part of your main stack.


-- Module Contains :

         - Template Sections: Resource , Output etc...

         - module parameters : to input custom values for your module.
         
         - So the structure of a module is:

                 - Organization::Service::Resource::MODULE 

-- a module can be part of a specific parent template and the module can have children and the children of the module can be a module itself.                 







------------------------------------------------------- Modules Parameters



-- So modules can have parameters and they'll look at your input custom values for your modules.

-- They will be defined the same way as template parameters and you can pass parent parameters to the module parameters,

-- So you provide parameters to your templates and then these are passed on to your module

-- but you can not perform constraint checking on your module parameters for now.




------------------------------------------------------- reference resources in a module 



-- So we can reference resources within the modules using the fully qualified logical name to get attributes out of modules, for example, using outputs.

-- So we can use the GetAtt and Ref intrinsic function to get to reference them and to get values out of them.








------------------------------------------------------- Module Hands ON




-- we're going to create a module that creates the restrictive S3 bucket.

     - So we have an S3 bucket,
     - Amazon KMS key to encrypt the data at rest in the bucket,
     - as well as a BucketPolicy that restricts access to the provided IAM roles and only allow HTTPS traffic to the bucket.



-- we'll have all this configuration as part of one module, and then we'll reuse this module and a template that will create an Amazon Kinesis Data Firehose and that will use this bucket as a destination.





-- open cloushell in and create folder like s3-module 

      - mkdir s3-module 

-- pip install cloudformation-cli

-- cfn init

-- choose "m"

-- give name of Mdoule like this    MyCompany::S3::Bucket::MODULE

-- So now this is creating the fragments for my module.

-- So now we can write the fragment itself either in JSON or YAML.

-- look cd fragments --> cat sample.json

-- we dont want sample.json so remove rm sample.json

-- cd fragments --> nano s3-bucket.yaml and paste the code below 






AWSTemplateFormatVersion: 2010-09-09
Description: Create a S3 bucket that follows MyCompany's standards
Parameters:
  KMSKeyAlias:
    Description: >-
      The alias for your KMS key. If you will leave this field empty KMS key
      alias won't be created along the key.
    Type: String
    AllowedPattern: '^(?!aws)([a-zA-Z0-9\-\_\/]){1,32}$|^$'
    MinLength: 0
    MaxLength: 32
    ConstraintDescription: >-
      KMS key alias must be at least 1 and no more than 32 characters long, can
      contain lowercase and uppercase letters, numbers, hyphens, underscores and
      forward slashes and cannot begin with aws.
  ReadOnlyArn:
    Description: >-
      Provide ARN of an existing Principal (role) that will be granted with read
      only access to the S3 bucket (e.g.
      'arn:aws:iam::123456789xxx:role/myS3ROrole'). If not specified, access
      will be granted to current AWS account:root only. CF deployment will fail
      and rollback for non-existing ARN.
    Type: String
    Default: ''
    AllowedPattern: >-
      ^(arn:aws:iam::\d{12}:role(\/|\/[\w\!\"\#\$\%\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\`\{\|\}\~]{1,510}\/)[\w\+\=\,\.\@\-]{1,64})$|^$
    ConstraintDescription: >-
      IAM role ARN must start with arn:aws:iam::<12-digit AWS account
      number>:role/[<path>/]<name of role>. The name of role must be at least 1
      and no more than 64 characters long, can contain lowercase letters,
      uppercase letters, numbers, plus (+), equal (=), comma (,), period (.), at
      (@), underscore (_), and hyphen (-). Path is optional and must not exceed
      510 characters.
  ReadWriteArn:
    Description: >-
      Provide ARN of an existing Principal (role) that will be granted with read
      and write access to the S3 bucket (e.g.
      'arn:aws:iam::123456789xxx:role/myS3RWrole'). If not specified, access
      will be granted to current AWS account:root only. CF deployment will fail
      and rollback for non-existing ARN.
    Type: String
    Default: ''
    AllowedPattern: >-
      ^(arn:aws:iam::\d{12}:role(\/|\/[\w\!\"\#\$\%\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\`\{\|\}\~]{1,510}\/)[\w\+\=\,\.\@\-]{1,64})$|^$
    ConstraintDescription: >-
      IAM role ARN must start with arn:aws:iam::<12-digit AWS account
      number>:role/[<path>/]<name of role>. The name of role must be at least 1
      and no more than 64 characters long, can contain lowercase letters,
      uppercase letters, numbers, plus (+), equal (=), comma (,), period (.), at
      (@), underscore (_), and hyphen (-). Path is optional and must not exceed
      510 characters.

Resources:
  KmsKey:
    Type: AWS::KMS::Key
    DeletionPolicy: Delete
    UpdateReplacePolicy: Delete
    Properties:
      Enabled: true
      EnableKeyRotation: true
      KeyPolicy:
        Version: 2012-10-17
        Statement:
          - Sid: 'Give AWS account:root full control over the KMS key'
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action:
              - 'kms:*'
            Resource: '*'
          - Sid: 'Give ReadOnlyRole access to use KMS key for decryption'
            Effect: Allow
            Principal:
              AWS: !Ref ReadOnlyArn
            Action:
              - 'kms:Decrypt'
              - 'kms:DescribeKey'
            Resource: '*'
          - Sid: 'Give the ReadWriteRole access to use KMS key for encryption and decryption'
            Effect: Allow
            Principal:
              AWS: !Ref ReadWriteArn
            Action:
              - 'kms:Encrypt'
              - 'kms:Decrypt'
              - 'kms:ReEncrypt'
              - 'kms:GenerateDataKey*'
              - 'kms:DescribeKey'
            Resource: '*'

  KmsKeyAlias:
    Type: AWS::KMS::Alias
    Properties:
      AliasName: !Sub 'alias/${KMSKeyAlias}'
      TargetKeyId: !Ref KmsKey

  Bucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: BucketOwnerFullControl
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              KMSMasterKeyID: !Ref KmsKey
              SSEAlgorithm: 'aws:kms'
      BucketName: !Sub '${AWS::StackName}-${AWS::AccountId}-${AWS::Region}'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true

  BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref Bucket
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: DenyIncorrectEncryptionHeader
            Effect: Deny
            Principal: '*'
            Action: 's3:PutObject'
            Resource: !Sub 'arn:aws:s3:::${Bucket}/*'
            Condition:
              StringEquals:
                's3:x-amz-server-side-encryption': AES256
          - Sid: DenyPublicReadACL
            Effect: Deny
            Principal: '*'
            Action:
              - 's3:PutObject'
              - 's3:PutObjectAcl'
            Resource: !Sub 'arn:aws:s3:::${Bucket}/*'
            Condition:
              StringEquals:
                's3:x-amz-acl':
                  - public-read
                  - public-read-write
                  - authenticated-read
          - Sid: DenyPublicReadGrant
            Effect: Deny
            Principal: '*'
            Action:
              - 's3:PutObject'
              - 's3:PutObjectAcl'
            Resource: !Sub 'arn:aws:s3:::${Bucket}/*'
            Condition:
              StringLike:
                's3:x-amz-grant-read':
                  - '*http://acs.amazonaws.com/groups/global/AllUsers*'
                  - '*http://acs.amazonaws.com/groups/global/AuthenticatedUsers*'
          - Sid: DenyNonHttpsConnections
            Effect: Deny
            Principal: '*'
            Action:
              - 's3:PutObject'
              - 's3:GetObject'
            Resource: !Sub 'arn:aws:s3:::${Bucket}/*'
            Condition:
              Bool:
                'aws:SecureTransport': false
          - Sid: 'Give ReadOnlyRole access to get objects from bucket and list bucket'
            Effect: Allow
            Principal:
              AWS: !Ref ReadOnlyArn
            Action:
              - 's3:GetObject'
              - 's3:GetObjectTagging'
              - 's3:ListBucket'
            Resource:
              - !Sub 'arn:aws:s3:::${Bucket}'
              - !Sub 'arn:aws:s3:::${Bucket}/*'
          - Sid: 'Give the ReadWriteRole access to get and put objects from and to bucket and list bucket and multipart uploads'
            Effect: Allow
            Principal:
              AWS: !Ref ReadWriteArn
            Action:
              - 's3:DeleteObject'
              - 's3:DeleteObjectTagging'
              - 's3:GetObject'
              - 's3:GetObjectTagging'
              - 's3:ListBucket'
              - 's3:PutObject'
              - 's3:PutObjectTagging'
            Resource:
              - !Sub 'arn:aws:s3:::${Bucket}'
              - !Sub 'arn:aws:s3:::${Bucket}/*'

Outputs:
  BucketArn:
    Description: ARN of the bucket created.
    Value: !GetAtt Bucket.Arn
  BucketName:
    Description: Name of the bucket created.
    Value: !Ref Bucket
  KmsKeyAlias:
    Description: Alias of SSE-KMS Customer Managed Key used to encrypt S3 bucket content.
    Value: !Ref KmsKeyAlias
  KmsKeyArn:
    Description: ARN of SSE-KMS Customer Managed Key used to encrypt S3 bucket content.
    Value: !GetAtt KmsKey.Arn





-- cd .. 

-- go back to s3-module 

-- now do cfn submit -v ,    run thsi command where u did cfn init (in folder or from root)
 
-- once u run the above cmnd it will come like this 





Validating your module fragments...
Run scan of template /home/cloudshell-user/fragments/s3-bucket.yaml
Module fragment might be valid, but there are warnings from cfn-lint (https://github.com/aws-cloudformation/cfn-python-lint):
        Consider using AWS::S3::BucketPolicy instead of AccessControl (from rule W3045: Controlling access to an S3 bucket should be done with bucket policies)
Creating CloudFormationManagedUploadInfrastructure
CloudFormationManagedUploadInfrastructure stack was successfully created
Successfully submitted type. Waiting for registration with token '8c826b78-add8-44a0-bf63-804ff512ae4d' to complete.
Registration complete.
{'ProgressStatus': 'COMPLETE', 'Description': 'Deployment is currently in DEPLOY_STAGE of status COMPLETED', 'TypeArn': 'arn:aws:cloudformation:us-east-1:298132369629:type/module/MyCompany-S3-Bucket-MODULE', 'TypeVersionArn': 'arn:aws:cloudformation:us-east-1:298132369629:type/module/MyCompany-S3-Bucket-MODULE/00000001', 'ResponseMetadata': {'RequestId': 'aecd8b8c-63dd-43d7-84c5-d8f41e972378', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'aecd8b8c-63dd-43d7-84c5-d8f41e972378', 'date': 'Tue, 01 Oct 2024 09:34:57 GMT', 'content-type': 'text/xml', 'content-length': '685', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}


-- now go to CF console --> Activated extensions ----> Modules ---> Filter extension type = Privately registered  , u will able to see the Module that we have created 

-- check the schema of the module 

-- we can also have example temlate how to use this module 


{
    "Resources": {
        "MyModule": {
            "Type": "MyCompany::S3::Bucket::MODULE",
            "Properties": {
                "KMSKeyAlias": "KMSKeyAlias",
                "ReadOnlyArn": "ReadOnlyArn",
                "ReadWriteArn": "ReadWriteArn"
            }
        }
    }
}







-- So next, well, we have to use this module

-- So now let's assume that we want to create that Firehose templates, And we want to reference the module.

-- So we're going to create a Firehose stream that writes into Amazon S3

-- to do so, we're going to have a Firehose destination 

-- type = give module name in the template 


Resources:
  FirehoseDestination: # Module logical name
    Type: MyCompany::S3::Bucket::MODULE
    Properties:
      KMSKeyAlias: !Sub "${AWS::StackName}"
      ReadWriteArn: !GetAtt FirehoseRole.Arn
      ReadOnlyArn: !Sub 'arn:aws:iam::${AWS::AccountId}:root'



-- We just pass on three different properties, the KMS key Alias, the Read Write ARN for Firehose, as well as the Read Only ARN for the role that would just pass.




-- create template.yaml File





AWSTemplateFormatVersion: '2010-09-09'
Description: "Create a Firehose stream that writes to S3"

Resources:
  FirehoseDestination: # Module logical name
    Type: MyCompany::S3::Bucket::MODULE
    Properties:
      KMSKeyAlias: !Sub "${AWS::StackName}"
      ReadWriteArn: !GetAtt FirehoseRole.Arn
      ReadOnlyArn: !Sub 'arn:aws:iam::${AWS::AccountId}:root'

  FirehoseRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AssumeRole1
            Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'

  FirehosePolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: "ReadWrite"
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: "KmsEncryptionDecryption"
            Effect: Allow
            Action:
              - 'kms:Decrypt'
              - 'kms:GenerateDataKey'
            Resource: !GetAtt FirehoseDestinationKmsKey.Arn # Get KmsKey Arn inside our module
            Condition:
              StringEquals:
                kms:ViaService: !Sub 's3:${AWS::Region}.amazonaws.com'
              StringLike:
                kms:EncryptionContext:aws:s3:arn: !Sub '${FirehoseDestinationBucket.Arn}/*' # Get Bucket Arn inside our module
          - Sid: FirehoseAccess
            Effect: Allow
            Action:
            - kinesis:DescribeStream
            - kinesis:GetShardIterator
            - kinesis:GetRecords
            - kinesis:ListShards
            Resource: !GetAtt Firehose.Arn
          - Sid: "S3ListBucket"
            Effect: Allow
            Action:
              - 's3:ListBucket'
              - 's3:ListBucketByTags'
              - 's3:ListBucketMultipartUploads'
              - 's3:GetBucketLocation'
            Resource: !GetAtt FirehoseDestinationBucket.Arn # Get Bucket Arn inside our module
          - Sid: "S3GetPutDeleteObject"
            Effect: Allow
            Action:
              - 's3:DeleteObject'
              - 's3:DeleteObjectTagging'
              - 's3:GetObject'
              - 's3:GetObjectTagging'
              - 's3:PutObject'
              - 's3:PutObjectTagging'
            Resource: !Sub '${FirehoseDestinationBucket.Arn}/*' # Get Bucket Arn inside our module
      Roles: 
      - !Ref FirehoseRole

  Firehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamName: !Sub "${AWS::StackName}"
      DeliveryStreamType: DirectPut
      S3DestinationConfiguration:
        BucketARN: !GetAtt FirehoseDestinationBucket.Arn # Get Bucket Arn inside our module
        RoleARN: !GetAtt FirehoseRole.Arn
        EncryptionConfiguration:
          KMSEncryptionConfig:
            AWSKMSKeyARN: !GetAtt FirehoseDestinationKmsKey.Arn # Get KmsKey Arn inside our module






-- deploy the stack 















======================================================== AWS CLOUDFORMATION  Generating CloudFormation templates: Imports, SAM, CDK & Macros =========================================================






------------------------------------------------------- Resource Import 



-- The idea is that with CloudFormation, all the resources created by CloudFormation are within your stacks but it is now possible to import resources created out of CloudFormation within your CloudFormation templates.

-- So the idea is that we don't need to delete and recreate the resources. We can just import them.

-- during an import operation, you're going to need a lot of things.

      - You're going to need you to have to have a templates, and the template must describe the entire stack which is the original stack resources as well as the target resources that you want to import.

      - As well as a unique identifier for each resource. eg : s3 bucket name 


-- So say we want to import an S3 bucket into a stack. First, we're going to write this template that will have the S3 bucket defined in it.

-- Then we're going to import them into CloudFormation using the import option.

-- And then all of a sudden the S3 buckets will be managed as part of the CloudFormation stack.

-- So each resource that you must import must have a DeletionPolicy attribute of any value and an identifier.

-- you can not import the same resource into multiple stacks because CloudFormation performs some checks.





------------------------------------------------------- CloudFormation validation on stack 


-- So, first of all, that the import to resources exists,

-- the properties and configuration values adhere to the resource schema,

-- the resources required properties are specified,

-- the resource to import does not belong to another stack.

-- there is no check of CloudFormation that the actual template configuration matches the actual configuration.

-- So it is recommended for you to run a Drift Detection on the imported resources after an import operation.


UseCases: The use cases for importing resources

    - to create a new stack from existing resources

    - import existing resources into an existing stack,

    - To move the resources between stacks.

    - To remove a resource from a stack

    - to remediate a detected drift,

    - as well as a moving nested stack from a parent stack and import it into another parent stack, 
    
    - or nesting an existing stack.





------------------------------------------------------- Resource Import  Hands On





Resources:
  ImportedTable:
    Type: AWS::DynamoDB::Table
    DeletionPolicy: Retain
    Properties:
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: id
          AttributeType: S
      KeySchema:
        - AttributeName: id
          KeyType: HASH

  ImportedBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain





-- first we're going to create manually a DynamoDB table. And an S3 bucket. But we will make sure that they are created manually,

-- then we'll use this template to import them into a CloudFormation template.

-- So if we look at the DynamoDB table, we have a PAY_PER_REQUEST BillingMode. and s3 

-- we can see, the DeletionPolicy is Retain for both of these things, because we must specify a DeletionPolicy, when we import resources.



-- go to DynamoDB --> Partition key = id --> Table settings = Customize settings -->  DynamoDB Standard -----> Read/write capacity settings = on-demand 

-- create s3 bucket also 

-- So next, what we have to do, is to import our CloudFormation resource, into a CloudFormation stack.

-- create one template.yaml


Resources:
  ImportedTable:
    Type: AWS::DynamoDB::Table
    DeletionPolicy: Retain
    Properties:
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: id
          AttributeType: S
      KeySchema:
        - AttributeName: id
          KeyType: HASH

  ImportedBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain







-- CF console --> create stack (choose existing resource) --> upload the template --> give table and bucket names --> import changes 

-- CloudFormation is going to check whether or not these two things are part of, existing CloudFormation resource templates.if not, then they're going to be imported.

-- you can also import ec2 



Resources:
  ImportedTable:
    Type: AWS::DynamoDB::Table
    DeletionPolicy: Retain
    Properties:
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: id
          AttributeType: S
      KeySchema:
        - AttributeName: id
          KeyType: HASH

  ImportedBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
  
  ImportedInstance:
    Type: AWS::EC2::Instance
    DeletionPolicy: Retain
    Properties:
      ImageId: ami-043097594a7df80ec
      InstanceType: t2.micro



-- create ec2 instance and paste ami id in code 

-- ImageId: ami-043097594a7df80ec should be match with your region 

-- once you done --> check tags for the ec2 


-- now if you wanted to delete this stack, this would not be possible, because, the DeletionPolicy of all these resources is Retain.

-- So we need to push an update to change the DeletionPolicy to Delete, as part of an update.




Resources:
  ImportedTable:
    Type: AWS::DynamoDB::Table
    DeletionPolicy: Delete
    Properties:
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: id
          AttributeType: S
      KeySchema:
        - AttributeName: id
          KeyType: HASH

  ImportedBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
  
  ImportedInstance:
    Type: AWS::EC2::Instance
    DeletionPolicy: Delete
    Properties:
      ImageId: ami-043097594a7df80ec
      InstanceType: t2.micro





-- do update on the stack with above stack 










------------------------------------------------------- AWS SAM



• SAM = Serverless Application Model

• Framework for developing and deploying serverless applications

• All the configuration is YAML code

• Generate complex CloudFormation from simple SAM YAML file

• Supports anything from CloudFormation: Outputs, Mappings, Parameters, Resources...

• SAM can use CodeDeploy to deploy Lambda functions

• SAM can help you to run Lambda, API Gateway, DynamoDB locally





------------------------------------------------------- AWS SAM  – Recipe



• Transform Header indicates it’s SAM template:

      • Transform: 'AWS::Serverless-2016-10-31'


• Write Code

      • AWS::Serverless::Function
      • AWS::Serverless::Api
      • AWS::Serverless::SimpleTable


• Package & Deploy: sam deploy (optionally preceded by “sam package”)

• Quickly sync local changes to AWS Lambda (SAM Accelerate): sam sync --watch






------------------------------------------------------- AWS SAM  Hands ON



-- we'll use a sample Hello World! application, and 

-- this SAM template is again, to be used to deploy a Lambda function

-- go to cloushell --> 

            sam init

            choose 1

            application = Example helloworld 

            runtime = python latest

            select options as per ur choice 

            cd sam-app

            ll

                  total 32
                  drwxr-xr-x. 2 cloudshell-user cloudshell-user 4096 Oct  1 13:21 events
                  drwxr-xr-x. 2 cloudshell-user cloudshell-user 4096 Oct  1 13:21 hello_world
                  -rw-r--r--. 1 cloudshell-user cloudshell-user    0 Oct  1 13:21 __init__.py
                  -rw-r--r--. 1 cloudshell-user cloudshell-user 8329 Oct  1 13:21 README.md
                  -rw-r--r--. 1 cloudshell-user cloudshell-user  679 Oct  1 13:21 samconfig.toml
                  -rw-r--r--. 1 cloudshell-user cloudshell-user 2447 Oct  1 13:21 template.yaml
                  drwxr-xr-x. 4 cloudshell-user cloudshell-user 4096 Oct  1 13:21 tests


            sam build   (update from documentation if it is not working with version)

            sam deploy --guided

            keep enter and y 




-- if we go into CloudFormation, this is creating a stack right now.

-- check resource it will create s3 bucket











------------------------------------------------------- AWS Cloud Development Kit (CDK)



• Define your cloud infrastructure using a familiar language:

    • JavaScript/TypeScript,Python,Java,and.NET


• Contains high level components called "constructs" , these constructs will be transformed later on into a CloudFormation template.

• The code is “compiled” into a CloudFormation template (JSON/YAML)

• You can therefore deploy infrastructure and application runtime code together

    • Great for Lambda functions
    • Great for Docker containers in ECS / EKS

-- can import/Migrate a cloudformation template to AWS CDK 









------------------------------------------------------- CDK vs SAM



• SAM:

    • Serverless focused
    • Write your template declaratively in JSON orYAML 
    • Great for quickly getting started with Lambda
    • Leverages CloudFormation

-- if you want to just write a Lambda function, SAM is probably gonna be a better choice.    


• CDK:


    • All AWS services
    • Write infra in a programming language JavaScript/TypeScript, Python, Java, and .NET
    • Leverages CloudFormation

-- if you want to write your entire infrastructure using the CDK,    

      - a CloudFormation and a program language in the CDK is going to be the way to go.









------------------------------------------------------- CDK Hands ON



steps.sh 


# 1. install the CDK
sudo npm install -g aws-cdk

# directory name must be cdk-app/ to go with the rest of the tutorial, changing it will cause an error
mkdir cdk-app
cd cdk-app/

# initialize the application
cdk init --language javascript

# verify it works correctly
cdk ls

# install the necessary packages
npm install @aws-cdk/aws-s3 @aws-cdk/aws-iam @aws-cdk/aws-lambda @aws-cdk/aws-lambda-event-sources @aws-cdk/aws-dynamodb


# 2. copy the content of cdk-app-stack.js into lib/cdk-app-stack.js



# go to  lib folder

cd lib 

# create one file eg: nano cdk-app-stack.js  and uplaod this code 


const cdk = require('aws-cdk-lib');
const { Stack } = cdk;
const s3 = require('aws-cdk-lib/aws-s3');
const iam = require('aws-cdk-lib/aws-iam');
const lambda = require('aws-cdk-lib/aws-lambda');
const lambdaEventSource = require('aws-cdk-lib/aws-lambda-event-sources');
const dynamodb = require('aws-cdk-lib/aws-dynamodb');
const { Construct } = require('constructs');

const imageBucket = "cdk-rekn-imagebucket";

class CdkAppStack extends Stack {
  /**
   *
   * @param {Construct} scope
   * @param {string} id
   * @param {cdk.StackProps=} props
   */
  constructor(scope, id, props) {
    super(scope, id, props);

    // The code that defines your stack goes here

    // ========================================
    // Bucket for storing images
    // ========================================
    const bucket = new s3.Bucket(this, imageBucket, {
      removalPolicy: cdk.RemovalPolicy.DESTROY,
    });
    new cdk.CfnOutput(this, "Bucket", { value: bucket.bucketName });

    // ========================================
    // Role for AWS Lambda
    // ========================================
    const role = new iam.Role(this, "cdk-rekn-lambdarole", {
      assumedBy: new iam.ServicePrincipal("lambda.amazonaws.com"),
    });
    role.addToPolicy(
      new iam.PolicyStatement({
        effect: iam.Effect.ALLOW,
        actions: [
          "rekognition:*",
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents",
        ],
        resources: ["*"],
      })
    );

    // ========================================
    // DynamoDB table for storing image labels
    // ========================================
    const table = new dynamodb.Table(this, "cdk-rekn-imagetable", {
      partitionKey: { name: "Image", type: dynamodb.AttributeType.STRING },
      removalPolicy: cdk.RemovalPolicy.DESTROY,
    });
    new cdk.CfnOutput(this, "Table", { value: table.tableName });

    // ========================================
    // AWS Lambda function
    // ========================================
    const lambdaFn = new lambda.Function(this, "cdk-rekn-function", {
      code: lambda.Code.fromAsset("lambda"),
      runtime: lambda.Runtime.PYTHON_3_8,
      handler: "index.handler",
      role: role,
      environment: {
        TABLE: table.tableName,
        BUCKET: bucket.bucketName,
      },
    });
    lambdaFn.addEventSource(
      new lambdaEventSource.S3EventSource(bucket, {
        events: [s3.EventType.OBJECT_CREATED],
      })
    );

    bucket.grantReadWrite(lambdaFn);
    table.grantFullAccess(lambdaFn);
  }
}

module.exports = { CdkAppStack };





# go to cdk-app 

cd ..









# 3. setup the Lambda function
mkdir lambda && cd lambda && touch index.py


nano index.py



#
# Lambda function detect labels in image using Amazon Rekognition
#

from __future__ import print_function
import boto3
import json
import os
from boto3.dynamodb.conditions import Key, Attr

minCofidence = 60


def handler(event, context):
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']

    rekFunction(bucket, key)


def rekFunction(bucket, key):
    print("Detected the following image in S3")
    print("Bucket: " + bucket + " key name: " + key)

    client = boto3.client("rekognition")

    response = client.detect_labels(Image={"S3Object": {"Bucket": bucket, "Name": key}},
                                    MaxLabels=10, MinConfidence=minCofidence)

    # Get the service resource
    dynamodb = boto3.resource("dynamodb")

    # Instantiate a table resource object
    imageLabelsTable = os.environ["TABLE"]
    table = dynamodb.Table(imageLabelsTable)

    # Put item into table
    table.put_item(
        Item={"Image": key}
    )

    objectsDetected = []

    for label in response["Labels"]:
        newItem = label["Name"]
        objectsDetected.append(newItem)
        objectNum = len(objectsDetected)
        itemAtt = f"object{objectNum}"
        response = table.update_item(
            Key={"Image": key},
            UpdateExpression=f"set {itemAtt} = :r",
            ExpressionAttributeValues={":r": f"{newItem}"},
            ReturnValues="UPDATED_NEW"
        )




# cd ..


# 4. bootstrap the CDK application
cdk bootstrap


-- go to cloudformation and check it will create stack for you 





# 5. (optional) synthesize as a CloudFormation template
cdk synth

   - this is going to generate the CF template for you 




# 6. deploy the CDK stack
cdk deploy

   - choose y 
    
   - go to CF "CdkAppStack" is being deployed.

   - check template in CF , 

   - So this is why the CDK is so helpful sometimes you can just have very very complex CloudFormation templates being generated using a few lines of code.





-- go to CdkAppStack --> Resources --> AWS::S3::Bucket --> to to s3 bucket and upload some images from google randoma images  into it 



-- so what's going to happen is that now the Lambda function is being triggered.

-- open dynamodb table created by CF --> explore items --> run 

-- we can see that a lot of objects have been found by recognition.

-- this entire application was deployed with just a little bit of code into a very complex CloudFormation template



# 7. empty the s3 bucket


# 8. destroy the stack
cdk destroy








-- open cloushell , run the above commands one by one










------------------------------------------------------- CloudFormation Macros 




-- So Macros allow you to do custom processing on CloudFormation templates.

      - For example, you can create a find-and-replace function.

      - You can create specific transformations



-- we'll go through a hands-on that we'll use account to duplicate a resource as many times as we count it.

-- for example, AWS::Serverless which takes an entire template written in SAM syntax and transform it into a complaint cloudformation template 

-- So to define a Macro,

          - you need a Lambda function that will take an input of what needs to be processed and output a CloudFormation templates.

          - A Resource of Type AWS::CloudFormation::Macro , Then you need to create this as a type of CloudFormation Macro which is a separate stack to define your Macro.


-- You can process:

          - you can process either your entire templates(Reference the MAcro in the Transform section in the template)

          - a snippet of the template,(Reference the Macro in Fn::Transform)


-- Then CloudFormation generates a ChangeSet that will include the process templates.

-- you'll be good to go to apply these templates into your CloudFormation stack.







------------------------------------------------------- How to Define Macro 




CompanyDefaultsMacro:
  Type: AWS::CloudFormation::Macro
  Properties:
    Name: CompanyDefaults
    FunctionName:
      Fn::GetAtt:
        - CompanyDefaultsLambdaFunction
        - Arn




Resources:
  Macro:
    Type: AWS::CloudFormation::Macro
    Properties:
      Name: Count
      FunctionName: !GetAtt CountMacroFunction.Arn







------------------------------------------------------- how to use Macro 



-- to use a Macro in a separate templates, you either have a transform at the very, very top.



Transform: Count

Resources:
  MyBucket:
    Type: AWS::S3::Bucket
    Count: 3
    Properties:
      Tags:
        - Key: TestKey
          Value: my bucket %d







------------------------------------------------------- Good to Know - Macro



-- you can not use Macros in the same template as where you are defining your Macros,

-- you cannot have Macros within the Macros.

-- Macros currently are not supported within StackSets

-- Macros are going to be evaluated in order from the deepest or the shallowest.(top-level macros are executed last)

-- In terms of billing for the Macro feature is going to be per AWS Lambda invocation.

-- transform hosted by AWS 

      - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-reference.html

-- Macros Example 


      - https://github.com/aws-cloudformation/aws-cloudformation-templates/tree/main/CloudFormation/MacrosExamples



  



------------------------------------------------------- Hands ON Macros 



-- we're going to create a macro called Count

-- this count macro is going to be applied at the top of our templates.

-- This macro will be applied. And then the idea is that again each resource will be duplicated without having to copy and paste the resource.

-- you can also use the percentage %D placeholder into any string value to replace by the iteration counting.

-- create count-macro.yaml





AWSTemplateFormatVersion: 2010-09-09
Description: >
  Count Macro
  A simple iterator for creating multipleidentical resources

Resources:
  Macro:
    Type: AWS::CloudFormation::Macro
    Properties:
      Name: Count
      FunctionName: !GetAtt CountMacroFunction.Arn
  
  CountMacroFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Runtime: python3.8
      Timeout: 360
      Role: !GetAtt AWSLambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import copy
          import json

          def process_template(template, parameters):
            new_template = copy.deepcopy(template)
            status = 'success'

            for name, resource in template['Resources'].items():
              if 'Count' in resource:

                # Check if the value of Count is referenced to a parameter passed in the template
                try:
                  refValue = new_template['Resources'][name]['Count'].pop('Ref')
                  # Convert referenced parameter to an integer value
                  count = int(parameters[refValue])
                  # Remove the Count property from this resource
                  new_template['Resources'][name].pop('Count')
                
                except AttributeError:
                  # Use numeric Count value
                  count = new_template['Resources'][name].pop('Count')
                
                print("Found 'Count' property with value {} in '{}' resource...multiplying!".format(count, name))

                # Remove the original resource from the template but take a local copy of it
                resourceToMultiply = new_template['Resources'].pop(name)

                # Create a new block of the resource muliplied with names ending in the iterator and the placeholders substituted
                resourcesAfterMultiplication = multiply(name, resourceToMultiply, count)

                if not set(resourcesAfterMultiplication.keys()) & set(new_template['Resources'].keys()):
                  new_template['Resources'].update(resourcesAfterMultiplication)
                else:
                  status = 'failed'
                  return status, template
              
              else:
                print("Did not find 'Count' property in '{}' resource...Nothing to do!".format(name))
            
            return status, new_template
        
          def update_placeholder(resource_structure, iteration):

            # Convert the json into a string
            resourceString = json.dumps(resource_structure)

            # Count the number of times the placeholder is found in the string
            placeHolderCount = resourceString.count('%d')

            # If the placeholder is found then replace it
            if placeHolderCount > 0:
              print ("Found {} occurrences of decimal placeholder in JSON, replacing with iterator value {}".format(placeHolderCount, iteration))

              # Make a list of the values that we will use to replace the decimal placeholders - the values will all be the same
              placeHolderReplacementValues = [iteration] * placeHolderCount

              # Replace the decimal placeholders using the list - the syntax below expands the list
              resourceString = resourceString % (*placeHolderReplacementValues,)

              # Convert the string back to JSON and return it
              return json.loads(resourceString)

            else:
              print("No occurrences of decimal placeholder found in JSON, therefore nothing will be replaced")
              return resource_structure
          
          def multiply(resource_name, resource_structure, count):
            resources = {}

            # Loop according to the number of times we want to multiply, creating a new resource each time
            for iteration in range(1, (count + 1)):
              print("Multiplying '{}', iteration count {}".format(resource_name, iteration))
              multipliedResourceStructure = update_placeholder(resource_structure, iteration)
              resources[resource_name + str(iteration)] = multipliedResourceStructure
            
            return resources
          
          def handler(event, context):
            result = process_template(event['fragment'], event['templateParameterValues'])

            return {
              'requestId': event['requestId'],
              'status': result[0],
              'fragment': result[1]
            }

  AWSLambdaExecutionRole:
     Type: AWS::IAM::Role
     Properties:
       AssumeRolePolicyDocument:
         Statement:
         - Action:
           - sts:AssumeRole
           Effect: Allow
           Principal:
             Service:
             - lambda.amazonaws.com
         Version: '2012-10-17'
       Path: "/"
       Policies:
       - PolicyDocument:
           Statement:
           - Action:
             - logs:CreateLogGroup
             - logs:CreateLogStream
             - logs:PutLogEvents
             Effect: Allow
             Resource: arn:aws:logs:*:*:*
           Version: '2012-10-17'
         PolicyName: !Sub ${AWS::StackName}-${AWS::Region}-AWSLambda-CW
       RoleName: !Sub ${AWS::StackName}-${AWS::Region}-AWSLambdaExecutionRole






-- which is going to create this macro for us.

-- So the resource type is of type CloudFormation macro

-- deploy stack 

-- So this is going to create lambda function and my lambda function is going to be backing my macro.



2

-- create template.yaml file 



Transform: Count

Resources:
  MyBucket:
    Type: AWS::S3::Bucket
    Count: 3
    Properties:
      Tags:
        - Key: TestKey
          Value: my bucket %d




-- in this example, we have a transformed counts on an S3 buckets.

-- as you can see this count property, is not recognized by CFN lint, because, well, it doesn't know about the accounts by a macro.

-- so what will happen is that we'll have three S3 buckets created as part of this templates and then the tags are going to change.

-- now deploy the template.yaml stack 

-- so my template is going to be based on to this lambda function because in the templates we are using this counts macro.

-- template --> View processed template , it will gives u entire template 

-- once complete the stack u can see 3 buckets created 










======================================================== AWS CLOUDFORMATION AWS CLI =========================================================




-- So we're going to upload this simple template into cloud shell to be able to apply directly within cloud formation, 





Metadata: 
  License: Apache-2.0
AWSTemplateFormatVersion: '2010-09-09'
Description: 'AWS CloudFormation Sample Template Sample template EIP_With_Association:
  This template shows how to associate an Elastic IP address with an Amazon EC2 instance
  - you can use this same technique to associate an EC2 instance with an Elastic IP
  Address that is not created inside the template by replacing the EIP reference in
  the AWS::EC2::EIPAssoication resource type with the IP address of the external EIP.
  **WARNING** This template creates an Amazon EC2 instance and an Elastic IP Address.
  You will be billed for the AWS resources used if you create a stack from this template.'
Parameters:
  InstanceType:
    Description: WebServer EC2 instance type
    Type: String
    Default: t3.small
    AllowedValues: [t2.nano, t2.micro, t2.small, t2.medium, t2.large, t2.xlarge, t2.2xlarge,
      t3.nano, t3.micro, t3.small, t3.medium, t3.large, t3.xlarge, t3.2xlarge,
      m4.large, m4.xlarge, m4.2xlarge, m4.4xlarge, m4.10xlarge,
      m5.large, m5.xlarge, m5.2xlarge, m5.4xlarge,
      c5.large, c5.xlarge, c5.2xlarge, c5.4xlarge, c5.9xlarge,
      g3.8xlarge,
      r5.large, r5.xlarge, r5.2xlarge, r5.4xlarge,
      i3.xlarge, i3.2xlarge, i3.4xlarge, i3.8xlarge,
      d2.xlarge, d2.2xlarge, d2.4xlarge, d2.8xlarge]
    ConstraintDescription: must be a valid EC2 instance type.
  KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instances
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: must be the name of an existing EC2 KeyPair.
  SSHLocation:
    Description: The IP address range that can be used to SSH to the EC2 instances
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: (\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})/(\d{1,2})
    ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x.
  LatestAmiId:
    Type:  'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
    Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2'
Resources:
  EC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      UserData: !Base64
        Fn::Join:
        - ''
        - [IPAddress=, !Ref 'IPAddress']
      InstanceType: !Ref 'InstanceType'
      SecurityGroups: [!Ref 'InstanceSecurityGroup']
      KeyName: !Ref 'KeyName'
      ImageId: !Ref 'LatestAmiId'
  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: !Ref 'SSHLocation'
  IPAddress:
    Type: AWS::EC2::EIP
  IPAssoc:
    Type: AWS::EC2::EIPAssociation
    Properties:
      InstanceId: !Ref 'EC2Instance'
      EIP: !Ref 'IPAddress'
Outputs:
  InstanceId:
    Description: InstanceId of the newly created EC2 instance
    Value: !Ref 'EC2Instance'
  InstanceIPAddress:
    Description: IP address of the newly created EC2 instance
    Value: !Ref 'IPAddress'






-- create parameter.json file 



[
  {
    "ParameterKey": "InstanceType",
    "ParameterValue": "t2.micro"
  },
  {
    "ParameterKey": "KeyName",
    "ParameterValue": "seds"
  },
  {
    "ParameterKey": "SSHLocation",
    "ParameterValue": "0.0.0.0/0"
  }
]




-- change keyname of urs 


-- upload these files into cloudshell 

-- enter the below command to deploy the stack from AWS Cli 

      aws cloudformation create-stack --stack-name example-cli-stack --template-body file://0-sample-template.yaml --parameters file://0-parameters.json


-- now delete the stack 


      aws cloudformation delete-stack --stack-name example-cli-stack










-------------------------------------------------------- Advanced Concepts & 3rd Party Tools




Former2
Former2 allows you to generate IaC (ex. CloudFormation templates) from existing resources https://github.com/iann0036/former2

          Everything happens in the browser (it’s a client-side web app)

          Requires IAM keys with ReadOnlyAccess

          The following outputs are currently supported:

          CloudFormation templates

          Terraform

          Troposphere

          CDK (Cfn Primitives) – TypeScript, Python, C#, Java

          CDK for Terraform – TypeScript

          Pulumi – TypeScript

          Diagram – an embedded version of draw.io


TaskCat
A tool that automates the testing of CloudFormation templates https://github.com/aws-quickstart/taskcat

      Deploys your template in multiple AWS Regions simultaneously

      Generates a report with a pass/fail result for each Region

      You provide

            AWS Regions and the number of AZs you want to include in the test

            Template parameters’ values



cfn-nag
A tool that looks for patterns in CloudFormation templates that may indicate insecure infrastructure https://github.com/stelligent/cfn_nag

Examples:

      IAM rule and Security Group rules that are too permissive (wildcards)

      Access logs and Encryption that aren’t enabled

      Password literals



CloudFormation cheatsheet
Summarizes the usage of !Ref and !GetAtt with CloudFormation resources https://theburningmonk.com/cloudformation-ref-and-getatt-cheatsheet/



aws-cfn-template-flip
A tool that converts CloudFormation templates between JSON and YAML formats https://github.com/awslabs/aws-cfn-template-flip



cfn-diagram
A tool to visualize CloudFormation/SAM/CDK templates as diagrams https://github.com/mhlabs/cfn-diagram

Generates https://draw.io and HTML diagrams

Select only the resources you want (filter by resource type/name)

Different layouts

Supports JSON and YAML



cfn-format
A tool that reads a CloudFormation template and outputs a cleanly-formatted copy adhering to CloudFormation standards https://github.com/awslabs/aws-cloudformation-template-formatter



awesome-cloudformation
Reference list for open-source projects related to CloudFormation: https://github.com/aws-cloudformation/awesome-cloudformation








------------------------------------------------------------- Template Validation




-- You can validate your CloudFormation template to catch syntax and semantic errors, before CloudFormation creates any resources

      - CloudFormation Console automatically validates the template after you specify input parameters

      - AWS CLI CloudFormation validate-template command


-- cfn-lint: https://github.com/aws-cloudformation/cfn-lint

      - Validate CloudFormation templates JSON/YAML against resource specification (properties and their values)


-- cfn-guard: https://github.com/aws-cloudformation/cloudformation-guard

      - Validate CloudFormation templates for compliance to organization policy guidelines

      - You define your own rules

      - Example: ensure users always create encrypted S3 buckets
 
      - Can be used as part of CI/CD pipeline









------------------------------------------------------------- Template Snippets & Samples



https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html


https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-sample-templates.html


https://github.com/awslabs/aws-cloudformation-templates



https://github.com/awslabs