

IMP NOTE : ALL the Topics have covered in the Solution architect Associate , here I have prepared the Most advanced topics for Associate Developer which was not covered in the solution architect associate





As per the recent update from the Amazon , they are going to shutdown some services , like 


1 Cloudsearch
2 Cloud9
3 Simple DB
4 forecast
5 S3 Select
6 DataPipeline
7 CodeCommit







=========================================================== CloudFront =========================================


-------------------- CloudFront Caching 

• The cache lives at each CloudFront Edge Location

• CloudFront identifies each object in the cache using the Cache Key 

• You want to maximize the Cache Hit ratio to minimize requests to the origin

• You can invalidate part of the cache using the CreateInvalidation API


-------------------- What is CloudFront Cache Key?

• A unique identifier for every object in the cache

• By default, consists of hostname + resource portion of the URL

• If you have an application that serves up content that varies based on user, device, language, location...

• You can add other elements (HTTP headers, cookies, query strings) to the Cache Key using "CloudFront Cache Policies"


-------------------- CloudFront Policies – Cache Policy

• Cache based on:

    • HTTP Headers: None – Whitelist

    • Cookies: None – Whitelist – Include All-Except – All

    • Query Strings: None – Whitelist – Include All-Except – All


• Control the TTL (0 seconds to 1 year), can be set by the origin using the Cache-Control header, Expires header...

• Create your own policy or use Predefined Managed Policies

IMP : • All HTTP headers, cookies, and query strings that you include in the Cache Key are automatically included and forward to ur  origin requests



------------------ 1 CloudFront Caching – Cache Policy HTTP Headers

- let's say we have an example request in french language


GET /blogs/myblog.html HTTP/1.1
Host: mywebsite.com
User-Agent: Mozilla/5.0 (Mac OS X 10_15_2....) 
Date: Tue, 28 Jan 2021 17:01:57 GMT 
Authorization: SAPISIDHASH fdd00ecee39fe.... Keep-Alive: 300
Language: fr-fr


-- so if we define none : 

• None:
   
    • Don’t include any headers in the Cache Key (except default)
    • Headers are not forwarded (except default)
    • Best caching performance

• Whitelist: 

    • only specified headers included in the Cache Key
      
       -     if u want to whitelist specific Headers and that may mean necessary because while you want to have the language as a Cache Key, then you specify which headers you want to include in the Cache Key,

    • Specified headers are also forwarded to Origin

       - so that the origin can actually respond to the request and give you the blog in the correct language.




---------------------------- 2 CloudFront Cache – Cache Policy Query Strings


- for example we have a request like 

 GET /image/cat.jpg?border=red&size=large HTTP/1.1


-- So query strings are what happens in the URL after a question mark. 

-- So for example, border equals red and size equals large. So here, we want a cat image. But apparently, it's going to be customized a little bit by the origin.

-- if you have 

• None : 

    • Don’t include any query strings in the Cache Key

    • Query strings are not forwarded

• Whitelist

    • Only specified query strings included in the Cache Key

    • Only specified query strings are forwarded


• Include All-Except : you specify which ones you don't want but the rest passes

    • Include all query strings in the Cache Key except the specified list

    • All query strings are forwarded except the specified list


• All

    • Include all query strings in the Cache Key

    • All query strings are forwarded

    • Worst caching performance




------------------------------ CloudFront Policies – Origin Request Policy 


• Specify values that you want to include in origin requests without including them in the Cache Key (no duplicated cached content)

• You can include extra :
      
      • HTTP headers: None – Whitelist – All viewer headers options
      • Cookies: None – Whitelist – All
      • Query Strings: None – Whitelist – All

    - but they will be forwarded to the origin but they're not going to be used in the Cache Key.


• Ability to add CloudFront HTTP headers and Custom Headers to an origin request that were not included in the viewer request (eg :  if you wanted to pass an API key or a secret header.)

• Create your own policy or use Predefined Managed Policies



----------------------------- Cache Policy vs. Origin Request Policy


GET /blogs/myblog.html HTTP/1.1
Host: mywebsite.com
User-Agent: Mozilla/5.0 (Mac OS X 10_15_2....) 
Date: Tue, 28 Jan 2021 17:01:57 GMT 
Authorization: SAPISIDHASH fdd00ecee39fe.... Keep-Alive: 300
Language: fr-fr


--  so the request will come with some query strings, some cookies, some headers, and then we will cache based on the cache policy.

-- For example, we want to cache here on the host name, the resource, and a header called authorization.



-- But then, your origin may need more than these three things to actually work and to actually serve properly the request.

-- So you may want to add the user agents,the session ID and the ref query string as part of your request to the origin.

-- So in this case, the request to the origin is going to be enhanced but then the caching will not happen based on what we forwarded to the origin request policy.

-- It's only going to be based on the cache policy.







Client ------------------------------(request)----------------------------> C.F --------------------------(forward)--------------------------------> Origin (EC2 instance)
                                                                             |

                   Cache Policy                                           EDGE Location                   Origin Request Policy (whitelist)

                Cache Key (cache based on)                                  |                                  Type                        Value
                                                                            |                               HTTP Headers         User-Agent, Authorization
                 - mywebsite.com 
                 - /content/stories/example-story.html                     CACHE                            Cookies               session_id
                 - Header: Authorization                                                                    Query Strings          ref


    


----------------------------------- CloudFront – Cache Invalidations


• In case you update the back-end origin, CloudFront doesn’t know about it and will only get the refreshed content after the TTL has expired

• However, you can force an entire or partial cache refresh (thus bypassing the TTL) by performing a CloudFront Invalidation

• You can invalidate all files (*) or a special path (/images/*)


------------------------------------ CloudFront – Cache Behaviors


• Configure different settings for a given URL path pattern

• Example: one specific cache behavior to images/*.jpg files on your origin web server

• Route to different kind of origins/origin groups based on the content type or path pattern
  
      • /images/*
      • /api/*  --  EG : Load balancer 
      • /* (default cache behavior) EG : S3 


• When adding additional Cache Behaviors, the Default Cache Behavior is always the last to be processed and is always /*



------ use case for cache behavior  EG : CloudFront – Cache Behaviors – Sign In Page


                                                                

                                                                        cache behaviours                                                 origins 
           Signed Cookies
       <------------------------->                                    
Users                                           <--------------------------->  /* (default)      -------------------->                      S3
                                          CF Distribution                         
                                                <--------------------------->  /login             ------------------->                EC2 instance (generated signed cookies)
       <------------------------->                                                                <--------------------              
            authenticate                                                                    Signed Cookies              



-- So the way we do it is that we define a cache behavior ,for /login and so the users who hit the /login page will be redirected to our EC2 instance.

-- And the role of our EC2 instance is to generate CloudFront signed cookies.

-- So these signed cookies are sent back to the user and the user will then use the signed cookies to be able to access our default cache behavior, which is any other URL, then /login and then access our S3 bucket files.

-- if the users are trying to access the default cache behavior without doing a login first, what we can do is that we can set up the cache behavior to only accept the request if signed cookies are present.

-- Therefore, we can redirect to the /login page and we're good to go.



---------------------------------------- CloudFront – Maximize cache hits by separating static and dynamic distributions



Static requests --------------> CDN Layer CloudFront (No headers / session caching rules Required for maximizing cache hits) ------(Static content)-----------> S3

Dynamic         --------------> CDN Layer CloudFront (Cache based on correct headers and cookie)------------------(Dynamic Content (REST, HTTP server))----------------> ALB + EC2 




---------------------------------------- CloudFront – ALB or EC2 as an origin





-- it's possible for CloudFront to access any custom HTTP backend.

-- So this includes as well an EC2 instance or an application load balancer.

-- So let's say we've developed an HTTP backend on top of an EC2 instance and so we want our users to access it through CloudFront.

-- How do we do?

-- we have 2 Patterens :


1 Ec2 as origin 

      - Well, they will access the edge locations of CloudFront and these edge locations will be making requests into our EC2 instances.

      - And therefore the EC2 instances must be public, otherwise the edge locations will not be able to access our EC2 instances because there's no private VPC connectivity in CloudFront.

      - So therefore, we must also have a security group that allows the list of all the public IP of the edge locations of CloudFront to make sure the security is compatible and works.

      - you can find a list of these CloudFront IPs in this URL right here.

                  http://d7uri8nf7uskq.cloudfront.net/tools/list-cloudfront-ips                



                                               Allow Public IP of Edge Locations                    
        users <-----------> Edge Location <------------------------------------------> EC2 Instances Must be Public


2 ALB as an Origin:

       - it must be public

       - and then the backend EC2 instances can be private because there is private VPC connection between the application load balancer and our EC2 instances.

       - We just need to make sure that the EC2 instances security group allows the security group of the load balancer.

       - So therefore, the users will be accessing the edge locations and then the public IPs of the edge locations must be allowed in the security group of the ALB to make sure connectivity can be established.

       
                                              Allow Public IP of Edge Locations                     Allow Security Group of Load Balancer

        users <-----------> Edge Location  <---------------------------------------------> ALB   <-------------------------------------------> EC2 Instances Can be Private

                Edge Location Public IPs                                  Application Load Balancer Must be Public







--------------------------------------- CloudFront Signed URL / Signed Cookies


• You want to distribute paid shared content to premium users over the world

• We can use CloudFront Signed URL / Cookie. We attach a policy with:

      • Includes URL expiration
      • Includes IP ranges to access the data from
      • Trusted signers (which AWS accounts can create signed URLs)

• How long should the URL be valid for?

     • Shared content (movie, music): make it short (a few minutes)
     • Private content (private to the user): you can make it last for years


• Signed URL = access to individual files (one signed URL per file)

• Signed Cookies = access to multiple files (one signed cookie for many files)



------------------------------------------ CloudFront Signed URL vs S3 Pre-Signed URL


          • CloudFront Signed URL:                                                                                                • S3 Pre-Signed URL:


• Allow access to a path, no matter the origin  (not only s3 , but HTTP, backend ....)                           • Issue a request as the person who pre-signed the URL

• Account wide key-pair, only the root can manage it                                                             • Uses the IAM key of the signing IAM principal

• Can filter by IP, path, date, expiration                                                                       • Limited lifetime

• Can leverage caching features



------------------------------------------ CloudFront Signed URL Process


• Two types of signers:

    1 • Either a trusted key group (recommended)
        
        • Can leverage APIs to create and rotate keys (and IAM for API security)


    2 • An AWS Account that contains a CloudFront Key Pair

        • Need to manage keys using the root account and the AWS console
        • Not recommended because you shouldn’t use the root account for this



• In your CloudFront distribution, create one or more trusted key groups

• You generate your own public / private key
       
       • The private key is used by your applications (e.g. EC2) to sign URLs
       • The public key (uploaded) is used by CloudFront to verify URLs




---------------------------------------------------------------- LAB Demo 

------------ Type 1 


-- open S3 and create private Bucket 

-- as u create private bucket , no one can access ur url through the S3 

-- Only access through by Cloud-Front directly coz, it is privtae bucket and we did not enable Static hosting also 

-- go to CF in console 

-- create ditrubtion on CF 

-- Origina Domain = load balancer / S3 -- these are the places wher u can host ur applications 

--  OAC--> Create control settings --> do not change any n create 

-- it is created access from S3 

-- Compress objects automatically : CloudFront can automatically compress certain files that it receives from the origin before delivering them to the viewer. CloudFront compresses files only when the viewer supports it, as specified in the Accept-Encoding header in the viewer request.


-- Default root object - optional = index.html  -----> must and should u have to give this , otherwise u won’t get o/p 


----- once u create distrubtion , the S3 bucket policy wil gwt generated copy that policy and paste in bucket policy 

-- now ur appn is getting deployed all over the world 

-- through the CF url customers will able to connect ur webiste through the edge locations 

-- once u change the content of ur website and do uploud again n if u do refresh u won't get new content 

-- u have too do "invalidate the Cache" 

-- go to CF and create invalidation for /index.html or /* , do upload again the file to s3 ,it will get latest file from the S3 and give latest content to customers 

-- By default, CloudFront caches files in edge locations for 24 hours. 


------------ Sign URL System 

--  now i want to make this private , and want to give access to only prime customers 

--  so do create Signed URL'S

-- for this first u have to create public key and do store this key in the Key group 

-- go to google and search for "RSA Key Generator" create 2048 bit key , copy public key and do paste in public key , create public key

-- now create key group and add this public key to this group , u can store upto 5 pulic keys in one group 

-- now go to distribution --> behaviour --> edit --> Restrict viewer access = yes --> save changes 

-- go to cli and enter the command 

-- store private key in the local 

 aws cloudfront sign --url https://d2wvy12e9tvxs2.cloudfront.net/ec2.py --key-pair-id K3T4KL9S53KCPW --private-key file://demo-private-key.pem --date-less-than 2024-07-26
   


-- change with your values 


-- it will get create one URL

         - remove % in the link if u get , the paste remaining url in browser and try

-- if u are getting from server then try 

          curl "url"











TYPE 2 : it is also another way but not recommend 

-- go to security credentials --> create cloudfront key pair --> download both public and private files 

-- now open terminals and enter the command like this 

      aws cloudfront sign --url https://dgw7w0gfg0nkb.cloudfront.net --key-pair-id APKAUK2QS6DOWVAB3OG5 --private-key file://pk-APKAUK2QS6DOWVAB3OG5.pem --date-less-than 2024-03-31


      -- it will generate one url 

      EG :    https://dgw7w0gfg0nkb.cloudfront.net?Expires=1711843200&Signature=Pa7-jzCpDyXKgwS6bqPh75zqiNnCHryUCWDgcQheZLwX7g0wyzLrBSiUmD2KNFtdnx-OKnYLO2zJSiLsORIQO1yDs5RBTCqW6y5BTGqE0-CUdQ5clls4LY4KKdwZWmRs2VyJtMMDNqiwjsID2nTHO8nRUkgWBB0Nx9FShrhsmMoqVYo2JnDmIWnLb8KE4r8vSxbKPmMKByRkqmUmHPSbR6ODct0njHdDbcDJuANZLh3NKXVPvfYNMGre1ipjwfPhz7neEbcZoMq3AYuXce83DzSQd2BN~P8lPKDyDOWy8C3kAHoUKUg~2tneTa9~Ksh2hFHHyOnIthSysoFKmsW5ug__&Key-Pair-Id=APKAUK2QS6DOWVAB3OG5%                                    





--------------------------------------- CloudFront – Multiple Origin

• To route to different kind of origins based on the content type

• Based on path pattern:

      • /images/*
      • /api/* 
      • /*



-------------------------------------- CloudFront – Origin Groups

• To increase high-availability and do failover

• Origin Group: one primary and one secondary origin

• If the primary origin fails, the second one is used





-------------------------------------- CloudFront – Field Level Encryption


• Protect user sensitive information through application stack

• Adds an additional layer of security along with HTTPS

• Sensitive information encrypted at the edge close to user

• Uses asymmetric encryption

• Usage: 

     • Specify set of fields in POST requests that you want to be encrypted (up to 10 fields)
     • Specify the public key to encrypt them


                                      
                                                Encrypt using Public Key

Client -----------(HTTPS)----------------------> EDGE Location ------------(HTTPS)--------------------> C.F -------------(HTTPS)-----------> ALB --------(HTTPS)------------------> WEB Servers

         POST /submit HTTP/1.1 
         Host: www.example.com                                             POST /submit HTTP/1.1                                                                               Decrypt using Private Key
                                                                           Host: www.example.com 



EG : 


LAB : https://325b057e.isolation.zscaler.com/profile/1233fc6e-6618-4022-a03b-96afce7da312/zia-session/?controls_id=5363df57-072a-41ec-8342-91ef13f84e51&region=bom&tenant=462f064b6b51&user=a88c06b13d4badbc3b1628a94e955f23c843810a6a0110d09b7d0213d4fe17aa&original_url=https%3A%2F%2Faws.amazon.com%2Fblogs%2Fsecurity%2Fhow-to-enhance-the-security-of-sensitive-customer-data-by-using-amazon-cloudfront-field-level-encryption%2F&key=sh-1&hmac=dfa6e28bf1bc65f977e7e1b8fb8cd99b505583c03072c2c6e52c6e286b86f799







------------------------------------------ CloudFront – Price Classes


• You can reduce the number of edge locations for cost reduction

• Three price classes:

       1. Price Class All: all regions – best performance

       2. Price Class 200: most regions, but excludes the most expensive regions

       3. Price Class 100: only the least expensive regions





------------------------------------------ CloudFront – Real Time Logs


• Get real-time requests received by CloudFront sent to Kinesis Data Streams

• Monitor, analyze, and take actions based on content delivery performance

• Allows you to choose:
  
      • Sampling Rate – percentage of requests for which you want to receive

      • Specific fields and specific Cache Behaviors (path patterns)



Real-time Processing  =          Users -----(Requests)--------> C.F -----(LOGS)-------> Kinesis Data Streams ------(records)--------------> Lambda


Near Real-time Processing  =       Users -----(Requests)--------> C.F -----(LOGS)-------> Kinesis Data Streams ------(records)--------------> Kinesis Data Firehose






===================================================== Containers on AWS ========================================

• Docker is a software development platform to deploy apps

• Apps are packaged in containers that can be run on any OS

• Apps run the same, regardless of where they’re run

  • Any machine
  • No compatibility issues
  • Predictable behavior
  • Less work
  • Easier to maintain and deploy
  • Works with any language, any OS, any technology


• Use cases: microservices architecture, lift-and-shift apps from on- premises to the AWS cloud, ...



---------------------------- Where are Docker images stored?

• Docker images are stored in Docker Repositories

• Docker Hub (https://hub.docker.com)

    • Public repository
    • Find base images for many technologies or OS (e.g., Ubuntu, MySQL, ...)


• Amazon ECR (Amazon Elastic Container Registry)

    • Private repository
    • Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)


-- Jfrog also we can store images





-------------------------- Docker vs.Virtual Machines

• Docker is ”sort of ” a virtualization technology, but not exactly

• Resources are shared with the host => many containers on one server

-- see pics in google for better understanding





-------------------------------- Getting Started with Docker



Dockerfile -------(Build)----------------> Docker Image ------------(Run)------------> Docker Container (Eg :python)
                                            |      |
                                            |      |
                                Push        |      |    Pull
                                            |      |
                                            |      |
              
                                        Docker Repositories

                                        Eg : DockerHub , ECR



-- Dockerfile  : which is defining how your Docker container will look. So we have a base Docker image , and we add some files and then we're going to build it.

-- DockerImage : And this will become a Docker image. And that Docker image, you can store it on a Docker repository , it's called a Push and you push it to either Docker hub which is a public repository, or Amazon is ECR
   
                 - Then you can pull back these images from these repositories and then you would run them.


-- Docker Container : And when you run a Docker image , it becomes a Docker container, which runs your code  you had built from your Docker build.


--- That is the whole process with Docker.





---------------------------------------------- Docker Containers Management on AWS

• Amazon Elastic Container Service (Amazon ECS)
     
     • Amazon’s own container platform


• Amazon Elastic Kubernetes Service (Amazon EKS)

     • Amazon’s managed Kubernetes (open source)


• AWS Fargate
  
     • Amazon’s own Serverless container platform
     • Works with ECS and with EKS


• Amazon ECR:
  
     • Store container images





--------------------------------------------------- 1 Amazon ECS - EC2 Launch Type


• ECS = Elastic Container Service

• Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters 
    
        - an ECS Cluster is made of things.And with the EC2 Launch Type, well these things are EC2 instances.

        - group of servers is called "clusters"


• EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances)

• Each EC2 Instance must run the ECS Agent to register in the ECS Cluster

• AWS takes care of starting / stopping containers

     - AWS is going to be starting or stopping the containers.
     - That means that whenever we have a new Docker container it's going to be placed accordingly on each EC2 Instance.
     





----------------------------------------------------  2 Amazon ECS – Fargate LaunchType


• Launch Docker containers on AWS

• You do not provision the infrastructure (no EC2 instances to manage)

• It’s all Serverless!, because we don't manage servers (there are servers behind., but we are not managing the servers)

• if we have an ECS Cluster , we just create task definition to define our ECS tasks.

• AWS just runs ECSTasks for you based on the CPU / RAM you need

   - So when we want to run a new Docker container, simple as that, it's going to be run, without us knowing where it's run and without an EC2 Instance to be created in the backend in our accounts for it to work.
   - So it's a little bit magic.


• To scale, just increase the number of tasks. Simple - no more EC2 instances





------------------------------------------------------- Amazon ECS – IAM Roles for ECS


 -- So let's take an example of the EC2 Launch Type in which we have an EC2 Instance running the ECS Agent on Docker.

 -- So in this case, we can create an EC2 Instance Profile which is only valued of course if you use EC2 Launch Type.

 1  EC2 Instance Profile (EC2 Launch Type only):

       • Used by the ECS agent
       • Makes API calls to ECS service
       • Send container logs to CloudWatch Logs
       • Pull Docker image from ECR
       • Reference sensitive data in Secrets Manager or SSM Parameter Store



 2 ECSTask Role:

-- our ECS tasks are going to get ECS Task Roles. And so this is valued for both EC2 Launch Type and Fargate.

-- And so here I have two tasks.And we can create a specific role per task.

           TASK A -----------(EC2 Task A Role)-------> s3 

           TASK B -----------(EC2 Task B Role)-------> DynamoDB

-- Well, why do we have different roles? 

ANS : Because each role allows you to be linked to different ECS services.

1  so, for example, the ECS Task A Role allows you to have your Task A, runs some API calls against Amazon S3

2  so, for example, the ECS Task B Role allows you to have your Task B, runs some API calls against Dynamodb


NOTE : you define the Task Role in the task definition of your ECS service.



• Allows each task to have a specific role

• Use different roles for the different ECS Services you run

• Task Role is defined in the task definition 

     -- task definition : The Task definitions view lists each task definition family you've created.

     -- You can perform the following actions:
        
         - Deploy the task definision as a service or a task.
         - Create a new revision






---------------------------------------------- Amazon ECS – Load Balancer Integrations


1  Application Load Balancer 

    -- supported and works for most use cases

    -- multiple ECS Tasks running. It's all in the ECS Cluster.

    -- And we want to expose these tasks as a HTP or HTTPS endpoint.

    -- Therefore we can run an Application Load Balancer in front of it and then our users will be going to the ALB and in the back end to the ECS tasks directly.

    -- the ALB is supported and will support most use cases, and that's a good choice.


2   Network Load Balancer

    -- recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link


3   Classic Load Balancer supported but not recommended (no advanced features – no Fargate)

    - you cannot link your Elastic Load Balancer to Fargate.






----------------------------------------------------- Amazon ECS – Data Volumes (EFS)


• Mount EFS file systems onto ECS tasks

   - So say you have an ECS cluster and in this case are represented both the EC2 Instance , as well as the Fargate Launch Type for my ECS Cluster.

   - And we want to mount a file system onto the ECS task to share some data. In that case, we use an Amazon EFS file system,


• Works for both EC2 and Fargate launch types

   - because it's a network file system is going to be compatible with both EC2 and the Fargate launch types. And it allows us to mount the file system directly onto our ECS tasks.


• Tasks running in any AZ will share the same data in the EFS file system

• Fargate + EFS = Serverless


• Use cases: persistent multi-AZ shared storage for your containers


IMP NOTE : 

    • Amazon S3 cannot be mounted as a file system ( S3 isn't a file system, it is object storage. )



--------------------------------------------------------- Capacity providers ECS 

-- The capacity provider view provides details about your Fargate and EC2 capacity providers.

-- For Amazon ECS on AWS Fargate, the Capacity Provider is FARGATE and a FARGATE_SPOT and the Type is FargateProvider. When you select AWS Fargate, these providers are added automatically. You cannot update or delete them.

-- For Amazon ECS on Amazon EC2, the Capacity Provider is the Auto Scaling group name and the Type is ASGProvider.

-- For Amazon ECS on Amazon EC2, you can create, update, and delete the capacity provider.

--  When you delete the capacity provide the capacity provider association is removed. You must go the Amazon EC2 console to delete the Auto Scaling group.






---------------------------------------------- LAB for ECS 

-- open ECS , give name for ur cluster

--  do check Fargate and ec2 instances 

-- now it create new ASG automatically for u , go with amazon linux 2 (os) , t2.micro (instance type) 

-- if u want to deploy these in ur vpc then select ur vpc , otherwise go with default vpc 

-- remaining all are default 

-- once the cluster is get created , automatically one ASG get created for u 

-- FARGATE / FARGATE_SPOT / Our ASG  = these are capacity providers 

-- go to democluster --> infrastructure 

        - we can launch the ec2 instances directly in the cluster through an ASG 

-- if u change desired capacity is 1 , it will create 1 ec2 for u 

-- democluster --> infrastructure --> the created instance is show in Container instances , these instance can be create by FARGATE / FARGATE_SPOT / Our ASG 



---- create an ECS service 

-- u need to create task definition, create new task definition

-- create TD with the name "nginxdemos-hello"

     - nginxdemos-hello : this is from the official dockerhub page from internet ,we are using this in this demo

-- Infrastructure requirements = AWS Fargate 

     - u can also do select ec2 , but here i want to be serverless

-- OS, Architecture, Network mode = Linux/x86_64

-- Task size = CPU = .5 vcpu and memory = 1 GB

-- IN Container – 1 

     - Name = nginxdemos-hello   and Image URI = nginxdemos/hello : this will directly pull image from the dockerhub repo

-- keep remaining are default

-- create TD with this configuration , 

-- let's launch this task definition as a service.

-- democluster --> create service 

-- Compute options = Launch type 

-- Launch type = Fargate

-- Application type = Service : Launch a group of tasks handling a long-running computing work that can be stopped and restarted. For example, a web application

             - Task : Launch a standalone task that runs and terminates. For example, a batch job.

-- for this demo choose service

-- choose family and revision

-- Service name = same as family name

-- create one new sg in Network field , allow HTTP from anywhere and public ip turned On

-- and also create NEW ALB for this demo in load balcer section

-- create service , wait for few minutes to create 

-- once the service is get created do observe the service 

-- the service is linked to the target group and this target group is present infront of LB , 

-- in the target , u can see ip address , this is ip of ur container 

-- now go to load balancer and copy the DNS , paste in browser , u will get nginx welcome page  , /hello with dns in browser , ---> get o/p as /hello 

-- now here we have one service , u can also launch some more 

-- cluster --> service --> democluster --> update service

-- put desired = 3   1 per AZ , now we have two more tasks being provisioned and they are provisioned on the Fargate engine.

-- So that means that behind the scenes, AWS is going to provision automatically the resource that it needs to launch these tasks.

-- now do refresh in the browser , it will refresh for 3 services , ALB is distributing the load equally

-- to avoid charges , make sure our desired capacity = 0 in services and ASG both 






-------------------------------------------------- ECS Service Auto Scaling


• Automatically increase/decrease the desired number of ECS tasks

• Amazon ECS Auto Scaling uses "AWS Application Auto Scaling" , we have three metrics we can scale on using the service.

    • ECS Service Average CPU Utilization
    • ECS Service Average Memory Utilization-Scale on RAM
    • ALB Request Count Per Target–metric coming from the ALB


• Target Tracking – scale based on target value for a specific CloudWatch metric

• Step Scaling – scale based on a specified CloudWatch Alarm

• Scheduled Scaling – scale based on a specified date/time (predictable changes)

• ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level)

    - IMP NOTE : Remember, that scaling your service, your ECS Service, at the task level is not equal to scaling your cluster of EC2 instances if you are in the EC2 launch type.

• Fargate Auto Scaling is much easier to setup (because Serverless)



------------------------------------------------ EC2 Launch Type – Auto Scaling EC2 Instances

• Accommodate ECS Service Scaling by adding underlying EC2 Instances

-- We have 2 types


1 • Auto Scaling Group Scaling

    • Scale your ASG based on CPU Utilization 
    • Add EC2 instances over time


2 • ECS Cluster Capacity Provider
 
     • Used to automatically provision and scale the infrastructure for your ECSTasks
     • Capacity Provider paired with an Auto Scaling Group
     • Add EC2 Instances when you’re missing capacity (CPU, RAM...)



IMP NOTE : So if you have to choose between Auto Scaling Group Scaling and ECS Cluster Capacity Provider, please use ECS Cluster Capacity Provider for your EC2 launch type.


USer -------------> CloudWatch Metric (ECS Service CPU Usage) ---------------(Trigger)-------------->CloudWatch Alarm -----------(scale)-------> ASG / Capacity providers





------------------------------------------------ ECS Rolling Updates


• When updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order

-- you will have two settings, the minimum healthy percent and the maximum percent.

-- So by default they're 100 and 200

-- EG : So your ECS service, for example, this one is running nine tasks represents an actual running capacity of 100%. 
    
         - And then if you set a minimum healthy percent of less than 100, this is going to say, "Hey you're allowed to terminate all the tasks on the right hand side, as long as we have enough tasks to have a percentage over the minimum healthy percent."

         - And in the maximum percent shows you how many new tasks you can create of the version two, to basically roll updates your service.

         - So this is how these two settings would impact your updates. 

         - so you will go ahead, create new tasks, then terminate all tasks and so on. All to make sure that all your tasks are going to be terminated and then updated to a newer version.


-- lets discuss 2 scenarios


------------------------------------------- 1 ECS Rolling Update – Min 50%, Max 100% 


 • Starting number of tasks: 4

   - In this case, we're going to lose four tasks to be terminated , so that we're running at 50% capacity.

   - Then two new tasks are going to be created, Now we're back at 100% capacity.

   - Then two old tasks are going to be terminated, we're back at 50% capacity.And two new tasks are going to be created, we're back at 100 capacity.

   - we have done a rolling updates.

   - In this case, we have been terminating tasks because we set the minimum to 50% and the maximum to 100%.



------------------------------------------- 2 ECS Rolling Update – Min 100%, Max 150%


• Starting number of tasks: 4

   - We cannot terminate a task because the minimum is 100%.

   - Therefore we can go into create two new tasks and this will bring our capacity to 150%. (total 6)

   - Then because we are above the minimum 100% we can terminate two old task and we're back at 100%.

   - Then we will create two new tasks and finally, terminates two old tasks. And this will have performed our rolling updates for our ECS service.




----------------------------------------- Amazon ECS – Task Definitions

• Task definitions are metadata in JSON form to tell ECS how to run a Docker container

• It contains crucial information, such as:
    
    • Image Name
    • Port Binding for Container and Host • Memory and CPU required
    • Environment variables
    • Networking information
    • IAM Role
    • Logging configuration (ex CloudWatch)

• Can define up to 10 containers in a Task Definition


-- Amazon ECS allows you to run and maintain a specified number of instances of a task definition simultaneously in an Amazon ECS cluster. This is called a service.



-------------------------------------- Amazon ECS – Load Balancing (EC2 Launch Type)


• We get a Dynamic Host Port Mapping if you define only the container port in the task definition

    - So if you have load balancing and you're using the EC2 launch type, then you're going to get what's called a Dynamic Host Port Mapping. If you define only the container port and the task definition.

Explanation : 

    - So we are running for example, an ECS task, and all of them have the container port set to 80 but the host port set to zero,meaning not set.

    - the host port is going to be random, is going to be dynamic.

    - so, each ECS task from within the EC2 instance, is going to be accessible from a different port on the host,the EC2 instance.

    - therefore, if you define an application load balancer , then you may say, well, it is difficult for the ALB to connect to the ECS task because the port is changing.

    - But the ALB when linked to an ECS service knows how to find the right port, thanks to the Dynamic Host Port Mapping feature.

NOTE : but it does not work with a classic load balancer because it is older generation.


• You must allow on the EC2 instance’s Security Group any port from the ALB’s Security Group



Question : You have a containerized application stored as Docker images in an ECR repository, that you want to run on an ECS cluster. You're trying to launch two copies of the same Docker container on the same EC2 container instance. The first container successfully starts, but the second container doesn't. You have checked that there's enough CPU and RAM on the EC2 container instance. What is the problem here?

ANS : The host port defined in the task definition






-------------------------------------- Amazon ECS – Load Balancing (Fargate)


• Each task has a unique private IP

• Only define the container port (host port is not applicable)

   -  because this is Fargate, there is no host


-- for example, with four tasks each task is going to get its own private IP through an Elastic Network Interface or ENI. And then each ENI is going to get the same container ports.

-- And therefore, when you have an ALB, then to connect to the Fargate task, it's just going to connect to all all of them on the same port on port 80.



• Example

   • ECS ENI Security Group

         • Allow port 80 from the ALB


   • ALB Security Group

         • Allow port 80/443 from web





------------------------------------------------------ Amazon ECS One IAM Role per Task Definition


-- you should know that IAM roles are assigned per task definition.

-- you have a task definition and then you assign an ECS task role. And this will allow you, for example, for your ECS tasks out of your task definition, to access the Amazon S3 service.

-- And therefore when you create an ECS service from this task definition then each ECS task automatically is going to assume and inherit this ECS task role.

NOTE :  you should know that the role is defined at the task definition level, not at this service level.so, therefore all the tasks within your service, are going to get access to Amazon S3.





----------------------------------------------------- Amazon ECS – Environment Variables


• Environment Variable

    • Hardcoded – e.g., URLs
    • SSM Parameter Store – sensitive variables (e.g., API keys, shared configs)
    • Secrets Manager – sensitive variables (e.g., DB passwords)


• Environment Files (bulk) – Amazon S3



---------------------------------------------------- Amazon ECS – Data Volumes (Bind Mounts)


Bind Mount :

-- With bind mounts, a file or directory on a host, such as an Amazon EC2 instance, is mounted into a container. Bind mounts are supported for tasks that are hosted on both Fargate and Amazon EC2 instances.

-- Bind mounts are tied to the lifecycle of the container that uses them. After all of the containers that use a bind mount are stopped, such as when a task is stopped, the data is removed.

-- For tasks that are hosted on Amazon EC2 instances, the data can be tied to the lifecycle of the host Amazon EC2 instance by specifying a host and optional sourcePath value in your task definition. 

The following are common use cases for bind mounts.

  - To provide an empty data volume to mount in one or more containers.
  
  - To mount a host data volume in one or more containers.
  
  - To share a data volume from a source container with other containers in the same task.

  - To expose a path and its contents from a Dockerfile to one or more containers.




• Share data between multiple containers in the same Task Definition

• Works for both EC2 and Fargate tasks

• EC2 Tasks – using EC2 instance storage

     • Data are tied to the lifecycle of the EC2 instance

• Fargate Tasks – using ephemeral storage
  
     • Data are tied to the container(s) using them
     • 20 GiB – 200 GiB (default 20 GiB)


• Use cases:

   • Share ephemeral data between multiple containers

   • “Sidecar”container pattern, where the “sidecar” container used to send metrics/logs to other destinations (separation of conerns)







----------------------------------------------------  Amazon ECS – Task Placement


• When an ECS task is started with EC2 Launch Type, ECS must determine where to place it, with the constraints of CPU and memory (RAM) and available port

• Similarly, when a service scales in, ECS needs to determine which task to terminate

• You can define:
  
    • Task Placement Strategy
    • Task Placement Constraints


• Note: only for ECS Tasks with EC2 LaunchType (Fargate not supported)



----------------------------------------------------  Amazon ECS – Task Placement Process


• Task Placement Strategies are a best effort


• When Amazon ECS places a task, it uses the following process to select the appropriate EC2 Container instance:

   1. Identify which instances that satisfy the CPU, memory, and port requirements

   2. Identify which instances that satisfy the Task Placement Constraints

   3. Identify which instances that satisfy the Task Placement Strategies

   4. Select the instances for task placement




---------------------------------------------------- Amazon ECS –Task Placement Strategies


• Binpack

    • Tasks are placed on the least available amount of CPU or Memory

    • Minimizes the number of EC2 instances in use (cost savings)


JSON :


"placementStrategy": [
    {
        "field": "memory",
        "type": "binpack"
    }
]



• Random

    • Tasks are placed randomly


JSON : 



"placementStrategy": [
    {
       
        "type": "random"
    }
]



• Spread


  • Tasks are placed evenly based on the specified value

  • Example: instanceId, attribute:ecs.availability-zone, ...


  JSON :

  "placementStrategy": [
    {
       
        "type": "spread",
        "field": "attribute:ecs.availability-zone"
    }
]



-------------------- You can mix them together


1 we can have a spread on availability zone and then a spread on instance ID

   
EG :

 "placementStrategy": [
    {

         "field": "attribute:ecs.availability-zone"
         "type": "spread",
       
    },

    {
          "field": "instanceid"
         "type": "spread",
    }
]




2  we can have a spread on availability zone and then a binpack on memory.

    
EG :

"placementStrategy": [
    {

         "field": "attribute:ecs.availability-zone"
         "type": "spread",
       
    },

    {
          "field": "memory"
         "type": "binpack",
    }
]






---------------------------------------------------- Amazon ECS –Task Placement Constraints


1  distinctInstance

     • Tasks are placed on a different EC2 instance

     - So you will never have two tasks on the same instance.

    
 "placementStrategy": [
    {
       
        "type": " distinctInstance"
        
    }
]


2   memberOf

    • Tasks are placed on EC2 instances that satisfy a specified expression

    • Uses the Cluster Query Language (advanced)

EG: 1 

    "placementStrategy": [
    {
       
        "type": "memberOf"
        "expression": "attribute:ecs.availability-zone in [ap-south-1,ap-south-2]"
        
    }
]

EG : 2 


"placementStrategy": [
    {
       
        "type": "memberOf"
        "expression": "attribute:ecs.instance-type =~ t2.*"
        
    }
]






---------------------------------------------------- Amazon ECR

• ECR = Elastic Container Registry

• Store and manage Docker images on AWS

• Private and Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)

• Fully integrated with ECS, backed by Amazon S3

• Access is controlled through IAM (permission errors => policy)

• Supports image vulnerability scanning, versioning, image tags, image lifecycle, ...




----------------------------------------------------  Amazon ECR – Using AWS CLI

• Login Command

   • AWS CLI v2

        aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com



• Docker Commands
 
   • Push

       docker push aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest


   • Pull
     
       docker pull aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest



• In case an EC2 instance (or you) can’t pull a Docker image, check IAM permissions





---------------------------------------------------- AWS Copilot


• CLI tool to build, release, and operate production-ready containerized apps
      
      - So Copilot is not a service,


• The idea is that we want to remove the difficulty of running apps on AppRunner, ECS and Fargate, by just using a CLI tool to deploy to these environments.

• Helps you focus on building apps rather than setting up infrastructure

• Provisions all required infrastructure for containerized apps (ECS,VPC, ELB, ECR...) is done for you by Copilot.

• Automated deployments with one command using CodePipeline

• Deploy to multiple environments

• Troubleshooting, logs, health status...



Microservices Architecture
Use CLI or YAML to describe the architecture of your applications   --------------------------------> AWS Copilot CLI for containerized applications (Well-architected infrastructure setup, Deployment Pipeline, Effective Operations and Troubleshooting)----------------> Amazon ECS / AWS Fargate / AWS App Runner



------------------------------------- LAB for Copilot

-- open cloud9 , create environment

-- all are default , create environment

sudo curl -Lo /usr/local/bin/copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux \
   && sudo chmod +x /usr/local/bin/copilot \
   && copilot --help


-- by using the above command we can install the copilot in the Identify

-- make sure that docker should be installed in the IDE , check by typing docker

-- clone the url from the github
  
      git clone https://github.com/aws-samples/aws-copilot-sample-service example

-- do cd example

-- this example folder contains docker file , and index.html files 

-- do copilot init

-- So this is the Copilot CLI and it's going to give us some questions. And with it we're going to be able to set up a containerized application on AWS.

-- Application name: copilot-guide

 Which workload type best represents your architecture?  [Use arrows to move, type to filter, ? for more help]
  > Request-Driven Web Service  (App Runner)
    Load Balanced Web Service   (Public. ALB by default. Internet to ECS on Fargate)
    Backend Service             (Private. ALB optional. ECS on Fargate)
    Worker Service              (Events to SQS to ECS on Fargate)
    Static Site                 (Internet to CDN to S3 bucket)
    Scheduled Job               (Scheduled event to State Machine to Fargate)


-- u will get this once u give ur application load balancer 

-- here we have LB service web app

-- Service name: web-app
 
   Which Dockerfile would you like to use for web-app?  [Use arrows to move, type to filter, ? for more help]
  > ./Dockerfile
    Enter custom path for your Dockerfile
    Use an existing image instead

-- choose 1st option

-- select N 

-- in web-app folder one manifest file will get created here 

-- now create an environment to run our application in.

    copilot env init --name prod --profile default --app copilot-guide\

-- choose default 

-- now now it's going to update all the resources 

-- now go to cloudformation and check StackSet-copilot-guide-infrastructure --> resource , it will create ecr repo , s3 bucket and bucket policy and KMS created for us

-- next what we have to do is to actually go ahead and provision our application.

-- once u run copilot env deploy --name prod  

-- it will gives u InvalidClientTokenId error , so to avoid this , u can do  click on cloud9 logo --> preferences --> aws settings --> disable temporary credentials 

-- now go to IAM --> create user --> with administration access policy --> generate Access key 

-- now do aws configure and give details and o/p format is JSON

-- now it is working , see what it is created for u 

-- check in ecs cluster got vreated but no services are there 

-- so now run copilot deploy

-- so it will search for our appn "web-app" and choose environment to run our application

-- So now it goes ahead and uses Docker to actually build our final Docker image.

-- Then it pushes that Docker image into ECR, and then from ECR is going to create an ECS service that will be referencing that image, and will be started on our ECS cluster.

-- now wait for 5-7 minutes , it will get created the whole process for u  do not close the window unitl it get created 

- Creating the infrastructure for stack copilot-guide-prod-web-app                [create complete]  [351.8s]
  - Service discovery for your services to communicate within the VPC             [create complete]  [0.0s]
  - Update your environment's shared resources                                    [update complete]  [172.8s]
    - A security group for your load balancer allowing HTTP traffic               [create complete]  [3.6s]
    - An Application Load Balancer to distribute public traffic to your services  [create complete]  [151.8s]
    - A load balancer listener to route HTTP traffic                              [create complete]  [1.1s]
  - An IAM role to update your environment stack                                  [create complete]  [16.3s]
  - An IAM Role for the Fargate agent to make AWS API calls on your behalf        [create complete]  [16.3s]
  - An HTTP listener rule for path `/` that forwards HTTP traffic to your tasks   [create complete]  [0.0s]
  - A custom resource assigning priority for HTTP listener rules                  [create complete]  [3.0s]
  - A CloudWatch log group to hold your service logs                              [create complete]  [7.3s]
  - An IAM Role to describe load balancer rules for assigning a priority          [create complete]  [16.3s]
  - An ECS service to run and maintain your tasks in the environment cluster      [create complete]  [122.5s]
    Deployments                                                                                      
               Revision  Rollout      Desired  Running  Failed  Pending                                       
      PRIMARY  1         [completed]  1        1        0       0                                             
  - A target group to connect the load balancer to your service on port 80        [create complete]  [15.3s]
  - An ECS task definition to group your containers and run them on ECS           [create complete]  [0.0s]
  - An IAM role to control permissions for the containers in your tasks


-- these are things that it will create all the complexity of thinking about what you need to actually create when you run an application on ECS is taken away by Copilot.

-- now do check in ecs , the service is get created 

-- now copy the link and paste in the broswer , u will get o/p

-- all the complexity will tAKEN BY Copilot for u
    
-- now delete resource , do run copilot app delete

-- wait for some time it will get deleted 

-- delete user in iam also 


---------------------------------------------------- Amazon EKS(Elastic Kubernetes service) Overview

• Amazon EKS = Amazon Elastic Kubernetes Service

• It is a way to launch managed Kubernetes clusters on AWS

• Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) application

• It’s an alternative to ECS, similar goal but different API

• EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers

• Use case: if your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes

• Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP...)

• For multiple regions, deploy one EKS cluster per region

• Collect logs and metrics using CloudWatch Container Insights



----------------------- Amazon EKS – Node Types

• Managed Node Groups

    • Creates and manages Nodes (EC2 instances) for you
    • Nodes are part of an ASG managed by EKS
    • Supports On-Demand or Spot Instances

• Self-Managed Nodes

    • Nodes created by you and registered to the EKS cluster and managed by an ASG
    • You can use prebuilt AMI - Amazon EKS Optimized AMI
    • Supports On-Demand or Spot Instances


• AWS Fargate

    • No maintenance required; no nodes managed


------------------ Amazon EKS – Data Volumes

• Need to specify StorageClass manifest on your EKS cluster

• Leverages a Container Storage Interface (CSI) compliant driver

• Support for...
   
   • Amazon EBS
   • Amazon EFS (works with Fargate) 
   • Amazon FSx for Lustre
   • Amazon FSx for NetApp ONTAP





===================================== Deploy an Application to Amazon ECS With EC2 | Docker | ECR | Fargate | Load balancer | AWS Project ===================




Step 1 : 

- Create one EC2 Instance and connect into it and install Docker on instance 

- sudo su

- yum update -y 

- curl -fsSL https://get.docker.com -o get-docker.sh

- yum install docker -y

- docker --version

- systemctl start docker

- systemctl status docker




Step 2 : 


- I would like to create a directory for iur project so,

       mkdir <name of ur folder>

- cd <folder that u have created>

- create one Dockerfile 

     vim Dockerfile


- Add content in Dockerfile

# Use the official CentOS 7 base image
FROM centos:centos7

# Install the Apache HTTP server package from the CentOS repository
RUN yum install httpd -y

# Copy the index.html file from the Docker build context to the default Apache document root directory in the container
COPY index.html /var/www/html/

# Specify the command to run when the container starts, which starts the Apache HTTP server in the foreground
CMD ["/usr/sbin/httpd","-D","FOREGROUND"]

# Expose port 80 to allow incoming HTTP traffic to the container
EXPOSE 80



- now create one index.html file 



<!DOCTYPE html>
<html>
  <head>
    <title>Application Deployment to Amazon ECS</title>
    <style>
      body {
        background-color: seaGreen;
        margin: 0;
        padding: 0;
      }
      .navbar {
        background-color: white;
        display: flex;
        justify-content: space-between;
        align-items: center;
        height: 50px;
        padding: 0 20px;
      }
      .navbar button {
        border: none;
        background-color: white;
        color: orange;
        font-size: 16px;
        font-weight: bold;
        cursor: pointer;
        outline: none;
        padding: 10px 20px;
        margin-right: 10px;
        border-radius: 20px;
        transition: all 0.3s ease;
      }
      .navbar button:hover {
        background-color: seaGreen;
        color: white;
      }
    </style>
  </head>
  <body>
    <div class="navbar">
      <button>Home</button>
      <button>Services</button>
      <button>About Us</button>
      <button>DevOps Courses</button>
    </div>
    <h1>WEB APPLICATION DEPLOYMENT TO ECS </h1>
    <p>Feels good learning Containerization with Docker and Deployment to Amazon ECS</p>
  </body>
</html>




- now build docker image 

      docker build -t project-image .              # project = u can give any name here 

















================================================= AWS Elastic Beanstalk ===============================================


Developer problems on AWS

  • Managing infrastructure
  • Deploying Code
  • Configuring all the databases, load balancers, etc
  • Scaling concerns


• Most web apps have the same architecture (ALB + ASG)

• All the developers want is for their code to run!

• Possibly, consistently across different applications and environments



------------------------------------- Elastic Beanstalk – Overview

• Elastic Beanstalk is a developer centric view of deploying an application on AWS

• It uses all the component’s we’ve seen before: EC2, ASG, ELB, RDS, ...

• Managed service

     • Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration, ...

     • Just the application code is the responsibility of the developer

• We still have full control over the configuration

• Beanstalk is free but you pay for the underlying instances



------------------------------------- Elastic Beanstalk – Components

• Application: collection of Elastic Beanstalk components (environments, versions, configurations, ...)

• Application Version: an iteration of your application code (v1, v2 , v3)

• Environment 

      • Collection of AWS resources running an application version (only one application version at a time in an environment),  where we can see we can actually update an application version within an environment from version one to version two.
   
      • Tiers: Web Server Environment Tier & Worker Environment Tier

      • You can create multiple environments (dev, test, prod, ...)



------------------------------------- Elastic Beanstalk – Supported Platforms

• Go
• Java SE
• Java withTomcat
• .NET Core on Linux
• .NET on Windows Server 
• Node.js
• PHP
• Python
• Ruby
• Packer Builder
• Single Container Docker 
• Multi-container Docker 
• Preconfigured Docker


------------------------------------- Web ServerTier vs. WorkerTier (environments)


 Web ServerTier :

 -- This is the traditional architecture that we know, where we have a load balancer and then it's sending traffic to an auto scaling group that has multiple EC2 instances that are going to be your web server.


 WorkerTier :

 -- So this time there is no clients accessing directly your EC2 instances. We're going to use an SQS queue, which is a message queue and the message will be sent into the SQS queue and the EC2 instances are going to be workers,

 -- because they're going to pull messages from the SQS queue to process them.

 • Scale based on the number of SQS messages , more SQS messages more ec2 instances

 • Can push messages to SQS queue from anotherWeb ServerTier




------------------------------------ Elastic Beanstalk Deployment Modes

1 Single Instance Great for dev

  -- In this case, you'll have one EC2 instance which will have an Elastic IP, potentially it can also launch an RDS database and so on,

  -- but it's all based on one instance with an Elastic IP. It's great for development purposes,


NOTE : but then if you wanted to scale a real Elastic Beanstalk mode, then you would go for high available with a load balancer,


2  High Availability with Load Balancer Great for prod

   -- which is great for production environments, in which case, you can have a load balancer distributing the loads across multiple EC2 instances that are managed for an auto scaling group and multiple available zones.

   -- And finally, you may have an RDS database that's also multi AZ with a master and a standby.





----------------------- 1st environment LAB 

Single Instance Great for dev

-- open Elasticbeanstalk --> Web server environment

-- choose nodejs

-- sample application --> single instance

-- choose Create and use new service role

-- sometimes the EC2 instance profile is not getting by default , so create ROLE for this 

    - IAM --> ROLES --> create role --> aws service --> ec2 --> attach permissions --> create role 

          AWSElasticBeanstalkMulticontainerDocker
          AWSElasticBeanstalkWebTier
          AWSElasticBeanstalkWorkerTier


-- now go directly to skip review

-- submit --> this will create our first environment




----------------------- 2nd environment LAB 



-- name = prod

-- sample application 

-- Presets  = high availability

-- select role , skip for review then submit , it will create for u 

-- it will take almsot 10 min

-- for this environment , the loadbalancer will created , SG with port 80 , and ASG 


NOTE : use the above method to create environment , in the new console there might be chance of getting error , so follow this only 





---------------------------------------- Beanstalk Deployment Options for Updates



1 • All at once (deploy all in one go) – fastest, but instances aren’t available to serve traffic for a bit (downtime)

2 • Rolling: update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy

3 • Rolling with additional batches: like rolling, but spins up new instances to move the batch (so that the old application is still available)

4 • Immutable: spins up new instances in a new ASG, deploys version to these instances, and then swaps all the instances when everything is healthy

5 • Blue Green: create a new environment and switch over when ready

6 • Traffic Splitting: canary testing – send a small % of traffic to new deployment



Explanation for Deployment Options



---------- 1 All at once (deploy all in one go)

-- Here is our four EC2 instances, and they all run the version one, which is blue, of our application.

-- Then we are going to do an all at once deployment. So we want deploy v2.

-- And what happens, that first Elastic Beanstalk will just stop the applications on all our EC2 instances.

-- So then I put it as gray, as in they don't run anything. for sometime

-- And then we will be running the new V2, because Elastic Beanstalk will deploy V2 to these instances.

-- So what do we notice? 

     • Fastest deployment

     • Application has downtime

     • Great for quick iterations in development environment

     • No additional cost



------------ 2 Elastic Beanstalk Deployment Rolling


• Application is running below capacity

• Can set the bucket size

• No additional cost

-- We have four instances running v1,and the bucket size will be two for the example.

-- So what happens is that the application on the instances will be stopped, and so they're gray.

-- But we still have the other two instances running v1, we have maybe half capacity here.

-- Then these first two instances will be updated, so they'll be running v2, and then we will roll on to the next bucket, or to the next batch.And so that's why it's called rolling.

-- first 2 will updated to 2nd version and then nxt 2 instancs updated to 2nd version , the application, at some point during the deployment, is running both versions simultaneously.

-- No additional cost

-- so if you set a very small bucket size and you have hundreds and hundreds of instances, it may be a very long deployment,

-- Right now, in this example, we have a bucket size of two and four instances, but we can have a bucket size of 2 and 100 instances. It will just take a very long time to upgrade everything.

-- Zero downtime



---------------- 3 Elastic Beanstalk Deployment Rolling with additional batches

-- so in this case, the application is not running under capacity, just like before.

-- Before, at some point, we were only running two instances out of four. So that was below capacity.

-- In this mode, we run at capacity,and we can also set the bucket size.

-- basically our application will still be running both versions simultaneously,

-- but at a small additional cost.

-- That additional batch, that we'll see in a second, will be removed at the end of the deployment.

-- the deployment is going to be long.

-- It's honestly a good way to deal with prod.

-- Zero downtime

EG : 

-- We have our four v1 EC2 instances, and the first thing we're going to do is deploy new EC2 instances, and they will have the v2 version on it. (now total they are 6 instances)

-- you can see that the additional two are running, already, the newer version.

-- Now we take the first batch to the first bucket of two and they get stopped, the application gets stopped,and the application gets updated to v2,

-- Then the process repeats again, just like in rolling.So the application running v1 gets stopped, and then the application is updated to v2.

-- so at the end, you can see, we have six EC2 instances running v2.

-- so at the end of it, the additional batch gets terminated and taken away.(now we have 4 insatnces)




---------------------------------- 4 Elastic Beanstalk Deployment Immutable


--  Zero downtime

--  New Code is deployed to new instances on a temporary ASG

• High cost, double capacity

• Longest deployment

• Quick rollback in case of failures (just terminate new ASG)

• Great for prod


EG : 

-- We have a current ASG with three applications v1 running on three instances.

-- And then we're going to have a new temporary ASG being created. At first, Beanstalk will launch one instance on it, just to make sure that one works.And if it works and it passes the health checks, it's going to launch all the remaining ones.

-- it's going to sort of merge the ASG with a temporary ASG. So it's going to move all the temporary ASG instances to the current ASG.

-- So now, in the current ASG, we have six instances, And when all of this is done and the temporary ASG is empty, then we have the current ASG that will terminate all the v1 applications, while the v2 applications are still there. And then, finally, the temporary ASG will just be removed.





Q: A company hosting their website on AWS Elastic Beanstalk. They want a methodology to continuously release new application versions with the ability to roll back very quickly in case if there're any issues. Also, the application must be running at full capacity while releasing new versions. Which Elastic Beanstalk deployment option do you recommend?

ANS : immutable , In this mode, a full set of new instances running the new version of the application in a separate Auto Scaling Group is launched. To roll back quickly, this mode terminates the ASG holding the new application version, while the current one is untouched and already running at full capacity.




--------------------------------- 5 Elastic Beanstalk Deployment Blue / Green

• Not a “direct feature” of Elastic Beanstalk

• Zero downtime and release facility

• Create a new “stage” environment and deploy v2 there

• The new environment (green) can be validated independently and roll back if issues

• Route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage environment

• Using Beanstalk, “swap URLs” when done with the environment test



-------------------------------- 6 Elastic Beanstalk - Traffic Splitting (Canary)

• Canary Testing

• New application version is deployed to a temporary ASG with the same capacity

• A small % of traffic is sent to the temporary ASG for a configurable amount of time

• Deployment health is monitored

• If there’s a deployment failure, this triggers an automated rollback (very quick)

• No application downtime

• New instances are migrated from the temporary to the original ASG

• Old application version is then terminated

-- this could be a big improvement on top of the blue/green technique



REF : https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html





--------------------------------- Deployment models LAB 


-- prod environment --> configuraations --> Updates, monitoring, and logging --> edit  --> select immutable --> apply 

-- it will take some time to update 

-- in the meanwhile go to google --> sample application node js beanstalk --> downlaod zip file for nodejs app

-- in this code --> index.html --> text coloumn --> change to blue colour

-- now upload and deploy zip file of this code

-- let's observe how it is creating in the events 

-- once it is create , check in brower , it will turn to blue 

-- here emv = green and prod = blue

-- now do environment swapping , that means prod become env and env become prod

-- And the reason we would do so is, for example, you take this environment, for example, prod. First, you're going to clone it.

-- So you're going to create a copy of prod,then you would deploy your new application to the new environment where you can perform some extensive testing. Call it prod number two.

-- in prod environment --> do swap environment domain --> choose env 

-- it will take sometime to update 

-- now check in browser , the pro become green and env become blue 





----------------------------------------------------------- Elastic Beanstalk CLI 


• We can install an additional CLI called the “EB cli” which makes working with Beanstalk from the CLI easier

• Basic commands are:
 
   • eb create
   • eb status 
   • eb health 
   • eb events 
   • eb logs
   • eb open
   • eb deploy
   • eb config
   • eb terminate

• It’s helpful for your automated deployment pipelines!


----------------------------------------------- Elastic Beanstalk Deployment Process


• Describe dependencies
  
      (requirements.txt for Python, package.json for Node.js)


• Package code as zip, and describe dependencies

      • Python: requirements.txt
      • Node.js: package.json

• Console: upload zip file (creates new app version), and then deploy

• CLI: create new app version using CLI (uploads zip), and then deploy


• Elastic Beanstalk will deploy the zip on each EC2 instance, resolve dependencies and start the application




------------------------------------------------------- Beanstalk Lifecycle Policy 


• Elastic Beanstalk can store at most 1000 application versions

• If you don’t remove old versions, you won’t be able to deploy anymore

• To phase out old application versions, use a lifecycle policy

   • Based on time (old versions are removed)
   • Based on space (when you have too many versions)

• Versions that are currently used won’t be deleted

• Option not to delete the source bundle in S3 to prevent data loss


-- All the applications are being registed in the amazon s3 bucket 

--  go to Elasticbean stalk --> appn versions --> activate lifecycle policy --> choose any one b/w the 2 options 




------------------------------------------------------- Elastic Beanstalk Extensions


• A zip file containing our code must be deployed to Elastic Beanstalk

• All the parameters set in the UI can be configured with code using files

• Requirements:

    • in the .ebextensions/ directory in the root of source code

    • YAML / JSON format

    • .config extensions (example: logging.config)

    • Able to modify some default settings using: option_settings

    • Ability to add resources such as RDS, ElastiCache, DynamoDB, etc...


• Resources managed by .ebextensions get deleted if the environment goes away


---- now add environmentvariables.config file  in the zip to the env environment (took from stphene codes)

-- u will see the environment variables in configuration section





------------------------------------------------------- Elastic Beanstalk Under the Hood


 Under the Hood : (The underlying implementation of a product (hardware, software, or idea)).doing work on something that isn't visible or apparent to someone else, or of something that is complex and not easily understood.


• Under the hood, Elastic Beanstalk relies on CloudFormation

• CloudFormation is used to provision other AWS services (we’ll see later)

• Use case: you can define CloudFormation resources in your .ebextensions to provision ElastiCache, an S3 bucket, anything you want!


-- go to CF and check in resource , that it has created for u 




------------------------------------------------------- Elastic Beanstalk Cloning

• Clone an environment with the exact same configuration

• Useful for deploying a “test” version of your application

• All resources and configuration are preserved:

    • Load Balancer type and configuration

    • RDS database type (but the data is not preserved)

    • Environment variables

• After cloning an environment, you can change settings 

-- we did during the lab 



------------------------------------------------------- Elastic Beanstalk Migration: Load Balancer


----------------------- 1 Elastic Beanstalk Migration: Load Balancer

• After creating an Elastic Beanstalk environment, you cannot change the Elastic Load Balancer type (only the configuration)

  -- if you want it to somehow upgraded from a Classic Load Balancer to an Application Load Balancer or from an Application Load Balancer to a Network Load Balancer. You will need to perform a migration and the steps are as such.

• To migrate:

    1. create a new environment with the same configuration except LB (can’t clone) -- u can't use clone 'coz it uses same configuration.

    2. deploy your application onto the new environment

    3. perform a CNAME swap or Route 53 update -- to shift traffic



----------------------- 2 RDS with Elastic Beanstalk

• RDS can be provisioned with Beanstalk, which is great for dev / test

• This is not great for prod as the database lifecycle is tied to the Beanstalk environment lifecycle

• The best for prod is to separately create an RDS database and provide our EB application with the connection string (for example using an environment variable.)



-------- Elastic Beanstalk Migration: Decouple RDS (How would we Decouple RDS if it's already in our Beannstock stack?)

1. Create a snapshot of RDS DB (as a safeguard)

2. Go to the RDS console and protect the RDS database from deletion

3. Create a new Elastic Beanstalk environment, without RDS, point your application to existing RDS (for example, using an environment variable.)

4. perform a CNAME swap (blue/green) or Route 53 update, confirm working

5. Terminate the old environment (RDS won’t be deleted)

6. Delete CloudFormation stack (in DELETE_FAILED state) (so we need to just go in CloudFormation and delete that CloudFormation stack manually.)








============================================================================= AWS CloudFormation ========================================================================

Managing your infrastructure as code


• CloudFormation is a declarative way of outlining your AWS Infrastructure, for any resources (most of them are supported)

• For example, within a CloudFormation template, you say:

    • I want a security group
    • I want two EC2 instances using this security group
    • I want two Elastic IPs for these EC2 instances
    • I want an S3 bucket
    • I want a load balancer (ELB) in front of these EC2 instances


• Then CloudFormation creates those for you, in the right order, with the exact configuration that you specify




---------------------------------------------- Benefits of AWS CloudFormation 

• Infrastructure as code

    • No resources are manually created, which is excellent for control

    • The code can be version controlled for example using Git

    • Changes to the infrastructure are reviewed through code



• Cost

    • Each resources within the stack is tagged with an identifier so you can easily see how much a stack costs you

    • You can estimate the costs of your resources using the CloudFormation template

    • Savings strategy: In Dev, you could automation deletion of templates at 5 PM and recreated at 8 AM, safely




• Productivity


   • Ability to destroy and re-create an infrastructure on the cloud on the fly

   • Automated generation of Diagram for your templates!

   • Declarative programming (no need to figure out ordering and orchestration)



• Separation of concern: create many stacks for many apps, and many layers. Ex:

   • VPC stacks
   • Network stacks 
   • App stacks



• Don’t re-invent the wheel


    • Leverage existing templates on the web!

    • Leverage the documentation





---------------------------------------------------------- How CloudFormation Works


• Templates must be uploaded in S3 and then referenced in CloudFormation



   Template ------(upload)-------------> S3 bucket ------------- (reference)-----------> AWS CloudFormation ---------------(create)-----------> Stack ----------(create)----> AWS Resources



• To update a template, we can’t edit previous ones. We have to re- upload a new version of the template to AWS


• Stacks are identified by a name within a region 

• Deleting a stack deletes every single artifact that was created by CloudFormation.





---------------------------------------------------------- Deploying CloudFormation Templates


1 • Manual way

     • Editing templates in CloudFormation Designer or code editor

     • Using the console to input parameters, etc...

     • We’ll mostly do this way in the course for learning purposes


2 • Automated way

     • Editing templates in a YAML file

     • Using the AWS CLI (Command Line Interface) to deploy the templates, or using a Continuous Delivery (CD) tool

     • Recommended way when you fully want to automate your flow



NOTE : there is no order in which CloudFormation should create your resources. When you write a CloudFormation template





---------------------------------------------------------- CloudFormation – Building Blocks


• Template’s Components

    • AWSTemplateFormatVersion – identifies the capabilities of the template “2010-09-09”

    • Description – comments about the template

    • Resources (MANDATORY) – your AWS resources declared in the template

    • Parameters – the dynamic inputs for your template

    • Mappings – the static variables for your template

    • Outputs – references to what has been created

    • Conditionals – list of conditions to perform resource creation


• Template’s Helpers

    • References
    • Functions




---------------------------------------------------------- LAB for Stacks


-------------- 1 creating source


-- make sure to use us-east-1 region only for creating stacks , 'coz all the templates have been designed for that region, especially when talking about AMI IDs, which are region specific.

-- open cloudformation --> choose yaml files for easy purpose --> upload a yaml file which has template for ec2 creation --> do submit 
 
EG for ec2 
     Resources:
     MyInstance:
       Type: AWS::EC2::Instance
       Properties:
          AvailabilityZone: us-east-1a
          ImageId: ami-0a3c3a20c09d6f377
          InstanceType: t2.micro



-- our first stack is being created , do refresh it will created for u 

-- do explore in CF Properties





------------ 2 Update and Delete Stack 


-- here only way to update is to upload a new version of template to update 

-- upload updated template yaml file 

EG for updated template 



---
Parameters:
  SecurityGroupDescription:
    Description: Security Group Description
    Type: String

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
      SecurityGroups:
        - !Ref SSHSecurityGroup
        - !Ref ServerSecurityGroup

  # an elastic IP for our instance
  MyEIP:
    Type: AWS::EC2::EIP
    Properties:
      InstanceId: !Ref MyInstance

  # our EC2 security group
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  # our second EC2 security group
  ServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Ref SecurityGroupDescription
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 192.168.1.1/32



-- in above template we are attaching 2 security groups and EIP to the instance

-- choose nxt --> SecurityGroupDescription , u will get this option 'coz in this code u have written so give Description --> nxt 

-- in the summary page in step 2 u have stack deatils with parameters and in Changeset preview , u can able to see the changes and modifiers 

-- a "change set" represents a list of things that is going to change as part of your CloudFormation updates.

-- do submit , u can see that in the same stack the update is happening , go n check all the resources are created through the template 

-- to delete stack ,jst do delete stack 






------------------------------------------------------------------  YAML Crash Course

EG for YAML code 

Parameters:
  KeyName:
    Description: The EC2 Key Pair to allow SSH access to the instance
    Type: 'AWS::EC2::KeyPair::KeyName'
Resources:
  Ec2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
      SecurityGroups:
        - !Ref InstanceSecurityGroup
        - MyExistingSecurityGroup
      KeyName: !Ref KeyName
      ImageId: ami-7a11e213
  InstanceSecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0




 • YAML and JSON are the languages you can use for CloudFormation

 • JSON is horrible for CF , 'coz of of many string interpolations and so on,

 • YAML is great in so many ways

 • Key value Pairs

 • Nested objects  (SG is nested object, resource , preperties etc )

 • Support Arrays ( - symbol means which represents an array)

 • Multi line strings 





------------------------------------------------------------------ 1 CloudFormation – Resources


• Resources are the core of your CloudFormation template (MANDATORY)

• They represent the different AWS Components that will be created and configured

• Resources are declared and can reference each other

• AWS figures out creation, updates and deletes of resources for us

• There are over 700 types of resources (!)

• Resource types identifiers are of the form:

          service-provider::service-name::data-type-name




--------------------------- How do I find Resources documentation?

• All the resources can be found here:

      https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/a ws-template-resource-type-ref.html


• Then, we just read the docs 

• Example here (for an EC2 instance):

     https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/a ws-resource-ec2-instance.html




--------------------------------CloudFormation – Resources FAQ

• Can I create a dynamic number of resources?

  - Yes, you can by using CloudFormation Macros and Transform
  


• Is every AWS Service supported?

   - Almost. Only a select few niches are not there yet

   - You can work around that using CloudFormation Custom Resources



------------------------------------------------------------------ 2 CloudFormation – Parameters


• Parameters are a way to provide inputs to your AWS CloudFormation template

• They’re important to know about if:

     • You want to reuse your templates across the company

     • Some inputs can not be determined ahead of time


• Parameters are extremely powerful, controlled, and can prevent errors from happening in your templates, thanks to types    -- IMP to know




------------------------------- When should you use a Parameter?


EG : 

Parameters:
  KeyName:
    Description: The EC2 Key Pair to allow SSH access to the instance
    Type: 'AWS::EC2::KeyPair::KeyName'


-- EG for the parameter 

• Ask yourself this:

       • Is this CloudFormation resource configuration likely to change in the future?

       • If so, make it a parameter


• You won’t have to re-upload a template to change its content





------------------------------- CloudFormation – Parameters Settings


• Parameters can be controlled by all these settings:

   • Type:
       
       • String
       • Number
       • CommaDelimitedList
       • List<Number>
       • AWS-Specific Parameter (to help catch invalid values – match against existing values in the AWS account)
       • List<AWS-Specific Parameter>
       • SSM Parameter (get parameter value from SSM Parameter store)
       • Description
       • ConstraintDescription (String)
       • Min/MaxLength
       • Min/MaxValue
       • Default
       • AllowedValues (array)
       • AllowedPattern (regex)
       • NoEcho (Boolean)


-- So you don't have to remember all of those, but what you need remember is that the parameters are not just strings. You can have constraints and validation, allowing you to make sure they are safe to use.
 





------------------------------- CloudFormation – Parameters Example

     1 • AllowedValues (array)

Parameters:
 InstanceType:
   Description: Choose an ec2 type
   Type: String
   AllowedValues:
      - t2.micro
      - t3.micro
      - t2.small


Resources:
  Ec2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
       InstanceType: !Ref InstanceType
       ImageId: xxxxxxxxxxxxxx


-- Here , we have a parameter called InstanceType. To choose an EC2 InstanceType of Type: String.

-- But we have defined AllowedValues being t2.micro, t2.small, or t2.medium with a Default being t2.micro. And this parameter is reused in the EC2Instance.

-- So thanks to it, we'll have a dropdown and the user can only select one of these three values, hence giving them choice while giving you control.




     2 NoEcho (Boolean)


Parameters:
 DBPassword:
   Description: db password
   Type: String
   NoEcho: true

Resources:
  MyDBInstance:
   Type: 'AWS::RDS::DBInstance'
    Properties:
      DBInstanceClass: db.t2.micro
      AllocatedStorage: 20
      Engine: mysql 
      MasterUsername: administration
      MasterUserpassword: !Ref DBPassword
      DBInstanceIdentifier: MydbInstance


-- So for example, say we want as a parameter to put in the database password, but of course it is a password so we have to keep it secret.

-- So we want to remove it from the logs and so on. So we'll have NoEcho: true , so that the password is not displayed anywhere.






------------------------------- How to Reference a Parameter?

• The Fn::Ref function can be leveraged to reference parameters

• Parameters can be used anywhere in a template

• The shorthand for this inYAML is !Ref

• The function can also reference other elements within the template


IMP : you need to make sure that your resources don't have the same name as your parameters.



------------------------------- CloudFormation – Pseudo Parameters


• AWS offers us Pseudo Parameters in any CloudFormation template

• These can be used at any time and are enabled by default

• Important pseudo parameters:

    EG ;

    AWS::AccountId

    AWS::Region

    AWS::StackId

    AWS::StackName

    AWS::NotificationARNs

    AWS::NoValue





------------------------------------------------------------------ 3 CloudFormation – Mappings



• Mappings are fixed variables within your CloudFormation template

• They’re very handy to differentiate between different environments (dev vs prod), regions (AWS regions), AMI types...

• All the values are hardcoded within the template


EG : 

Mappings: 
  Mapping01: 
    Key01: 
      Name: Value01
    Key02: 
      Name: Value02
    Key03: 
      Name: Value03



RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309


-- So here, based on the region you have, so us-east-1, us-west-1, or eu-west-1, and based on the architecture you're using, for example, HVM64 or HVMG2, this is going to give you a different AMI ID every time.

-- Well, we know that the AMIs are specific based on the region, so of course it makes sense to have a different AMI per region.



--------------------------------- Accessing Mapping Values (Fn::FindInMap)



• We use Fn::FindInMap to return a named value from a specific key

• !FindInMap [ MapName,TopLevelKey, SecondLevelKey ]


EG :

AWSTemplateFormatVersion: "2010-09-09"
Mappings: 
  RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309
Resources: 
  myEC2Instance: 
    Type: "AWS::EC2::Instance"
    Properties: 
      ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", HVM64]
      InstanceType: m1.small


-- we have an EC2 instance that is using an ImageId. And this ImageId is using the FindInMap function.

-- to use this FindInMap function, we first need to use a map name. So here we have the RegionMap. Then we have the top level key.

-- So what we want to use in here, we have a reference to the pseudo parameter AWS::Region. So if you launch this template in us-east-1, it's going to be us-east-1, and if you launch it to us-west-1, automatically this pseudo parameter is going to resolve to us-west-1.
 
-- And then finally, the type of architecture you want, for example, HVM64. And this works great because, well, AMIs are region specific, and so you want to make sure you have the right AMI for the right region and the right architecture.



--------------------------------- When would you use Mappings vs. Parameters?



• Mappings are great when you know in advance all the values that can be taken and that they can be deduced from variables such as 

   • Region
   • Availability Zone
   • AWS Account
   • Environment (dev vs prod) 
   • etc...

• They allow safer control over the template

• Use parameters when the values are really user specific





------------------------------------------------------------------ 4  CloudFormation –  Outputs



• The Outputs section declares optional outputs values that we can import into other stacks (if you export them first)!

• You can also view the outputs in the AWS Console or in using the AWS CLI

• They’re very useful for example if you define a network CloudFormation, and output the variables such as VPC ID and your Subnet IDs

• It’s the best way to perform some collaboration cross stack, as you let expert handle their own part of the stack



-------------------------- CloudFormation – Outputs Example

• Creating a SSH Security Group as part of one template

• We create an output that references that security group


Outputs:
  StackSSHSecurityGroup:
    Description: The SSH SG
    Value: !Ref MycompanywideSShSG
    Export:
      Name: SSH SecurityGroup




-------------------------- CloudFormation – Outputs Cross-Stack Reference

• We then create a second template that leverages that security group

• For this, we use the Fn::ImportValue function

• You can’t delete the underlying stack until all the references are deleted





------------------------------------------------------------------ 5 CloudFormation –  Conditions


• Conditions are used to control the creation of resources or outputs based on a condition

• Conditions can be whatever you want them to be, but common ones are:
   • Environment (dev / test / prod) 
   • AWS Region
   • Any parameter value

• Each condition can reference another condition, parameter value or mapping


-------------------------- How to define a Condition

  Conditions:
     CreateprodResource: !Equals [!Ref EnvType, prod]
  

• The logical ID is for you to choose. It’s how you name condition

• The intrinsic function (logical) can be any of the following:

   • Fn::And
   • Fn::Equals
   • Fn::If
   • Fn::Not
   • Fn::Or


----------------- How to use a Condition

• Conditions can be applied to resources / outputs / etc...

   




-------------------------------------------------------------------- CloudFormation – Intrinsic Functions (built-in functions)


mk = must know 



• Ref              mk
• Fn::GetAtt       mk
• Fn::FindInMap    mk
• Fn::ImportValue   mk
• Fn::Join
• Fn::Sub
• Fn::ForEach
• Fn::ToJsonString
• Condition Functions (Fn::If, Fn::Not, Fn::Equals, etc...)     mk
• Fn::Base64        mk
• Fn::Cidr
• Fn::GetAZs 
• Fn::Select
• Fn::Split
• Fn::Transform 
• Fn::Length



--------------------------- 1  Intrinsic Functions – Fn::Ref

• The Fn::Ref function can be leveraged to reference

     • Parameters – returns the value of the parameter

     • Resources – returns the physical ID of the underlying resource (e.g., EC2 ID)

     - it won’t work for conditions 
     

• The shorthand for this inYAML is !Ref

MyEIP:
  Type: "AWS::EC2::EIP"
  Properties:
    InstanceId: !Ref MyEC2Instance



--------------------------- 2 Intrinsic Functions – Fn::GetAtt


• Attributes are attached to any resources you create

• To know the attributes of your resources, the best place to look at is the documentation



-- The following example template returns the SourceSecurityGroup.OwnerAlias and SourceSecurityGroup.GroupName of the load balancer with the logical name myELB.


EG 

AWSTemplateFormatVersion: 2010-09-09
Resources:
  myELB:
    Type: AWS::ElasticLoadBalancing::LoadBalancer
    Properties:
      AvailabilityZones:
        - eu-west-1a
      Listeners:
        - LoadBalancerPort: '80'
          InstancePort: '80'
          Protocol: HTTP
  myELBIngressGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: ELB ingress group
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          SourceSecurityGroupOwnerId: !GetAtt myELB.SourceSecurityGroup.OwnerAlias
          SourceSecurityGroupName: !GetAtt myELB.SourceSecurityGroup.GroupName



-- in general So anytime we use Ref, we're going to get the reference ID out of it.

-- But if we use GetAtt to get an attribute, we have the option to get more information out of this EC2 instance. So here we have the AvailabilityZone. So for example, we can know in which AZ an instance was launched, for example, us-east-1b. You get the Id again,

-- you could get the PrivateDNSName, the PrivateIp, the PublicDNSName, and the PublicIp. So while the Ref gives you usually a reference to the ID of the resource you have created, the GetAtt allows you to get more out of the resource and you can only get what CloudFormation supports in terms of attributes that are defined in documentation







--------------------------- 3 Intrinsic Functions – Fn::FindInMap

• We use Fn::FindInMap to return a named value from a specific key

• !FindInMap [ MapName,TopLevelKey, SecondLevelKey ]


TopLevelKey: The top-level key name. Its value is a list of key-value pairs.

SecondLevelKey: The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.



Return value:

  The value that's assigned to SecondLevelKey.


EG 

Mappings: 
  RegionMap: 
    us-east-1: 
      HVM64: "ami-0ff8a91507f77f867"
      HVMG2: "ami-0a584ac55a7631c0c"
    us-west-1: 
      HVM64: "ami-0bdb828fd58c52235"
      HVMG2: "ami-066ee5fd4a9ef77f1"
    eu-west-1: 
      HVM64: "ami-047bb4163c506cd98"
      HVMG2: "ami-31c2f645"
    ap-southeast-1: 
      HVM64: "ami-08569b978cc4dfa10"
      HVMG2: "ami-0be9df32ae9f92309"
    ap-northeast-1: 
      HVM64: "ami-06cd52961ce9f0d85"
      HVMG2: "ami-053cdd503598e4a9d"
Resources: 
  myEC2Instance: 
    Type: "AWS::EC2::Instance"
    Properties: 
      ImageId: !FindInMap
        - RegionMap
        - !Ref 'AWS::Region'
        - HVM64
      InstanceType: m1.small






--------------------------- 4 Intrinsic Functions – Fn::ImportValue


• Import values that are exported in other stacks

• For this, we use the Fn::ImportValue function

-- The intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references. 

-- In the following example template snippets, Stack A exports VPC security group values and Stack B imports them.

EG :

Stack A Export


Outputs:
  PublicSubnet:
    Description: The subnet ID to use for public web servers
    Value:
      Ref: PublicSubnet
    Export:
      Name:
        'Fn::Sub': '${AWS::StackName}-SubnetID'
  WebServerSecurityGroup:
    Description: The security group ID to use for public web servers
    Value:
      'Fn::GetAtt':
        - WebServerSecurityGroup
        - GroupId
    Export:
      Name:
        'Fn::Sub': '${AWS::StackName}-SecurityGroupID'



Stack B Import


Resources:
  WebServerInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.micro
      ImageId: ami-a1b23456
      NetworkInterfaces:
        - GroupSet:
            - Fn::ImportValue: 
              'Fn::Sub': '${NetworkStackNameParameter}-SecurityGroupID'
          AssociatePublicIpAddress: 'true'
          DeviceIndex: '0'
          DeleteOnTermination: 'true'
          SubnetId: Fn::ImportValue: 
            'Fn::Sub': '${NetworkStackNameParameter}-SubnetID'



-- Declaration

   Fn::ImportValue: sharedValueToImport


-- Alternatively, you can use the short form:

   !ImportValue sharedValueToImport




------- Important to know : 

 - You can't use the short form of !ImportValue when it contains the short form of !Sub.

 # do not use
!ImportValue
  !Sub '${NetworkStack}-SubnetID' 


- Instead, you must use the full function name, for example:

Fn::ImportValue:
  !Sub "${NetworkStack}-SubnetID"







--------------------------- 5 Intrinsic Functions – Fn::Base64


• Convert String to it’s Base64 representation

     !Base64 "valueToEncode"


• Example: pass encoded data to EC2 Instance’s UserData property


-- The intrinsic function Fn::Base64 returns the Base64 representation of the input string. This function is typically used to pass encoded data to Amazon EC2 instances by way of the UserData property.


EG:

Resources:
  WebServerInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.micro
      ImageId: ami-a1b23456
        UserData:
          Fn::Base64: |
            #!/bin/bash
            # Use this for your user data (script from top to bottom)
            # install httpd (Linux 2 version)
            yum update -y
            yum install -y httpd
            systemctl start httpd





--------------------------- 6 Intrinsic Functions – Condition Functions


Conditions:
     CreateprodResource: !Equals [!Ref EnvType, prod]
  

• The logical ID is for you to choose. It’s how you name condition

• The intrinsic function (logical) can be any of the following:

   • Fn::And
   • Fn::Equals
   • Fn::If
   • Fn::Not
   • Fn::Or




Ref for other functions  : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-base64.html







-------------------------------------------------------------------- CloudFormation – Rollbacks


-- they're very important to understand at the exam.


• If Stack Creation Fails: you have two options.

     • Default: everything rolls back (gets deleted). We can look at the log

     • Option to disable rollback and troubleshoot what happened



• If it is a problem with Stack Update Fails:

     • The stack automatically rolls back to the previous known working state, again, deleting anything that was newly created.

     • Ability to see in the log what happened and error messages



• Rollback Failure? Fix resources manually then issue ContinueUpdateRollback API from Console

    • Or from the CLI using continue-update-rollback API call




----------------- LAB for failuers 


-- create one yaml file , make some wrong configurations in it , for eg : ImageId


ami-0a3c3a20c09d6f377


-- create stack with the yaml file which has wrong ami-id, it has 2 Security Groups


subbu1.yaml


---
Parameters:
  SecurityGroupDescription:
    Description: Security Group Description
    Type: String

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-123345
      InstanceType: t2.micro
      SecurityGroups:
        - !Ref SSHSecurityGroup
        - !Ref ServerSecurityGroup

  # an elastic IP for our instance
  MyEIP:
    Type: AWS::EC2::EIP
    Properties:
      InstanceId: !Ref MyInstance

  # our EC2 security group
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  # our second EC2 security group
  ServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Ref SecurityGroupDescription
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 192.168.1.1/32



-- choose Stack failure options = Preserve successfully provisioned resources , create stack

-- here it is failed to create resource 'coz , i have not given description for security group

-- but i did not give description for "ServerSecurityGroup" but the SSHSecurityGroup is created 

-- now u cannot update the stack , so delete the stack , when there is create failure , u should delete the stack and troubleshoot and upload again



now create another yaml file with correct configuration 


subbu2.yaml 


---
Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro


-- now upload this file , 


-- choose  Stack failure options = Roll back all stack resources

-- create stack , now the resource got created 

-- now update the stack by uploading the 1st example yaml file (subbu1.yaml) file 

-- here u will get update option , 'coz u have created resources successfully 

-- upload the 1st yaml file 

-- now here provide group description

-- choose Stack failure options = Roll back all stack resources

-- create stack 

-- here the securitygropus were created but due to wrong ami-id , it is failed to create , so as per the condition that we ahve given (rollback), so, that means that my server security group and SSH security groups should disappear.




-- now update the stack with the same yaml file subbu1.yaml , bit choose Stack failure options = Preserve successfully provisioned resources

-- This would create SSH and server security groups, but it would not roll them back in case there is a rollback happening, a stack failure.

-- So, this is up to you to choose what you want, but both the behaviors can be desirable based on what you're trying to do.









-------------------------------------------------------------------- CloudFormation – Service Role


-- So CloudFormation can use service roles. What are they?


• IAM role that allows CloudFormation to create/update/delete stack resources on your behalf

   - Well, they are iam roles that you create and they're dedicated to CloudFormation and they allow CloudFormation to actually create update and delete stack resources on your behalf.


• Give ability to users to create/update/delete the stack resources even if they don’t have permissions to work with the resources in the stack


• Use cases:

    • You want to achieve the least privilege principle

    • But you don’t want to give the user all the required permissions to create the stack resources


• User must have iam:PassRole permissions which is a necessary permission to give a role to a specific service in AWS.




----------- LAB IAm roles with CF


-- open IAM --> create new role --> aws service --> CF --> give s3 full access --> create role 


-- now create stack , upload subbu2.yaml file for demo and in the permission sections = select IAM role that we have created now ,

-- So if I don't specify, then iam role is going to use my own personal permissions. 

-- But if I want to specify an iam role, I can look at this DemoRole for CFN with S3 capabilities. AWS CloudFormation will use this role for all stack operations. Other users that have permissions to operate on this stack will be able to use this role, even if they don't have permission to pass it. Ensure that this role grants the least privilege.

-- but this one, and because this one is just powered with Amazon S3 permissions,then actually my stack will fail because my stack is actually creating an EC2 instance.






-------------------------------------------------------------------- CloudFormation – Capabilities


• We have  CAPABILITY_NAMED_IAM and CAPABILITY_IAM 
       
       - So they are capabilities you need to give to CloudFormation whenever your CloudFormation template is going to create or update IAM resources, such as when you create a IAM user, a role, a group, a policy, and so on, through your CloudFormation templates.

      • Necessary to enable when you CloudFormation template is creating or updating IAM resources (IAM User, Role, Group, Policy, Access Keys, Instance Profile...)

      • Specify CAPABILITY_NAMED_IAM if the resources are named, otherwise, just CAPABILITY_IAM.

      - And the reason we do so is that we want to explicitly acknowledge the fact that CloudFormation is going to create IAM resources.


• CAPABILITY_AUTO_EXPAND

     - which is when your CloudFormation template is including macro and nested stacks, so stacks within stacks, to perform dynamic transformations.

     • Necessary when your CloudFormation template includes Macros or Nested Stacks (stacks within stacks) to perform dynamic transformations

     • You’re acknowledging that your template may change before deploying



• InsufficientCapabilitiesException

     • Exception that will be thrown by CloudFormation if the capabilities haven’t been acknowledged when deploying a template (security measure)

     - while launching a template, that means that the CloudFormation templates was requiring capabilities, but you haven't acknowledged them.

     - So, as a security measure, you need to redo the templates, upload, and launch using, this time, these capabilities. It's just an extra argument in your API call, or a box to tick on your AWS console.



-------------- lab

-- create one capabilities.yaml file 



AWSTemplateFormatVersion: '2010-09-09'
Description: An example CloudFormation that requires CAPABILITY_IAM and CAPABILITY_NAMED_IAM

Resources:
  MyCustomNamedRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: MyCustomRoleName
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: [ec2.amazonaws.com]
            Action: ['sts:AssumeRole']
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2FullAccess
      Policies:
        - PolicyName: MyPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 's3:*'
                Resource: '*'

Outputs:
  RoleArn:
    Description: The ARN of the created IAM Role
    Value: !GetAtt MyCustomNamedRole.Arn



-- here CloudFormation template, which actually has an IAM role being created. And it's named, so there's a role name, MyCustomRoleName. And it's using a managed policy, which is the AmazonEC2FullAccess. So we're dealing with IAM, and so as such, we are creating some IAM roles.


-- now try to create this stack in console with this file 

-- u will have a checkbox at last while u try to create this stack , do check and submit , then only u will able to create this stack 

-- the role has been created . check in console 







--------------------------------------------------------------------  1 CloudFormation – DeletionPolicy Delete


• DeletionPolicy:
  
     •  DeletionPolicy is a setting you can apply to resources on your CloudFormation templates, which allows you to Control what happens when the CloudFormation template is deleted or when a resource is removed from a CloudFormation template

     • Extra safety measure to preserve and backup resources

• Default DeletionPolicy=Delete 

-- So by default, we've seen that when we delete a CloudFormation template, all the resources within are also deleted. That means that the default DeletionPolicy is delete, so you don't have to specify it because it is the default.

EG:

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
    DeletionPolicy: Delete  




EG 2 :

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  myS3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete


-- in the above example , We have an S3 buckets and we have DeletionPolicy=Delete.

-- It will work only if the S3 bucket is empty. If it's not empty, then the delete will fail. So the way to fix this if you wanted to would be to either manually delete everything within the S3 bucket and then continue with the deletion of your CloudFormation templates.

-- Or it would be for you to implement a custom resource to actually delete everything within the S3 bucket before automatically having the S3 bucket go away.






----------------------------- 2 CloudFormation – DeletionPolicy Retain

• DeletionPolicy=Retain: 

     • Specify on resources to preserve in case of CloudFormation deletes

     • Works with any resources




EG :

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  myS3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain



-- So example here, we have a S3, and we know that by default, it would be deleted when I delete my CloudFormation template, 

-- but maybe we actually wanna keep it, keep the data within because we care about the data of this bucket. And so we would specify at the bottom DeletionPolicy retain.

-- And so even if I delete my CloudFormation templates, this S3 Bucket would stay, and this works with any resources.





----------------------------- 3 CloudFormation – DeletionPolicy Snapshot


• DeletionPolicy=Snapshot

• Create one final snapshot before deleting the resource

• Examples of supported resources:

    • EBS Volume, ElastiCache Cluster, ElastiCache ReplicationGroup

    • RDS DBInstance, RDS DBCluster, Redshift Cluster, Neptune DBCluster, DocumentDB DBCluster



----------- LAB 

-- create one yaml file that has DeletionPolicy


Resources:
  MySG:
    Type: AWS::EC2::SecurityGroup
    DeletionPolicy: Retain
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  MyEBS:
    Type: AWS::EC2::Volume
    DeletionPolicy: Snapshot
    Properties:
      AvailabilityZone: us-east-1a
      Size: 1
      VolumeType: gp2



-- in above example , there is a security group and the DeletionPolicy is retained. So that means that if I delete my transformation stack, this security group should stay.

-- And there is an EBS volume, and the DeletionPolicy is a snapshot. So that means that upon deleting the stack, the volume should go away, but a snapshot should be created first.

-- let's see how it is working 

-- upload yaml file and create one stack 

-- once the resource is created , do delete the stack

-- u can observe that MySG = DELETE_SKIPPED , 'coz we have given retain 

-- ebs volume is deleted but the snapshot is already created for u go n check in console 

-- u can delete manually through console , if u want to delete 







--------------------------------------------------------------------  CloudFormation – Stack Policies


• During a CloudFormation Stack update, all update actions are allowed on all resources (default)


-- but sometimes, you may want to protect your stack against updates, or part of your stack against updates. This is where Stack policies come in.


• A Stack Policy is a JSON document that defines the update actions that are allowed on specific resources during Stack updates


EG:

{
  "Statement" : [
  {
    "Effect" : "Allow",
    "Principal" : "*",
    "Action" : "update:*",
    "Resource" : "*",
   },

  {
    "Effect" : "Deny",
    "Principal" : "*",
    "Action" : "Update:*",
    "Resource" : "LogicalResourceId/ProductionDatabase"
  }
  ]
}


-- So here, we have an example where the first statement is saying "Allow update*" on everything, meaning that everything in your CloudFormation Stack can be updated,

-- second part is saying "Deny update*" on Resource Production Database. That means that whatever is named "Production Database" in your CloudFormation Stack is going to be protected against any kind of updates,



• Protect resources from unintentional updates

• When you set a Stack Policy, all resources in the Stack are protected by default

• Specify an explicit ALLOW for the resources you want to be allowed to be updated






--------------------------------------------------------------------  CloudFormation – Termination Protection


• To prevent accidental deletes of CloudFormation Stacks, use TerminationProtection


-- once u create stack --> stack actions --> set TerminationProtection





--------------------------------------------------------------------  CloudFormation – Custom Resources


• Used to

     • define resources not yet supported by CloudFormation

     • define custom provisioning logic for resources can that be outside of CloudFormation (on-premises resources, 3rd party resources...)

     • have custom scripts run during create / update / delete through Lambda functions (Eg : running a Lambda function to empty an S3 bucket before being deleted)


• Defined in the template using

      - AWS::CloudFormation::CustomResource or 
      
      - Custom::MyCustomResourceTypeName (recommended)


• Backed by a Lambda function (most common) or an SNS topic




------------------------------- How to define a Custom Resource?


• Ser viceToken specifies where CloudFormation sends requests to, such as Lambda ARN or SNS ARN (required & must be in the same region)

• Input data parameters (optional)


EG :

Resources:
  MyCustomResourceUsingLambda:
   Type: Custom::MylambdaResource
   properties:
      ServiceToken: arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME
      #input values (optional)
      ExamplePreperty: "exampleValue"




------------------------------- Use Case – Delete content from an S3 bucket


• You can’t delete a non-empty S3 bucket

• To delete a non-empty S3 bucket, you must first delete all the objects inside it

• We can use a custom resource to empty an S3 bucket before it gets deleted by CloudFormation


EG : 

-- Whenever we run delete stack on CloudFormation, then your custom resource backed by Lambda function is going to run API calls to empty your S3 bucket.

-- when your S3 bucket is emptied, then CloudFormation is going to actually attempt to delete your S3 bucket, and everything will work.





---------------- LAB 

-- u have 2 options to delete non-empty s3 bucket using cloudformation





IMP : if you are getting error while uploading the yaml file thorugh the local system , then try to upload in the s3 bucket copy the https://.... url of object and paste in the CF console



Option -1 



-- in this demo , we are going to create lambda function , s3 bucket , lambdaexecutionrole, and custom resource 

-- as per the documentation we can not delete the s3 bucket directly which has objects in it , but through the custom resources with lambda function , we can delete the s3 bucket directly

-- create one yaml files which will create an s3 bucket , CustomResourceLambdaFunction , CustomResourceLambdaExecutionRole , CustomResource



EG :  delete-resource.yaml


----
Resources:

    CustomResourceLambdaExecutionRole:
        Type: 'AWS::IAM::Role'
        Properties:
            AssumeRolePolicyDocument:
                Version: 2012-10-17
                Statement:
                    - Effect: Allow
                      Principal:
                          Service: lambda.amazonaws.com
                      Action:
                          - 'sts:AssumeRole'
            Policies:
                - PolicyName: LoggingPolicy
                  PolicyDocument:
                      Version: 2012-10-17
                      Statement:
                          - Effect: Allow
                            Action:
                                - logs:CreateLogGroup
                                - logs:CreateLogStream
                                - logs:PutLogEvents
                            Resource: '*'
                - PolicyName: S3Policy
                  PolicyDocument:
                      Version: 2012-10-17
                      Statement:
                          - Effect: Allow
                            Action:
                                - s3:List*
                                - s3:DeleteObject
                            Resource: '*'

    CustomResourceLambdaFunction:
        Type: 'AWS::Lambda::Function'
        Properties:
            Code:
                ZipFile: |
                    import cfnresponse
                    import boto3

                    def handler(event, context):
                        print(event)
                        print('boto version ' + boto3.__version__)

                        # Globals
                        responseData = {}
                        ResponseStatus = cfnresponse.SUCCESS
                        s3bucketName = event['ResourceProperties']['s3bucketName']

                        if event['RequestType'] == 'Create':
                            responseData['Message'] = "Resource creation successful!"

                        elif event['RequestType'] == 'Update':
                            responseData['Message'] = "Resource update successful!"

                        elif event['RequestType'] == 'Delete':
                            # Need to empty the S3 bucket before it is deleted
                            s3 = boto3.resource('s3')
                            bucket = s3.Bucket(s3bucketName)
                            bucket.objects.all().delete()

                            responseData['Message'] = "Resource deletion successful!"

                        cfnresponse.send(event, context, ResponseStatus, responseData)

            Handler: index.handler
            Runtime: python3.12
            Role: !GetAtt CustomResourceLambdaExecutionRole.Arn

    CustomResource:
        Type: Custom::CustomResource
        Properties:
            ServiceToken: !GetAtt CustomResourceLambdaFunction.Arn
            s3bucketName: !Ref S3Bucket
        DependsOn: S3Bucket

    S3Bucket:
      Type: AWS::S3::Bucket
      Properties:
        AccessControl: Private
        BucketName: custom-resource-s3-bucket1


-- once u upload this file this will create s3 bucket , CustomResourceLambdaFunction , CustomResourceLambdaExecutionRole , CustomResource 

-- now go to bucket and upoad some files in it , now do dleete the stack u can able to delete the s3 bucket (in this process , the custom resource will delete first and then remaing will delete after this )



-- suppose u have s3 bucket already and have files in it so , u have to change 2 items in the 'CustomResource' resource:

     1 - Change the `DependsOn` property to the logical name of the S3 bucket in your template

     2  - change the parameter passed to the custom resource to reference your S3 bucket.  So you should change this line:

                `s3bucketName: !Ref S3Bucket`

        - to this:

                `s3bucketName: !Ref <your s3 bucket logical ID>`


--  DependsOn: < logical name of ur bucket >

--  s3bucketName: !Ref <your s3 bucket logical ID>

-- this is how u can automate the process


-- it will take almost 30 min to give status 

-- once u delete the stack , u will 2 cases 

1 Delete a custom resource that's stuck in DELETE_FAILED status

2 Delete a custom resource that's stuck in DELETE_IN_PROGRESS status




1 Delete a custom resource that's stuck in DELETE_FAILED status


-- To delete your stack, complete the following steps:

1.    Open the CloudFormation console.

2.    Choose the stack that contains your custom resource that's stuck in DELETE_FAILED status.

3.    Choose Actions, and then choose Delete Stack.

4.    In the pop-up window that provides a list of resources to retain, choose the custom resource that's stuck in DELETE_FAILED status. Then, choose Delete.

5.    Choose Actions, and then choose Delete Stack.

The status of your stack changes to DELETE_COMPLETE.

Note: Your custom resource isn't a physical resource, so you don't have to clean up your custom resource after stack deletion.




2 Delete a custom resource that's stuck in DELETE_IN_PROGRESS status


-- To force the stack to delete, you must manually send a SUCCESS signal. The signal requires the ResponseURL and RequestId values, which are both included in the event that's sent from CloudFormation to Lambda.

-- open lambda --> monitoring --> choose log it has values

Received event: {
  "RequestType": "Delete",
  "ServiceToken": "arn:aws:lambda:us-east-1:111122223333:function:awsexamplelambdafunction",
  "ResponseURL": "https://cloudformation-custom-resource-response-useast1.s3.us-east-1.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A111122223333%3Astack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2%7CMyCustomResource%7Ce2fc8f5c-0391-4a65-a645-7c695646739?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170313T0212304Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=QWERTYUIOLASDFGBHNZCV%2F20190415%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=dgvg36bh23mk44nj454bjb54689bg43r8v011uerehiubrjrug5689ghg94hb",
  "StackId": "arn:aws:cloudformation:us-east-1:111122223333:stack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2",
  "RequestId": "e2fc8f5c-0391-4a65-a645-7c695646739",
  "LogicalResourceId": "MyCustomResource",
  "PhysicalResourceId": "test-MyCustomResource-1URTEVUHSKSKDFF",
  "ResourceType": "Custom::PingTester"

}


--  To send a SUCCESS response signal in the response object to the delete request, run the following command in your local command-line interface. Be sure to include the values that you copied from above values 

curl -H 'Content-Type: ''' -X PUT -d '{
    "Status": "SUCCESS",
    "PhysicalResourceId": "test-CloudWatchtrigger-1URTEVUHSKSKDFF",
    "StackId": "arn:aws:cloudformation:us-east-1:111122223333:stack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2
  ",
    "RequestId": "e2fc8f5c-0391-4a65-a645-7c695646739",
    "LogicalResourceId": "CloudWatchtrigger"
  }' 'https://cloudformation-custom-resource-response-useast1.s3.us-east-1.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A111122223333%3Astack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2%7CMyCustomResource%7Ce2fc8f5c-0391-4a65-a645-7c695646739?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170313T0212304Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=QWERTYUIOLASDFGBHNZCV%2F20190415%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=dgvg36bh23mk44nj454bjb54689bg43r8v011uerehiubrjrug5689ghg94hb
  '



-- u will able to delete successfully 




Optione -2 


-- Go to AWS Console > CloudFormation.

-- Deploy this cloud formation template to create the required role with necessary policy.



AWSTemplateFormatVersion: "2010-09-09"
Description: >
  This CloudFormation template creates a role assumed by CloudFormation
  during CRUDL operations to mutate resources on behalf of the customer.

Resources:
  ExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      MaxSessionDuration: 8400
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: resources.cloudformation.amazonaws.com
            Action: sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: ResourceTypePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                - "s3:DeleteObject"
                - "s3:GetBucketTagging"
                - "s3:ListBucket"
                - "s3:ListBucketVersions"
                - "s3:PutBucketTagging"
                - "cloudformation:ListExports"
                Resource: "*"
Outputs:
  ExecutionRoleArn:
    Value:
      Fn::GetAtt: ExecutionRole.Arn





-- Once the stack is successfully created, copy the ARN of the IAM role created.


Activate the Extension:

    - Go to AWS Console > CloudFormation
    - On the left menu select ‘Public extensions’.
    - Choose ‘Third party’ under the publisher.
    - Search for ‘AwsCommunity::S3::DeleteBucketContents’ and select it click on activate.
    - Under the execution role, provide the ARN of the role created above.
    - Click ‘Activate’.

-- You are now ready to use this extension in your cloudformation template.    


-- As a final step, update your bucket creation template to look like below:


Resources:

  MyS3Bucket:

    Type: AWS::S3::Bucket

    Properties:

      BucketName: MyBucketName

  Deleter:

    Type: AwsCommunity::S3::DeleteBucketContents

    Properties:

      BucketName: !Ref MyS3Bucket


-- Now whenever you delete the stack, the bucket is automatically emptied and then deleted by cloud formation without any hassle !








--------------------------------------------------------------------  CloudFormation – StackSets


• Create, update, or delete stacks across multiple accounts and regions with a single operation/template

• Target accounts to create, update, delete stack instances from StackSets

• When you update a stack set, all associated stack instances are updated throughout all accounts and regions

• Can be applied into all accounts of an AWS Organization

• Only Administrator account (or Delegated Administrator) can create StackSets


        https://www.youtube.com/watch?v=SC6o6FnVt-M







======================================================================= AWS Integration & Messaging =======================================================================

SQS, SNS & Kinesis




---------------------------- Section Introduction


• When we start deploying multiple applications, they will inevitably need to communicate with one another

• There are two patterns of application communication

     1) Synchronous communications  (application to application) 

     2) Asynchronous / Event based  (application to queue to application)


• Synchronous between applications can be problematic if there are sudden spikes of traffic

• What if you need to suddenly encode 1000 videos but usually it’s 10?


• In that case, it’s better to decouple your applications,

   • using SQS: queue model
   • using SNS: pub/sub model
   • using Kinesis: real-time streaming model


• These services can scale independently from our application!





---------------------------------------------------------------------- 1 Amazon SQS What’s a queue?


-- SQS is a simple queuing service.

-- So we have an SQS queue, and it is going to contain messages. And to contain messages, well, something needs to send messages into our SQS queue and whatever sends a message into our SQS queue is called a producer.

-- So it's possible for us to have one producer, but also have more. You can have multiple producers sending many messages into an SQS queue. And the message could be whatever he wants. For example, it could be process this order, or process this video. Whatever message you create goes into the queue.

-- Then something needs to process the messages from the queue and receive them, and it's called consumers.

-- So consumers will poll the messages from the queue, so that means that they will ask the queue, do you have any message for me? And the queue says, yes, here it is. And the consumer will poll these messages and get some information.

-- And then with that message, it will process it and then delete it back from the queue. And you may have multiple consumers consuming messages from an SQS queue.

-- So a queuing service is here to be a buffer to decouple between your producers and your consumers.



               Producer -----------------(send messagaes)-----------------> SQS Queue -----------------(Poll messages)---------------> Consumer




--------------------------------------- Amazon SQS – Standard Queue



• Oldest offering (over 10 years old)

• Fully managed service, used to decouple applications


• Attributes:

     • Unlimited throughput, unlimited number of messages in queue

     • Default retention of messages: 4 days, maximum of 14 days

     • Low latency (<10 ms on publish and receive)

     • Limitation of 256KB per message sent


• Can have duplicate messages (at least once delivery, occasionally)

• Can have out of order messages (best effort ordering)





--------------------------------------- SQS – Producing Messages


-- So, messages that are up to 256 kilobytes are sent into SQS by producers. how does this happen?

-- Well, the producers will send the messages to SQS using an SDK, software development kits. And the API to send a message to SQS is called SendMessage API.

• Produced to SQS using the SDK (SendMessage API)

• The message, it will be written, it will be is persisted in SQS until a consumer deletes it , which signifies that the message has been processed.

• Message retention: default 4 days, up to 14 days


• Example: send an order to be processed

    • Order id
    • Customer id
    • Any attributes you want


-- so you will send a message into the SQS queue with maybe some information, such as the order ID, the customer ID, and any attributes you may want. For example, the address, and so on.

-- And then your consumer, that is in application rights, will have to deal with that message itself.

• SQS standard: unlimited throughput




--------------------------------------- SQS – Consuming Messages


• Consumers (running on EC2 instances, servers, or AWS Lambda)...

• Poll SQS for messages (receive up to 10 messages at a time)

• Process the messages (example: insert the message into an RDS database)

• Delete the messages using the DeleteMessage API



 


--------------------------------------- SQS – Multiple EC2 Instances Consumers


• Consumers receive and process messages in parallel

--  so each consumer will receive a different set of messages by calling the poll function.

-- if somehow a message is not processed fast enough by a consumer, it will be received by other consumers, and so this is why we have at least once delivery.

• At least once delivery

• Best-effort message ordering

• Consumers delete messages after processing them

     -- when the consumers are done with the messages, they will have to delete them, otherwise, other consumers will see these messages.

-- if we need to increase the throughputs because we have more messages, then we can add consumers and do horizontal scaling to improve the throughput of processing.

• We can scale consumers horizontally to improve throughput of processing




--------------------------------------- SQS with Auto Scaling Group (ASG)


-- Well, that means that your consumers will be running on EC2 instances inside of an Auto Scaling group and they will be polling for messages from the SQS queue.

-- But now your Auto Scaling group has to be scaling on some kind of metric, and a metric that is available to us is the Queue Length. It's called "ApproximateNumberOfMessages."

-- It is a CloudWatch Metric that's available in any SQS queue. And we could set up an alarm, such as whenever the queue length go over a certain level, then please set up a CloudWatch Alarm, and this alarm should increase the capacity of my Auto Scaling group by X amount.

-- this will guarantee that the more messages you have in your SQS queue, maybe because there's a surge of orders on your websites, the more EC2 instances will be provided by your Auto Scaling group, and you will accordingly process these messages at a higher throughputs.





--------------------------------------- SQS to decouple between application tiers


-- the use case is to decouple between applications, so application tiers.

-- So, for example, let's take an example of an application that processes videos. 

-- We could have just one big application that's called a front-end that will take the request and whenever a video needs to be processed, it will do the processing and then insert that into an S3 bucket.

-- But the problem is that processing may be very, very long to do and it may just slow down your websites if you do this in the front-end here.

-- So instead, you can decouple your application here the request of processing a file and the actual processing of a file can happen in two different applications.

-- therefore, whenever you take a request to process a file, you will send a message into an SQS queue.

-- Now, when you do the request to process, that file will be in the SQS queue and you can create a second processing tier called the back-end processing application that will be in its own Auto-Scaling group to receive these messages, process these videos, and insert them into an S3 bucket.

-- So as we can see here with this architecture, we can scale the front-end accordingly, and we can scale the back-end accordingly as well, but independently.

-- because the SQS queue has unlimited throughputs and it has unlimited number of messages in terms of the queue, then you are really safe, and this is a robust and scalable type of architecture.




   requests ------------ fronted ---------- (SendMessage)----------> SQS Queue -----------(ReceiveMessages) ----------> Back-end processing Application(video processing) -------------(insert) -----> S3





--------------------------------------- Amazon SQS - Security


• Encryption:

    • So we have encryption in-flight encryption by sending and producing messages  HTTPS API

    • At-rest encryption using KMS keys

    • Client-side encryption if the client wants to perform encryption/decryption itself (It's not something that's supported by SQS)


• Access Controls: IAM policies to regulate access to the SQS API

• SQS Access Policies (similar to S3 bucket policies)

     • Useful for cross-account access to SQS queues

     • Useful for allowing other services (SNS, S3...) to write to an SQS queue




--------------------------------------- LAB for SQS standard queue


-- refer to Solution Architect course




-----------------------------------------------------  SQS Queue Access Policy


-- SQS Queue Access Policies. And there are two good use cases for SQS Queue Access Policies.

-- They're similar to S3 Bucket policies in terms that they are resource policies, so JSON IAM policies that you're going to add directly onto your SQS Queue.


----------------- 1st use case : Cross Account Access




Account 444455556666 (SQS Queue (queue1))---------------(poll for messages)-------> Account 111122223333(ec2 insances)




-- Say you have a queue in an account and another account needs to access that queue. Maybe it has an EC2 Instance.

-- So for that EC2 Instance to be able to pull message across accounts, what you need to do is to create a Queue Access Policy look like this , and you attach it to the SQS Queue in the first account.


{
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_Send_Receive",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "111122223333"
         ]
      },
      "Action": [
         "sqs:SendMessage",
         "sqs:ReceiveMessage"
      ],
      "Resource": "arn:aws:sqs:*:444455556666:queue1"
   }]
}


-- What this Queue access policy will do 

-- it will allow the principle of AWS to be 111122223333, which represents the account on the right hand side on the sqs ReceiveMessage on this resource right here.

-- so this Queue Access Policy is really what will allow your EC2 Instance to pull from the SQS Queue in another account.





----------------------- 2nd use case : Publish S3 Event Notifications To SQS Queue


-- for example, when you have an S3 Bucket, and it will publish event notifications to an SQS Queue.

-- for example, you upload an object into an S3 Bucket, and what you want is to get automatically a message sent to the SQS Queue.



      Upload object ------------------>  S3 Bucket (bucket1) ---------(Send message)-------------> SQS Queue (queue1)


-- As you can see the SQS Queue, wIll need to give permission to the S3 Bucket to write a message to it.



{
"Version": "2012-10-17", "Statement" : [{
"Effect": "Allow",
"Principal": { "AWS": "*"},
"Action": [ "sqs:SendMessage" ],
"Resource": "arn:aws:sqs:us-east-1:444455556666:queue1", "Condilon": {
"ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:bucket1" },
"StringEquals": { "aws:SourceAccount": "<bucket1_owner_account_id>" }, }
}] }


-- look at the details for example, the action is sqs:SendMessage,

-- the condition is that the sourceArn of the bucket represents the S3 Bucket named bucket1, 

-- and that the source accounts needs to be the account owner of the S3 buckets.

-- So once you have this, then the S3 bucket is allowed to write to an SQS Queue.




---------------------------------------------- LAB for s3 event notifications 


-- create standard queue , IMP : encryption = disable

-- now create one s3 bucket --> properties --> create event notificatin for all prefix and suffix  --> choose destination is SQS queue , try to create event notificatin

              link : https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html#step1-create-sqs-queue-for-notification

-- here , u will get an Error "Unable to validate the following destination configurations"

-- to resolve this error , we need to modify it to allow our S3 Bucket to write into our SQS Queue.

-- go to queue --> Access policy --> c.o info (Access policy (Permissions) ) --> in the last right corner down side u will get documentation link --> open that Walkthrough: Configuring SNS or SQS 

-- the SQS queue will look like this 



{
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
        {
            "Sid": "example-statement-ID",
            "Effect": "Allow",
            "Principal": {
                "Service": "s3.amazonaws.com"
            },
            "Action": [
                "SQS:SendMessage"
            ],
            "Resource": "SQS-queue-ARN",
            "Condition": {
                "ArnLike": {
                    "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1"
                },
                "StringEquals": {
                    "aws:SourceAccount": "bucket-owner-account-id"
                }
            }
        }
    ]
}



-- paste this sqs queue policy in the sqs standard queue that u have created

-- u have modify some items in this policy like   "Resource": "SQS-queue-ARN",  "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1" ,  "aws:SourceAccount": "bucket-owner-account-id"

-- once u change these , save policy  , now go to s3 and try to create event notification , now u will able to create successfully

-- now do upload any file in the s3 and check in SQS , u will get details of that object 










----------------------------------------------  SQS – Message Visibility Timeout


-- we have a consumer doing a ReceiveMessage request, and therefore, a message will be returned from the queue. Now the visibility timeout begins.

• After a message is polled by a consumer, it becomes invisible to other consumers

• By default, the “message visibility timeout” is 30 seconds

-- That means that during these 30 seconds, the message has to be processed, if you do so, that means that if the same or other consumers do a message request API call, then the message will not be returned.

• After the message visibility timeout is over, the message is “visible” in SQS

-- So effectively, during the visibility timeout, that message is invisible to other consumers.

-- But after the visibility timeout is elapsed and if the message has not been deleted, then the message will be put back into the queue and therefore, another consumer or the same consumer doing a receive message API call will receive the message again, the same message as before.



IMP to know 

• If a message is not processed within the visibility timeout, it will be processed twice Because it will be received by two different consumers, or twice by the same consumer.

-- And so, if a consumer is actively processing a message but knows that it needs a bit more time to process the message because otherwise, it will go out of the visibility timeout window, there is an API called "ChangeMessageVisibility."

• A consumer could call the ChangeMessageVisibility API to get more time

-- so if a consumer knows that a message needs a bit more time to be processed and you don't want to process that message twice, then the consumer should call the ChangeMessageVisibility API to tell SQS, hey, do not make that message visible for now, I just need a bit more time to process this message.

-- how do you set this message visibility timeout ?
   
   • If visibility timeout is high (hours), and consumer crashes, re-processing will take time

   - if you set it to a really, really high value by default, say, hours, and the consumer crashes, then it will take hours until this message reappears, re-becomes visible in your SQS queue, and that will take a lot of time.

   • If visibility timeout is too low (seconds), we may get duplicates

   - If you set it to something really, really low, like a few seconds, what happens that if the consumer doesn't end up having enough time to process that message for whatever reason, then it will be read many times by different consumers and you may get duplicate processing.


-- So the idea is that the visibility timeout should be set to something reasonable for your application and your consumer should be programmed that if they know they need a bit more time, then they should call the ChangeMessageVisibility API to get more time and increase the timeout of that visibility window.





-------------- LAB to observe Visibility Timeout 

-- create one std queue, open duplicate tab for this same queue

-- open 2 windows , in 1st window send some message and go to 2nd window try to refresh and poll for message  ,

-- in 2nd window u can't get any messages 'coz it is in visibility timeout period , after 30 sec it will visible to U

-- the more u poll , the receive count will increase for same message 

--  u can also change visibility time , but default is better 





Question : You have an SQS Queue where each consumer polls 10 messages at a time and finishes processing them in 1 minute. After a while, you noticed that the same SQS messages are received by different consumers resulting in your messages being processed more than once. What should you do to resolve this issue?

ANS : increase Visibility Timeout

EXP : SQS Visibility Timeout is a period of time during which Amazon SQS prevents other consumers from receiving and processing the message again. In Visibility Timeout, a message is hidden only after it is consumed from the queue. Increasing the Visibility Timeout gives more time to the consumer to process the message and prevent duplicate reading of the message. (default: 30 sec., min.: 0 sec., max.: 12 hours)








---------------------------------------------- Amazon SQS – Dead Letter Queue (DLQ)


• If a consumer fails to process a message within the Visibility Timeout...the message goes back to the queue!

• We can set a threshold of how many times a message can go back to the queue

• After the "MaximumReceives" threshold is exceeded, the message goes into a Dead Letter Queue (DLQ)

• Useful for debugging!

• DLQ of a FIFO queue must also be a FIFO queue

• DLQ of a Standard queue must also be a Standard queue

• Make sure to process the messages in the DLQ before they expire:

      • Good to set a retention of 14 days in the DLQ





---------------------------------------------- SQS DLQ – Redrive to Source 


• Feature to help consume messages in the DLQ to understand what is wrong with them

-- So you have your messages now you know they haven't been processed in the source queue and therefore they are in the Dead Letter Queue and you're going to do a manual inspection and debugging of these messages.

-- then you're going to fix your consumer code understand why the message wasn't processed in case the message was correct.

-- And then what you can do is re drive that message from the dead letter queue into the source SQS queue

-- what's going to happen with this is that the consumer can now reprocess that message without even knowing that the message went into the Dead Letter Queue and then the message processing has happened and as a cool feature. 

• When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches without writing custom code






---------------------------------------------- LAB DLQ 


-- create one std queue and one dead letter queue with all default values 

-- now go to std queue --> change visibility time = 5 , and add DLQ , Maximum receives = 3 (keep below 5 ) , do save this queue

-- go to std queue --> send message --> poll message 

-- So here we're receiving the messages once and after five seconds, we will receive it a second time. And after, again, five seconds, we're going to receive a third time,

-- stop polling now , try to poll for messages again , u can not see any messages 

-- go to DLQ --> poll for messages --> u will see that message 

-- let me show you how to redrive that message from the DLQ into this first queue.

-- go to DLQ queue --> start DLQ redrive 

-- go to main Std queue --> start polling for messagaes , u will get message from the DLQ 






---------------------------------------------- Amazon SQS – Delay Queue


• Delay a message (consumers don’t see it immediately) up to 15 minutes

• Default is 0 seconds (message is available right away), That means that as soon as you send the message into an SQS queue, the message will be available right away to be read 

• Can set a default at queue level , to say all the messages should be delayed by X number of seconds

• Can override the default on send using the DelaySeconds parameter 

    - every time you send a message you can set a per-message delay if you wanted to using the DelaySeconds parameter.


-- SQS Delay Queues is a period of time during which Amazon SQS keeps new SQS messages invisible to consumers. In SQS Delay Queues, a message is hidden when it is first added to the queue. (default: 0 mins, max.: 15 mins)



Producer--------------------(Send messages)-----> SQS queue ----------(Poll messages) -------------> Consumer



----------------------------- LAB for Delay Queue


-- create new Delayqueue 

-- set Delivery delay = 10 sec , so that messages will wait 10 seconds before being read by a consumer.

-- create Queue , now type some message and poll first , now c.o send message , u will get message after 10 sec 

-- so, as you can see, there was a delay between the send and the actual delivery of that message.





-------------------------------------------------------------------------------  Certified Developer concepts


---------------------------------------------- 1 Amazon SQS - Long Polling


• When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue

• This is called Long Polling

-- So why do we do long polling ?

• LongPolling decreases the number of API calls made to SQS while increasing the efficiency and decreasing the latency of your application.

-- we do long polling because we are doing less API calls into the SQS queue. And on top of it, we know that as soon as the message will arrive in the SQS queue, then the SQS queue will send it back to the consumer. 

-- So we are increasing the efficiency, because we do less API calls, so less CPU cycles are used. And we are also decreasing the latency because as soon as a message will be received by your SQS queue, it will be received by your consumer.

• The wait time can be between 1 sec to 20 sec (20 sec preferable)

• Long Polling is preferable to Short Polling

• Long polling can be enabled at the queue level or at the API level using "ReceiveMessageWaitTimeSeconds"


--------important thing that you need to understand the difference is in short polling Amazon SQS sends the response right away even if the query found is no message 

------- long polling Amazon SQS asked you sends and empty response only if the polling wait time expires and that is why we use long polling if we want to save API call costs.





---------------------------------------------- 2 SQS Extended Client


• Message size limit is 256KB, how to send large messages, e.g. 1GB?


• Using the SQS Extended Client (Java Library) , which does something very simple that you could implement in any other language, but the idea is that it will use an Amazon S3 buckets as a repository for the large data.


IMP :


-- for example , your producer wants to send a large message into SQS, but first what's going to happen is that the actual large message will end up in Amazon S3,

-- what will be sent into your SQS queue is it will be a small metadata message that has a pointer to the larger message in your Amazon S3 buckets.

-- So the SQS queue will contain small messages, and your Amazon S3 bucket will contain large objects.

-- your consumer when it reads from the SQS queue using this library, the SQS extended clients, then it will consume this small metadata message,

-- which will say to the consumer, "Hey, go read "that bigger message out of Amazon S3," and the consumer will be able to read and retrieve large messages from S3.

-- So a typical use case for this is if you're processing video files, you don't send the entire video file into your SQS queue node, you upload that video file into your Amazon S3 bucket, 

-- and you send a small message with a pointer to that video file into your SQS queue. And that allows you to accommodate any message size really through this pattern.



----------- WORKFLOW Structure


        Producer -------> Small metadata message ------------> SQS Queue ------------> Small metadata message -----------> Consumer
           |                                                                                                                 |
           |                                                                                                                 |
           |                                                                                                                 |
           |                                                                                                                 |
            -------------(Send large message to S3)-----------> S3 Bucket ------------(Retrieve large message from S3)--------
   
                         



---------------------------------------------- 3 SQS – Must know API


-- we'll, you will maybe see some API calls given through by the exam. And so it's just normal API calls


• CreateQueue (MessageRetentionPeriod), DeleteQueue

    - So CreateQueue is used to create a queue, and you can use the argument "MessageRetentionPeriod" to set how long a message should be kept in a queue before being discarded.

    - DeleteQueue is used to delete a queue and delete all the messages in the queue at the same time.


• PurgeQueue: delete all the messages in queue 


• SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage

    - when we are sending messages, we use to SendMessage API, and if we want to send messages with a delay, we can use the DelaySeconds parameter.

    - ReceiveMessage is to do polling, and DeleteMessage is to delete a message once it has been processed by a consumer.

• MaxNumberOfMessages: default 1, max 10 (for ReceiveMessage API)

    - So when you receive a message, by default, the parameter MaxNumberOfMessages is set to 1. That means that you receive one message at a time, but you can receive up to 10 messages in a time in SQS. So you can set the max number of messages parameter for the ReceiveMessage API to 10 to receive a batch of messages from SQS.


• ReceiveMessageWaitTimeSeconds: Long Polling

    - telling your consumer how long to wait before getting a response for from the queue. And this is equivalent of enabling long polling. 
    

• ChangeMessageVisibility: change the message timeout

    - And the ChangeMessageVisibility, it is used to change the message timeout in case you need more time to process a message.


• if you want to use Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility this helps decrease the number of API calls you're doing into API and therefore helps decrease your costs



--------------------------- LAB for Long Polling in AWS 


-- go to std queue --> edit --> Receive message wait time = 20 (0 - 20 sec ) --> save 

-- this is saying that you should wait up to 20 seconds to receive a message if the queue is empty.

-- send and receive messagaes , here I'm gonna go ahead and start a consumer. Now this consumer is doing long polling, and so that means that only one API call is happening, and it's waiting for a message coming from the SQS queue because right now there is none.

-- But if I say say hello world and just press Send, as soon as I press Send, the message was received by my consumer.

-- This was extremely low latency, because my consumer was in long polling mode, and it was waiting for a message from SQS thanks to the WaitMessageTime setting that we set from before.





------------------------------------------------------------------------ Amazon SQS – FIFO Queue


• FIFO = First In First Out (ordering of messages in the queue)



    Producer --------(Send messages 4,3,2,1) ---------> SQS Queue -----------(Poll messages 4,3,2,1)------------> Consumer


-- Now, because we have such a constraint and guarantee about ordering, this SQS queue has limited throughput.

• Limited throughput: 300 msg/s without batching, 3000 msg/s with Batching

• Exactly-once send capability (by removing duplicates) 

• Messages are processed in order by the consumer

-- So FIFO queue is something you should see whenever you have decoupling, but also the need to maintain the ordering of messages, and also make sure that you are on this throughput's constraints that you are not sending too many messages into SQS,

-- the FIFO queue is ends with .fifo only while u create any FIFO queue 




---------------- LAB 


-- create our first FIFO Queue , with all default settings 

-- send message and observe the things 




---------------------------------------------- SQS FIFO – Deduplication


• De-duplication interval is 5 minutes

• Two de-duplication methods:

      • Content-based deduplication: will do a SHA-256 hash of the message body , And if the same message body is encountered twice, the same hash will be the same twice. And so therefore, the second message will be refused.

      • Explicitly provide a Message Deduplication ID directly while sending a message. And if the same deduplication ID is encountered twice then the message will be gone.



---------------------------------------------- SQS FIFO – Message Grouping


-- The second advanced concept we need to look at is message grouping.

• If you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order

• To get ordering at the level of a subset of messages, specify different values for MessageGroupID

      • The idea is that the Messages that share a common Message Group ID will be in order within the group

      • Each Group ID can have a different consumer ( So you can enable parallel processing!) on your SQS FIFO queue.


• Ordering across groups is not guaranteed


for example , And we're grouping messages into three groups: A, B, and C. 

-- Say we have message A1, A2, A3. Then we can have a consumer for the group A.

-- Then we have another group of messages: B1, B2, B3, B4. We can have another consumer for that group B.

--  for consumer group C , we have C1 and C2.

-- The idea is that, for example, maybe sometimes you don't need the total ordering of all the messages. But you want ordering of all the messages for a specific customer ID.

-- for that one specific customer ID, you can use this as your message group ID.
     
      

                   ----------------(A3, A2 , A1) --------------------> Consumer for Group “A”

      SQS FIFO     ----------------(b4, b3, b2 , b1) --------------------> Consumer for Group “B” 
                  
                   ----------------(c2 , c1) --------------------> Consumer for Group “C”



Question: You are running an SQS FIFO queue with 10 message groups (defined by MessageGroupID). What's the maximum number of consumers can consume simultaneously?

ANS: 10

EXP : You can have as many consumers as "MessageGroupID" for your SQS FIFO queues.





--------------------------- LAB 


-- go to FIFO queue --> edit --> enable contenct based deduplication --Save 

-- now try to send message with group ID , u willl receive message , now try to send message again u can't send 'coz the deduplication is occuring here , now change the contect with same group id , u will able to send the message 'coz we have changed our content 


IMP to know 

-- here during the polling period only u can able to delete the messages once it gets polled after u cannot delete in the FIFO queue(The receipt handle has expired.)

-- where u can delete in std queue

-- now try to delete during the polling time , u can able to delete the message 







---------------- the differences are , IMP to Know

1  Message Order

    -- Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they are sent. Occasionally (because of the highly-distributed architecture that allows high throughput), more than one copy of a message might be delivered out of order.

    -- FIFO queues offer first-in-first-out delivery and exactly-once processing: the order in which messages are sent and received is strictly preserved.

2  Delivery

    -- Standard queues guarantee that a message is delivered at least once and duplicates can be introduced into the queue.

    -- FIFO queues ensure a message is delivered exactly once and remains available until a consumer processes and deletes it; duplicates are not introduced into the queue.

3  Transactions Per Second (TPS)

    -- Standard queues allow nearly-unlimited number of transactions per second.

    -- FIFO queues allow to process up to 3000 messages per second per API action.

4  Regions

    -- Standard queues are available in all the regions.

    -- FIFO queues are currently available in limited regions only.

5  AWS Services Supported

    -- Standard Queues are supported by all AWS services.

    -- FIFO Queues are currently not supported by all AWS services like: CloudWatch Events, S3 Event Notifications, SNS Topic Subscriptions, Auto Scaling Lifecycle Hooks, AWS IoT Rule Actions, AWS Lambda Dead Letter Queues.






----------------------------------------------------------------- Amazon Simple Queue Service (Amazon SQS) temporary queues


-- Temporary queues help you save development time and deployment costs when using common message patterns such as request-response.

-- You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.

-- The following are the benefits of temporary queues:

   - They serve as lightweight communication channels for specific threads or processes.
   - They can be created and deleted without incurring additional costs.
   - They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.

-- To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client.

-- This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. 

-- The key concept behind the client is the virtual queue. 

-- Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. 

-- Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.





----------------------------------------------------------------- Amazon Simple Notification Service (SNS) --------------------------------------------------------------


• What if you want to send one message to many receivers?


Method 1 : Direct integration 

   - So you could have a direct integration where for example a buying service application could send an email notification, then send a message to a fraud service send a message to a shipping service and maybe even send a message into an SQS Queue.

   - This is cumbersome because every time you have to add a new receiving service. You need to create and write that integration.


Method 2 :  Pub / Sub (Publish-subscribe.)
    
   - The idea is that the buying service will send a message into an SNS topic which is publishing a message into a topic. And that topic will have many subscribers.

   - And each of the subscriber will be able to receive that message from the SNS topic and have it for their own.



---------------------------------------- Amazon SNS


• The “event producer” only sends message to one SNS topic

• As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications 

• Each subscriber to the topic will get all the messages (note: new feature to filter messages), except if you're using a feature to filter messages and it is possible as well.

• Up to 12,500,000 subscriptions per topic

• 100,000 topics limit , and you can increase that limit as well.


        SNS -----------------(publish)------------> SQS / Lambda / Kinesis Data Firehose / HTTP(S) Endpoints / Emails / SMS and Mobile Nonficanons 


Note: Kinesis Data Firehose is now supported, but not Kinesis Data Streams.




------------------------ SNS integrates with a lot of AWS services


• Many AWS services can send data directly to SNS for notifications

CloudWatch Alarms / AWS Budgets / Lambda / DynamoDB / S3 Bucket (Events) / Auto Scaling Group (Notifications) / RDS Events / AWS DMS (New Replic) / CloudFormation (State Changes) ------------(publish)--------> SNS



----------------------- Amazon SNS – How to publish


• Topic Publish (using the SDK)
  • Create a topic
  • Create a subscription (or many)
  • Publish to the topic

• Direct Publish (for mobile apps SDK)
  • Create a platform application
  • Create a platform endpoint
  • Publish to the platform endpoint
  • Works with Google GCM, Apple APNS, Amazon ADM...



----------------------  Amazon SNS – Security

• Encryption:
  • In-flight encryption using HTTPS API
  • At-rest encryption using KMS keys
  • Client-side encryption if the client wants to perform encryption/decryption itself

• Access Controls: IAM policies to regulate access to the SNS API

• SNS Access Policies (similar to S3 bucket policies)
  • Useful for cross-account access to SNS topics
  • Useful for allowing other services ( S3...) to write to an SNS topic 




-------------------------------- SNS + SQS: Fan Out


-- The idea is that you want a message to be sent to multiple SQS queues, but if you send them individually to every SQS queue, there can be problems associated with it.

-- For example, if your application crashes in between, if there are delivery failures, or if you add more SQS queues down the road. Therefore we want to use the fan-out pattern.

• The idea is that you're going to Push once in SNS, receive in all SQS queues that are subscribers

    - The idea is that you're going to push once an SNS topic and then you're going to subscribe as many SQS queues as you want to the SNS topic. These queues are subscribers and they will all receive the messages sent into SNS.

-- So for example, we have a buying service that wants to send messages to two SQS queues. What it will do instead, it will send one message into an SNS topic, 

-- and the queues are subscribers of that SNS topic so that the fraud service and the shipping service can read all messages from their own SQS queues.

• Fully decoupled, no data loss 

• SQS allows for: data persistence, delayed processing and retries of work

• Ability to add more SQS subscribers over time

• Make sure your SQS queue access policy allows for SNS to write

• Cross-Region Delivery: works with SQS Queues in other regions

    - we have cross-region delivery. So it's definitely possible for an SNS topic in one region to send messages to SQS queues in other regions if the security allows it.






------------------------------------------- Next, so how do we use this pattern for other purposes?



------------------------------------- 1 Application: S3 Events to multiple queues


• For the same combination of: event type (e.g. object create) and prefix (e.g. images/) you can only have one S3 Event rule

   - So there is a limitation of S3 event rules is that for a combination of event type, for example, an object is being created and a prefix, for example, images/, you can only have one S3 event rule.


----------- But what if you want to send the same S3 event notification to multiple SQS queues?


• If you want to send the same S3 event to many SQS queues, use fan-out

    

       S3 object created -----------(events)----------> A S3 ----------------> SNS Topic -----(FAN-OUT)------> SQS Queues / Lambda function 




------------------------------------- 2 Application: SNS to Amazon S3 through Kinesis Data Firehose


-- Another architecture is that you can send data directly from SNS to Amazon S3 through Kinesis Data Firehose.

-- So because SNS has a direct integration with KDF

• SNS can send to Kinesis and therefore we can have the following solutions architecture:

     

          Buying Service ----------------------> SNS Topic --------------- Kinesis Data Firehose ------------- Amazon S3 / Any supported KDF Destination





------------------------------------- 3 Amazon SNS – FIFO Topic


-- So we can apply the fan-out pattern to FIFO topics as well.

• FIFO = First In First Out (ordering of messages in the topic)

 
     Producer -----------(Send messages 4,3,2,1)-------------- SNS TOPIC --------------(Receive messages 4,3,2,1)-----------------> Subscribers SQS FIFO


• Similar features as SQS FIFO:

     • Ordering by Message Group ID (all messages in the same group are ordered)

     • Deduplication using a Deduplication ID or Content Based Deduplication


• Can have SQS Standard and FIFO queues as subscribers

• Limited throughput (same throughput as SQS FIFO)




----------------- SNS FIFO + SQS FIFO: Fan Out


• In case you need fan out + ordering + deduplication







------------------------------------- SNS – Message Filtering


• JSON policy used to filter messages sent to SNS topic’s subscriptions

• If a subscription doesn’t have a filter policy, it receives every message





---------------------------------------------------------- Kinesis Overview -------------------------------------------------


• Makes it easy to collect, process, and analyze streaming data in real-time

• Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data...

-- As long as the data is generated fast and in real time, this counts as a real time stream of data.



------------------ So there are four services that comprise Kinesis.


• Kinesis Data Streams: capture, process, and store data streams

• Kinesis Data Firehose: load data streams into AWS data stores

• Kinesis Data Analytics: analyze data streams with SQL or Apache Flink

• Kinesis Video Streams: capture, process, and store video streams




-------------------------------- 1 Kinesis Data Streams


-- Kinesis Data Streams is a way for you to stream big data in your systems.

-- So a Kinesis Data Stream is made of multiple shards, and shards are numbered. Number one, number two, all the way to number N.

-- And this is something you have to provision ahead of time. So when you start with Kinesis Data Streams, you're saying, hey, I want a stream with six shards. And so the data is going to be split across all the shards.

-- And the shards are going to be defining your stream capacity in terms of ingestion and consumption rates.

-- then we have producers. So producers send data into Kinesis Data Streams, and producers can be manyfold. like they could be Applications / Client / Kinesis Producer Library (KPL)/ Kinesis Agent

-- So all these producers do the exact same thing. They rely on the SDK at a very, very low level, and they're going to produce records into our Kinesis Data Stream.



--- RECORD : So a record, at its fundamental, is made of two things, it's made of a partition key and it is made of the data blob, or the value that is up to 1MB.

        - So the partition key will define and help determine in which shard will the record go to.       

        - And the data blob is the value itself. (Up to 1 MB)

        - So when you have the producers sending data to Kinesis Data Streams, they can send data at a rate of one megabytes per second, or a thousand messages per second, per shard.
    
        - So if you have six shards, you get six megabytes per second, or 6,000 messages per second, overall,



-- Now, once the data is in Kinesis Data Streams, it can be consumed by many consumers, and these consumers, again, can have many forms like Apps (KCL, SDK) / Lambda / Kinesis Data Firehose / Kinesis Data Analytics


-- So when the consumer receives a record, it receives, again, the partition key, also a sequence number which represents where the record was in the shard, as well as the data blob, so the data itself.


--- Now we have different consumption modes for Kinesis Data Streams.

     - We have two megabytes per second of throughput shared for all the consumers, per shard,

                                 OR

     - you get two megabytes per second, per shard, per consumer if you are enabling the enhanced consumer mode, the enhanced fan-out.



-- So again, producers send data to Kinesis Data Streams. It stays in there for a while, and then it is read by many different consumers.




WORKFLOW : 


  Producers --------------------------------------------(Record , 1 MB/sec or 1000 msg/sec per shard) -----------> KDS (it has shards)-----------------(Record, 2 MB/sec (shared) Per shard all consumers or 2 MB/sec (enhanced) Per shard per consumer)------------------> consumers (Apps (KCL, SDK) / Lambda / Kinesis Data Firehose / Kinesis Data Analytics) 

   Applications / Client                                   Partition Key and Data Blob (upto1MB)                                                                       Partition Key , Sequence no.,  Data Blob 
   / Kinesis Producer Library (KPL)                                                                                                                                            
   / Kinesis Agent     



Question : You have a Kinesis data stream with 6 shards provisioned. This data stream usually receiving 5 MB/s of data and sending out 8 MB/s. Occasionally, your traffic spikes up to 2x and you get a ProvisionedThroughputExceededException exception. What should you do to resolve the issue?

ANS : Add more shards

EXP : The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity.





---------------------------------------------------------------- some properties of Kinesis Data Streams.


• Retention between 1 day to 365 days

• Ability to reprocess (replay) data

• Once data is inserted in Kinesis, it can’t be deleted (immutability)

• Data that shares the same partition goes to the same shard (ordering)

• Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent

• Consumers:

     • Write your own: Kinesis Client Library (KCL), AWS SDK

     • Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics,




---------------------------------------------------------------- Kinesis Data Streams – Capacity Modes


• Provisioned mode:
  • You choose the number of shards provisioned, scale manually or using API
  • Each shard gets 1MB/s in (or 1000 records per second)
  • Each shard gets 2MB/s out (classic or enhanced fan-out consumer) 
  • You pay per shard provisioned per hour

• On-demand mode:
  • No need to provision or manage the capacity
  • Default capacity provisioned (4 MB/s in or 4000 records per second)
  • Scales automatically based on observed throughput peak during the last 30 days
  • Pay per stream per hour & data in/out per GB




---------------------------------------------------------------- Kinesis Data Streams Security 


-- it is deployed within a region. And so you have your shards.

• Control access / authorization using IAM policies

• Encryption in flight using HTTPS endpoints

• Encryption at rest using KMS

• You can implement encryption/decryption of data on client side (harder)

• VPC Endpoints available for Kinesis to access within VPC

• Monitor API calls using CloudTrail




---------------------------------------------------------------- Kinesis Producers 


• Puts data records into data streams

• Data record consists of:

     • Sequence number (unique per partition-key within shard)
     • Partition key (must specify while put records into stream)
     • Data blob (up to 1 MB)

• Producers:

     • AWS SDK: simple producer
     • Kinesis Producer Library (KPL): C++, Java, batch, compression, retries
     • Kinesis Agent: monitor log files


• Write throughput: 1 MB/sec or 1000 records/sec per shard

• PutRecord API : the API to send data into Kinesis is called the PutRecord API.

• Use batching with PutRecords API to reduce costs & increase throughput 



IMP to Know :


-- while you produce to Kinesis data streams with a partition key.

-- if one device is very chatty and sends a lot of data it may overwhelm a shard. Also, you need to make sure your partition key is very well distributed to avoid the concept of a hot partition,

-- because then you will have one shard that's going to have more throughputs than the others and they will bring some imbalance.

-- So you need to think about a way to have a distributed partition key.

-- For example, if you have six shards and 10,000 users the user ID is very distributed.

-- But if you have six shards and you just look at Chrome, Firefox and Safari as web browser and the name of the web browser as a partition key, 

-- then it's going to be very hot maybe for Chrome, because there are many, many Chrome users in the world versus Firefox or Safari.


        Use highly distributed partition key to avoid “hot partition”




-------------- Kinesis - ProvisionedThroughputExceeded


-- So when we produce to Kinesis data streams from our applications, we know that we can produce 1 megabyte per second or 1,000 records per second and so as long as we do so it goes well.

-- But if we start over-producing into a shard we're going to get an exception because we are going over the provision throughput.

-- So we get the ProvisionedThroughputExceeded exception.


-- the solution for this exception is 

      • Use highly distributed partition key

      • Retries with exponential backoff , to ensure that we can retry these exceptions

      • Increase shards (scaling) , you need to scale the shard, maybe it's called shard-splitting to split a shard into and augment the throughputs , by increasing the number of shards this can help address throughput exceptions 






---------------------------------------------------------------- Kinesis Data Streams Consumers


• Get data records from data streams and process them

-- consumers can be 

• AWS Lambda
• Kinesis Data Analytics
• Kinesis Data Firehose
• Custom Consumer (AWS SDK) – Classic or Enhanced Fan-Out
• Kinesis Client Library (KCL): library to simplify reading from data stream




---------------------------------------------------------------- Kinesis Consumers – Custom Consumer



(look picture in pdf for better understanding )



1 Shared (Classic) Fan-out Consumer  (Pull Model)


-- So when you write your consumer in the classic shared throughput way you have your Kinesis Data Stream with a lot of shards and you get two megabytes per second per shard across all consumers.

-- That means that if you look at shard, number one just for this example we can write a Consumer Application A that is going to issue a get records API call to get records from the shard one, 

-- but as possible for us to have many different applications reading from the same Kinesis Data Stream.

-- So consumer Application B will also issue get records API calls, 

-- the Consumer Application C will also issue get records across all consumers.

-- Now, what happens in this instance is that they are all sharing two megabytes per second per shard across all consumers.

-- That means that in this instance we have three consumers sharing, two megabytes per second. That means that each consumer can get a maximum of approximately 666 kilobytes per second of data.

-- So as you can see, there's a limit to how many consumers we can have in the more consumer applications we add onto Kinesis Data Stream. The more we're put limitation we're going to have.




2  Enhanced Fan-out Consumer (Push Model)


-- So in this case we get two megabytes per second, per consumer, per shard. So it's not across all consumers it's per consumer, per shard.

-- So that means that the Consumer Application A will use a new API code called SubscribeToShard() and this will make the shard, send the data push the data into our Consumer Application A at the rate of two megabytes per second.

-- If the second Consumer Application B issues another subscribed to shard then this Consumer Application B as well. Will get the data pushed by the shard into the Consumer Application at the rate of two megabytes per second.

-- So as we can see here we have three consumer applications and we get six megabytes per second of throughputs for this one shard.





----------------------------------------------------------- Kinesis Consumers Types Difference





Shared (Classic) Fan-out Consumer - pull                                                    Enhanced Fan-out Consumer - push


• Low number of consuming applications                                                 • Multiple consuming applications for the same stream

• Read throughput: 2 MB/sec per shard across all consumers                             • 2 MB/sec per consumer per shard

• Max. 5 GetRecords API calls/sec  per shard                                                   

• Latency ~200 ms                                                                      • Latency ~70 ms

• Minimize cost ($)                                                                    • Higher costs ($$$)

• Consumers poll data from Kinesis using GetRecords API call                           • Kinesis pushes data to consumers over HTTP/2 (SubscribeToShard API)

• Returns up to 10 MB (then throttle for 5 seconds) or up to 10000 records             • Soft limit of 5 consumer applications (KCL) per data stream (default)





----------------------------------------------------------- Kinesis Consumers – AWS Lambda


-- it is a way for you to consume data without using servers.

-- So we have Kinesis Data Stream and say has three shards. so we are going to have lambda functions and their role will be to process records and save the record into dynamodb which is a serverless database.

-- So the Lambda functions are going to call get batch onto the Kinesis Data Stream.

-- And the data is going to be sent to lambda the functions by partition key to be processed.

-- The Lambda functions can then send data into dynamodb and we have a way to process our Kinesis Data Stream using a serverless mechanism.


• Supports Classic & Enhanced fan-out consumers

• Read records in batches

• Can configure batch size and batch window

• If error occurs, Lambda retries until succeeds or data expired

• Can process up to 10 batches per shard simultaneously





------------------------------------------------------------- LAB for Kinesis Data Streams 


#!/bin/bash

# get the AWS CLI version
aws --version

# PRODUCER

# CLI v2
aws kinesis put-record --stream-name test --partition-key user1 --data "user signup" --cli-binary-format raw-in-base64-out

# CLI v1
aws kinesis put-record --stream-name test --partition-key user1 --data "user signup"


# CONSUMER 

# describe the stream
aws kinesis describe-stream --stream-name test

# Consume some data
aws kinesis get-shard-iterator --stream-name test --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON

aws kinesis get-records --shard-iterator <>





-- open Kinesis --> choose data streams --> Data stream capacity  = provisioned (1 shard is enough for demo ) ----> create stream

-- in the data streams section , u will able to see producers and consumers and monitoring also 

-- let's keep it simple. We just want to write and read from our stream,

-- so therefore, we're going to use the SDK for producing and for consuming.

-- to do so , open cli in AWS Console 

-- As an alternative, you could be using your own terminal or CLI if it's preconfigured,

-- we have follow some commands to do this LAB 

-- 1st command 

          aws --version

-- u will get version of ur AWS CLI

-- 2nd cmd 

          aws kinesis put-record --stream-name test --partition-key user1 --data "user signup" --cli-binary-format raw-in-base64-out


-- here instead of test (replace ur data stream name ) , press enter , u will get 


-- {
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49651005991449160444837921106981744793678338524213936130"
}


-- u will get this ID's of shard and sequence

-- do same for login and logout messages 

--  aws kinesis put-record --stream-name mystream --partition-key user1 --data "user login" --cli-binary-format raw-in-base64-out


--  aws kinesis put-record --stream-name mystream --partition-key user1 --data "user logout" --cli-binary-format raw-in-base64-out



-- if you waited a little bit, and went into monitoring, and look at the stream metrics,

-- So next we want to be able to consume from Kinesis Data Stream.

-- So to do so, we're going to first describe the stream, to get some information around what this stream is made of, because we need to be able to consume from a specific shard.

--  3 rd cmd # describe the stream 
 
         aws kinesis describe-stream --stream-name test

-- change test , put ur datastream name 

-- as you can see here, we have this StreamDescription. 

-- We have one shard called shardId-0000000000000, and so we need to keep this in our mind. To be able to read from this stream.

IMP : 

-- so when you use the CLI, the SDK at a very, very low level, you need to specify from which shard you are reading from.

-- But if you are using Kinesis Client Library, all of this is handled for you by the library itself.

-- But we are using the CLI, so we have to specify the shard ID.




-- 4th cmd  # Consume some data

        aws kinesis get-shard-iterator --stream-name test --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON


-- "ShardIterator": "AAAAAAAAAAGSYiojmBSCJQpNaQz0qWSwWlj8yGA5j6QASza3PdcI4rAGHwggraCdkYpPqT47zlgorq92fYLSsrhfQuckhtteKHuzZ6O1AYq9P5G1dOyh4Xd4CDxEecYe0YZBiYlrR3/ySB1xt2IkjHIibWcyvyaQBrzHexR0d5jj8sY0dpSPkYUh4YlBkSodUSoIFqbywWwJQRmn1eqSKjZWEIExBGtIWF1WKkNVTB1qZKlUrarQpg=="

-- u will get like this 


-- here , we have the shard-iterator-type is TRIM-HORIZON.this means that you're going to read from the very beginning of the stream, so it will read all the records that were sent for from the beginning.

-- this ShardIterator can be reused to consume records,


-- 5th cmd 

       aws kinesis get-records --shard-iterator <"ShardIterator" content>


eg :  aws kinesis get-records --shard-iterator "AAAAAAAAAAGSYiojmBSCJQpNaQz0qWSwWlj8yGA5j6QASza3PdcI4rAGHwggraCdkYpPqT47zlgorq92fYLSsrhfQuckhtteKHuzZ6O1AYq9P5G1dOyh4Xd4CDxEecYe0YZBiYlrR3/ySB1xt2IkjHIibWcyvyaQBrzHexR0d5jj8sY0dpSPkYUh4YlBkSodUSoIFqbywWwJQRmn1eqSKjZWEIExBGtIWF1WKkNVTB1qZKlUrarQpg=="


-- So this consumption mode I'm doing right now, by using the low level API, describing the stream, getting a ShardIterator, and getting records is using the shared consumption mode.

-- This is not using enhanced Fan-Out,which in my opinion, should be using the Kinesis Client Library, Consumer Library for you to really leverage, and have a nice API to do so.


-- once u enter this u will get all the records like this 


  "Records": [
        {
            "SequenceNumber": "49651005991449160444837921106981744793678338524213936130",
            "ApproximateArrivalTimestamp": "2024-04-11T02:57:35.479000+00:00",
            "Data": "dXNlciBzaWdudXA=",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921106982953719497975830815965186",
            "ApproximateArrivalTimestamp": "2024-04-11T03:03:05.345000+00:00",
            "Data": "dXNlciBsb2dpbg==",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921106984162645317591353343868930",
            "ApproximateArrivalTimestamp": "2024-04-11T03:03:18.207000+00:00",
            "Data": "dXNlciBsb2dvdXQ=",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921107080876710886870538971512834",
            "ApproximateArrivalTimestamp": "2024-04-11T03:29:42.186000+00:00",
            "Data": "dXNlciBzaWdudXA=",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921107082085636706486336377323522",
            "ApproximateArrivalTimestamp": "2024-04-11T03:29:59.749000+00:00",
            "Data": "dXNlciBsb2dpbg==",
            "PartitionKey": "user1"
        },

  ]



-- we get a batch of records out of it. So we used to have record one right here, which is PartitionKey user1.

-- moe open in google , base64 data decode online , decode the data 



IMP : it will expire after 5 min ,  If u get timeout error like this , do this again from step 1 within below 5 min

-- An error occurred (ExpiredIteratorException) when calling the GetRecords operation: Iterator expired. The iterator was created at time Thu Apr 11 03:18:15 UTC 2024 while right now it is Thu Apr 11 03:25:34 UTC 2024 which is further in the future than the tolerated delay of 300000 milliseconds.
         





---------------------------------------------------------------- Kinesis Client Library (KCL)


• A Java library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload

• Each shard is to be read by only one KCL instance

     • 4 shards = max. 4 KCL instances

     • 6 shards = max. 6 KCL instances



IMP : When using Kinesis Client Library, each shard is to be read-only by one KCL instance. So, if you have 10 shards, then the maximum KCL instances you can have is 10.



--------------------------- how the Kinesis Client Library works ?


• Progress is checkpointed into DynamoDB (needs IAM access) 

    - So, the Kinesis Client Library will be reading from our Kinesis Data Stream and the progress of how far it's been reading is going to be checkpointed into DynamoDB, and so your application running KCL will need IAM access to DynamoDB.


• Track other workers and share the work amongst shards using DynamoDB

    - It will be able, thanks to DynamoDB to track the other workers of your KCL application and share the work among shards.


• KCL can run on EC2, Elastic Beanstalk, and on-premises as long as they have correct IAM credentials.

• Records are read in order at the shard level

-- there are two versions of the Kinesis Client Library,

     • KCL 1.x (supports shared consumer)

     • KCL 2.x (supports shared & enhanced fan-out consumer)




------------------------------------ KCL Example: 4 shards


look at picture in pdf for better understanding 



-- if we look at an example of 4 shards into our stream, we can have a DynamoDB table to check on the progress,

-- so we can run two KCL apps of the same coherent application running on two different EC2 instances.

-- in this case, thanks to DynamoDB, they will know how to share the work, so the first KCl app is going to be reading from shard 1 and 2, and the second KCL app is going to be reading from shard 3 and 4.

-- Now, the progress of how far the app has been reading into the Kinesis Data Stream will be checkpointed into DynamoDB.

-- And so, for example, if one of these application goes down, DynamoDB and KCL apps working together, will know that an app will go down, and so reading from the other shards will be resumed from where it was checkpointed.




------------------------------------ KCL Example: 4 shards, Scaling KCL App


-- if you have 4 shards and now you run 4 KCL applications, then it will be each reading from one shard.

-- And therefore the progress will be resumed from DynamoDB and checkpointed again.

-- But we can not have more KCL apps than shards, because well, otherwise one will be doing nothing.




------------------------------ KCL Example: 6 shards, Scaling Kinesis


-- So if you want to read to scale Kinesis, you can scale Kinesis and add 6 shards, so now we still have our 4 KCL applications, but now we have six shards in Kinesis in the streams.

-- And so again, they will detect this change, and working together with DynamoDB, they will again, split the work between each KCL application and the shard assignments.




--------------------------------- KCL Example: 6 shards, Scaling KCL App


-- So that means that once we have 6 shards Kinesis Data Stream then we can have 6 KCL applications reading from them, and checkpointing the progress into DynamoDB.





---------------------------------------------------------------- Kinesis Operation – Shard Splitting 



• Used to increase the Stream capacity (1 MB/s data in per shard)

    - So Shard Splitting is used to split a shard into two. So that means we have more shards and so it's used to increase the Stream capacity.

    - So if we add more shards by splitting a shard we get an extra one megabyte per second of throughput per shard.


• Used to divide a “hot shard”

    - So it's used for example, when you want to divide a hot shard,

    - for example , we have 3 shards into our KDS , here shard 2 is very hot, a lot of data is sent to it.

    - here , We're going to split the shard and we're gonna get shard 4 and 5 out of it

    - and by having the new shards, we've increased our stream throughput from three megabytes per second to four megabytes per second.

    - So we've increased the capacity but because we are being built in terms of how many shards we have in our stream we've also increased the cost of our Kinesis Data Stream .


• The old shard is closed and will be deleted once the data is expired

    - The old shard is going to be closed or Shard two will not be written two anymore and the old data will be expired after some time.

    - So depending on what your retention period is between one to 365 days, you will have to wait this long.
  
    - Then the data is expired and then finally, the shard will be deleted.


• No automatic scaling (manually increase/decrease capacity)

    - There is no auto-scaling in Kinesis Data Streams although you can find a way to create your own auto-scaling and there is a solution architecture for that 
    
    - but there's no setting to do auto-scaling with Kinesis Data Streams and so it is for you to manually increase or decrease capacity.


• Can’t split into more than two shards in a single operation




---------------------------------------------------------------- Kinesis Operation – Merging Shards


-- the opposite operation of Shard Splitting is Merging Shards .

• So you realize that you want to Decrease the Stream capacity and save costs

• Can be used to group two shards with low traffic (cold shards) , you're going to merge them together into a new shard.

-- So in this example, Shard one and four are merged into Shard six and this is used to decrease capacity and the cost. (look at picture for better understanding from pdf)

• Old shards are closed and will be deleted once the data is expired

• Can’t merge more than two shards in a single operation

-- with Shard Splitting and Shards Merging you've seen how to scale up and down Kinesis.







---------------------------------------------------------------------- Kinesis Data Firehose (delivery stream)



-- So it is a very helpful service that can take data from producers. And producers can be everything we've seen for Kinesis Data Stream, so applications, clients, SDK, or the Kinesis agents can all produce into your Kinesis Data Firehose.

-- But also, a Kinesis Data Stream can produce into Kinesis Sata Firehose. Amazon CloudWatch logs and events can produce into Kinesis Data Firehose.

-- all these applications are going to send records into Kinesis Data Firehose, and then Kinesis Data Firehose can optionally choose to transform the data using a Lambda function, but this is optional.

-- once the data is transformed optionally, then it can be written in batches into destinations.

-- So Kinesis Data Firehose take data from sources. Usually the most common is going to be Kinesis Data Streams and it's going to write this data into destinations without you writing any kind of code, because Kinesis Data Firehose knows how to write data.



---------------------- So there are three kinds of destinations with Kinesis Data Firehose.


1 AWS Destinations :

    1 the first one is Amazon S3, so you can write all your data into Amazon S3.

    2 The second one is Amazon Redshift, which is a warehousing database. to do so, it first writes the data into Amazon S3. And then Kinesis Data Firehose will issue a copy command, and this copy command is going to copy data from Amazon S3 into Amazon Redshift.

    3 Amazon OpenSearch


2 There are also some third party partner destinations.

    - So Kinesis Data Firehose can send data into Datadog, Splunk, New Relic, MongoDB. And this list can get bigger and bigger over time,


3 Custom Destinations

    - if you have your own API with an HTTP endpoint, it is for you to send data from Kinesis Data Firehose into a custom destination.



-- so once the data is sent into all these destinations, you have two options.

      - You can also send all the data into an S3 bucket as a backup or just send the data that was failed to be written into these destinations into a failed S3 buckets. 



• Fully Managed Service, no administration, automatic scaling, serverless

    • AWS: Redshift / Amazon S3 / OpenSearch
    • 3rd party partner: Splunk / MongoDB / DataDog / NewRelic / snowflake ...
    • Custom: send to any HTTP endpoint


• Pay for data going through Firehose

• Near Real Time - Why?

     - because we write data in batches from Firehose to the destination.

     • Buffer interval: 0 seconds (no buffering) to 900 seconds

     • Buffer size: minimum 1MB

     - So in case you have buffering, well, that makes Kinesis Data Firehose a near real-time service.

     - And if you don't have buffering, if you have zero seconds for your buffer interval, it's still going to be considered near real-time because going to take a few seconds to deliver your data into your destination.



• Supports many data formats, conversions, transformations, compression

• Supports custom data transformations using AWS Lambda

• Can send failed or all data to a backup S3 bucket 




-------------------------------------------------------------- diff b/w Kinesis Data Streams vs Firehose



          Kinesis Data Streams                                                           Kinesis Data Firehose


• Streaming ser vice for ingest at scale                                      • Load streaming data into S3 / Redshift / OpenSearch / 3rd party / custom HTTP / snowflake


• Write custom code (producer /consumer)                                      • Fully managed


• Real-time (~200 ms)                                                         • Near real-time


• Manage scaling (shard splitting / merging)                                  • Automatic scaling

• Data storage for 1 to 365 days                                              • No data storage

• Supports replay capability                                                  • Doesn’t support replay capability






--------------------------------------------------------------  Kinesis Data Firehose Hands on


-- create KDF ---> source = KDS --> destinations = s3 --> select KDS ARN ---> buffer interval = 60 sec for demo --> create bucket and choose that bucket as destination

-- create KDF,  reamin all are default 

-- here we have our producer is KDS , so go to KDS and create records as we did before like for signup , login and logout

-- so three records have been sent. And what I can do now is go into Amazon S3 and see if they have appeared in Amazon S3.

-- currently, there are zero objects in my bucket. That's because the Kinesis Data Firehose has a buffer of 60 seconds. So we need to wait 60 seconds until the data makes it into Amazon S3.

-- after 60 sec , it will store into s3 bucket , open n see 

-- now delete the resources 




-------------------------------------------------------------- Kinesis Data Analytics (Managed Apache Flink)


-- there are two flavors of it.


--------------- 1 Kinesis Data Analytics for SQL applications


-- So it sits in the center. And the two data sources that it's able to read from are Kinesis Data Streams and Kinesis Data Firehose.

-- So you can read from either of those, and then you can apply SQL statements to perform your real-time analytics.

-- It's also possible for you to join some reference data by referencing it from an Amazon S3 bucket. This will, for example, allow you to enrich the data in real-time.

-- Then you can send data to various destinations, and there are two of them.

      1 The first one is a Kinesis data stream. So you can create a stream out of a Kinesis Data Analytics real-time query, or  
      
      2 you can send it directly into Kinesis Data Firehose, 
      
      
-- each with their own use cases.If you send directly into Kinesis Data Firehose, then you can send into Amazon S3, Amazon Redshift, or Amazon OpenSearch, or any other Firehose destinations.

-- Whereas if you send it into a Kinesis data stream, you can do real-time processing of that stream of data using AWS Lambda or whatever applications you are running on EC2 instances.

-- remember the diagram(structure, get picture from pdf )

-- this is for Kinesis Data Analytics for SQL Applications.


• Real-time analytics on Kinesis Data Streams & Firehose using SQL

• Add reference data from Amazon S3 to enrich streaming data

• Fully managed, no servers to provision

• Automatic scaling

• Pay for actual consumption rate

• Output:

      • Kinesis Data Streams: create streams out of the real-time analytics queries

      • Kinesis Data Firehose: send analytics query results to destinations

• Use cases:

    • Time-series analytics 
    • Real-time dashboards 
    • Real-time metrics




------------------  2 Kinesis Data Analytics for Apache Flink


• Use Flink (Java, Scala or SQL) to process and analyze streaming data

   - So Flink are special applications you need to write as code. And what it allows you is that you can actually run these Flink applications on the cluster that's dedicated to it on Kinesis Data Analytics.


-- But it's all behind the scenes. And with Apache Flink, you can read from two main data sources, you can read from Kinesis Data Streams or Amazon MSK.


• So with this service, you Run any Apache Flink application on a managed cluster on AWS , the idea is that Flink is going to be a lot more powerful than just standard SQL.

     - So if you need advanced querying capability, or to read streaming data from other services such as Kinesis Data Streams or Amazon MSK, which is managed Kafka on AWS, then you would use this service.
     
     • provisioning compute resources, parallel computation, automatic scaling

     • application backups (implemented as checkpoints and snapshots)

     • Use any Apache Flink programming features

     • Flink does not read from Firehose (use Kinesis Analytics for SQL instead)
         
         - You cannot read from Kinesis Data Firehose. If you need to read and do real-time analytics on Kinesis Data Firehose, then you must use Kinesis Data Analytics for SQL.





--------------------------------------------------------------  Ordering data into Kinesis



-- how data is being ordered for Kinesis and SQS FIFO. Because even though these technologies look similar and have some similar capabilities, they're actually very, very, very different.


• Imagine you have 100 trucks (truck_1, truck_2, ... truck_100) on the road sending their GPS positions regularly into AWS.

      - So let's have a little case study. Imagine you have 100 trucks on the road, and each truck will have a truck ID. and they're on the road and they're going to send their GPS positions very regularly into AWS.


• You want to consume the data in order for each truck, so that you can track their movement accurately.

• How should you send that data into Kinesis?

• Answer : send using a “Partition Key” value of the “truck_id”

--  the value of that partition key is the truck ID. So the truck one will send it for the  partition key truck one and then truck two will send for partition key truck two .....

• The same key will always go to the same shard


------- EG : see picture in pdf for better understanding 


Explanation :


-- So we have our Kinesis Data Stream and it has three shards , to simplify things, I'm not going to show you 100 trucks, but five should be enough. So we have five trucks, and they're on the road and they're sending the data into Kinesis.

-- As I said, we choose the partition key to be truck ID.

-- So that means that my truck one, when it's sending it's GPS data, it will send it to Kinesis with the partition key, truck one and Kinesis will say, okay, partition key truck one, I will hash it I mean we'll do a computation. And in this instance, it figures out that truck one should go into shard number one.

-- So my data will go into shard number one.

-- Then the truck two will be sending its data as well and will send a partition key of truck two. And Kinesis will look at this partition key and say I've hashed it and now it looks like you should go into shard two.

-- Same for truck three so truck three will be on the road. And it will send truck three as the partition key. But this time, the Kinesis Data Stream service will hash that truck three as the key and say you should go to shard one and that's fine.

-- It just says it doesn't have to be shard three it just says, this partition key should go to shard one.

-- Now for truck four, it will go to shard three and for truck five, it will go to shard two.

-- So this is the idea now we have a repartition and it's called partition hence the name partition key of each truck on each shard based on the partition key.

-- because truck one keeps on sending the same partition key which is truck one, then the data will always go to the same shard.

-- Hence so on the next data point for the truck one will be in shard one and the next data point for truck three will be in shard one as well and so on.

-- So anytime the truck one sends data, it will be in shard one and anytime the blue truck, the shard three sends data then it will be in shard one as well, because we are specifying to use the same partition key over time

-- So we see here is that truck one and three will always have the data into shard one.

-- Now if we look at the shard two, then only truck two and five will have the data into shard two.

-- if you look at shard three, in this example, we only have the truck four that will send its data into shard three.

-- So now imagine you have 100 trucks and maybe five shards, then each shard on average will have about 20 trucks.

-- But there is no linkage directly, you can tell between the truck and each shard.

-- Kinesis will have to hash the partition key to determine which shard to go to. 

-- The idea is though that as soon as we have a stable partition key, then each truck will be sending this data to the same shard and therefore we will have the data in order for each truck at the shard level.






-------------------------------------------------------------- Ordering data into SQS


• For SQS standard, there is no ordering.

• For SQS FIFO, if you don’t use a Group ID, messages are consumed in the order they are sent, with only one consumer

• You want to scale the number of consumers, but you want messages to be “grouped” when they are related to each other

• Then you use a Group ID (similar to Partition Key in Kinesis)





-------------------------------------------------------------- Kinesis vs SQS ordering


• Let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO


 • Kinesis Data Streams:

      • On average you’ll have 20 trucks per shard , thanks to the hashing, so each truck will be designated one shard and will stay in that shard forever.

      • Trucks will have their data ordered within each shard

      • The maximum amount of consumers in parallel we can have is 5 , because we have five shards and we need one consumer per shard.

      • Can receive up to 5 MB/s of data , So the Kinesis Data Stream though because it has five shards can receive up to five megabytes per second of data, which is quite a high throughput.


• SQS FIFO

      • You only have one SQS FIFO queue

      • You will have 100 Group ID, because we have 100 trucks, then we can create 100 group ID, each equal to the truck ID.

      • You can have up to 100 Consumers (due to the 100 Group ID) , Each consumer will be hooked to one specific group ID.

      • You have up to 300 messages per second (or 3000 if using batching)



---- conclusion


-- sometimes is going to be better to use SQS FIFO. If you want to have a dynamic number of consumers based on the number of group IDs,

-- sometimes it could be better to use Kinesis Data Stream if you have say 10,000 trucks and you need to send it lot of data, and also have data ordering per shard in your Kinesis Data Stream. 








--------------------------------------------------------------  SQS vs SNS vs Kinesis

SQS:                                                          SNS:                                         Kinesis

• Consumer“pulldata”                                    • Push data to many subscribers                        • Standard:pulldata (2 MB per shard)

• Data is deleted after being consumed                  • Upto 12,500,000subscribers                           • Enhanced-fanout:pushdata (2 MB per shard per consumer)

• Canhaveasmanyworkers (consumers) as we want           • Data is not persisted(lost if not delivered)         • Possibility to replay data

• Noneedtoprovision throughput                          • Pub/Sub                                              • Meantforreal-timebigdata,analytics and ETL

• Ordering guarantees only on FIFO queues               • No need to provision throughput                      • Ordering at the shard level

• Individual message delay capability                   • Integrates with SQS for fan- out architecture        • Data expires after X days
                                                          pattern     

                                                        • FIFO capability for SQS  FIFO                        • Provisioned mode or on- demand capacity mode   









================================================== AWS Monitoring, Troubleshooting & Audit (CloudWatch, X-Ray and CloudTrail) ========================================


-- we have our application it's in the cloud. It's running. And your manager calls you at 2:00 a.m. and say it's not running anymore.

-- What happened?

-- Well we've deployed our application but we forgot to turn on monitoring.

-- Monitoring is so important and it will make sure that your applications are running the right way. 

-- Such as you can see what's happening with the logs with the metrics with tracing and audits who made what's in your AWS infrastructure.





---------------------------------------- Why Monitoring is Important


• We know how to deploy applications

     • Safely
     • Automatically
     • Using Infrastructure as Code
     • Leveraging the best AWS components!


• Our applications are deployed, and our users don’t care how we did it...

• Our users only care that the application is working!

     • Application latency: will it increase over time?

     • Application outages: customer experience should not be degraded

     • Users contacting the IT department or complaining is not a good outcome

     • Troubleshooting and remediation


• Internal monitoring:

     • Can we prevent issues before they happen? 
     • Performance and Cost
     • Trends (scaling patterns)
     • Learning and Improvement





---------------------------------------- Monitoring in AWS


1 • AWS CloudWatch:

      • Metrics: Collect and track key metrics
      • Logs: Collect, monitor, analyze and store log files
      • Events: Send notifications when certain events happen in your AWS 
      • Alarms: React in real-time to metrics / events


2 • AWS X-Ray:

      • Troubleshooting application performance and errors , so we'll see the latency and we'll see the errors just live.
      • Distributed tracing of microservices
      - X-Ray is kind of a new service that is not very popular yet, but I think it is one of the most awesome ones.


3 • AWS CloudTrail:

      • Internal monitoring of API calls being made
      • Audit changes to AWS Resources by your users






---------------------------------------- AWS CloudWatch Metrics


• CloudWatch provides metrics for every services in AWS , you need to understand what the metric means.

• Metric is a variable to monitor (CPUUtilization, NetworkIn...), and then based on how the metric is behaving, it gives you an idea of how the service is behaving, and you can do some troubleshooting based on this.

• Metrics belong to namespaces

• Dimension is an attribute of a metric (instance id, environment, etc...).

• Up to 30 dimensions per metric

• Metrics have timestamps

• Can create CloudWatch dashboards of metrics


-- CloudWatch is all about Alarms,Events and Logs

-- it is regional only 

-- CW is used to monitor performance of all as resource 

-- to monitor the resource , CW need Host level metrics also known as default metrics 

     1 CPU 

     2 Network 

     3 Disk --- volume 

     4 Status Check 

-- memory not comes under HLM 

-- memory is custom metrics 

-- 2 types of moitoring 

     1 Basic and 
     2 Detailed Moitoiring 

-- Basic is free and it will take every 5 min data

-- deatiled monitoiring : every 1 min data points , billable


-- u can create alaram in CW 

-- alarms can do actions like (terminate , reboot ,stop , recover ) 

-- Alarm has 3 States : 

     1 In Alarm : > 90 

     2 OK : < 90

     3 Insufficient : ec2 stopped due to some reasons



-- we also have concept called " Composite Alarms"

- CW alarms are single metric

- Composite Alarms are monitoring the states of multiple alarms 

 eg: AND or OR conditions 




IMP : In CloudWatch Logs, Log Retention Policy defined at ........................... level.


ANS : Log Groups 


---------------------------------------- some  terminologies to understand for the Cloudwatch 


1  NameSpaces : Containers for monitoring data , it is a way to keep things seperate 

      -- NameSpace has got a name , it can be anythng as long as it stays within the rule set 

      -- All aws data goes to aws namespace ---> AWS/Service

      -- Namespace contains related metric 


2  Metric : it is a collection of related Data Points , in the time ordered structures 

      Eg: cpu usage network IN/OUT 


3  Data Point : let us say we have metric called CPU Utilization , everytime any server measures its utilization and send it into cloudwatch that goes into the CPU utilization metrics and each one of those measures so everytime the server reports the cpu that measure is called "Data Point" 


     --it has 2 components 

     1 timestamp 

     2 value 


Note : CPU utilization metric could contain data from many servers ,so how do we seperate data for this? so use " Dimesions"


4  Dimensions : these are Name Value Pairs that seperate data point for different things or perspective withn the same metric 


      -- while sending data points to cloud watch ,AWS also send in , these two 

      A  Name = InstanceID , Value =I-xxxx

      B  name = InstanceType , value =t2.micro.......


5  ALARM : CW also allows to take actions based on metrics which is done using Alarms 

      -- two states 

      A  OK --> Everything is working fine 

      B  ALARM --> Something bad has happened 

      C insufficient Data ---> instance has stopped or not exists




REF : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html






---------------------------------------- EC2 Detailed monitoring 


• EC2 instance metrics have metrics “every 5 minutes”

• With detailed monitoring (for a cost), you get data “every 1 minute”

• Use detailed monitoring if you want to scale faster for your ASG! , it gives you some benefits for your ASG, if you want to scale out and in faster.

• The AWS Free Tier allows us to have 10 detailed monitoring metrics


• Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)







---------------------------------------- CloudWatch Logs for EC2 


• By default, no logs from your EC2 machine will go to CloudWatch

• You need to run a CloudWatch agent on EC2 to push the log files you want

• Make sure IAM permissions are correct

• The CloudWatch log agent can be setup on-premises too



------------------------------------- Hands On 

--- u ave to install CW Agent 

-- by deafult CW agent do not have permission to push logs to cloudwatch logs 

-- so we should create ROLE here , and give CWFULL access policy and this role is attach to the ec2 

1  create IAM role and give CW permissions 

2  attach The role to the ec2 instance ,make sure ur kernal version is 5.10....

3  login to te ec2 and install CW agent 

4  configure the files 

5  start the CW agent service 

6  see the logs in CW loogs 

-- crtae IAM role 

-- give CLOUDWATCHFILLACES permissions and attach role to the ec2 

-- now loginto the ec2 and enter cmnds 

1 make sure ur kernal version is 5.10....


- login as root user like sudo -s

2 in real time u always do Yum Update 

3 yum install -y awslogs  --- to intall CW Agent

4 once  u install cw packages , u have to create 2 files 

- go to cd /etc/awslogs/

- press enter 

- once u do ls , two files will be created 

-  1  awscli.conf and  2 awslogs.conf

- do sudo cat awscli.conf

- change region as per ur requriemnt , sudo vim awscli.conf (to change the region)

- do cat another file awslogs.conf

- file = var/logs/messages (this is appn log path) , it is the place where it will collects the logs 

- log_stream_name = from here logs coming 

5 now u have to start awslogs , once u start awslogs automatically the log group generated once u start 

6 start the cloudwaatch Agent service

- systemctl start awslogsd 

- press nter 

7 once u pres nter the backend process will run and all the system logs will push to the CW logs 

8 go n check in log groups 

- /var/log/messages  log group created 

- we have given log_stream as Instance _id open n check the log group 

- once u open instnce id u can check all the system logs 

9  u can also push appn logs to CW agent 


- insted of var/log/messages u put log path   




---------------------------------------- CloudWatch Custom Metrics


• Possibility to define and send your own custom metrics to CloudWatch

• Example: memory (RAM) usage, disk space, number of logged in users ...

• Use API call "PutMetricData"

• Ability to use dimensions (attributes) to segment metrics

     • Instance.id
     • Environment.name

IMP • Metric resolution (StorageResolution API parameter – two possible value):

      • Standard: 1 minute (60 seconds)
      • High Resolution: 1/5/10/30 second(s) – Higher cost


• Important: Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly)

     - Something good to know is that with custom metrics, when you push a metric in the past or in the future, this works as well.

     - So this is a very important exam point.

     - So if you are pushing a metric up to two weeks in the past or two hours in the future, you're not going to get an error from CloudWatch,

     - This is going to accept your metric as is. And so that means you need to make sure that your EC2 instance time is currently configured. If you want the metrics to be synchronized with the actual time from AWS.

    - See Timestamp in documentation


-- for more custom metrics examples 

REF : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html





----------------------------------------  LAB for custom metrics (Memory)


Expore topic : till now we only see Host level metrics , now find how to get metrics for memory 

- By default, we cannot monitor Memory metrics on EC2 Instances.

- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-scripts-intro.html

- use this link for the documentation 

or u can do in another process

- create ec2 instance 

- crate one role and attach policies 

-  attach policies cloudwatchfullacces and AmazonSSMFullAccess

-  why SSM? 

ANs: i have to store particular value(json document which is used to fetch the memory utilixation from aws ec2 and send it to AWS cloudwatch ) in ssm

-  create a parameter in the system manager(system manager --> application --> parameter) with the name (u can give any name) 

eg :  /alarm/AWS-CWAgentLinConfig 

- go system manager -->paramter store --> give name --> in the value place copy json script 

{
	"metrics": {
		"append_dimensions": {
			"InstanceId": "${aws:InstanceId}"
		},
		"metrics_collected": {
			"mem": {
				"measurement": [
					"mem_used_percent"
				],
				"metrics_collection_interval": 60
			}
		}
	}
}


-- ceate ec2 and attach role to ec2 and create with userdata with cloudwatch agent to install

userdata: 

#!/bin/bash
wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip
unzip AmazonCloudWatchAgent.zip
sudo ./install.sh
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:/alarm/AWS-CWAgentLinConfig -s


-- check whether clouwtch agent is instaled or not by using 

-- sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status

-- if it is running it is success , after one minute

-- now go to CLoudwatch and --> all metrics --> CW Agent --> give instacne id and chck logs of memory






---------------------------------------- CloudWatch Logs


-- So CloudWatch Logs is the perfect place to store your application logs in AWS.

-- And to do so, you must first define log groups.

• Log groups: arbitrary name, usually representing an application

• Log stream: instances within application / log files / containers

• Can define log expiration policies (never expire, 1 day to 10 years...)

• CloudWatch Logs can send logs to:

     • Amazon S3 (exports)
     • Kinesis Data Streams
     • Kinesis Data Firehose
     • AWS Lambda
     • OpenSearch

• Logs are encrypted by default

• you Can setup KMS-based encryption with your own keys



---------------------------------- what types of logs can go into CloudWatch Logs? (CloudWatch Logs - Sources)


• we can send the logs using the SDK, CloudWatch Logs Agent, CloudWatch Unified Agent

• Elastic Beanstalk: which is used to collect logs from the application directly into CloudWatch.

• ECS: ECS will send the logs directly from the containers into CloudWatch.

• AWS Lambda: collection from function logs , will send logs from the functions themselves.

• VPC Flow Logs: will send logs specific to your VPC metadata network traffic.

• API Gateway : will send all the requests made to the API Gateway into CloudWatch Logs.

• CloudTrail : you can send the logs directly based on the filter.

• Route53: will log all the DNS queries made to its service.




---------------------------------------- what if you wanted to query the logs in CloudWatch Logs? (CloudWatch Logs Insights)


-- So, it's a querying capability within CloudWatch Logs which allows you to write your query.

-- You specify the timeframe you want to apply your query to and then automatically you're going to get a result as a visualization.

-- This visualization can also be exported either as a result or added to a dashboard for being able to return it whenever you want.

• Search and analyze log data stored in CloudWatch Logs

• Example: find a specific IP inside a log, count occurrences of “ERROR” in your logs...

      - So there are lots of simple queries provided as part of the console for CloudWatch Logs Insights. 

      - For example, you can find the most 25 most recent events, or you can have a look at how many events had exceptions or errors in your logs, or you can look for a specific IP and so on ..
 

• Provides a purpose-built query language

      • Automatically discovers fields from AWS services and JSON log events

      • Fetch desired event fields, filter based on conditions, calculate aggregate statistics, sort events, limit number of events...

      • Can save queries and add them to CloudWatch Dashboards


• Can query multiple Log Groups in different AWS accounts

• It’s a query engine, not a real-time engine





---------------------------------------- CloudWatch Logs – S3 Export


-- CloudWatch Logs can be exported into several destinations.


1 CloudWatch Logs – S3 Export

     • Log data can take up to 12 hours to become available for export

        - So this is for a batch export to send all your logs into Amazon S3, and this export can take up to 12 hours to be completed.

     • The API call to initiate this export is called CreateExportTask.

     • So because this is a batch export, this is Not near-real time or real-time... use Logs Subscriptions instead




---------------------------------------- CloudWatch Logs Subscriptions


• Get a real-time log events from CloudWatch Logs for processing and analysis

• Send to Kinesis Data Streams, Kinesis Data Firehose, or Lambda

• Subscription Filter – filter which logs are events delivered to your destination

      - So, the subscription filter can send data into Kinesis Data Streams. This would be a great choice if you wanted to use, for example, the integration with Kinesis Data Firehose, Kinesis Data Analytics, or Amazon EC2, or Lambda.

      - You can also directly send it into Kinesis Data Firehose. From there, you can send it in near real-time fashion into Amazon S3 or the OpenSearch Service,

      - So you can write your own custom Lambda function, or you can use a managed Lambda function that is sending data in real-time into the OpenSearch Service.





---------------------------------------- CloudWatch Logs Aggregation Multi-Account & Multi Region


-- thanks to these subscription filters, it is possible for you to aggregate data from different CloudWatch Logs into different accounts and different regions into a common destination such as the Kinesis Data Stream in one specific accounts. And then Kinesis Data Firehose. And then in near real-time into Amazon S3.

-- So that is very possible, and that is a way for you to perform log aggregation.



---------------------------------------- Cross-Account Subscription


• Cross-Account Subscription – send log events to resources in a different AWS account (KDS, KDF)

-- So, let's say you have a sender account and the recipient accounts.

-- So you create a CloudWatch Log subscription filter in sender account , and then this gets sent into a subscription destination, which is a virtual representant of the Kinesis Data Stream in the recipient accounts.

-- Then you attach a destination access policy to allow the first account to actually send data into this destination.

-- Then you create an IAM role in the recipient account, which has the permission to send records into your Kinesis Data Stream, and you make sure that this role can be assumed by the first account.

-- when you have all these things in place, then it is possible for you to send data from CloudWatch Logs in one account into a destination in another account.




----------------------------------------  Live Tail


-- So first let's create a log group

-- go to log group --> create log stream called demostream 

-- go to demostream  , we have option called tailing  , do tailing 

-- select log group as u want and select Select log streams , apply filter 

-- So that means that as events are being posted into Cloudwatch logs, they are going to appear here in our Live Tail,

-- now go to demostream  --> actions --> create log event (Hello world)

-- now check in live tail , it will be there 

-- So this is quite a nice way because if you have log streaming very fast, it can all appear here. And then from this we can get more information around when this happened, the group and so on.

-- So it's just a very cool, very easy feature to debug your CloudWatch logs. And from a pricing perspective,

-- you only get a few hours a day, so maybe one hour a day of free usage of Live Tail. So please make sure to cancel and close your lifetail session so that you don't have any cost, but you have one hour of free every day.




---------------------------------------- CloudWatch Logs Agent & Unified Agent


• For virtual servers (EC2 instances, on-premise servers...)

• CloudWatch Logs Agent

      • Old version of the agent

      • Can only send to CloudWatch Logs


• CloudWatch Unified Agent

      • Collect additional system-level metrics such as RAM, processes, etc...
      • Collect logs to send to CloudWatch Logs
      • Centralized configuration using SSM Parameter Store




---------------------------------------- CloudWatch Unified Agent – Metrics


• Collected directly on your Linux server / EC2 instance

• CPU (active, guest, idle, system, user, steal)

• Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops)

• RAM (free, inactive, used, total, cached)

• Netstat (number of TCP and UDP connections, net packets, bytes)

• Processes (total, dead, bloqued, idle, running, sleep)

• Swap Space (free, used, used %)


• Reminder: out-of-the box metrics for EC2 – disk, CPU, network (high level)


-- If you want more granularity think CloudWatch Unified Agents,





---------------------------------------- CloudWatch Logs Metric Filter


• CloudWatch Logs can use filter expressions 

    • For example, find a specific IP inside of a log
    • Or count occurrences of “ERROR” in your logs 
    • Metric filters can be used to trigger alarms


• So, one thing you should know is that when you create a filter, it  do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.


• Ability to specify up to 3 Dimensions for the Metric Filter (optional)



-- So, let's take an example. We have CloudWatch Logs agent installed on an EC2 instance, which is streaming the logs into CloudWatch Logs.

-- Then an actual metric is going to be created out of it. This is your metric filter.

-- From there, we can, for example, integrate it with a CloudWatch alarm to say that, "Hey, if we count five times error "in less than a minute in your logs, "you may want to know about it "and be alerted in an SNS topic."



 
 CloudWatch Logs Agent (Ec2 instance)--------------(stream)-----------> CW Logs --------------------> Metric Filters ------------------> CW Alarm -----------> SNS 





----------------------------------------  CloudWatch Logs Metric Filter - Hands On


-- see in course 




---------------------------------------- CloudWatch Alarms


• Alarms are used to trigger notifications for any metric

• Various options (sampling, %, max, min, etc...)

• Alarm States:

    • OK :  means that it's not triggered.

    • INSUFFICIENT_DATA  : means that there's not enough data for the alarm to determine a state.

    • ALARM : which is that your threshold has been breached and therefore a notification will be sent.


• Period:

    • Length of time in seconds to evaluate the metric

    • High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec




Question : A CloudWatch Alarm set on a High-Resolution Custom Metric can be triggered as often as ......................

ANS : 10 sec

EXP : If you set an alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds.








---------------------------------------- CloudWatch Alarm Targets 


-- alarms have three main targets.


1 • Stop,Terminate, Reboot, or Recover an EC2 Instance

2 • Trigger Auto Scaling Action

3 • Send notification to SNS (from which you can do pretty much anything)




---------------------------------------- CloudWatch Alarms – Composite Alarms


• CloudWatch Alarms are on a single metric

-- but then if you wanted to have multiple metrics, use Composite Alarms

• Composite Alarms are monitoring the states of multiple other alarms

-- So the composite alarm is the action of combining all these other alarms together.

• you can use AND and OR conditions to be able to be very flexible in terms of the condition you're checking for.

• Helpful to reduce “alarm noise” by creating complex composite alarms

   - it's very helpful to reduce alarm noise because you can create complex composite alarms and saying, 
   
   - for example, if the CPU is high and the network is high, then don't alert me because I only wanna know when the CPU is high and the network is low, this kind of things.




---------------------------------------- EC2 Instance Recovery


• Status Check:

     • Instance status = check the EC2 VM

     • System status = check the underlying hardware


• Recovery: Same Private, Public, Elastic IP, metadata, placement group






---------------------------------------- CloudWatch Alarm: good to know


• Alarms can be created based on CloudWatch Logs Metrics Filters

• To test alarms and notifications, set the alarm state to Alarm using CLI

    aws cloudwatch set-alarm-state --alarm-name "myalarm" --state-value ALARM --state-reason "testing purposes"





---------------------------------------- CloudWatch Alarms Hands On


-- create once ec2 instance , it will take 5 minutes to get some metrics 

-- go to cw and go to all alaram --> select metric -->choose ec2 --> paste instance is --> choose CPUUtilization -->  CPUUtilization > 90 --> Additional configuration Datapoints to alarm = 3 of 3 --> choose ec2 action as u want --> create alaram

-- So now this alarm obviously does have insufficient data, so we need to wait 15 minutes for it 

-- we could go into the EC2 instance and launch a way to get the CPU very high for 15 minutes, but this would be a very, very long, 

-- or we can use the API call name, Set alarm state, to really see what would happen if this alarm went into the breach phase.

-- open cloushell , go to goole and search for cloudwatch alarm state , enter this below cmd as ur names 

   -  aws cloudwatch set-alarm-state --alarm-name <value> --state-value <value> --state-reason <value>

   eg : aws cloudwatch set-alarm-state --alarm-name terminateec2 --state-value ALARM --state-reason "Testing"


-- paste this in cli , it will turn to in alarm state , then as per rule it will going to terminate the ec2 instance 






---------------------------------------- CloudWatch Synthetics Canary


• Configurable script that monitor your APIs, URLs, Websites, ...

• Reproduce what your customers do programmatically to find issues before customers are impacted

• Checks the availability and latency of your endpoints and can store load time data and screenshots of the UI

• Integration with CloudWatch Alarms

• Scripts written in Node.js or Python

• Programmatic access to a headless Google Chrome browser

• Can run once or on a regular schedule



---------------------------------------- CloudWatch Synthetics Canary Blueprints


• Heartbeat Monitor – load URL, store screenshot and an HTTP archive file

• API Canary – test basic read and write functions of REST APIs

• Broken Link Checker – check all links inside the URL that you are testing

• Visual Monitoring – compare a screenshot taken during a canary run with a baseline screenshot

• Canary Recorder – used with CloudWatch Synthetics Recorder (record your actions on a website and automatically generates a script for that)

• GUI Workflow Builder – verifies that actions can be taken on your webpage (e.g., test a webpage with a login form)





---------------------------------------- CloudWatch Synthetics Canary Hands On


-- cloudwatch ---> application signals or Application logs ----> Synthetics Canaries 

-- create canary with any application link (amazon,google.....) --> use hearbeat canary --> remaing all are default --> create canary and wait for 2-3 minutes 

-- this will gives the info about the application , it is working fine or not 







----------------------------------------------------- Amazon EventBridge (formerly CloudWatch Events)



-- Amazon EventBridge is a serverless service that uses events to connect application components together, making it easier for developers to build scalable event-driven applications.

-- Event Bridge is an Event Bus that helps in integrating different AWS Services , and custom applications and SAAS Applications 

-- Earlier we have Cloud watch events , the only negative point is that , it did not support SAAS applications and custom applications outside from the AWS 

-- EventBridge was build for the same purpose , u can create effortless event-driven architectures

Eg 1 : u want to receive SNS notification every time production EC2 instance are terminated , this will be done by EventBridge

Eg 2 : automated deployment using code pipelines at 11PM everyday

Eg 3 : if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


-- these all things will done by the event Bridge and u can schedule the events also 

-- It is fully managed service 

-- pay what u use model 


Different Parts in EventBridge:


Working flow -----


Event Producer --> Event --> Amazon EventBridge event Bus --> Rule ---> AWS Lambda , AWS Kinesis Data Firehose, Amazon Simple Notification Service


--  Event producer : AWS Service / Custom applications / Third Party SaaS providers,

--  From the producers the event will generated , this will transform to the bus and it move to rules , in rules there are multiple rules and this rules will send to multiple targets 

-- A single rule may have multiple targets and max target is 5 ,all targets will process in the event in parallel


Important terms in EventBridge :

1 Event  : An Event indicated a change in an environment 

2 Rule   : A rule matches incoming events and routes them to targets for processing  

3 Target : target application of ur rule. A target process events 

Targets can include ec2 , lambda functions , kinesis streams , Ecs tasks,  , the target receive events in JSON format 

4 Event Bus : An event bus receives events. When u create a rule , u associate it with a specific event bus and rules is matched only to events received by that event bus 

- rule can not be create standAlone , it should have it's parent as EventBus

Components of EventBridge :

1 EventBridge Rule : A rule matches incoming events and sends them to targets for processing.

2 EventBridge Pipes : A pipe connects an event source to a target with optional filtering and enrichment.

3 EventBridge Schedule : A schedule invokes a target one-time or at regular intervals defined by a cron or rate expression.

4 EventBridge Schema registry : Schema registries collect and organize schemas.


EventBus : Usecases

if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


In the above example , the working flow will be like 


User deletes his subscription --> custom event is trigged --> rights custom event would be pass into the EventBus --> rule matching --> then it reaches to the targets , once the rule is matched  here then it sends to targets --> 1 the targets are Feedback service 2 Schedule call service 3 offers service 





stepehe note

-- formerly known as CloudWatch Events

• Schedule: Cron jobs (scheduled scripts)

    - Schedule Every hour ----------> Lambda (Trigger script on Lambda function)


-- but not just a schedule like every hour, it can also react to an event pattern. 

• Event Pattern: Event rules to react to a service doing something

    - So there are event rules that can react to a service doing something. For example, you can react to the event of IAM root user sign in in the console. 
    
    - So when that happens, maybe you want to send a message into an SNS topic and receive an email notification, so that if anyone is using the root account, then you will receive an email,

    - which may be a good security feature for your accounts.


          IAM Root User Sign in Event (EVENT)-------------> SNS Topic with Email Notification



• you have different destinations like Trigger Lambda functions, send SQS/SNS messages...


• Event buses can be accessed by other AWS accounts using Resource-based Policies

• You can archive events (all/filter) sent to an event bus (indefinitely or set period)

• Ability to replay archived events ,which is super handy for debugging, super handy for troubleshooting, and for fixing production as well.





----------------------------------------------------- Amazon EventBridge – Schema Registry


-- EventBridge receives a lot of events from different places and so therefore, you need to understand what the events are going to look like and remember, these events are in this adjacent format we just saw.

• EventBridge can analyze the events in your bus and infer the schema

• The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus

• Schema can be versioned



----------------------------------------------------- Amazon EventBridge – Resource-based Policy


• Manage permissions for a specific Event Bus

• Example: allow/deny events from another AWS account or AWS region

• Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region







======================================= practicals for event bridge 


Example 1 

1  Schedule AWS Lambda Functions Using EventBridge


-- lambda scheduling use cases 

- Automated backups at EOD of ur applications 

- backend cleaning (including logs and temp files )

- consolidated reports after business hours , so lambda can trigger  Athena queries and can run any database queries and send to business stack holders through SNS 


- to schedule events u have two options 

1 to schedule  at fixed rate -- for every 1 minute 

2 Cron job (10 * * * * )

-- open console 

-- open lambda --> python 3.9 --> create function 

-- write python code to invoke lambda function

import json
from datetime import datetime

def lambda_handler(event, context):
    # TODO implement
    currentTime = datetime.now()
    print("Time at which Lambda invoked" + str(currentTime))


-- give empty JSON to test {}

-- now go to Amazon eventBridge in console 

-- c.o create rule --> schedule --> continue to create rule --> schedule that run at regular intervals--> select 1 minute --> select lambda function --> create rule 

-- u can check in CloudWatch logs , for invoking is there or not 

--  the logs are generated 


-- if u want to monitor ur invocations --> Metrics --> all Metrics --> query --> choose AWS/events --> Myrule name 

- metric name = COUNT(invocation)

- filter by = Rulename = rule name that u have created in lambda function to test the function 

- choose number in right corner to see the no. of invocations 


-------------------------------------------

Example 2 : EventBridge with SNS 


-- AS we know that EventBridge will use for 2 process 

1 event-event process : when the event has occurred according to our rule then it will trigger to the targets 

2 schedule process : do schedule 

-- in the above example we have seen Schedule event

-- now event to event 

- for example the ec2 is stopped then it will send alert to the subscribers on SNS Topic 

-- open SNS in console --> create one topic --> standard --> create topic --> open topic -->create subscription --> add protocol email or phone number 

-- when ever the instance is getting stooped then I would like send an alert to all the subscribers 

-- this is called event-event process 

-- create one ec2 instance 

-- create rule in event bridge --> name --> rule with an event pattern --> in event pattern = select ec2 --> select event type as u want --> in Target1 choose AWS Service = SNS topic select SNS topic --> create rule 

-- do stop the instance 

-- u will get notification once the instance is get stopped 


--------------- now for schedule event 

-- create another rule , that is based on the time to stop the instance 

-- select schedule pattern option --> A schedule that runs at a regular rate, such as every 10 minutes. --> target = terminateinstanceAPI call --> u can also add another target to get notification 
 

-- the instance will get terminated after the time that u have specified 


-- this is simple basic example for schedule pattern 




-------------------------------------------------------------------- AWS X-Ray


-- when you do debugging production ,  the good old way:

     • Test locally
     • Add log statements everywhere 
     • Re-deploy in production , and from the logs try to figure out what is breaking, what is happening.

-- It's really painful. It's not best practices.

• Log formats differ across applications using CloudWatch and analytics is hard.

• Debugging: monolith “easy”, distributed services “hard”

• No common views of your entire architecture!


-- here comes x-ray 

-- So, X-ray is going to give you a visual analysis of your application.

-- We'll see that basically as a client doing a request to our application we will see how many of these requests fail or don't fail. And then, from the application we'll see what it does.

-- So, it will call other IPs, it will call SNS it will call a DynamoDB Table. And so, as you can see, we'll be able to trace exactly, visually what happens when we talk to our EC2 instance.






-------------------------------------------------------------------- AWS X-Ray advantages


• Troubleshooting performance (bottlenecks)

• You can Understand dependencies in a microservice architecture, because you can visually see what is happening and how all your microservices interact with one another.

• Pinpoint service issues

• Review request behavior

• Find errors and exceptions 

•  We can answer questions Are we meeting time SLA? , in terms of latency or time to process a request?

• Where I am throttled? we can understand which service really slows down, throttles us.

• Identify users that are impacted by our errors 




-------------------------------------------------------------------- X-Ray compatibility


-- So, X-ray has a lot of compatibility.

• AWS Lambda
• Elastic Beanstalk
• ECS
• ELB
• API Gateway
• EC2 Instances or any application server (even on premise)


-- So, they really made X-Ray try to be as wide as possible and as applicable as to any application they can.




-------------------------------------------------------------------- X-Ray, how does it work? (AWS X-Ray Leverages Tracing)


-- it leverages something called tracing.

• Tracing is an end to end way to following a “request”

• So, when I make a request to, for example, my application server,  each component that will deal with a request that could be my database, that could be my gateway, my load balancer, my application server. Each component dealing with the request adds its own “trace” 

• Tracing is made of segments (+ sub segments)

• Annotations can be added to traces to provide extra-information

• So, when all these things are together, Ability to trace:

      • Every request
      • Sample request (as a % for example or a rate per minute)


• X-Ray Security:

       • IAM for authorization
       • KMS for encryption at rest


-- So, once you get all these traces, basically, X-Ray provides its magic and provide this nice little graph




-------------------------------------------------------------------- AWS X-Ray How to enable it?


-- Well, you have two ways 


1 Your code (Java, Python, Go, Node.js, .NET) must import the AWS X-Ray SDK

    • Very little code modification needed

    • The application SDK will then capture:

          • Calls to AWS services
          • HTTP / HTTPS requests
          • Database Calls (MySQL, PostgreSQL, DynamoDB)
          • Queue calls (SQS)


2) Install the X-Ray daemon or enable X-Ray AWS Integration

• X-Ray daemon works as a low level UDP packet interceptor (Linux / Windows / Mac...)

       - The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API.

       - So, if we run on a machine, on-premise server or EC2 instance, we need to install the daemon.

       - And the daemon is basically a little program that works as a low level UDP packet interceptor.

       - It can be running on Linux, Windows, and Mac. , so, you have to install it on your machine.


• AWS Lambda / other AWS services that already have integration with X-Ray then they will run the daemon for you and you don't have to worry about it.

• Each application must have the IAM rights to write data to X-Ray

 

Question : So, a very common question is, my X-Ray application works on my computer when I test locally, but doesn't work on my EC2 machine, why?

 -- the answer is probably because on your machine you're running the X-Ray daemon, but when you deploy to your EC2 instance, it's not running the X-Ray daemon, and therefore, X-Ray doesn't see your calls.



  EC2 Instance (Applicaxon Code + AWS X-Ray SDK ---------> X-Ray Daemon Running on machine) ------------------(Send batch every 1 second to AWS X-Ray)-----------> X-Ray




-------------------------------------------------------------------- The X-Ray magic


• X-Ray service collects data from all the different services

• Service map is computed from all the segments and traces

• X-Ray is graphical, so even non technical people can help troubleshoot



-------------------------------------------------------------------- AWS X-Ray Troubleshooting


• If X-Ray is not working on EC2

       • Ensure the EC2 IAM Role has the proper permissions

       • Ensure the EC2 instance is running the X-Ray Daemon


• To enable on AWS Lambda:

     • Ensure it has an IAM execution role with proper policy (AWSX-RayWriteOnlyAccess)

     • Ensure that X-Ray is imported in the code

     • Enable Lambda X-Ray Active Tracing




-------------------------------------------------------------------- AWS X-Ray Hands On


-- demp app will get errors , so create ur own demo app and we will deploy this on cloudformation

-- so here U r using cloud formation , so make sure that to use us-east-1 region for better o/p


-- create one yaml file for this demo



Eg :



AWSTemplateFormatVersion: '2010-09-09'
Description: >
  AWS CloudFormation template to create a new VPC
  or use an existing VPC for ECS deployment
  in Create Cluster Wizard. Requires exactly 1
  Instance Types for a Spot Request.
Parameters:
  Email:
    Type: String
    Default: UPDATE_ME  # <- change to a valid "abc@def.xyz" email (without quotes)
  FrontendImageUri:
    Type: String
    Default: public.ecr.aws/xray/scorekeep-frontend:latest
  BackendImageUri:
    Type: String
    Default: public.ecr.aws/xray/scorekeep-api:latest
  EcsClusterName:
    Type: String
    Description: >
      Specifies the ECS Cluster Name with which the resources would be
      associated
    Default: scorekeep-cluster
  EcsAmiId:
    Description: AMI ID
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/recommended/image_id
  EcsInstanceTypeT2:
    Type: CommaDelimitedList
    Description: >
      Specifies the EC2 instance type for your container instances.
      Defaults to t2.micro.
    Default: t2.micro
    ConstraintDescription: must be a valid EC2 instance type.
  EcsInstanceTypeT3:
    Type: CommaDelimitedList
    Description: >
      Specifies the EC2 instance type for your container instances.
      Defaults to t3.micro.
    Default: t3.micro
    ConstraintDescription: must be a valid EC2 instance type.
  KeyName:
    Type: String
    Description: >
      Optional - Specifies the name of an existing Amazon EC2 key pair
      to enable SSH access to the EC2 instances in your cluster.
    Default: ''
  VpcId:
    Type: AWS::EC2::VPC::Id

  SubnetId1:
    Type: AWS::EC2::Subnet::Id
  SubnetId2:
    Type: AWS::EC2::Subnet::Id

  SecurityGroupId:
    Type: String
    Description: >
      Optional - Specifies the Security Group Id of an existing Security
      Group. Leave blank to have a new Security Group created
    Default: ''
  AsgMaxSize:
    Type: Number
    Description: >
      Specifies the number of instances to launch and register to the cluster.
      Defaults to 1.
    Default: '1'
  IamRoleInstanceProfile:
    Type: String
    Description: >
      Specifies the Name or the Amazon Resource Name (ARN) of the instance
      profile associated with the IAM role for the instance
    Default: ScorekeepInstanceProfile
  SecurityIngressFromPort:
    Type: Number
    Description: >
      Optional - Specifies the Start of Security Group port to open on
      ECS instances - defaults to port 0
    Default: '80'
  SecurityIngressToPort:
    Type: Number
    Description: >
      Optional - Specifies the End of Security Group port to open on ECS
      instances - defaults to port 65535
    Default: '80'
  SecurityIngressCidrIp:
    Type: String
    Description: >
      Optional - Specifies the CIDR/IP range for Security Ports - defaults
      to 0.0.0.0/0
    Default: 0.0.0.0/0
  EcsEndpoint:
    Type: String
    Description: >
      Optional - Specifies the ECS Endpoint for the ECS Agent to connect to
    Default: ''
  RootEbsVolumeSize:
    Type: Number
    Description: >
      Optional - Specifies the Size in GBs of the root EBS volume
    Default: 30
  EbsVolumeSize:
    Type: Number
    Description: >
      Optional - Specifies the Size in GBs of the data storage EBS volume used by the Docker in the AL1 ECS-optimized AMI
    Default: 22
  EbsVolumeType:
    Type: String
    Description: Optional - Specifies the Type of (Amazon EBS) volume
    Default: 'gp2'
    AllowedValues:
      - ''
      - standard
      - io1
      - gp2
      - sc1
      - st1
    ConstraintDescription: Must be a valid EC2 volume type.
  RootDeviceName:
    Type: String
    Description: Optional - Specifies the device mapping for the root EBS volume.
    Default: /dev/xvda
  DeviceName:
    Type: String
    Description: Optional - Specifies the device mapping for the EBS volume used for data storage. Only applicable to AL1.
    Default: /dev/xvdcz
  UseSpot:
    Type: String
    Default: 'false'
  IamSpotFleetRoleArn:
    Type: String
    Default: ''
  SpotPrice:
    Type: String
    Default: ''
  SpotAllocationStrategy:
    Type: String
    Default: 'diversified'
    AllowedValues:
      - 'lowestPrice'
      - 'diversified'
  UserData:
    Type: String
    Default: |
      #!/bin/bash
      echo ECS_CLUSTER=scorekeep-cluster >> /etc/ecs/ecs.config;echo ECS_BACKEND_HOST= >> /etc/ecs/ecs.config;
  IsWindows:
    Type: String
    Default: 'false'
  ConfigureRootVolume:
    Type: String
    Description: Optional - Specifies if there should be customization of the root volume
    Default: 'true'
  ConfigureDataVolume:
    Type: String
    Description: Optional - Specifies if there should be customization of the data volume
    Default: 'false'
  AutoAssignPublicIp:
    Type: String
    Default: 'INHERIT'
Conditions:
  UseT2MicroInstance:
    !Or
    - !Or
      - !Equals [!Sub '${AWS::Region}', 'us-east-1']
      - !Equals [!Sub '${AWS::Region}', 'us-east-2']
      - !Equals [!Sub '${AWS::Region}', 'us-west-1']
      - !Equals [!Sub '${AWS::Region}', 'us-west-2']
      - !Equals [!Sub '${AWS::Region}', 'ap-south-1']
      - !Equals [!Sub '${AWS::Region}', 'ap-northeast-3']
      - !Equals [!Sub '${AWS::Region}', 'ap-northeast-2']
      - !Equals [!Sub '${AWS::Region}', 'ap-southeast-1']
      - !Equals [!Sub '${AWS::Region}', 'ap-southeast-2']
      - !Equals [!Sub '${AWS::Region}', 'ap-northeast-1']
    - !Or
      - !Equals [!Sub '${AWS::Region}', 'ca-central-1']
      - !Equals [!Sub '${AWS::Region}', 'eu-central-1']
      - !Equals [!Sub '${AWS::Region}', 'eu-west-1']
      - !Equals [!Sub '${AWS::Region}', 'eu-west-2']
      - !Equals [!Sub '${AWS::Region}', 'eu-west-3']
      - !Equals [!Sub '${AWS::Region}', 'sa-east-1']
      - !Equals [!Sub '${AWS::Region}', 'cn-northwest-1']
  CreateEC2LCWithKeyPair:
    !Not [!Equals [!Ref KeyName, '']]
  SetEndpointToECSAgent:
    !Not [!Equals [!Ref EcsEndpoint, '']]
  CreateNewSecurityGroup:
    !Equals [!Ref SecurityGroupId, '']
  CreateWithSpot: !Equals [!Ref UseSpot, 'true']
  CreateWithASG: !Not [!Condition CreateWithSpot]
  CreateWithSpotPrice: !Not [!Equals [!Ref SpotPrice, '']]
  IsConfiguringRootVolume: !Equals [!Ref ConfigureRootVolume, 'true']
  IsConfiguringDataVolume: !Equals [!Ref ConfigureDataVolume, 'true']
  IsInheritPublicIp: !Equals [!Ref AutoAssignPublicIp, 'INHERIT']
Resources:
  ScorekeepTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    Properties: 
      ContainerDefinitions: 
      - Cpu: '256'
        Image: !Ref FrontendImageUri
        MemoryReservation: '256'
        Name: scorekeep-frontend
        PortMappings: 
          - ContainerPort: '80'
      - Cpu: '512'
        Environment: 
          - Name: AWS_REGION
            Value: !Sub ${AWS::Region}
          - Name: NOTIFICATION_TOPIC
            Value: !Sub arn:aws:sns:${AWS::Region}:${AWS::AccountId}:scorekeep-notifications
          - Name: NOTIFICATION_EMAIL
            Value: !Ref Email
        Image: !Ref BackendImageUri
        MemoryReservation: '512'
        Name: scorekeep-api
        PortMappings: 
          - ContainerPort: '5000'
      - Cpu: '256'
        Essential: true
        Image: amazon/aws-xray-daemon
        MemoryReservation: '128'
        Name: xray-daemon
        PortMappings: 
          - ContainerPort: '2000'
            HostPort: '2000'
            Protocol: udp
      Cpu: '1024'
      ExecutionRoleArn: !Ref ECSExecutionRole
      Family: scorekeep
      Memory: '900'
      NetworkMode: host
      RequiresCompatibilities: 
        - EC2
      TaskRoleArn: scorekeepRole
  UserTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-user"
      KeySchema:
        HashKeyElement: {AttributeName: id, AttributeType: S}
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  SessionTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-session"
      KeySchema:
        HashKeyElement: {AttributeName: id, AttributeType: S}
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  GameTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-game"
      AttributeDefinitions:
        - AttributeName: "id"
          AttributeType: "S"
        - AttributeName: "session"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "id"
          KeyType: "HASH"
      GlobalSecondaryIndexes:
        - IndexName: "session-index"
          KeySchema:
            - AttributeName: "session"
              KeyType: "HASH"
          ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
          Projection: { ProjectionType: ALL }
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  MoveTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-move"
      AttributeDefinitions:
        - AttributeName: "id"
          AttributeType: "S"
        - AttributeName: "game"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "id"
          KeyType: "HASH"
      GlobalSecondaryIndexes:
        - IndexName: "game-index"
          KeySchema:
            - AttributeName: "game"
              KeyType: "HASH"
          ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
          Projection: { ProjectionType: ALL }
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  StateTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-state"
      AttributeDefinitions:
        - AttributeName: "id"
          AttributeType: "S"
        - AttributeName: "game"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "id"
          KeyType: "HASH"
      GlobalSecondaryIndexes:
        - IndexName: "game-index"
          KeySchema:
            - AttributeName: "game"
              KeyType: "HASH"
          ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
          Projection: { ProjectionType: ALL }
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  NotificationTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: "scorekeep-notifications"
  ECSExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement: 
          - 
            Effect: "Allow"
            Principal: 
              Service: 
                - "ecs-tasks.amazonaws.com"
            Action: 
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
      RoleName: "scorekeepExecutionRole"
  ECSTaskRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement: 
          - 
            Effect: "Allow"
            Principal: 
              Service: 
                - "ecs-tasks.amazonaws.com"
            Action: 
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess"
        - "arn:aws:iam::aws:policy/AmazonSNSFullAccess"
        - "arn:aws:iam::aws:policy/AWSXrayFullAccess"
      RoleName: "scorekeepRole"
  ScorekeepECSRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: ScorekeepECSRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Action: "sts:AssumeRole"
          Principal:
            Service: ["ec2.amazonaws.com"]
          Effect: "Allow"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
  ScorekeepInstanceProfile:
    DependsOn: ScorekeepECSRole
    Type: AWS::IAM::InstanceProfile
    Properties: 
      InstanceProfileName: !Ref IamRoleInstanceProfile
      Roles: 
        - ScorekeepECSRole
  ScorekeepECSCluster:
    Type: AWS::ECS::Cluster
    Properties: 
      ClusterName: !Ref EcsClusterName
  EcsSecurityGroup:
    Condition: CreateNewSecurityGroup
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: ECS Allowed Ports
      VpcId: !Ref VpcId
      SecurityGroupIngress:
       - IpProtocol: 'tcp'
         FromPort: '80'
         ToPort: '80'
         CidrIp: 0.0.0.0/0
  EcsInstanceLc:
    DependsOn:
    - ScorekeepECSCluster
    - ScorekeepInstanceProfile
    Type: AWS::AutoScaling::LaunchConfiguration
    Condition: CreateWithASG
    Properties:
      ImageId: !Ref EcsAmiId
      InstanceType: !Select [ 0, !If [ UseT2MicroInstance, !Ref EcsInstanceTypeT2, !Ref EcsInstanceTypeT3 ] ]
      AssociatePublicIpAddress: !If [ IsInheritPublicIp, !Ref "AWS::NoValue", !Ref AutoAssignPublicIp ]
      IamInstanceProfile: !Ref IamRoleInstanceProfile
      KeyName: !If [ CreateEC2LCWithKeyPair, !Ref KeyName, !Ref "AWS::NoValue" ]
      SecurityGroups: [ !If [ CreateNewSecurityGroup, !Ref EcsSecurityGroup, !Ref SecurityGroupId ] ]
      BlockDeviceMappings:
        - !If
          - IsConfiguringRootVolume
          - DeviceName: !Ref RootDeviceName
            Ebs:
              VolumeSize: !Ref RootEbsVolumeSize
              VolumeType: !Ref EbsVolumeType
          - !Ref AWS::NoValue
        - !If
          - IsConfiguringDataVolume
          - DeviceName: !Ref DeviceName
            Ebs:
              VolumeSize: !Ref EbsVolumeSize
              VolumeType: !Ref EbsVolumeType
          - !Ref AWS::NoValue
      UserData:
        Fn::Base64: !Ref UserData
  EcsInstanceAsg:
    DependsOn:
    - ScorekeepTargetGroup
    - EcsInstanceLc
    Type: AWS::AutoScaling::AutoScalingGroup
    Condition: CreateWithASG
    Properties:
      VPCZoneIdentifier:
        - !Ref SubnetId1
        - !Ref SubnetId2
      LaunchConfigurationName: !Ref EcsInstanceLc
      MinSize: '0'
      MaxSize: !Ref AsgMaxSize
      DesiredCapacity: !Ref AsgMaxSize
      Tags:
        -
          Key: Name
          Value: !Sub "ECS Instance - ${AWS::StackName}"
          PropagateAtLaunch: true
        -
          Key: Description
          Value: "This instance is the part of the Auto Scaling group which was created through ECS Console"
          PropagateAtLaunch: true
      TargetGroupARNs:
      - !Ref ScorekeepTargetGroup
  ScorekeepService:
    DependsOn:
    - ScorekeepTaskDefinition
    - UserTable
    - SessionTable
    - GameTable
    - MoveTable
    - StateTable
    - NotificationTopic
    - ECSExecutionRole
    - ECSTaskRole
    - ScorekeepECSCluster
    - EcsInstanceAsg
    Type: AWS::ECS::Service
    Properties:
      Cluster: scorekeep-cluster
      DeploymentConfiguration:   
        MaximumPercent: 100
        MinimumHealthyPercent: 0
      DesiredCount: 1
      LaunchType: EC2
      SchedulingStrategy: REPLICA
      ServiceName: scorekeep-service
      TaskDefinition: !Ref ScorekeepTaskDefinition
  ScorekeepTargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: ScorekeepTargetGroup
      TargetType: instance
      Port: 80
      Protocol: HTTP
      VpcId: !Ref VpcId
  ScorekeepLoadBalancer:
    DependsOn: EcsSecurityGroup
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: scorekeep-lb
      Scheme: internet-facing
      SecurityGroups: 
        - !Ref EcsSecurityGroup
      Subnets: 
        - !Ref SubnetId1
        - !Ref SubnetId2
      Tags:
        - Key: Name
          Value: !Join [_, [!Ref 'AWS::StackName']]
      Type: application
  ScorekeepLoadBalancerListener:
    DependsOn:
    - ScorekeepLoadBalancer
    - ScorekeepTargetGroup
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties: 
      DefaultActions:
        - Type: forward
          TargetGroupArn: !Ref ScorekeepTargetGroup
      LoadBalancerArn: !Ref ScorekeepLoadBalancer
      Port: 80
      Protocol: HTTP
Outputs:
  LoadBalancerUrl:
    Description: The URL of the ALB
    Value: !GetAtt ScorekeepLoadBalancer.DNSName




Question : You have a Classic ECS cluster that you want to enable IAM roles for your ECS tasks so that they can make API requests to AWS services. Which ECS configuration option should you enable in /etc/ecs/ecs.config?


ANS : ECS_ENABLE_TASK_IAM_ROLE





-- if you have a look at it, this CloudFormation template actually deploys an ECS cluster.

-- then we're going to have a front-end Image a backend API all using X-Ray so that we see data in the X-Ray folder and then it's going to be running on the T2 or T3 micro.

             -  Default: public.ecr.aws/xray/scorekeep-frontend:latest

             -  Default: public.ecr.aws/xray/scorekeep-api:latest



-- open CF , upload this template 

-- now all are default in 2nd step except u have to add 2 subnets and VPC , And the reason we do so is that this is we indicate to the templates where we want to deploy our resources. 

-- all are default , acknowledge then submit 

-- this is going to create a CloudFormation template for us.

-- check the resources that template created for u 

-- add notificatin subscription , if u want 

-- So how do we use this web application. go to Outputs , open link in the new tab 

-- c.o create --> give sample name --> rules (tic tac toe)--> create 

-- c.o View traces for this session , it will show you all the traces in X-ray session

-- play the game , What's going to happen is that as we play the games this is going to send traces into X-Ray.

-- now go to X-ray , So this is my service map, and this shows the dependencies of all my components in AWS and how they relate towards the API calls that have been made.

-- if any error presents , it will highlight in clours 

-- u can debug and find the error and do trouble shoot 

-- delete the stack





-------------------------------------------------------------------- X-Ray Instrumentation in your code


-- some advanced concepts for X-Ray , and the first thing I want to show you is how to instrument your code.


• Instrumentation means the measure of product’s performance, diagnose errors, and to write trace information. So it is a field in Software Engineering to do all these things.

• To instrument your application code, you use the X-Ray SDK

 
- Example app.js - Express


var app = express();

var AWSXRay = require('aws-xray-sdk');
app.use(AWSXRay.express.openSegment('MyApp'));

app.get('/', function (req, res) {
  res.render('index');
});

app.use(AWSXRay.express.closeSegment());



REF : https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs.html


-- So here is an example of how we can instrument our node js code with the X-Ray SDK.

-- so once we add some code, for example here, requiring the X-Ray SDK and using it in our express app, then our code will be instrumented.

-- That means that we will get trace information from our code into the X-Ray service.

• Many SDK require only configuration changes

• You can modify your application code to customize and annotation the data that the SDK sends to X- Ray, using interceptors, filters, handlers, middleware...





-------------------------------------------------------------------- X-Ray Concepts


• Segments: each application / service will send them

• Subsegments: if you need more details in your segment

• Trace: segments collected together to form an end-to-end trace

• Sampling: decrease the amount of requests sent to X-Ray, reduce cost

• Annotations: Key Value pairs used to index traces and use with filters

   - So annotations is are extremely important in X-ray. If you want to be able to search your traces with new indexes.


• Metadata: Key Value pairs, not indexed, not used for searching


• The X-Ray daemon / agent has a config to send traces cross account:
  
    • make sure the IAM permissions are correct – the agent will assume the role

    • This allows to have a central account for all your application tracing






-------------------------------------------------------------------- X-Ray Sampling Rules


• with the sampling rules, we are able to control the amount of data that you send to the X-Ray service and record. the more data you send you X-Ray, the more you're going to pay.

• You can modify sampling rules without changing your code

• By default there is a Sampling Rules, which says that the X-Ray SDK records the first request each second (Blue Part), an five percent(Green Part) of any additional requests.

• One request per second is the reser voir, which ensures that at least one trace is recorded each second as long the service is serving requests.

• Five percent is the rate at which additional requests beyond the reservoir size are sampled.



-------------------------------------------------------------------- X-Ray Custom Sampling Rules


• You can create your own rules with the reservoir and rate


• Example 1 – Higher minimum rate for POSTs


• Rule name: POST minimum

• Priority: 100

• Reservoir: 10

• Rate: 0.10 (.1 if configured using a JSON document)

• Service name: *

• Service type: *

• Host: *

• HTTP method: POST

• URL path: *

• Resource ARN: *


--  in this example ,  Reservoir: 10 , iThat means that 10 requests per second will be sent into X-Ray and then 10% of the other ones will be sent.

-- So here we have a higher minimum rate and we send more requests into X-Ray.




Example – Debugging rule to trace all requests for a problematic route

         -- A high-priority rule applied temporarily for debugging.

• Rule name: DEBUG – history updates

• Priority: 1

• Reservoir: 1

• Rate: 1

• Service name: Scorekeep

• Service type: *

• Host: *

• HTTP method: PUT

• URL path: /history/*

• Resource ARN: *


-- Whereas here we want you to have debugging. And so we say we want to have all requests. So one a reservoir= 1  and one of rate = 1.

-- That means that all requests will be sent into X-Ray. So we don't want to lose any traces. And this is very helpful when we want you to debug to find what's going on for every single trace.

-- Obviously, in production, this will be very, very expensive because now we are sending a lot of data into X-Ray, 

-- but it's very helpful temporarily to change these custom sampling rules to see what's going on.

-- the cool thing about it is that if you change your sampling rules in the X-Ray console, you don't have to restart your applications. You don't have to do anything with your X-Ray in SDK.

-- Automatically the daemon, the extra daemon knows how to get sampling rules and correctly send the right amount of data into the X-Ray service.





-------------------------------------------------------------------- X-Ray Write APIs (used by the X-Ray daemon)



{
  "Version" : "2012-10-17",
  "Statement" : [
    {
      "Effect" : "Allow",
      "Action" : [
        "xray:PutTraceSegments",
        "xray:PutTelemetryRecords",
        "xray:GetSamplingRules",
        "xray:GetSamplingTargets",
        "xray:GetSamplingStatisticSummaries"
      ],
      "Resource" : [
        "*"
      ]
    }
  ]
}


REF : arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess



-- So this is a managed policy called the X-Ray Write Only Access and you can see it has five line items


1  PutTraceSegments: Uploads segment documents to AWS X-Ray

     - as the name indicates, it uploads a segment documents into AWS X-Ray that's necessary to have if you want to write into X-Ray.


2  • PutTelemetryRecords: Used by the AWS X-Ray daemon to upload some information about how many segments were received, rejected and backend connection errors. So this helps with the metrics.

     - SegmentsReceivedCount, 
     
     - SegmentsRejectedCounts, 
     
     - BackendConnectionErrors...


3  • GetSamplingRules: Retrieve all sampling rules (to know what/when to send)

     - So usually when we write stuff, we have a lot of puts because this was how the APIs are named in AWS whenever you're write it says, "put."

     - So do you know why?

     - we saw that whenever we were changing the sampling rules in the X-Ray console, all the X-Ray daemons were automatically updated to know when to send data into X-Ray.

     - So for your X-Ray daemon to be able to know how the sampling rules are changing, then the GetSamplingRule authorization and permission is necessary.


• GetSamplingRules same applies for  GetSamplingTargets & GetSamplingStatisticSummaries: advanced , 

• The X-Ray daemon needs to have an IAM policy authorizing the correct API calls to function correctly





-------------------------------------------------------------------- X-Ray Read APIs


{
  "Version" : "2012-10-17",
  "Statement" : [
    {
      "Sid" : "AWSXrayReadOnlyAccess",
      "Effect" : "Allow",
      "Action" : [
        "xray:GetSamplingRules",
        "xray:GetSamplingTargets",
        "xray:GetSamplingStatisticSummaries",
        "xray:BatchGetTraces",
        "xray:BatchGetTraceSummaryById",
        "xray:GetDistinctTraceGraphs",
        "xray:GetServiceGraph",
        "xray:GetTraceGraph",
        "xray:GetTraceSummaries",
        "xray:GetGroups",
        "xray:GetGroup",
        "xray:ListTagsForResource",
        "xray:ListResourcePolicies",
        "xray:GetTimeSeriesServiceStatistics",
        "xray:GetInsightSummaries",
        "xray:GetInsight",
        "xray:GetInsightEvents",
        "xray:GetInsightImpactGraph"
      ],
      "Resource" : [
        "*"
      ]
    }
  ]
}



-- this is more complicated, but this is a managed policy for reading.

-- as you can see, all these things as says get, get, get, get, get, and we have one called batch get trace which also says gets.



• GetServiceGraph:is to get the main graph that we saw in the console.

• BatchGetTraces: Retrieves a list of traces specified by ID. Each trace is a collection of segment documents that originates from a single request.

• GetTraceSummaries: Retrieves IDs and annotations for traces available for a specified time frame using an optional filter. To get the full traces, pass the trace IDs to BatchGetTraces.

• GetTraceGraph: Retrieves a service graph for one or more specific trace IDs.





-------------------------------------------------------------------- X-Ray with Elastic Beanstalk


• AWS Elastic Beanstalk platforms include the X-Ray daemon so we don't need to include it

• You can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextensions/xray-daemon.config)

• Make sure to give your instance profile the correct IAM permissions so that the X-Ray daemon can function correctly

• Then make sure your application code is instrumented with the X-Ray SDK to send these traces

• Note:The X-Ray daemon is not provided for Multicontainer Docker



----------------- in the console , u have to add some options to enable X-Ray 


1 in the monitoring stage enable , X-Ray 

2 make sure that the EC2 instances have an IAM role that allow them to connect into X-Ray. --> IAM --> roles --> beanstalk-instancerole --> AWSElasticBeanstalkWebTier  it has x-ray permissions 





-------------------------------------------------------------------- ECS + X-Ray integration options


-- theory part around how to integrate ECS with X-Ray



-------------- 1 ECS Cluster : X-Ray Container as a Daemon


-- you have an ECS Cluster and one way to run the X-Ray Daemon is to use the container as a Daemon itself.



what does that mean?


-- That means we have our two EC2 instance. For example, in our ECS cluster, and remember we manage those EC2 instances and so we're going to run a Daemon task, a Daemon container of the X-Ray Daemon.

-- That means that the X-Ray Daemon Container will be running on every single EC2 instances. If you have 10 EC2 instances in your ECS cluster, then you will have 10 containers, one on each EC2 instance, running as a Daemon Container.

-- so that means that the X-Ray agent is now running on all these EC2 instances. 

-- And so you can launch your App Containers on the EC2 instances, and after matting them correctly from the networking standpoint to hit the X-Ray Daemon with a UDP port. Then you can run all your applications.

-- So, in this case, you will just have one X-Ray Daemon Container per EC2 instance.



-------------- 2 ECS Cluster : X-Ray Container as a “Side Car”


-- That means you still have your EC2 instances, but now you're going to run one X-Ray Daemon Container alongside each application container, and they will connect from a networking stand point.

-- So this is why it's called a Side Car, it's because the X-Ray Daemon now runs side-to-side as our application container, and it's a Side car.

-- if we look at this now, we have one Side Car per App Container. So, if you have 20 App Containers under one EC2 instance then we'll have 20 X-Ray Side Car.




-------------- 3 Fargate Cluster : X-Ray Container as a “Side Car”


-- Now, Fargate Clusters, we don't have control over the EC2 instances, it's just an ECS Cluster that we don't have any control over the underlying instances.

-- So, we can not use the X-Ray Daemon Container, we also have to use the X-Ray Container as a Side Car pattern.

-- So, if you want to launch a Fargate task, it would be the App Container and the X-Ray Side Car, here and there.






---------------------------------------------------------------- AWS Distro for OpenTelemetry


-- So OpenTelemetry is a project and AWS has created a distribution of that project that is AWS supported and they call it Secure and Production Ready.


--------- So what is OpenTelemetry?

ANS : OpenTelemetry is a way to get a single set of APIs, library, agents and collector services to collect distributed traces and metrics for your applications.

-- The idea is that it's very similar to X-Ray, but it's open-source.



-- It can also help you collect metadata from your AWS resources and services.

-- so you have agents and these agents can be auto-instrumented to collect traces without you even changing your code which looks very similar to X-Ray.

-- All these traces and these metrics can be sent to multiple AWS services as well as partner solutions.

     • X-Ray,CloudWatch,Prometheus...


• Instrument your apps running on AWS (e.g., EC2, ECS, EKS, Fargate, Lambda) as well as on-premises

• Migrate from X-Ray to AWS Distro for Temeletry if you want to standardize with open-source APIs from Telemetry or send traces to multiple destinations simultaneously





AWS Distro for OpenTelemetry ----------------(Collect Traces ----> Collect Metrics -------> AWS Resources and Contextual Data) ----------------> AWS X-Ray / Amazon CloudWatch / Amazon Managed Service for Prometheus / Partner Monitoring Solunons






---------------------------------------------------------------- AWS CloudTrail


• Provides governance, compliance and audit for your AWS Account

• CloudTrail is enabled by default!

• Get an history of events / API calls made within your AWS Account by:

• Console
• SDK
• CLI
• AWS Services

• Can put logs from CloudTrail into CloudWatch Logs or S3

• A trail can be applied to All Regions (default) or a single Region.

• If a resource is deleted in AWS, investigate CloudTrail first!

-- By default, CloudTrail trails created via the AWS Management Console will have global service events enabled. It is recommended that you only have one trail allocated to global service events per account in order to reduce duplicate events.


EPV : AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible.






---------------------------------------------------------------- CloudTrail Events



1 • Management Events:

     • Operations that are performed on resources in your AWS account

     • Examples:

          • Configuring security (IAM AttachRolePolicy)

          • Configuring rules for routing data (Amazon EC2 CreateSubnet)

          • Setting up logging (AWS CloudTrail CreateTrail)

          • By default, trails are configured to log management events.

          • Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)



2 • Data Events:

      • By default, data events are not logged (because high volume operations)

      - So what are Data Events?

      • Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject):as you can see, these can be happening a lot on an S3 bucket. so this is why they're not logged by default and u have the option can separate Read and Write Events

      - So a Read Event will be a GetObject whereas a wright Event would be a DeleteObject or a PutObject.

      - Another kind of event you can have in a CloudTrail are AWS Lambda function execution activities.

      • AWS Lambda function execution activity (the Invoke API)

          - So whenever someone uses the Invoke API so you can get insights about how many times your Lambda functions are being evoked.

          - this could be really high volumes, if your Lambda functions are executed a lot.




3  CloudTrail Insights (u have to pay)


-- So when we have so many Management Events across all types of services and so many APIs happening very quickly in your accounts, 

-- it can be quite difficult to understand what looks odd, what looks unusual and what doesn't.

-- so this is where CloudTrail Insights comes in.


-- So with CloudTrail Insights and you have to enable it and you have to pay for it, it will analyze your events and try to detect unusual activity in your accounts.

• Enable CloudTrail Insights to detect unusual activity in your account:

    EG : 
    
    • inaccurate resource provisioning

    • hitting service limits

    • Bursts of AWS IAM actions

    • Gaps in periodic maintenance activity


• CloudTrail Insights analyzes normal management events to create a baseline

• And then continuously analyzes write events to detect unusual patterns

    • Anomalies appear in the CloudTrail console
    • Event is sent to Amazon S3
    • An EventBridge event is generated (for automation needs)






---------------------------------------------------------------- CloudTrail Events Retention


• Events are stored for 90 days in CloudTrail and then afterwards they're deleted,

• To keep events beyond this period, log them to S3 and use Athena



---------------------------------------------------------------- LAB 

-- open cloud trail --> event history u can able to see all the events which u have done in the aws account through the root user or normal user 

-- for lab , lets create a new cloud trail 

-- create new s3 bucket , all the API events that will send to s3 and stored in this bucket 

-- Log file SSE-KMS encryption  and Log file validation = uncheck no need for this demo 

-- management events --> create trail that's it 

-- wait for 5 min atleast to see the current time and date iin the trail 

-- after 5 min go n check in the s3 bucket --> it will create log files 

-- in the mean while create one simple ec2 instance for example purpose\

-- check in the history it will shows u the instance running in the trail

-- this is how u will get all the API calls that U have made in ur AWS Account 






------------------------------------------------------- Amazon EventBridge – Intercept API Calls


-- A very important cloud trail integration you need to know about is the one with Amazon EventBridge to intercept any API calls.

-- So let's say you wanted to receive an SNS notification, anytime a user would delete a table in DynamoDB by using the DeleteTable API Call.

-- So what happens that whenever we do an API call in AWS, as you know, the API call itself is going to be logged in CloudTrail. That's for any API call.

-- But also  these all these API calls will end up as events as well in Amazon EventBridge.

-- so we can look for that very specific delete table API call, and create a rule out of it. And this rule will have a destination the destination being Amazon SNS and therefore, we can create alerts.



 User -----------(DeleteTable API Call)------> DynamoDB -----------(Log API call)-----> CloudTrail (any API call) -----------(event)------> Amazon EventBridge -------(alert) ------------> SNS





-------------- few more examples on how you can integrate Amazon Eventbridge and CloudTrail.


1 

   -- For example, say, you wanted to be notified whenever a user was assuming a role in your accounts.

   -- So the AssumeRole is an API in the IAM service and therefore, is going to be logged by CloudTrail.

   -- And then using EventBridge integration, we can trigger a message into an SNS topic.


   User ------ (AssumeRole) ---------> IAM Role --------(API Call logs) ----------> CloudTrail --------(event)---------> EventBridge -------> SNS





2 

    -- Similarly, we can also intercept API calls that, for example, change the Security Group inbound rules. So the Security Group call is called AuthorizeSecurityGroupIngress, and it's an EC2 API call.

    -- So these are going to be logged again by CloudTrail and then they will appear in EventBridge and then we can trigger a notification in SNS.



    User -------------(AuthorizeSecurityGroupIngress)--------> EC2(Security Group) ------------- (API Call logs) -------------> CloudTrail ------------(event)------> EventBridge ---------> SNS






------------------------------------------------------- CloudTrail vs CloudWatch vs X-Ray


• CloudTrail:

   • Audit API calls made by users / services / AWS console
   • Useful to detect unauthorized calls or root cause of changes


• CloudWatch:

   • CloudWatch Metrics over time for monitoring
   • CloudWatch Logs for storing application log
   • CloudWatch Alarms to send notifications in case of unexpected metrics
   - CloudWatch is really just for overall metrics,


• X-Ray:

   • Automated Trace Analysis & Central Service Map Visualization
   • Latency, Errors and Fault analysis
   • Request tracking across distributed systems
   - X-Ray is a lot more granular, trace-oriented type of service







======================================================================== AWS Lambda ============================================================================


It’s a serverless world


What’s serverless?


• Serverless is a new paradigm in which the developers don’t have to manage servers anymore...

• They just deploy code

• They just deploy... functions !

• Initially... Serverless == FaaS (Function as a Service)

• Serverless was pioneered by AWS Lambda but now also includes anything that’s managed: “databases, messaging, storage, etc.”

• Serverless does not mean there are no servers... it means you just don’t manage / provision / see them





------------------------------------------------------ Serverless in AWS


• AWS Lambda
• DynamoDB
• AWS Cognito
• AWS API Gateway 
• Amazon S3
• AWS SNS & SQS
• AWS Kinesis Data Firehose 
• Aurora Serverless
• Step Functions
• Fargate



------------------------------------------------------ Why AWS Lambda


Amazon EC2 :

  • Virtual Servers in the Cloud
  • Limited by RAM and CPU
  • Continuously running
  • Scaling means intervention to add / remove servers


Amazon Lambda : 

   • Virtual functions – no servers to manage!
   • Limited by time - short executions
   • Run on-demand
   • Scaling is automated!




------------------------------------------------------ Benefits of AWS Lambda


• Easy Pricing:

    • Pay per request and compute time 

    • Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time


• Integrated with the whole AWS suite of services

• Integrated with many programming languages

• Easy monitoring through AWS CloudWatch

• Easy to get more resources per functions (up to 10GB of RAM!)

• Increasing RAM will also improve CPU and network!





------------------------------------------------------ AWS Lambda language support


• Node.js (JavaScript)
• Python
• Java (Java 8 compatible)
• C# (.NET Core)
• Golang
• C# / Powershell
• Ruby
• Custom Runtime API (community supported, example Rust)


• Lambda Container Image : this Lambda container image is quite special.

     • The container image must implement the Lambda Runtime API , so it's not any container image that can run on Lambda. There needs to be some prerequisites about how that container image is built.

     • ECS / Fargate is preferred for running arbitrary Docker images


-- So the exam, if they ask you to run a container on Lambda, unless that container does implement the Lambda runtime API, you will run  that container on ECS or Fargate.





------------------------------------------------------ AWS Lambda Integrations Main ones


1 API Gateway : So API Gateway is to create a REST API, and they will invoke our Lambda functions.


2 Kinesis : Kinesis will be using Lambda to do some data transformations on the fly.


3 DynamoDB :  DynamoDB will be used to create some triggers, so whenever something happens in our database a Lambda function will be triggered.


4 S3 : A Lambda function would be triggered anytime, for example, a file is created in S3.


5 CloudFront :  CloudFront, this will be Lambda@edge,


6 CloudWatch Events : CloudWatch Events or EventBridge. This is whenever things happen in our infrastructure on AWS, and we want to be able to react to things.

             - For example, say we have a cut pipeline, state changes and we want to do some automations based on it, we can use a Lambda function.


7 CloudWatch Logs : CloudWatch Logs, to stream these logs, wherever you want.


8 SNS : SNS to react to notifications and your SNS topics.


9 SQS : SQS to process messages from your SQS queues.


10 Cognito : Cognito to react to whenever, for example, a user login to your database.




-- So, these are just the main ones. There are tons of Lambda integrations.





------------------------------------------------------ Example: Serverless Thumbnail creation


-- So let's say we have an S3 bucket, and we want to create thumbnails on the fly.

-- So there will be an event which is that the new image will be uploaded in Amazon S3.

-- This will trigger, through an S3 event notification, a Lambda function.

-- that Lambda function will have code to generate a thumbnail. That thumbnail maybe pushed and uploaded into another S3 bucket or the same S3 bucket, which would be a smaller version of that image.

-- And also, our Lambda function may want to insert some data into DynamoDB, around some metadata for the image, for example the image, name, size, creation date, etc..




New image in S3 -------------(trigger)----------> AWS Lambda Function Creates a Thumbnail -------------(Push)-------> New thumbnail in S3 / Metadata in DynamoDB




------------------------------------------------------ Example: Serverless CRON Job


-- So CRON is a way on your EC2 instances, for example, to generate jobs every five minutes, or every Monday at 10:00 AM, etc

-- But you need to run CRON on a virtual server. So an EC2 two instance and so on.

-- so while your instance is not running, or at least your CRONs are not doing anything, then your instance time is wasted.

-- so, as such, you can create a CloudWatch Event rule or an EventBridge rule that will be triggered every 1 hour. And every 1 hour it will be integrated with a Lambda function that will perform your task.

-- So this is a way to create a serverless CRON,

-- in this example, CloudWatch Events is serverless and Lambda functions are serverless too.




------------------------------------------------------ AWS Lambda Pricing: example


• You can find overall pricing information here:

     https://aws.amazon.com/lambda/pricing/


• Pay per calls:

     • First 1,000,000 requests are free

     • $0.20 per 1 million requests thereafter ($0.0000002 per request)


• Pay per duration: (in increment of 1 ms)

     • 400,000 GB-seconds of compute time per month for FREE

     • == 400,000 seconds if function is 1GB RAM

     • == 3,200,000 seconds if function is 128 MB RAM

     • After that $1.00 for 600,000 GB-seconds


• It is usually very cheap to run AWS Lambda so it’s very popular





------------------------------------------------------ Lambda Hands On


-- open lambda in console ---> check how it will work on the intro page 

-- c.o scale seamlessly , to observe how this will work and how it will get scale automatically

-- Lambda responds to events : Once you create Lambda functions, you can configure them to respond to events from a variety of sources. Try sending a mobile notification, streaming data to Lambda, or placing a photo in an S3 bucket.


-- Scale seamlessly : Lambda scales up and down automatically to handle your workloads, and you don't pay anything when your code isn't running.

       - * Your first 1 million requests or 400,000 GB-seconds of compute per month are free. Costs in this demo are based on a 128 MB function with a 1 second invocation duration.



-- choose blue print --> hello world python --> this will give lambda function code --> create function


-- create test event and do test and observe the result 



------------------------------------------------------ Lambda – Synchronous Invocations


• Synchronous: CLI, SDK, API Gateway, Application Load Balancer

   -  So you're doing a synchronous invocation when you're using the CLI, and the SDK, the API Gateway, or even an Application Load Balancer.


-- What does that mean by synchronous?


• Results is returned right away : that means that you're waiting for the results, and then the result will be returned to you.

• Any Errors that come backs to you, must be handled on the client side. (retries, exponential backoff, etc...)

-- That means that if my lambda function fails, and I just invoked it from the console, I want to click on the retry button to retry it.

-- So that means any time there is an error on lambda, the client has to figure out what to do. Do you want to retry it, do you want to do an exponential backoff, etc etc.

-- So synchronous means a direct invocation that you wait the result of.



Example 1 :


-- So the CLI and the SDK would just invoke our lambda function, the lambda function would do something, and give us our response.


             ------------------------------------(invoke)---------------------------------->
  SDK/CLI                                                                                        Lambda     (Do something)
             <------------------------------------(Response)---------------------------------



Example 2 :


--  when we'll use the API Gateway ,the API Gateway will proxy its request to the lambda function, so it will invoke the lambda function for you; 

-- the lambda function will give the response to your API Gateway, which will give you the response.

-- so in this schema, we're just waiting for the response that makes it a synchronous type of invocation.


           -------------------------(invoke) ----------->               ------------(proxy)----------------> 
   Client                                                   API Gateway                                        AWS LAmbda  (Do something)
   
           <------------------(Response)------------------              <------------------------------------





------------------------------------------------------ Lambda - Synchronous Invocations - Services


-- Well, first of all, any time it's user-invoked then it's going to be synchronous.


• User Invoked:

      • Elastic Load Balancing (Application Load Balancer)

      • Amazon API Gateway

      • Amazon CloudFront (Lambda@Edge)

      • Amazon S3 Batch


• Service Invoked:

     • Amazon Cognito

     • AWS Step Functions


• Other Services:

     • Amazon Lex

     • Amazon Alexa

     • Amazon Kinesis Data Firehose





------------------------------------------------------ Lambda - Synchronous Invocations Hands On


-- actually when we had our code over function, and we just did test, then this was a synchronous invocation because we are waiting for the results of the invocation, and we get it here in this window.

-- If it took maybe two minutes for this invocation to complete, we would have waited two minutes to get the execution results.

-- The other thing we can test the synchronous invocation with is using the CLI.

-- open cloudshell in console 

-- So let's use this CLI to list our functions.

      aws lambda list-functions


-- if u r using nrml CLI , then add --region "ur region name"

-- So next we want to do a synchronous invocation from the CLI of our lambda function,

-- for this this just go into our code and under a synchronous.sh we have the different commands. So I'm on Linux Mac because this is CloudShell and this is using the CLI V2 versions.




aws lambda list-functions --region eu-west-1


# LINUX / MAC
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --region eu-west-1 response.json

# WINDOWS POWERSHELL
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\" }' --region eu-west-1 response.json

# WINDOWS CMD
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload "{""key1"":""value1"",""key2"":""value2"",""key3"":""value3""}" --region eu-west-1 response.json





-- based on ur OS , u can use the commands 

-- here i am using on linux , so 1st once i choose 

       aws lambda invoke --function-name mydemo --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }'  response.json



-- now do cat response.json

-- I mean, my synchronize invocation has worked.





------------------------------------------------------ Lambda Integration with ALB


-- So, for now, lambda functions, they can be either invoked with CLI or the SDK, But sometimes if you want to expose them to the internet, you want to allow people to use them through an HTTP or an HTTPS endpoint.

-- so as such, you have two ways for doing this. 
   
       - The first way is to use an Application Balancer, or 
       
       - an API Gateway,


• for it to work, The Lambda function must be registered in a target group 



      Client <-----------(HTTP/HTTPS)---------> Application Load Balancer (ALB) <-------------(INVOKE SYNC)--------------> Lambda (Target Group)



-- So your clients will be invoking and sending requests in the form of HTTP or HTTPS to your ALB, which will be invoking synchronously your lambda function in a target group,

-- because synchronously, because we're waiting for the lambda function to get back to the application balancer, which will in turn, return a response to the client.



Question : "How does the load balancer convert an HTTP request into a lambda invocation?"


ANS : 


1 ALB to Lambda: HTTP to JSON

    - the HTTP gets transformed into a JSON document.


-- Example Application Load Balancer request event


{
    "requestContext": {
        "elb": {
            "targetGroupArn": "arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/lambda-279XGJDqGZ5rsrHC2Fjr/49e9d65c45c6791a"
        }
    },
    "httpMethod": "GET",
    "path": "/lambda",
    "queryStringParameters": {
        "query": "1234ABCD"
    },
    "headers": {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "accept-encoding": "gzip",
        "accept-language": "en-US,en;q=0.9",
        "connection": "keep-alive",
        "host": "lambda-alb-123578498.us-east-1.elb.amazonaws.com",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36",
        "x-amzn-trace-id": "Root=1-5c536348-3d683b8b04734faae651f476",
        "x-forwarded-for": "72.12.164.125",
        "x-forwarded-port": "80",
        "x-forwarded-proto": "http",
        "x-imforwards": "20"
    },
    "body": "",
    "isBase64Encoded": false
}




-- elb = there is the ELB information, so which ELB invokes and what is the target group.

-- Then we get some information around the HTTP method, so it was a GET and the path was /lambda.

-- We get the query string parameters as key/value pairs, so each query string will be appearing in the JSON document.

-- We'll get the headers as key/value pairs,

-- we'll get the body for POST, PUT, and the value is base 64 encoded, so do you need to decode it or not.

-- for this information, there is a translation of the entire HTTP request to JSON.




2  Lambda to ALB conversions: JSON to HTTP

-- So, similarly, our lambda function should return something, a JSON document and the ALB will convert this back into HTTP.


Example response document format


{
    "statusCode": 200,
    "statusDescription": "200 OK",
    "isBase64Encoded": False,
    "headers": {
        "Content-Type": "text/html"
    },
    "body": "<h1>Hello from Lambda!</h1>"
}



-- So if we look at the response from the lambda function, it's very simple.

-- It needs to include a status code and a description,

-- as well as the response headers as key/value pairs,

-- finally the body of the response and the flag, whether or not it is base 64 encoded.





------------------------------------------------------ ALB Multi-Header Values


• So, if we have our client talking to our ALB, ALB can support multi header values (ALB setting) which is to have multi-header values.

-- What does that mean?

-- That means that if we pass in multiple headers with the same value or query strings with the same value,


• When you enable multi-value headers, HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects.



EG :

1 HTTP

     http://example.com/path?name=foo&name=bar


2 JSON

     ”queryStringParameters”: {“name”: [“foo”,”bar”] }



-- So, in this example, name=foo&name=bar have the same name, but different values. 

-- Then we can enable the setting and both the headers and the query string parameters will be converted as an array into the lambda function.

-- So that means that when my ALB invokes my lambda function for the queryStringParameters JSON,

-- what I will see is name, and instead of one value, I will see an array of values. So, foo and bar.

-- "How do we support multi-header values?"

       ANS :It's just a setting on the ALB and this is what it does.






------------------------------------------------------ lambda and ALB Hands On


-- open console lambda --> choose scratch --> give name --> select python 3.9 or above --> create function .

IMP -- now i have to create one load balancer for my lambda application , create TG , make sure SG have HTTP inbound rule and target have Lambda function as target and enable Health checks for TG 

-- so now if we look at our lambda code , it is just simple code that just returns the status 200

-- Now let's add a little bit of logging.

import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }


-- once u add print (event) , here we can see that the event that was passed is being printed to the console.

-- this will be helpful, when we want to see the event pass to another function from the load balancer.

-- now go to LB --> copy DNS and paste in browser --> it will downlaod one file in that file u have o/P

-- 'body': json.dumps('Hello from Lambda!') , 'coz of this we r getting o/p as in document form 

--  what we would like to have this response not to be downloaded, but instead to be displayed in my web browser.

-- so u have make changes in the code , the code will look like this 


import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        "statusCode": 200,
        "statusDescription": "200 OK",
        "isBase64Encoded": False,
        "headers": {
            "Content-Type": "text/html"
        },
        "body": "<h1>Hello from Lambda!</h1>"
    }



-- the content type to be text html, and therefore this is deployed in our web browser,

-- So this is how a Lambda function is properly sent in data into our ALB, and then our ALB showing the data directly from the web browser itself.

-- if u want to allow multi head values , u can enable , go to TG --> Attributes --> Multi value headers

-- but if you enable multi-value header, you need to change your response a little bit. ---- Remember

-- delete LB 






------------------------------------------------------ Lambda – Asynchronous Invocations





-- we've seen synchronous invocations,

-- let's go into asynchronous invocation. So they're for service that will invoke another functions behind the scenes,

• for example,  S3, SNS, CloudWatch Events...

-- for example , we have an S3 bucket and an S3 event notification for new files.

-- This will go into the Lambda Service, and because it's asynchronous something will happen. The events are going to be placed in an internal Event Queue.

-- So we have an Event Queue here and your Lambda function is going to be reading off that Event Queue.

-- The Lambda function then will try to process these events, but if somehow things go wrong, the Lambda function will automatically attempt to retry.

-- So that means that it will be three tries in total.

-- The first one will happen right away, then the second one will happen a minute after, and the third one will happen two minutes after the second one.

-- So the idea is that our Lambda function is going to retry three times in total.

-- And then, once the retries happen, that means that our Lambda function is possibly going to process those same events multiple times and so this could be a problem.

-- And so if you lambda function is not "idempotent" this could be big problem, so meaning your Lambda function should be idempotent.

            - Idempotentcy means that, in case of retries,the result is the same.


-- So then, if you have a retry happening, what will happen is that you will see duplicate log entries in CloudWatch Logs because your Lambda function will try over and over and over again.

-- So we can define a DLQ, or dead-letter queue, for after the retries are done.

-- So that means that in case there's a failed processing and we never were able to succeed because of the retries,

-- then the Lambda function can send some events to SQS or SNS for further processing later on.

-- this is the whole idea behind asynchronous invocations.


"Why we will use asynchronous versus synchronous?"

ANS : 

- Well, first of all, some services must use asynchronous, so you have no choice,

- and the second one is, for example, say you need to speed up processing and you don't need to wait for the results, then you can start 1000 files being processed at the same time,

- you just wait at the end for all to be processed in parallel, you don't wait for each individual results, and so that speeds up your processing time.


• Asynchronous invocations allow you to speed up the processing if you don’t need to wait for the result (ex: you need 1000 files processed)




                            
                                                    | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
                                                    |  Lambda Service                                                         |
                                                    |                                                                         |
                                                    |
                                                    |                                            - - - - - - - -              |
                                                    |                                            |              |
                                                    |                                            |   retries    |             |
                                                    |                                            |              |               
                                                    |                                            |              |             |
S3 bucket ----------------(New file event)---------------> Event Queue ----------(Read)-------> Function -------              |

                                                    |                                             |
                                                    |                                             |
                                                    |                                             |  DLQ for failed processing
                                                    |                                             |
                                                    |                                         SQS or SNS 
                                                    |
                                                    |
                                                    | - - - - - - - - - - - - - - - - - - - - - - - - -- - - - - - - - - - - - - 





Summary :


• The events are placed in an Event Queue

• Lambda attempts to retry on errors

     • 3 tries total
     • 1 minute wait after 1st , then 2 minutes wait

• Make sure the processing is idempotent (in case of retries)

• If the function is retried, you will see duplicate logs entries in CloudWatch Logs

• Can define a DLQ (dead-letter queue) – SNS or SQS – for failed processing (need correct IAM permissions)








------------------------------------------------------ Lambda - Asynchronous Invocations - Services


• Amazon Simple Storage Service (S3) , when we have S3 Event Notifications invoking Lambda functions.

• Amazon Simple Notification Service (SNS), so when we receive notifications and we trigger a Lambda function.

• Amazon CloudWatch Events / EventBridge, which will basically have our Lambda functions react to events happening in our AWS infrastructure.

• AWS CodeCommit (CodeCommit Trigger : new branch, new tag, new push)

• AWS CodePipeline (invoke a Lambda function during the pipeline, Lambda must callback)


----- other -----


• Amazon CloudWatch Logs (log processing)

• Amazon Simple Email Service

• AWS CloudFormation

• AWS Config

• AWS IoT

• AWS IoT Events





------------------------------------------------------ Lambda – Asynchronous Invocations Hands On


--  so i have my lambda function 

-- But if we invoke this asynchronously, we cannot do it from the console, we have to do it from the CLI.

-- open cloudshell , based on ur OS , u can use the below cmnds 



# LINUX / MAC
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --invocation-type Event --region eu-west-1 response.json

# WINDOWS POWERSHELL
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\" }' --invocation-type Event --region eu-west-1 response.json

# WINDOWS CMD
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload "{""key1"":""value1"",""key2"":""value2"",""key3"":""value3""}" --invocation-type Event --region eu-west-1 response.json



-- I am using linux 

-- so the whole purpose of an asynchronous invocation is that we do not have the results back to us.


EG : aws lambda invoke --function-name my-demo --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --invocation-type Event  response.json


-- it will gives u 202 response ,So that means that the lambda function has been successfully invoked but we don't know the results.

-- go to cloud watch --> fucntion log group --> open recent logs --> we can see the function was invoked and the requests was successful.

-- But we don't know this, if it was successful or not, because this was an asynchronous invocation.

-- now go to code and purposefully do make changes in code , for getting errror like this 


 #return event['key1']  # Echo back the first key value
 raise Exception('Something went wrong')



-- now do enter same commands again , but u will get same response and go n check in CW , u will get error in logs regardless of whether or not the function succeeded or failed because we don't want to know the results, due to the nature of asynchronous.

-- So what we can do instead is maybe set up a dead letter queue.

-- now go to sqs in console --> create one std queue 

-- now come to lambda --> refresh the page --> configuration --> open IAM role for lambda --> attch AmazonSQSFullAccess permissions 

-- lambda --> configuration --> asynchronous configuration --> edit --> add SQS 

-- test function , it will fail we know that 

-- So, if we try to go now into our CloudShell, and invoke this function again, we'll know that the invocation itself will fail because the lambda function function was coded to fail.

-- But now what's going to happen is that the DLQ is going to kick into effect, so we're gonna have 2 retry attempts, and after the retry attempts have failed then the message should go into Amazon SQS.

-- now go to sqs and check for message we have one message 





------------------------------------------------------ CloudWatch Events / EventBridge


-- how we can integrate CloudWatch Events or EventBridge with Lambda.

-- 2 ways 


1 CRON or Rate EventBridge Rule ------------------(Trigger Every 1 hour)------------> AWS Lambda Funchon Perform a task


2 CodePipeline EventBridge Rule ------------------(Trigger on State Changes) ------------> AWS Lambda Function Perform a task





------------------------------------------------------  CloudWatch Events / EventBridge (Hands ON)

-- create one function with python 3.9

-- make sure this function is being invoked by EventBridge.

-- Eventbridge --> rule --> create new rule --> Rule type = Schedule --> c.o continue to create rule ---> A schedule that runs at a regular rate, such as every 10 minutes. (1 min)--> choose target as aws lambda fun---> create rule

-- do refresh page of lambda u will see invocation of Eventbridge

-- wait for 1 min --> now check in cloudwatch invocation happens

-- now add print(event) in code and deploy the changes and wait for 1 Min

-- now do check in CW recent logs, u will see the event info

-- do disable our rule 




------------------------------------------------------ lambda and S3 Events Notifications


• S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...

    - So just a reminder on S3 event notifications, it is a way for you to get notified whenever an object is created, removed, restored, when there is a replication happening.

• Object name filtering possible (*.jpg)

    - You can filter by prefix and by suffix. And the use case is, the classic one is to generate thumbnail images of every image uploaded into Amazon S3.
    
    • Use case: generate thumbnails of images uploaded to S3


-- So you have your events into Amazon S3, and S3 can send it to three things,


1 Amazon S3 -------(Events)-------> SNS ---------> SQS

 
EXP : your events into Amazon S3, send to SNS and from an SNS topic, we can do a fan out pattern to send to multiple SQ-Q,  we can sent it into an SQ-Q


2 Amazon S3 -------(Events)-------> SQS ---------> Lambda Function


EXP : we can sent it into an SQ-Q , and have a Lambda function directly read off that SQ-SQ,


3 Amazon S3 -------(Events)-------(async)----------> Lambda Function ----------- (DLQ) ---------> SQS


EXP : we could have an Amazon S3 event notification, directly invoke our Lambda function and this is an asynchronous invocation.

- this Lambda function could do whatever it wants with that data, and then in case things go wrong, we can set up a dead-letter queue, for example an SQS, as we've seen from before.



• S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer

• If two writes are made to a single non- versioned object at the same time, it is possible that only a single event notification will be sent

• If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.




------------------------------------------------------ Simple S3 Event Pattern – Metadata Sync


  S3 bucket -------------(New file event)------------> lambda (Update metadata table) -----------> DynamoDB Table / Table in RDS


EXP : So, here is a simple pattern. An S3 bucket will have a new file event into Lambda. And Lambda function will process that file maybe insert that data into DynamoDB Table or even a table in RDS database.





------------------------------------------------------ lambda and S3 Events Notifications Hands On


-- create one lambda function and one s3 bucket , make sure both are in same region

-- s3 buket --> properties --> event notificatin --> create event notification for all object creation

-- So this event notification is enabled to send data into lambda function.

print(event) in code 


import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }



-- now do upload a file in the s3 bucket

-- now what's going to happen is that this should trigger an event into my Lambda function and to see whether or not this has worked,

-- lambda --> cloudwatch --> u will see that records have created with all the event info 







------------------------------------------------------ Lambda – Event Source Mapping


-- So we have seen asynchronous processing, we have seen synchronous processing, and now we are going to see Event Source Mapping. So this is the last category of how Lambda can process events in AWS.

--  it applies to 


   • Kinesis Data Streams
   • SQS & SQS FIFO queue
   • DynamoDB Streams


• Common denominator: records need to be polled from the source

    - So Lambda needs to ask the service to get some records and then the records will be returned.

    - So that means that Lambda needs to poll from these services.


• Your Lambda function is invoked synchronously



            Poll
         ---------------->
Kinesis                         Inside the lambda function (Event Source Mapping(internal)) ----------------(INVOKE WITH EVENT BATCH)-----------------> Lambda Function
         <----------------
            Return



-- We have Kinesis and the Lambda service, and if we configure Lambda to read from Kinesis, 

-- then they will be internally an Event Source Mapping that will be created. And that is responsible for polling Kinesis, and returning and getting the results back from Kinesis.

-- So Kinesis will return a batch to us.

-- then once this Event Source Mapping has some data for Lambda to process, it's going to invoke our Lambda function synchronously, with an event batch.







------------------------------------------------------ So there are two categories of Event Source Mapper. (Streams and Queues)



------------------------------------------ 1 Streams & Lambda (Kinesis & DynamoDB)


• So in case of streams, they will be an Event Source Mapping. They will create an iterator for each shard,

    - so each Kinesis shard or DynamoDB Stream shard. And the items will be processed in order at the shard level.


• so we can configure where to start to read from. You can read it with just the new items, or from the beginning of the shard or from a specific timestamp.


• Whenever an item is processed from a shard, whether it be from Kinesis or DynamoDB,Processed items aren't removed from the stream (other consumers can read them)


-- So the use case for this is either low traffic or high traffic.


• Low traffic: use batch window to accumulate records before processing , So to make sure that you invoke another function efficiently.

-- then if you have a very high throughput stream and you want to speed up processing,

• You can process multiple batches in parallel at shard level 

    • up to 10 batches per shard
    • in-order processing is still guaranteed for each partition key,




------------------------------------------ Streams & Lambda – Error Handling


• By default, if your function returns an error, the entire batch is reprocessed until the function succeeds, or the items in the batch expire.

    - this is very imp ,Having an error in a batch can block your processing. 


• To ensure in-order processing, processing for the affected shard is paused until the error is resolved

-- so you can manage that in several ways.

• You can configure the event source mapping to:

     • discard old events

     • restrict the number of retries

     • split the batch on error (to work around Lambda timeout issues)


• Discarded events can go to a Destination





------------------------------------------ 2 Lambda – Event Source Mapping SQS & SQS FIFO





            Poll
         ---------------->
SQS                        Inside the lambda function (Event Source Mapping(internal)) ----------------(INVOKE WITH EVENT BATCH)-----------------> Lambda Function
         <----------------
            Return


-- The SQS queue will be polled by a Lambda Event Source Mapping.And then whenever a batch is returned, your Lambda function will be invoked synchronously with the event batch.

-- So in the case of SQS, the Event Source Mapping will poll SQS using Long Polling. So it's going to be efficient.

• Specify batch size (1-10 messages)

• Recommended: Set the queue visibility timeout to 6x the timeout of your Lambda function

• To use a DLQ

      • set-up on the SQS queue, not Lambda (DLQ for Lambda is only for async invocations)

      • Or use a Lambda destination for failures


---- but not on Lambda. Why?

ANS : Because the DLQ for Lambda only works for asynchronous invocations. And this is a synchronous invocations.



• Lambda also supports in-order processing for FIFO (first-in, first-out) queues, And the number of Lambda functions that will be scaling to process your queue will be equal to the number of active message groups.

• For standard queues, items aren't necessarily processed in order.

• for a standard queue, Lambda will scale as fast as possible to read all the messages in your standard queue.

• When an error occurs, batches are returned to the queue as individual items and might be processed in a different grouping than the original batch.

• Occasionally, the event source mapping might receive the same item from the queue twice, even if no function error occurred.

• Lambda deletes items from the queue after they're processed successfully.

• You can configure the source queue to send items to a dead-letter queue if they can't be processed.





------------------------------------------ Lambda Event Mapper Scaling


• Kinesis Data Streams & DynamoDB Streams:

     • One Lambda invocation per stream shard

     • If you use parallelization, up to 10 batches processed per shard simultaneously


• SQS Standard:

     • Lambda adds 60 more instances per minute to scale up

     • Up to 1000 batches of messages processed simultaneously


• SQS FIFO:

     • Messages with the same GroupID will be processed in order

     • The Lambda function scales to the number of active message groups







------------------------------------------------------ Lambda event source mapping Hands On


-- create one function with python 3.8 ,  make sure that lambda role has "AWSLambdaSQSQueueExecutionRole" permisssions 

-- create one SQS std queue ,

-- now go to lambda --> add triggers --> sqs --> create trigger

-- go to code --> make some changes

import json

def lambda_handler(event, context):
    # TODO implement
    print(event)
    return "success"


-- now let's test our Lambda function by simply going into SQS and sending a message.

-- send some message from SQS

--  so, because my Lambda function is continuously pulling from SQS, then it should process this message.

-- open logs in cloudwatch , u will able to see the message in the cloudwatch records

-- now go to queue --> see u have 0 messages 'coz the messages has been processed by our lambda function.

-- now disable the sqs trigger in the lambda 

-- now add 2nd type trigger called kinesis to lambda , do try by using aws documentation 






------------------------------------------------------ Lambda – Event and Context Objects



EventBridge -------------(invoke)----------------> Lambda Function


EVENT Object :


  • JSON-formatted document contains data for the function to process

  • Contains information from the invoking service (e.g., EventBridge, custom, ...)

  • Lambda runtime converts the event to an object (e.g., dict type in Python)

  • Example: input arguments, invoking service arguments, ...


  -- So let's take an example where your Lambda function is invoked, for example, by a EventBridge rule.

  -- So what's going to happen is that EventBridge is going to create an event and that event is going to be passed to your Lambda function

  -- your Lambda function will receive that event, and it's called the event object.

  -- the event object includes a lot of detail around, well, the event itself. Where it was emitted from and the service itself will include a lot of data related to that event in the event.


Context Objects :

    • Provides methods and properties that provide information about the invocation, function, and runtime environment

    • Passed to your function by Lambda at runtime

    • Example: aws_request_id, function_name, memory_limit_in_mb, ...


    -- this is more some metadata around your function, such as the request ID from AWS, your function name, the log group associated with your Lambda function, the memory limit, and so on.

    -- So this event object and this context object are very different but very complementary.





------------------------------------------------------ Access Event & Context Objects using Python


import time

def lambda_handler(event, context):   
    print("Lambda function ARN:", context.invoked_function_arn)
    print("CloudWatch log stream name:", context.log_stream_name)
    print("CloudWatch log group name:",  context.log_group_name)
    print("Lambda Request ID:", context.aws_request_id)
    print("Lambda function memory limits in MB:", context.memory_limit_in_mb)
    # We have added a 1 second delay so you can see the time remaining in get_remaining_time_in_millis.
    time.sleep(1) 
    print("Lambda time remaining in MS:", context.get_remaining_time_in_millis())




-- so the handler has an event and has a context, the event will have information, for example, such as the source of the event or the region of the event and so on that we can print to the console 

-- the context will have information, such as the request ID, the function ARN, the function name, the memory limits in megabytes, and then, for example, some information about CloudWatch Logs, such as the stream name or the group name.






------------------------------------------------------ Lambda – Destinations


-- So, the problem is that, when we've been doing asynchronous invocations or event mappers,

-- it was really hard for us to see if it had failed or succeeded.

-- So the idea with destinations is to send the result of an asynchronous invocation or the failure of an event mapper into somewhere.


• So here is a very cool new feature from Nov 2019: Can configure to send result to a destination

• Asynchronous invocations - can define destinations for successful and failed event:

    • Amazon SQS 
    • Amazon SNS
    • AWS Lambda
    • Amazon EventBridge bus

• Note: AWS recommends you use destinations instead of DLQ now (but both can be used at the same time)


• Event Source mapping: This is only used for when you have an event batch that gets discarded because we can't process it. So we can, instead, send that event batch into Amazon SQS or Amazon SNS,

IMP • Note: that if you have an Event Source mapping reading from SQS, you can set either a failed destination or you can set up a DLQ directly on your SQS Queue.






------------------------------------------------------ Lambda – Destinations Hands on


-- for this demo , choose s3 event notificatin invoke funtion , 

-- now go to sqs create two queues 1 for success and 2nd is for failure 

-- go to lambda function --> add destination --> asynchronous invocation --> onfailure --> sqs --> fail sqs --> save

-- go to lambda function --> add destination --> asynchronous invocation --> onsuccess --> sqs --> success sqs --> save

-- So now we need to test the fact that these two destinations are working properly.

-- now upload any file in the s3 bucket , go to sqs refresh the page , u will find one message , u will find the body of the file 

-- So we get both the event source, as well as the event response and some extra information into the body of my SQS queue message,

-- now try for failure , go to code and raise exception

import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    raise Exception("this is failure sqs")


-- now upload some files in bucket , my lambda function is going to be invoked asynchronously and is going to raise an exception.

-- I should not see any messages right away. in SQS , do you know why?

ANS : because S3 invoking my lambda function is an asynchronous type of invocation. 


    - Then if we go into the configuration remember, and go to the asynchronous invocation, well we have two retry attempts that are going to be running.

    - And then once the retry attempts are run, then the destination will be invoked as a failure.

    - so wait for sometime (3-4 min)


-- now do refresh in SQS , u will find one message for failure and poll that message 

-- So we could debug what happened in here, in our lambda function to make sure that it doesn't happen next time.





------------------------------------------------------ Lambda Execution Role (IAM Role)


• Grants the Lambda function permissions to AWS services / resources

• Sample managed policies for Lambda:
  
      • AWSLambdaBasicExecutionRole – Upload logs to CloudWatch.

      • AWSLambdaKinesisExecutionRole – Read from Kinesis

      • AWSLambdaDynamoDBExecutionRole – Read from DynamoDB Streams

      • AWSLambdaSQSQueueExecutionRole – Read from SQS

      • AWSLambdaVPCAccessExecutionRole – Deploy Lambda function in VPC

      • AWSXRayDaemonWriteAccess – Upload trace data to X-Ray.



• When you use an event source mapping to invoke your function, Lambda uses the execution role to read event data.

• Best practice: create one Lambda Execution Role per function



------------------------------------------------------ Lambda Resource Based Policies


• Use resource-based policies to give other accounts and AWS services permission to use your Lambda resources

• Similar to S3 bucket policies for S3 bucket

• An IAM principal can access Lambda:

      • if the IAM policy attached to the principal authorizes it (e.g. user access and we have full permissions, so we can access our Lambda function, this is what we've been doing so far,)

      • OR if the resource-based policy authorizes (e.g. service access) , this is more helpful when you have a service to service access.



• When an AWS service like Amazon S3 calls your Lambda function, the resource-based policy gives it access.





------------------------------------------------------  Lambda Resource Based Policies Hands On


--  go to IAM --> search for lambda permissions --:> explore those permissions it have 




------------------------------------------------------ Lambda Environment Variables


• Environment variable = key / value pair in “String” form

• Adjust the function behavior without updating code

• The environment variables are available to your code

• Lambda Service adds its own system environment variables as well

• Helpful to store secrets (encrypted by KMS)

• Secrets can be encrypted by the Lambda service key, or your own CMK




------------------------------------------------------ Lambda Environment Variables Hands ON 



-- create one lambda function with python 3.8

--  so we're going to pass environment variables in unencrypted form to learn their function.

-- modify the lambda code first 

import json
import os 

def lambda_handler(event, context):
    # TODO implement
    return os.getenv("ENVIRONMENT_NAME")


-- deploy changes

-- os package  = this is going to allow us to get access to environment variables.

-- So we're going to create this environment variable called "ENVIRONMENT_NAME" and we're going to give it a value and the value of which will be retrieved by the lambda function and return.

-- lambda function --> configuration --> environment variables --> edit --> ENVIRONMENT_NAME = give value ---> save 

-- So for now this environment variable is unencrypted.

-- now do test ur code , change ur variable name and test again 

-- so the code did not change, so here we see the impact that environment variables can have on to the return values and the behavior of our code, 




------------------------------------------------------ Lambda Logging & Monitoring


-- how Lambda does logging, monitoring, and tracing.


• CloudWatch Logs: 

      • AWS Lambda execution logs are stored in AWS CloudWatch Logs

      • Make sure your AWS Lambda function has an execution role with an IAM policy that authorizes writes to CloudWatch Logs , this is included in the Lambda basic execution role as we've seen before.


• CloudWatch Metrics: 
   
      • AWS Lambda metrics are displayed in AWS CloudWatch Metrics UI or the Lambda UI.

      • they will represent information about your Invocations, Durations, Concurrent Executions

      • Error count, Success Rates, Throttles

      • Async Delivery Failures

      • Iterator Age (Kinesis & DynamoDB Streams)



------------------------------------------------------ Lambda Tracing with X-Ray


• Enable in Lambda configuration (Active Tracing)

• It will Runs the X-Ray daemon for you

• Use AWS X-Ray SDK in Code

• Ensure Lambda Function has a correct IAM Execution Role

      • The managed policy is called AWSXRayDaemonWriteAccess


IMP • Environment variables to communicate with X-Ray

        • _X_AMZN_TRACE_ID: contains the tracing header 

        • AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR

 IMP    • AWS_XRAY_DAEMON_ADDRESS: the X-Ray Daemon IP_ADDRESS:PORT

     



------------------------------------------------------ Lambda Tracing with X-Ray Hands On


-- lambda function ---> configuration --> monitor --> enable active tracing 

-- take s3 event notifications examples ---> write code for success --> upload object --> wait for 5 minutes --> go n check in x-ray console 




------------------------------------------------------ Lambda@Edge and CloudFront Functions 


-- Customization At The Edge : Well we know we deploy our functions and our applications in a specific region, but sometimes using, for example, CloudFront, we have the Edge locations are distributing our content.

• Many modern applications execute some form of the logic at the edge, before reaching the application itself. So these are called Edge Functions,


• Edge Function:

      • A code that you write and attach to CloudFront distributions

      • Runs close to your users to minimize latency


• CloudFront provides two types: CloudFront Functions & Lambda@Edge

• You don’t have to manage any servers, deployed globally

• Use case: customize the CDN content

• Pay only for what you use

• Fully serverless



------------------------- CloudFront Functions & Lambda@Edge Use Cases


• Website Security and Privacy
• Dynamic Web Application at the Edge
• Search Engine Optimization (SEO)
• Intelligently Route Across Origins and Data Centers 
• Bot Mitigation at the Edge
• Real-time Image Transformation
• A/B Testing
• User Authentication and Authorization
• User Prioritization
• User Tracking and Analytics



------------------------- CloudFront Functions




              Viewer Request                               Origin Request
          ------------------------>                    -------------------------> 
  Client                                  CloudFront                                      Origin 
          <------------------------                    <------------------------
              Viewer Response                               Origin Response 




-- So the client will do a request into CloudFront, and this is called a viewer request, because the client views it.

-- Then CloudFront will do an origin request into your origin server, the server will reply to CloudFront,

-- so we have an origin response, and finally CloudFront sends that response to the client, so we have a viewer response.




• CloudFront Functions are Lightweight functions written in JavaScript and they modified the viewer request and response.

• They're used  For high-scale, latency-sensitive CDN customizations

• which gives you Sub-ms startup times, millions of requests/second

• Used to change Viewer requests and responses:

       • Viewer Request: after CloudFront receives a request from a viewer we can modify this 

       • Viewer Response: before CloudFront forwards the response to the viewer


• Native feature of CloudFront (manage code entirely within CloudFront)

IMP - Remember, CloudFront Functions high performance, high scale only for the viewer request and response.






------------------------- Lambda@Edge


-- Now Lambda@Edge is a bit more, so you can modify all of them.

• Lambda functions written in NodeJS or Python

• Scales to 1000s of requests/second

• Used to change CloudFront requests and responses:
 
     • Viewer Request – after CloudFront receives a request from a viewer

     • Origin Request – before CloudFront forwards the request to the origin

     • Origin Response – after CloudFront receives the response from the origin

     • Viewer Response – before CloudFront forwards the response to the viewer


• Author your functions in one AWS Region (us-east-1), then CloudFront replicates to its locations





              Viewer Request                               Origin Request
          ------------------------>                    -------------------------> 
  Client                                  CloudFront                                      Origin 
          <------------------------                    <------------------------
              Viewer Response                               Origin Response 




------------------------- CloudFront Functions vs. Lambda@Edge



                                        CloudFront Functions                                               Lambda@Edge         


Runtime Support                          JavaScript                                                         Node.js, Python

# of Requests                            Millions of requests per second                                    Thousands of requests per second

CloudFront Triggers                      - Viewer Request/Response                                          - Viewer Request/Response  and - Origin Request/Response

Max. Execution Time                      <1ms                                                                5 – 10 seconds

Max. Memory                              2MB                                                                 128MB upto 10GB

Total Package Size                       10 KB                                                                 1MB–50MB

Network Access, File System Access         No                                                                   YES

Access to the Request Body               NO                                                                      YES

Pricing                                  Free tier available, 1/6th price of @Edge                             No free tier, charged per request & duration





------------------------- CloudFront Functions vs. Lambda@Edge - Use Cases


CloudFront Functions


• Cache key normalization

      • Transform request attributes (headers, cookies, query strings, URL) to create an optimal Cache Key


• Header manipulation

      • Insert/modify/delete HTTP headers in the request or response


• URL rewrites or redirects


• Request authentication & authorization

      • Create and validate user-generated tokens (e.g., JWT) to allow/deny requests      





Lambda@Edge 


• Longer execution time (several ms)

• Adjustable CPU or memory

• Your code depends on a 3rd libraries (e.g., AWS SDK to access other AWS services)

• Network access to use external services for processing

• File system access or access to the body of HTTP requests





------------------------------------------------------ Lambda in VPC 



Lambda by default


   • By default, your Lambda function is launched outside your own VPC (in an AWS-owned VPC)

   • Therefore it cannot access resources in your VPC (RDS, ElastiCache, internal ELB...)



Lambda in VPC

   • You must define the VPC ID, the Subnets and the Security Groups

   • Lambda will create an ENI (Elastic Network Interface) in your subnets

   • your Lambda function needs a  AWSLambdaVPCAccessExecutionRole




                                     |- - - - - - - - - - - - - - - - 
                                     | Private Subnet 
                                     |
                                     |
                                     |
                                     |
                                     |
                                     |
                                     |
  Lambda Function ------------------ |--------> Elastic Network Interface (ENI) (Lambda SG) ----------------> Amazon RDS In VPC (RDS Security group)
                                     |
                                     |
                                     |- - - - - - - - - - - - - - - - 



-- we have our RDS security group around our Amazon RDS database in our VPC.

-- this Lambda function, we want to give it VPC access. Therefore once we've set it up correctly

-- it will create an ENI, an elastic network interface, alongside the Lambda security group

-- to access your RDS database, Your Lambda is going to go through your ENI. You know, it's invisible, we don't see it, but this is how it happens behind the scenes.

-- So it will go through the ENI into your Amazon RDS database.

-- so for this to work, we need to make sure that the RDS security group does allow network access from the Lambda security group, just like for EC2 instances and load balancer






------------------------------------------------------ Lambda in VPC – Internet Access


-- What if we deploy a Lambda function in a VPC, can we access the public internet?


• By default , A Lambda function in your VPC does not have internet access

• Deploying a Lambda function in a public subnet does not give it internet access or a public IP


------------- So what can we do then?


• Deploying a Lambda function in a private subnet gives it internet access if you have a NAT Gateway / Instance

    - The NAT gateway or instance will be talking to the internet gateway of our VPC and the internet gateway will give us access to the external API.


------------ Next, what if you want you to access DynamoDB?


-- we can access DynamoDB either through the public route and through your internet gateway.

-- So this would work once a NAT is put in place.

• You can use VPC endpoints to privately access AWS services without a NAT


IMP : So if you deploy a Lambda function in a private subnet, note that your CloudWatch Logs work even if you have no end points or NAT gateway. CloudWatch Logs is something that functions no matter what.





 ------------------------------------------------------ Lambda in VPC Hands On


-- create one lambda function with py 3.8(use 3.11 now)

-- create one sg for lambda without any inbound rules 

--  go to configuration --> permissions --> add AWSLambdaENIManagementAccess to the IAM role 

-- So back into our Lambda function. Now let's go into the configuration and make sure that we can deploy this within our VPC and add subnets 

-- because right now the Lambda function is within the AWS cloud and it has internet access but it doesn't have VPC access.

-- if right now we publish our Lambda function in three subnets, And these are public subnets because they have access to the internet, as we know, The Lambda function can still not have access to the internet.

-- It needs instead to be published, deployed into a private subnets and have a NAT gateway or NAT instance in a public subnets to route traffic to.

-- our Lambda function is now deployed within our VPC. wait for 5 min to update

-- test ur code , now go to ENI we can see here three network interfaces have been created and the this respond to the network interfaces of my Lambda function within my VPC.
                
-- So these network interfaces each in one different AZ. So each in one different subnets is what allows my Lambda function to communicate with our VPC.








------------------------------------------------------ Lambda Function Improvement



Lambda Function Configuration


• RAM:

     • From 128MB to 10GB in 1MB increments

     • The more RAM you add, the more vCPU credits you get

     • At 1,792 MB of RAM, a function has the equivalent of one full vCPU

     • After 1,792 MB, you get more than one CPU, and need to use multi-threading in your code to benefit from it (up to 6 vCPU)


• If your application is CPU-bound (computation heavy), increase RAM

    - So if your application is CPU bound, that means that it has a lot of computations, and you want to improve the performance of your application, 
    
    - that means to decrease the amount of time your function will run for, then you need to increase your application,your lambda function RAM.


• Timeout: default 3 seconds, maximum is 900 seconds (15 minutes)

    - by default has a timeout of three seconds. That means that if your lambda function runs for more than three seconds, it will error out with a timeout,

    - Anything above 15 minutes is not a good use case for lambda and is something maybe that's going to be better for Fargate, ECS, or EC2.





------------------------------------------------------ Lambda Execution Context 


• The execution context is a temporary runtime environment that initializes any external dependencies of your lambda code

• Great for database connections, HTTP clients, SDK clients...

• The execution context is maintained for some time in anticipation of another Lambda function invocation

• The next function invocation can “re-use” the context to execution time and save time in initializing connections objects

• The execution context includes the /tmp directory

     - which is a space where you can write files and they will be available across executions.



------------------------------------------------------ Initialize outside the handler


BAD : The DB connection is established At every function invocation


import json
import time

def lambda_handler(event, context):
    connect_to_db()
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
def connect_to_db():
      time.sleep(3)




GOOD! : The DB connection is established Once And re-used across invocations

import json
import time

def connect_to_db():
      time.sleep(3)


connect_to_db()

def lambda_handler(event, context):
   
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }






------------------------------------------------------ Lambda Functions /tmp space


-- what if you need to write some temporary files and reuse them?

ANS : You can use the /tmp space.

• If your Lambda function needs to download a big file to work...

• If your Lambda function needs disk space to perform operations...

• You can use the /tmp directory

• Max size is 10GB

• The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations (helpful to checkpoint your work)

• For permanent persistence of object (non temporary), use S3

• To encrypt content on /tmp, you must generate KMS Data Keys




------------------------------------------------------ Lambda Function Improvement Hands On



step 1 :


-- lambda function --> configuraation --> general configuration --> keep timeout sec = 3 

-- now modify code for this demo 


import json
import time

def lambda_handler(event, context):
    time.sleep(2)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }


-- here we make sleep = 2 sec , it will work 'coz we have given timeout sec = 3 , and see the executed time 

-- But what happens if we make the Lambda function sleep five seconds?

-- make sleep(5), do test 

-- the Lambda function will fail. Why? Because it will timeout. So we got an error message here saying, Hey the tasks timed out after three seconds

-- now change timeout = 6 secs in configuration 

-- now u will get response without any error



Step 2 :


------ Last thing to optimize your Lambda function performance is around where you set the initialization of your function.

-- So if you're connecting to a DB, modify code

import json
import time

def lambda_handler(event, context):
    connect_to_db()
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
def connect_to_db():
      time.sleep(3)


-- where we connect to the database within the Lambda handler,

-- that means that every time we invoke our function this function connects to DB is going to be run it's going to take three seconds 

-- because it takes a long time to connect your database and then is going to return the results you have.

-- test again , it will take 3 sec again because we are connecting to database every single time, 



Step 3 :


-- instead of doing the connection to the database within the Lambda handler, you do it outside of it.


import json
import time

def connect_to_db():
      time.sleep(3)


connect_to_db()

def lambda_handler(event, context):
   
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }



-- deploy and run the code , at first INIT time it will take 3 sec , then do test again 

-- it will take less time , now my function is much quicker because we've done the database initialization again outside of the function handler.

-- Imagine that instead of here, instead of sleeping, you actually connect to the database and you get a database object out of it that you can use within your Lambda handler






------------------------------------------------------ Lambda Layers


-- They're a newer feature of Lambda, they allow us to do two things.


• Custom Runtimes

      • Ex: C++ https://github.com/awslabs/aws-lambda-cpp

      • Ex: Rust https://github.com/awslabs/aws-lambda-rust-runtime



• Externalize Dependencies to re-use them:

     




------------------------------------------------------ Lambda Layers Hands On


-- create function with py 3.8

-- So we're going to use a layer provided by AWS because it is very complicated for us to create a layer

        https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/


--  go to layer in lambda function --> c.o layers --> add layer --> pythonSciPy layer ----> select latest version

-- copy code in website and paste in function 

-- So we can see the first two lines is import numpy as np, and imports a site by spatial and this function right here.

-- So this means that these things are being imported. 

-- And when you have imports on stuff that Lambda function do not know usually you have to pack it with dependencies just the way we did it for node.JS and NPM.

-- But it turns out that thanks to the layer the dependencies are actually available to our Lambda function, but we didn't have to pack them as dependencies from moving in our code, which is quite cool.

-- run code 

--  important part to remember is that the code itself of the function was using a library here.

-- And that library came from the Lambda layer we had added from before.





------------------------------------------------------ Lambda – File Systems Mounting


• Lambda functions can access EFS file systems if they are running in a VPC

• Configure Lambda to mount EFS file systems to local directory during initialization

• Must leverage EFS Access Points

• Limitations: watch out for the EFS connection limits (one function instance = one connection) and connection burst limits



-- see Lambda – Storage Options in documentation




------------------------------------------------------ Lambda Concurrency



-------------------- Lambda Concurrency and Throttling


-- In Lambda, concurrency is the number of in-flight requests that your function is currently handling. There are two types of concurrency controls available:


1 Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges.


2 Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account.






-- So the more we invoke our Lambda functions, the more we will have concurrent executions of our Lambda functions. We know this because Lambda can scale very, very easily and fast.

• Concurrency limit: up to 1000 concurrent executions

• Can set a “reserved concurrency” at the function level (=limit)

       EG : this Lambda function can only have "up to 50 concurrent executions.


• Each invocation over the concurrency limit will trigger a “Throttle”

• Throttle behavior:

       • If synchronous invocation => return ThrottleError - 429

       • If asynchronous invocation => retry automatically and then go to DLQ


• If you need a higher limit, open a support ticket




-------------------- Lambda Concurrency Issue


• If you don’t reserve (=limit) concurrency, the following can happen:

IMP : the concurrency limit applies to all the functions in your accounts, and so you have to be careful because if one function goes over the limit, it's possible that your other functions get throttled.




-------------------- Concurrency and Asynchronous Invocations


-- let's take the example of S3 event notifications. So we are uploading files into our S3 buckets,

-- and this creates a new file event that will invoke our Lambda functions,

-- say we are putting many, many files at the same time. So we get many, many different Lambda concurrent executions happening.

• If the function doesn't have enough concurrency available to process all events, additional requests are throttled.

• For throttling errors (429) and system errors (500-series), Lambda returns the event to the queue and attempts to run the function again for up to 6 hours. So there's a lot of retries that happens due to the throttling and so on.

       - 'coz it is asynchronous mode


• The retry interval increases exponentially from 1 second after the first attempt to a maximum of 5 minutes.





-------------------- Cold Starts & Provisioned Concurrency


• Cold Start:

      • New instance => code is loaded and code outside the handler run (init)

      • If the init is large (code, dependencies, SDK...) this process can take some time.

      • First request served by new instances has higher latency than the rest , user is unhappy

      - that may impact your users. So if your user is maybe waiting three seconds to get a request response, that may be very, very slow for them and 

      - they may experience a cold start and may be unhappy with your product.


• Provisioned Concurrency:

      • Concurrency is allocated before the function is invoked (in advance)

      • So the cold start never happens and all invocations have low latency

      • Application Auto Scaling can manage concurrency (schedule or target utilization),to make sure that you have enough reserved Lambda functions to be ready to be used and minimize this cold start problem.



• Note:
 
     • Note:cold starts inVPC have been dramatically reduced in Oct & Nov 2019

     - https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/





-------------------- Reserved and Provisioned Concurrency


https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html






------------------------------------------------------ Lambda external Dependencies


------------ Lambda Function Dependencies


-- we have been doing some pretty simple Lambda functions. Just some code, no external dependencies.

-- In the real world, you definitely need to add more dependencies with the packages and so on.

• If your Lambda function depends on external libraries: for example AWS X-Ray SDK, Database Clients, etc...

• You need to install the packages alongside your code and zip it together

      • For Node.js, use npm & “node_modules” directory

      • For Python, use pip --target options

      • For Java, include the relevant .jar files


• Upload the zip straight to Lambda if less than 50MB, else to S3 first

• Native libraries work: they need to be compiled on Amazon Linux

• AWS SDK comes by default with every Lambda function





------------------------------------------------------ Lambda external Dependencies Hands On


-- we're going to create a Lambda function with dependencies.


-- create 2 files 


1 index.js


// Require the X-Ray SDK (need to install it first)
const AWSXRay = require('aws-xray-sdk-core')

// Require the AWS SDK (comes with every Lambda function)
const AWS = AWSXRay.captureAWS(require('aws-sdk'))

// We'll use the S3 service, so we need a proper IAM role
const s3 = new AWS.S3()

exports.handler = async function(event) {
  return s3.listBuckets().promise()
}



2 steps.sh


#!/bin/bash

# You need to have nodejs / npm installed beforehand
npm install aws-xray-sdk

# Set proper permissions for project files
chmod a+r * 

# You need to have the zip command available
zip -r function.zip .

# create the Lambda function using the CLI
aws lambda create-function --zip-file fileb://function.zip --function-name lambda-xray-with-dependencies --runtime nodejs14.x --handler index.handler --role arn:aws:iam::001736599714:role/DemoLambdaWithDependencies





-- open cloushell

-- you need to create a Lambda folder.

       - mkdir lambda 
       - cd lambda
       - ll


-- the first thing is that you need to add this index.js file.

       - sudo yum install -y nano
       - nano index.js    (paste index.js content into this file )
       - ctrl + x  , then y , then enter
       - cat index.js



-- let's go into the steps to install some dependencies, and make sure the function will be packaged correctly to be uploaded into the Lambda service.

-- if we look at this index.js 

       - we can see that it requires the xray-sdk-core, and this is something that we need to bundle with our Lambda function.

       - Then we are opening the AWS SDK to talk to Amazon S3, and we return a ListBuckets operation into Amazon S3.


-- So what we need to do is to make sure that our Lambda function does have access to the xray-sdk for AWS.

       - npm install aws-xray-sdk        (which is going to install locally, all the necessary files and folders to have access to this SDK.)

       - ll  u will have the files 


-- Now in here, we need to edit the permissions of these files.

      - chmod a+r * 


-- Next, we need to zip all the, all the files in here into a zip file called functions.zip.

       - zip -r function.zip .

       - ll


-- now u have zip file , finally, we need to upload this .zip file into the Lambda console.

-- create lambda function through the CLI 

--  to do so , first u gave to create one role here IAM --> create new role for the lambda --> add AWSLambdaBasicExecutionRole permission ---> create role

-- now copy the role of ARN and paste in the cmnd 

  

      aws lambda create-function --zip-file fileb://function.zip --function-name lambda-xray-with-dependencies --runtime nodejs20.x --handler index.handler --role arn:aws:iam::298132369629:role/lambdawithdependencyrole


-- the function got created , go n check in console 

-- now do test , u will get errror , 'coz the lambda try to access s3 

-- add s3readonlypermissions in iam role

-- this version is not supporting , anyhow we can do this with external dependencies





------------------------------------------------------ Lambda and CloudFormation 


-- So we can use CloudFormation to upload Lambda function. And so the way we do it, we have two ways.


1 Lambda and CloudFormation – inline

     - so we would define our Lambda code inline of our CloudFormation templates.

     • Inline functions are very simple

     • Use the Code.ZipFile proper ty

     • You cannot include function dependencies with inline functions


2 Lambda and CloudFormation – through S3

     
     • You must store the Lambda zip in S3

     • You must refer the S3 zip location in the CloudFormation code

           • S3Bucket
           • S3Key: full path to zip
           • S3ObjectVersion: if versioned bucket


     • If you update the code in S3, but don’t update S3Bucket, S3Key or S3ObjectVersion, CloudFormation won’t update your function

          - this is why versioning is recommended because if you're enabled versioning and you overwrite the file and you specify a new S3 object version 
          
          - then CloudFormation will pick up the change and update your Lambda function.




------------------------------------------------------ Lambda and CloudFormation – through S3 Multiple accounts


-- say you will have an account which contains an S3 bucket with your Lambda code.

-- Now you want to deploy this Lambda code into Account 2 and Account 3.

-- How do this ?

-- first of all, we need to launch CloudFormation in Account 2. And the S3 bucket is going to be referencing is the S3 bucket in Account 1.

-- Now you need to ask yourself, how do we make sure that Account 2 has access to the Lambda code in Account 1?

ANS : - Well, we can use a bucket policy and a bucket policy on the S3 bucket in Account 1 should allow CloudFormation to access the code.

      - But also we can define an execution role on your CloudFormation service for the template itself which will allow to get and list to the S3 bucket in Account 1.

      - the two things combined is going to allow CloudFormation to retrieve the code from the S3 bucket and therefore create your Lambda function.

      - do same things for account 3 , 4.... also



------------------------------------------------------ Lambda and CloudFormation  Hands On


-- we're going to create a CloudFormation template upload it that will create a Lambda function for us.


lambda.yaml




Parameters:
  S3BucketParam:
    Type: String
  S3KeyParam:
    Type: String
  S3ObjectVersionParam:
    Type: String

Resources:
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: root
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:*
            Resource: arn:aws:logs:*:*:*
          - Effect: Allow
            Action:
            - xray:PutTraceSegments
            - xray:PutTelemetryRecords
            - xray:GetSamplingRules
            - xray:GetSamplingTargets
            - xray:GetSamplingStatisticSummaries
            Resource: "*"
          - Effect: Allow
            Action: 
            - s3:Get*
            - s3:List*
            Resource: "*"

  LambdaWithXRay: 
    Type: "AWS::Lambda::Function"
    Properties: 
      Handler: "index.handler"
      Role: 
        Fn::GetAtt: 
          - "LambdaExecutionRole"
          - "Arn"
      Code: 
        S3Bucket: 
          Ref: S3BucketParam
        S3Key: 
          Ref: S3KeyParam
        S3ObjectVersion:
          Ref: S3ObjectVersionParam
      Runtime: "nodejs20.x"
      Timeout: 10
      # Enable XRay
      TracingConfig:
        Mode: "Active"



-- here , we see that there are three parameters available to us. The S3 Bucket parameter, The S3 Key parameter and the S3 object version parameter.

-- these will be helpful to tell CloudFormation where to get the function zip from in Amazon S3.

-- Next we create resources.

-- the first resource we create is a Lambda execution role. So it's an IAM role. 

-- There is a policy document which allows Lambda functions to assume this IAM role  (AssumeRolePolicyDocument) 

-- as well as the policy itself. the policy contains multiple statements.

       - The first one is actions on CloudWatch logs

       - The second one is action on x-ray to be able to send traces to x-ray

       - the last one and finally some allow operations on S3, Get * , List * to be able to read it from Amazon S3.

-- Next to function itself, Lambda with x-ray.

-- So it's a function in which we have to specify the handlers so Index.handler

-- the role which is the Lambda execution role Arn that we get and retrieve thanks to this interesting function get attributes that will be get gotten from this Lambda role right here

-- So as we can see, the entire configuration of our Lambda function can be done through CloudFormation which is really awesome.




-- create one bucket in s3 , enable versioning (IMP), 

-- open cloud shell , cd lambda --> pwd --> copy path of it --> actions --> download file  and add /function.zip at last

        /home/cloudshell-user/lambda/function.zip


-- So now we can reuse this function.zip , upload this file in s3 bucket 

-- So now we are able to reference this function, zip directly from our CloudFormation templates.

-- now go to CF --> upload lambda.yaml file 

-- now add Parameters

       - S3BucketParam        = bucket Name 

       - S3KeyParam           = function.zip

       - S3ObjectVersionParam = BVhDrj7iqDmMhHmGC.TbSfmgQZZSNAzq (c.o on object --> versions)



-- create stack 

-- As soon as we opened this function we get an information message. This function belongs to an application because there was an integration between CloudFormation and Lambda that was detected by the Lambda console.

-- So it knows that it is managed by a CloudFormation template.

-- module is not supporting here  is not supporitng here but overall idea is that to understnad the concept 

-- u can also see x-ray tracing is enabled 




------------------------------------------------------ Lambda Container Images


-- Lambda was supporting container images and this is a new feature. And this allows to deploy Lambda Function as a container of images up to 10 gigabytes from ECR


• Deploy Lambda function as container images of up to 10GB from ECR

• Pack complex dependencies, large dependencies in a container

           - So Docker is very famous to allow you to help to put your application code, the dependencies, and the data sets it needs all together on top of a base image.

           - And that base image must implement the Lambda Runtime API.

           - So the idea is that to make it seemingly simple, Lambda runs a virtual machine, the container. And so you use the base image of that container,

           - you add on your application code and your dependencies and you pack this as an image that Lambda can run

           - because this base image implements the Lambda Runtime API. And this allows you to run your containers onto Lambda,

           - but it's not any Docker container. The base image must implement the Lambda Runtime API. So these base images exist for multiple languages.



• Base images are available for Python, Node.js, Java, .NET, Go, Ruby

• Can create your own image as long as it implements the Lambda Runtime API

• Test the containers locally using the Lambda Runtime Interface Emulator

• Unified workflow to build apps




            |-----------------------------------------------------
            |       Application Code                             |
            |------------------------------------                |
            |      Dependencies, datasets                        |
            |------------------------------------                |
            |  Base Image must implement the Lambda Runtime API  |
            |----------------------------------------------------
                           |
                           |  Build Publish
                           |
                           |

                        Amazon ECR

                           |
                           |   Deploy
                           |
                           |
                         Lambda
                          
                           



• Example: build from the base images provided by AWS 


# Use an image that implements the Lambda Runtime API

    FROM amazon/aws-lambda-nodejs:12

# Copy your application code and files

    COPY app.js package*.json ./

# Install the dependencies in the container

    RUN npm install

# Function to run when the Lambda function is invoked

    CMD [ "app.lambdaHandler" ]




------------------------------------------------------ Lambda Container Images – Best Practices


• Strategies for optimizing container images:


1 • Use AWS-provided Base Images

       • Stable,Built on AmazonLinux2, 'coz these base images are already cached by Lambdaservice 

       - so if you use them, then the Lambda service has to pull less information out of your containers.


2 • Use Multi-Stage Builds

    
       • Build your code in larger preliminary images, copy only the artifacts you need in your final container image, discard the preliminary steps

       - Therefore, the final image is going to be much smaller and much more simple.


3 • Build from Stable to Frequently Changing

       • Make your most frequently occurring changes as late in your Dockerfile as possible

       - for example installing base packages on your image should be as early as possible.


4 • Use a Single Repository for Functions with Large Layers

       • ECR compares each layer of a container image when it is pushed to avoid uploading and storing duplicates




• Use them to upload large Lambda Functions (up to 10 GB) 

       - instead of having some code pushed to Lambda as is, you can create a very large container image and use that as the basis for your Lambda Function.






------------------------------------------------------ AWS Lambda Versions


• When you work on a Lambda function, we work on $LATEST

     $LATEST (mutable) -  we can able to change / edit 



• When we’re ready to publish a Lambda function, we create a version

• Versions are immutable

     $LATEST (mutable) ---------> V1 (Immutable) and V2 (immutable)


• Versions have increasing version numbers (V1,v2,v3.......)


• Versions get their own ARN (Amazon Resource Name)

• Version = code + configuration (nothing can be changed - immutable)

• Each version of the lambda function can be accessed




------------------------- But what if he wants to give your end user a standard endpoint?



So for this, we can use Lambda Aliases.



------------------------------------------------------ AWS Lambda Aliases


• Aliases are ”pointers” to Lambda function versions

• We can define a “dev”, ”test”, “prod” aliases and have them point at different lambda versions

• Aliases are mutable

• Aliases enable Canary deployment by assigning weights to lambda functions

     - Because we can assign weights to the Lambda function versions we point to.

     - So for PROD for example, say we want to switch from the V1 function all the way to the V2 function.

     - Instead of switching the pointer, we can say 95% of the traffic is going to go to V1. And only 5% of the traffic is going to go to V2.

     - the purpose of this is that now we're testing V2 in PROD, making sure it works before we switch the full power onto V2 and 100% of the traffic there.



• Aliases enable stable configuration of our event triggers / destinations

• Aliases have their own ARNs

IMP : • Aliases cannot reference aliases , They can only reference version 




                                                          USER

                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                   --------------------    |       ---------------------      
                                  |                        |                           |
                                  |                        |                           |
                                  |                        |                           |
                                  |                        |                           |
                                  |                        |                           |
                         DEV Alias (mutable)         PROD Alias (mutable)        TEST Alias (mutable)

                                  |                       |          |                  |
                                  |                       |          |                  |
                                  |                       |    95%   |   5%             |
                                  |                       |          |                  |
                                  |                       |          |-------------     |
                                  |                       |                        |    |
                                                                                   |    |
                          $LATEST (mutable)          V1 (Immutable)                 V2 (Immutable)  






 ------------------------------------------------------ AWS Lambda Versions Aliases Hands On


 -- create lambda function with py 3.8

 -- So versions are here to allow us to fix a amount of code and removables and settings in time,

 -- say you are a specific version. And the idea is that we want to author our versions over time to allow our lambda function to evolve.

 -- make code like this to test 


 import json

def lambda_handler(event, context):
    # TODO implement
    return "this is version 1"



-- So say we're very happy with this code and we want to author this as the version one of our function.

-- actions --> publish new version --> publish --> here u were not able to  change ur code which is immutable

-- if u want to edit , u can do in main function

-- now go to function code and make it as v2 

-- do publish new version 2

-- main function --> versions --> u will get all versions here



--- now coming to Aliases 

-- main function --> aliases --> create aliases --> dev ---> latest version --> create

-- main function --> aliases --> create aliases --> Test ---> 2 --> create ,  it is latest published version

-- main function --> aliases --> create aliases --> Prod ---> 1 --> create ,  it is most stable version of our function 


--  we have created versions and Aliases 

-- in aliases , we have see the weight for deployment

-- So let's go into prod and we want to edit this alias because we want to upgrade from V1 to V2

-- go to aliases --> prod --> edit --> select weighted alias as V2 version --> give some portion of traffic for testing purpose (eg : 50%)

-- So now our alias "prod" has a weight between two versions.

-- open prod and do test , here 50% of traffic for v1 and remaing for v2 

-- then when you're happy with your production and say okay now my new function version is working just as expected,

-- edit prod from v1 to v2 






------------------------------------------------------ Lambda & CodeDeploy



• CodeDeploy can help you automate traffic shift for Lambda aliases

• CodeDeploy Feature is integrated within the SAM framework(Serverless Application Model)



    

  


                                                          Make X vary over time until X = 100%
                                                      | -----------------------------------------------   
                                                      |                                                |
                                                      |                                                |  
                                                      |                                                |
                                                      |                                                |
                                                      |           PROD Alias (lambda) -----(100 – X%)------>    V1                 
                                                      |                 |
                                                      |                 |
                                                      |                 | --------------V2
                                                      |                                  

                                                 Code Deploy ---------------> X%



-- So we have a PROD Alias, and say we want to upgrade it from the Lambda function Version 1 to the lambda function Version 2.

-- So we want to shift the traffic from 100% of V1 to V2.

-- CodeDeploy will be making the X, in this example, vary over time until X equals 100.

-- So that means that X will be first equal to 10%. And so we'll have 90% on V1 and 10% on V2.

-- finally, all the way up to 100% on V2 and 0% on V1.




------------- what strategies that CodeDeploy have?


• Linear: grow traffic every N minutes until 100%

      • Linear10PercentEvery3Minutes 

      • Linear10PercentEvery10Minutes

• Canary: try X percent then 100%

      • Canary10Percent5Minutes

      • Canary10Percent30Minutes


• AllAtOnce: immediate 

      - This is the quickest and most dangerous

      - because if we haven't tested our V2 function, then things can fail.



• Can create Pre & Post Traffic hooks to check the health of the Lambda function

     - the idea is that if anything goes wrong, then the traffic hooks can be failing or a CloudWatch alarm can be failing,

     - your CodeDeploy can know that something is going wrong. And therefore, it will do a rollback and put the traffic back all onto V1.





------------------------------------------------------ Lambda & CodeDeploy – AppSpec.yml


version 0.0

Resources :
  - myLambdaFunction:
       Type: AWS::lambda::Function
       Properties:
          Name: myLambdaFunction
          Alias: myLambdaFunctionAlias
          CurrentVersion: 1
          targetVersion: 2



-- • Name (required) – the name of the Lambda function to deploy

• Alias (required) – the name of the alias to the Lambda function

• CurrentVersion (required) – the version of the Lambda function traffic currently points to

• TargetVersion (required) – the version of the Lambda function traffic is shifted to





------------------------------------------------------ Lambda – Function URL


-- What if you wanted to just expose your Lambda function as an HTTP endpoint without having to go through the hassle of using API Gateway or an application balancer?



• Dedicated HTTP(S) endpoint for your Lambda function

• A unique URL endpoint is generated for you (never changes)
 
           • https://<url-id>.lambda-url.<region>.on.aws (dual-stack IPv4 & IPv6)


• Invoke via a web browser, curl, Postman, or any HTTP client

• Access your function URL through the public Internet only
   
      • Doesn’t support PrivateLink (Lambda functions do support)


• Supports Resource-based Policies & CORS configurations

• Can be applied to any function alias or to $LATEST (can’t be applied to other function versions)

• Create and configure using AWS Console or AWS API

• Throttle your function by using Reserved Concurrency





------------------------------------------ Lambda – Function URL Security


• Resource-based Policy , this gets attached to your Lambda function and this is going to be able to say

       • Authorize other accounts / specific CIDR / IAM principals


• Cross-Origin Resource Sharing (CORS)

       • If you call your Lambda function URL from a different domain



                example.com
              <------------------------>      <--------------------> S3 Bucket (Static Website Hosting)
       users                            C.F
              <------------------------
                api.example.com      |
                                     |
                                     |
                               Lambda Function URL      


        - So in this example, our S3 bucket is fronted by CloudFront to which we have a custom URL as example.com. But the API is hosted as a Lambda function URL,

        - Because the domains are different, you need to set the CORS setting on your Lambda function URL to make things work.



-- if u have 

• AuthType NONE – allow public and unauthenticated access

      • Resource-based Policy is always in effect (must grant public access)


• AuthType AWS_IAM – IAM is used to authenticate and authorize requests

      • Both Principal’s Identity-based Policy & Resource-based Policy are evaluated

      • Principal must have lambda:InvokeFunctionUrl permissions

      • Same account – Identity-based Policy OR Resource-based Policy as ALLOW

      • Cross account – Identity-based Policy AND Resource Based Policy as ALLOW






------------------------------------------------------ Lambda – Function URL Hands On


-- create a function with py 3.9

-- publish this function 

-- create alias for this version

-- lambda function --> alias --> function URL --> select Identity policy (None) --> save 

-- url is generated , paste in browser u will o/P

-- so we were able to access our Lambda Function publicly, through a URL.






------------------------------------------------------ Lambda and CodeGuru Profiling 


• Gain insights into runtime performance of your Lambda functions using CodeGuru Profiler

• CodeGuru creates a Profiler Group for your Lambda function

• Supported for Java and Python runtimes

• Activate from AWS Lambda Console

• When activated, Lambda adds:

       • CodeGuru Profiler layer to your function

       • Environment variables to your function

       • AmazonCodeGuruProfilerAgentAccess policy to your function





------------------------------------------------------ AWS Lambda Limits to Know - per region



• Execution:
  
      • Memory allocation: 128 MB – 10GB (1 MB increments)

      • Maximum execution time: 900 seconds (15 minutes)

      • Environment variables (4 KB)

      • Disk capacity in the “function container” (in /tmp): 512 MB to 10GB

      • Concurrency executions: 1000 (can be increased)



• Deployment:

      • Lambda function deployment size (compressed .zip): 50 MB

      • Size of uncompressed deployment (code + dependencies): 250 MB

      • Can use the /tmp directory to load other files at startup

      • Size of environment variables: 4 KB





------------------------------------------------------ AWS Lambda Best Practices



• Perform heavy-duty work outside of your function handler

      • Connect to databases outside of your function handler

      • Initialize the AWS SDK outside of your function handler

      • Pull in dependencies or datasets outside of your function handler



• Use environment variables for:

      • Database Connection Strings, S3 bucket, etc... don’t put these values in your code

      • Passwords, sensitive values... they can be encrypted using KMS


• Minimize your deployment package size to its runtime necessities.

      • Break down the function if need be

      • Remember the AWS Lambda limits

      • Use Layers where necessary


• Avoid using recursive code, never have a Lambda function call itself









====================================================================== Amazon DynamoDB (NoSQL Serverless Database)=======================================================



---------------------------------------------------------- Traditional Architecture


• Traditional applications leverage RDBMS databases

• These databases have the SQL query language

• Strong requirements about how the data should be modeled

• Ability to do query joins, aggregations, complex computations

• Vertical scaling (getting a more powerful CPU / RAM / IO)

• Horizontal scaling (increasing reading capability by adding EC2 / RDS Read Replicas)





---------------------------------------------------------- NoSQL databases


• NoSQL databases are non-relational databases and are distributed

• NoSQL databases include MongoDB, DynamoDB, ...

• NoSQL databases do not support query joins (or just limited support)

• All the data that is needed for a query is present in one row

• NoSQL databases don’t perform aggregations such as “SUM”, “AVG”, ...

• NoSQL databases scale horizontally

        - That means that if you need more write or read capacity, you can behind the scenes have more instances and it will scale really well.



• There’s no “right or wrong” for NoSQL vs SQL, they just require to model the data differently and think about user queries differently




---------------------------------------------------------- Amazon DynamoDB


• Fully managed, highly available with replication across multiple AZs

• NoSQL database - not a relational database

• Scales to massive workloads, distributed database

• Millions of requests per seconds, trillions of row, 100s of TB of storage

• Fast and consistent in performance (low latency on retrieval)

• Integrated with IAM for security, authorization and administration

• Enables event driven programming with DynamoDB Streams

• Low cost and auto-scaling capabilities

• Standard & Infrequent Access (IA) Table Class

-- our "DynamoDB supports "Document and key-value(JSON) pair" 




---------------------------------------------------------- DynamoDB - Basics


• DynamoDB is made of Tables

• Each table has a Primary Key (must be decided at creation time)

• Each table can have an infinite number of items (= rows)

• Each item has attributes (can be added over time – can be null)

         - But these attributes can also be nested. So it's a bit more powerful than columns,

         - they can be added over time, you don't need to define them all at creation time of your table, and some of them can be null.

         - so it's completely fine for an attribute to be missing in some data.


• Maximum size of an item is 400KB

• Data types supported are:
 
         • Scalar Types – String, Number, Binary, Boolean, Null

         • Document Types – List, Map

         • Set Types – String Set, Number Set, Binary Set




---------------------------------------------------------- DynamoDB – Primary Keys


--------- So you have two options for primary keys.



--------- • Option 1: Partition Key (HASH)

       • Partition key must be unique for each item 

       • Partition key must be “diverse” so that the data is distributed

       • Example:“User_ID” for a users table


  

        User_ID              First_Name        Last_Name      Age

        7791a3d6             subbu             kattamedi      20
        u63527db             jncjdnc                          33
        7357336gv            lsidmckdmc                       23



--  User_ID  = Partition Key (primary key),    

--  First_Name        Last_Name      Age    = Attributes

-- here , missing attributes is fine in dynamoDB



--------- • Option 2: Partition Key + Sort Key (HASH + RANGE)

                   • The combination must be unique for each item

                   • Data is grouped by partition key , this is why it's very important to choose a good partition key.

                   • Example: users-games table,“User_ID” for Partition Key and “Game_ID” for Sort Key

                         - That means that users can attend multiple games.



          

          User_ID          Game_ID            Score         Result

          7791a3d6          2233              20              win
          98377366          3421              40             lost
          98377366          2123              33              win


-- “User_ID” for Partition Key and “Game_ID” for Sort Key  and    Score and Result = attributes

-- here for 2 and 3 are same partiton key but different Sort key





---------------------------------------------------------- DynamoDB – Partition Keys (Exercise)


• We’re building a movie database

• What is the best Partition Key to maximize data distribution?

       • movie_id

       • producer_name

       • leader_actor_name

       • movie_language


• “movie_id” has the highest cardinality so it’s a good candidate

• “movie_language” doesn’t take many values and may be skewed(not accurate or exact) towards English so it’s not a great choice for the Partition Key





---------------------------------------------------------- DynamoDB – Read/Write Capacity Modes


• Control how you manage your table’s capacity (read/write throughput)


• Provisioned Mode (default) 

      • You specify the number of reads/writes per second

      • You need to plan capacity beforehand

      • Pay for provisioned read & write capacity units


• On-Demand Mode

      • Read/writes automatically scale up/down with your workloads

      • No capacity planning needed

      • Pay for what you use, more expensive ($$$)


• You can switch between different modes once every 24 hours      

       






---------------------------------------------------------- R/W Capacity Modes – Provisioned


• Table must have provisioned read and write capacity units

• Read Capacity Units (RCU) – throughput for reads

• Write Capacity Units (WCU) – throughput for writes

• Option to setup auto-scaling of throughput to meet demand

• Throughput can be exceeded temporarily using “Burst Capacity”

• If Burst Capacity has been consumed, you’ll get a “ProvisionedThroughputExceededException”

• It’s then advised to do an exponential backoff retry





---------------------------------------------------------- DynamoDB – Write Capacity Units (WCU)


• One Write Capacity Unit (WCU) represents one write per second for an item up to 1 KB in size

• If the items are larger than 1 KB, more WCUs are consumed


• Example 1: we write 10 items per second, with item size 2 KB

ANS :     • We need  10 * (2 KB / 1 KB)  = 20 WCU's


• Example 2: we write 6 items per second, with item size 4.5 KB

ANS : • We need  6 * (5 KB / 1 KB)  = 30 WCU's , because the 4.5 kilobytes is always going to be rounded to the upper kilobyte by DynamoDB to get an idea of how many WCUs you've consumed.


• Example 3: we write 120 items per minute, with item size 2 KB

ANS : we need (120 / 60 ) * (2KB / 1KB)  = 4 WCU'S , We need to do a small computation which is 120 divided by 60 to get items per second,


--  1 WCU = 2.5 million Writes 



---------------------------------------------------------- Strongly Consistent Read vs. Eventually Consistent Read




-- So if you consider DynamoDB, it's a serverless database, of course, but behind the scenes, there are servers.

-- You just don't see them or manage them.

-- So we have servers, and let's just consider three servers right now to make it very, very simple, but obviously a lot more,

-- your data is going to be distributed and replicated across all the servers.

-- Now, if you consider your application, your application is going to do writes to one of these servers,

-- internally DynamoDB is going to replicate these writes across different servers, such as Server 2 and Server 3.

-- Now, when your application reads from DynamoDB, there is a chance that you're going to read not from Server 1 but from Server 2.

-- Now, two things can happen



1 • Eventually Consistent Read (default) 

        • If we read just after a write, it’s possible we’ll get some stale(old) data because of replication has not happened yet


2 • Strongly Consistent Read

       • If we read just after a write, we will get the correct data

       • For this, we need to Set “ConsistentRead” parameter to True in API calls (GetItem, BatchGetItem, Query, Scan)    

       • Consumes twice the RCU

       - so it's going to be a more expensive query and also may have a slightly higher latency.  





---------------------------------------------------------- DynamoDB – Read Capacity Units (RCU)



• One Read Capacity Unit (RCU) represents one Strongly Consistent Read per second, or two Eventually Consistent Reads per second, for an item up to 4 KB in size


      - For ECR , 1 RCU = 2 Reads per second of 4KB size

      - For SCR, 1RCU = 1 read per second of 4KB Size 


• If the items are larger than 4 KB, more RCUs are consumed

      - it's going to be rounded up to the nearest upper four kilobytes.


• Example 1: 10 Strongly Consistent Reads per second, with item size 4 KB

ANS : 


-- we know the basic of 

   -- For ECR , 1 RCU = 2 Reads per second of 4KB size

   -- For SCR, 1RCU = 1 read per second of 4KB Size 


-- here it have mentioned SCR , so 1RCU = 1 read per second of 4KB Size 


we need 10 * (4Kb / 4KB) = 10 RCU'S




• Example 2: 16 Eventually Consistent Reads per second, with item size 12 KB 

ANS : here it is ECR so,


we need (16 / 2) * (12KB / 4KB)  = 8 * 3 = 24 RCU'S




• Example 3: 10 Strongly Consistent Reads per second, with item size 6 KB

ANS : 10 * (8KB / 4KB) = 20 RCU's (we must roundup 6KB to 8KB)

        - here we do not have 6 we have only 4 and 8 KB so,go for near (only up) so 8KB 



- example 4 : An application doing Strongly Consistent reads of 10 items per second, with each item 10 KB in size. What RCU do you choose?

Ans : 10 KB gets rounded to 12 KB. 10 * (12 KB / 4 KB) = 30 RCU.


- example 5 : An application doing Eventually Consistent reads of 12 items per second, with each item 16 KB in size. What RCU do you choose?

ANS : We can do 2 Eventually Consistent reads per seconds for items of 4 KB with 1 RCU. 12 * (16 KB / 4 KB) = 48 / 2 = 24 RCU.






---------------------------------------------------------- DynamoDB – Partitions Internal



-- how DynamoDB works in the backend with partitions.?


• Data is stored in partitions , partitions are just copies of your data that live on specific servers.

       - when your application does writes into DynamoDB, what's going to happen is that your application will send a partition key, a sort key maybe, and some attributes.


• Partition Keys go through a hashing algorithm to know to which partition they go to

      - So if we take the partition key of ID_13, it's going to go through this internal hash function of DynamoDB, t's going to say, "Hey, any time I see ID_13, this is going to go into Partition 1."

      - if you have ID_45 in the second row, then ID_45 is going to go through the hash function, and the hash function will say, "Hey, this ID_45 should go to Partition 2."



-- So, obviously, if you have what's called a hot partition, the data is always going to be the hot key, the data is always going to be into the same partition.


• To compute the number of partitions:

     • # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by capacity = (RCU's Total / 3000) + (WCU's Total / 1000)


     • # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by Size     = Total Size / 10 GB


      • # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠            = ceil(max(# 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by capacity, # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by Size ))



• WCUs and RCUs are spread evenly across partitions






---------------------------------------------------------- DynamoDB – Throttling


• If we exceed provisioned RCUs or WCUs, we get “ProvisionedThroughputExceededException”

• Reasons:

     • Hot Keys – one partition key is being read too many times (e.g., popular item) from a specific partition,

     • Hot Partitions

     • Very large items, remember RCU and WCU depends on size of items


• Solutions:

     • Exponential backoff when exception is encountered (already in SDK)

     • Distribute partition keys as much as possible (how we can choose a really good partition key.)

     • If RCU issue, we can use DynamoDB Accelerator (DAX)





---------------------------------------------------------- R/W Capacity Modes – On-Demand


• Read/writes automatically scale up/down with your workloads

• No capacity planning needed (WCU / RCU)

• Unlimited WCU & RCU, no throttle, more expensive

• You’re charged for reads/writes that you use in terms of RRU and WRU

• Read Request Units (RRU) – throughput for reads (same as RCU)

• Write Request Units (WRU) – throughput for writes (same as WCU)

• 2.5x more expensive than provisioned capacity (use with care)

• Use cases: unknown workloads, unpredictable application traffic, ...






---------------------------------------------------------- DynamoDB - Basic Operations 



--------------------------- DynamoDB – Writing Data


• PutItem

     • Creates a new item or fully replace an old item (same Primary Key)

     • ConsumesWCUs


• UpdateItem

     • Edits an existing item’s attributes or adds a new item if it doesn’t exist , with UpdateItem, we only edit a few attributes, not every other attribute.

     • Can be used to implement Atomic Counters – a numeric attribute that’s unconditionally incremented



• Conditional Writes

     • Accept a write/update/delete only if conditions are met, otherwise returns an error

     • Helps with concurrent access to items

     • No performance impact




--------------------------- DynamoDB – Reading Data


• GetItem

      • Read based on Primary key

      • Primary Key can be HASH or HASH+RANGE

      • Eventually Consistent Read (default)

      • Option to use Strongly Consistent Reads (more RCU - might take longer)

      • ProjectionExpression can be specified to retrieve only certain attributes




--------------------------- DynamoDB – Reading Data (Query)


• Query returns items based on:

       • KeyConditionExpression
           
            • Partition Key value (must be = operator) – required

            • SortKeyvalue(=,<,<=,>,>=,Between,Beginswith)–optional

      
      • FilterExpression

            • Additional filtering after the Query operation (before data returned to you)

            • Use only with non-key attributes (does not allow HASH or RANGE attributes)




• Returns:

       • The number of items specified in Limit

       • Or upto 1MB of data

• Ability to do pagination(Pagination is splitting a database output into manageable chunks or pages)  on the results

• Can query table, a Local Secondary Index, or a Global Secondary Index






--------------------------- DynamoDB – Reading Data (Scan)


• Scan the entire table and then filter out data (inefficient)

• Returns up to 1 MB of data – use pagination to keep on reading (page 1, page2 ,....)

• Consumes a lot of RCU

• Limit impact using Limit or reduce the size of the result and pause

• For faster performance, use "Parallel Scan"

     • Multiple workers scan multiple data segments at the same time

     • Increases the throughput and RCU consumed

     • Limit the impact of parallel scans just like you would for Scans


• Can use ProjectionExpression & FilterExpression (no changes to RCU)




--------------------------- DynamoDB – Deleting Data


• DeleteItem

     • Delete an individual item

     • Ability to perform a conditional delete


• DeleteTable

     • Delete a whole table and all its items

     • Much quicker deletion than calling DeleteItem on all items




--------------------------- DynamoDB – Batch Operations


• Allows you to save in latency by reducing the number of API calls

• Operations are done in parallel for better efficiency

• Part of a batch can fail; in which case we need to try again for the failed items


• BatchWriteItem

     • Up to 25 PutItem and/or DeleteItem in one call

     • Up to 16 MB of data written,up to 400 KB of data per item

     • Can’t update items (use UpdateItem)

     • UnprocessedItems for failed write operations(exponential backoff or add WCU)

          - you can retry the items within the UnprocessedItems. So two options to process them correctly.

          - Either you use an exponential backup strategy to keep on trying with longer and longer time until it succeeds,

          - if you consistently get these UnprocessedItems and scaling issues, then of course, you need to add write capacity units to allow your batch operations to complete efficiently.


• BatchGetItem

      • Return items from one or more tables

      • Up to 100 items,up to 16 MB of data

      • Items are retrieved in parallel to minimize latency

      • UnprocessedKeys for failed read operations (exponential backoff or add RCU)





--------------------------- DynamoDB – PartiQL


-- we have specific API calls to do specific things, but sometimes all that you know, as a data engineer or whatever, as a developer, may be SQL.

-- so you can use SQL on DynamoDB by using PartiQL.


• SQL-compatible query language for DynamoDB

• Allows you to select, insert, update, and delete data in DynamoDB using SQL , But this time, instead of doing the DynamoDB specific APIs you can just use SQL.

• Run queries across multiple DynamoDB tables , but you cannot do joins

• Run PartiQL queries from:

     • AWS Management Console
     • NoSQL Workbench for DynamoDB
     • DynamoDB APIs
     • AWS CLI
     • AWS SDK


-- the goal of it is really not to add new capabilities to DynamoDB because you have the same capabilities, but it's just to use SQL to write these API calls against DynamoDB.





-------- see hands on in course for basic API'S







---------------------------------------------------------- DynamoDB – Conditional Writes



-- So this is for the write operations.


• For PutItem, UpdateItem, DeleteItem, and BatchWriteItem

• You can specify a Condition expression to determine which items should be modified:

    • attribute_exists
    • attribute_not_exists
    • attribute_type
    • contains (for string)
    • begins_with (for string)
    • ProductCategory IN (:cat1, :cat2) and Price between :low and :high • size (string length)


• Note: Filter Expression filters the results of read queries, while Condition Expressions are for write operations





---------------------------------------------------------- Conditional Writes – Example on Update Item



-- To perform a conditional update, you use an UpdateItem operation with a condition expression. The condition expression must evaluate to true in order for the operation to succeed; otherwise, the operation fails.



EG :


aws dynamodb update-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --update-expression "SET Price = Price - :discount" \
    --condition-expression "Price > :limit" \
    --expression-attribute-values file://values.json



-- we say that we want to set the price to be equal to the current price minus discounts only if the price is over to a specific limit.

-- That's our condition expression.

-- We pass in the file://values.json , which has the discount to be a number of 150 and the limit to be a number of 500.


{
    ":discount": { "N": "150"},
    ":limit": {"N": "500"}
}



-- So that means that if we have this item in our DynamoDB table with a key 456 if we apply this Update Item command is going to be transformed to be now having a price of 500.


{
    "Id": { "N": "456"},
    "Price": {"N": "650"},
    "ProductCategory": {"S": "Sporting Goods"}
}


             |
             |
             |
             |


{
    "Id": { "N": "456"},
    "Price": {"N": "500"},
    "ProductCategory": {"S": "Sporting Goods"}
}


-- That's because indeed before the price was strictly over the limit of 500 because 650 is greater than 500.

-- But if we apply yet again the same command now it's not going to succeed. The price will never go below 500 

-- because, well, the condition expression would evaluate to being false.







---------------------------------------------------------- Conditional Writes – Example on Delete Item



• attribute_not_exists

      • Only succeeds if the attribute doesn’t exist yet (no value)


aws dynamodb update-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "attribute_not_exists(price)"



-- For example, we're saying hey, delete an item or delete many items if you have a batch right item we can do delete one item for our product catalog if the attribute price doesn't exist.



• attribute_exists

     • Opposite of attribute_not_exists


aws dynamodb update-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "attribute_exists(ProductReviews.onestar)"






---------------------------------------------------------- Conditional Writes – Do Not Overwrite Elements


• attribute_not_exists(partition_key)

     • Make sure the item isn’t overwritten


• attribute_not_exists(partition_key) and attribute_not_exists(sort_key)

     • Make sure the partition / sort key combination is not overwritten






---------------------------------------------------------- Conditional Writes – Example Complex Condition



aws dynamodb delete-item \
    --table-name ProductCatalog \
    --key '{"Id":{"N":"456"}}' \
    --condition-expression "(ProductCategory IN (:cat1, :cat2)) and (Price between :lo and :hi)" \
    --expression-attribute-values file://values.json



-- The arguments for --expression-attribute-values are stored in the values.json file.


{
    ":cat1": {"S": "Sporting Goods"},
    ":cat2": {"S": "Gardening Supplies"},
    ":lo": {"N": "500"},
    ":hi": {"N": "600"}
}


-- we can pass in this values.json file we're specifying category 1 and category 2 as well as the low of 500 and the high of 600.

-- And so when we apply Delete Item in case the item does belong to this range then we're good to go.

-- But as we can see, for example, even though the products category Sporting Good is compliant with our condition, which is the first part of our condition,

{
    "Id": { "N": "456"},
    "Price": {"N": "650"},
    "ProductCategory": {"S": "Sporting Goods"}
}


-- the price is 650, which is not between 500 and 600. And so the whole condition is false and the item will not be deleted in this case.






---------------------------------------------------------- Conditional Writes – Example of String Comparisons


• begins_with – check if prefix matches

• contains – check if string is contained in another string



aws dynamodb delete-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "begins_with(Pictures.FrontView, :v_sub)" \
    --expression-attribute-values file://expression-attribute-values.json



-- The arguments for --expression-attribute-values are stored in the expression-attribute-values.json file.


{
    ":v_sub":{"S":"http://"}
}



-- So here for example, we're saying, hey I want to make sure to delete items if it begins with http.://




--You can check for an element in a set or look for a substring within a string by using the contains function. If the condition expression evaluates to true, the operation succeeds; otherwise, it fails.


-- The following example uses contains to delete a product only if the Color String Set has an element with a specific value.



aws dynamodb delete-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "contains(Color, :v_sub)" \
    --expression-attribute-values file://expression-attribute-values.json



-- The arguments for --expression-attribute-values are stored in the expression-attribute-values.json file.


{
    ":v_sub":{"S":"Red"}
}







---------------------------------------------------------- DynamoDB Indexs – Local Secondary Index (LSI)



-- same partition_key 

-- any Sort Key


• Alternative Sort Key for your table (same Partition Key as that of base table)

• The Sort Key consists of one scalar attribute (String, Number, or Binary)

• Up to 5 Local Secondary Indexes per table

• Must be defined at table creation time

      - So you cannot create them after your tables being created. So you need to have some careful thinking into how you want your table to be designed.


• Attribute Projections – can contain some or all the attributes of the base table (KEYS_ONLY, INCLUDE, ALL)


-- right now we can do queries on user ID and game ID, is very simply,

-- but we can not do a query on a user ID and game timestamp. For this, we need to do a scan and then do some service side and some client site filtering.

-- So to do so, if you wanted to do a query based on user ID and game timestamp. We would need to create an LSI and define the LSI on the attributes game timestamp.




          User_ID          Game_ID       Game_TS                            Score         Result

          7791a3d6          2233        “2021-03-15T17:43:08”               20              win
          98377366          3421        “2021-06-20T19:02:32”               40              lost
          98377366          2123        “2021-02-11T04:11:31”               33               win




-- User_ID , Game_ID   = PartitionKey  

--  Game_TS   = LSI

-- Game_TS , Score ,Result = attributes






---------------------------------------------------------- DynamoDB – Global Secondary Index (GSI)


-- ANY partition_key 

-- any Sort Key


• Alternative "Primary Key" (HASH or HASH+RANGE(Sort)) from the base table

• Speed up queries on non-key attributes

• The Index Key consists of scalar attributes (String, Number, or Binary)

• Attribute Projections – some or all the attributes of the base table (KEYS_ONLY, INCLUDE, ALL)

• Must provision RCUs & WCUs for the index

• the GSI are powerful because Can be added/modified after table creation



-- So let's take a very simple table in which we have user ID, game ID and game timestamp.

-- With this table we're able to query based on user ID. So it gives me all the games of this user,

-- but we cannot query on game ID as we can see right now, if you want to query on game ID, it would be extremely difficult when you do a scan and then filter client's site.


1 TABLE (query by “User_ID”)

           User_ID          Game_ID       Game_TS                            

          7791a3d6          2233        “2021-03-15T17:43:08”               
          98377366          3421        “2021-06-20T19:02:32”              
           98377366         2123         “2021-02-11T04:11:31”            


   User_ID  = Partition Key
   
   Game_ID  = Sort Key

   Game_TS = Attributes






-- So instead we're going to create a GSI, a global secondary index,

-- Which is going to give us the ability to query by game ID.

-- So now the partition key for the GSI becomes the game ID. The sort key may be for example, game timestamp. This is what you want to query on.

-- And the attributes become user ID because we've done a projection of the user ID attributes.


2 INDEX GSI (query by “Game_ID”)


                            Game_ID       Game_TS                      User_ID  

                            2233        “2021-03-15T17:43:08”           7791a3d6  
                            3421        “2021-06-20T19:02:32”           98377366 
                            2123         “2021-02-11T04:11:31”          98377366 


   User_ID = Attributes
   
   Game_ID =  Partition Key

   Game_TS = Sort Key 





---------------------------------------------------------- DynamoDB – Indexes and Throttling


• Global Secondary Index (GSI):

       • If the writes are throttled on the GSI, then the main table will be throttled! (So this is a very, very important that comes up in the exam.)

       • Even if the WCU on the main tables are fine , if there are throttling on the GSI, then the main table, no matter what, we'll be throttling       
 
       • Choose your GSI partition key carefully!

       • Assign your WCU capacity carefully!

       - Global Secondary Index (GSI) uses an independent amount of RCU and WCU and if they are throttled due to insufficient capacity, then the main table will also be throttled.



• Local Secondary Index (LSI):

       • Uses the WCUs and RCUs of the main table

       • No special throttling considerations





---------------------------------------------------------- (LSI + GSI) Hands On


-- create one table in dynamoDB

-- partition_key = user_id with string type 

-- sortkey = Game_TS

-- choose customize settings 

-- choose provisioned

-- read capacity --> off --> 1

-- write capacity --> off --> 1 

-- we can create either a local index or a global index. Now local indexes can only be created at table creation time, whereas global indexes can be created afterwards.

-- So as you can see, we do not have the option to specify a different Partition key, so user_id will remain the Partition key for this index, 

-- but we can specify a different Sort key.

-- create LSI --> Game_ID (string)

-- create Table 

-- let's have a look at how we can query it right now.

-- open table --> explore items --> query

-- So if I go into Query, as we can see, I can query either a table or an index and have two options.

-- So as we can see this local index allows me to query by a different Sort key,

-- now go to tables --> GSI --> create GSI , it will take more time to create, so wait to create   (https://repost.aws/knowledge-center/create-gsi-dynamodb)

-- we could enter a completely different Partition key, for example, game_id, and a Sort key could be, for example, game_ts,

-- go back to the query , u can able to query with game_id , game_ts








---------------------------------------------------------- DynamoDB - PartiQL


• Use a SQL-like syntax to manipulate DynamoDB tables

• Supports some (but not all) statements:

     • INSERT
     • UPDATE 
     • SELECT 
     • DELETE

• It supports Batch operations




---------------------------------------------------------- DynamoDB – Optimistic Locking


-- So DynamoDB has a feature called Optimistic Locking.


• So the idea is that you can do Conditional Writes in DynamoDB.

      - DynamoDB has a feature called “Conditional Writes”

-- What does that mean?

• A strategy to ensure an item hasn’t changed before you update/delete it

• Each item has an attribute that acts as a version number


-- I want to write this only if this condition is met and this is called Optimistic Locking.




for EG : we have a DynamoDB Table ,we have an item with a user ID, a first name and a version, which is one.


User_ID     First_Name      Version

12345        subbu             1


-- Now two clients at the same time want to update this item because they think that the first name is wrong.

-- the client 1 is saying, Hey, I want to update this to name = John, only if version = 1.

-- the client 2 says, I want to update name = Lisa only if version = 1.

-- What's going to happen is that's one of these requests, obviously, will make it to DynamoDB first.

-- so then DynamoDB will go ahead, and maybe it's the second one, will update the first name to Lisa. And will also update the version to 2,


User_ID     First_Name      Version

12345        Lisa             2


-- Now what's going to happen out of this is that the client 1 updates will not go through

-- because now that DynamoDB says, Hey, you told me I should do this update only if version = 1 but it turns out that now my version is = 2

-- therefore, the client will get a error message saying, Hey, you don't have the right data.

-- So this feature of Conditional Writes or Optimistic Locking





---------------------------------------------------------- DynamoDB Accelerator (DAX)


• Fully-managed, highly available, seamless in-memory cache for DynamoDB

• Microseconds latency for cached reads & queries

• Doesn’t require application logic modification (compatible with existing DynamoDB APIs) , And you just create a DAX cluster and you're good to go.

• Solves the “Hot Key” problem (too many reads)

       - So if you're reading a very specific key or a very specific item, too many times then you may get throttling on your RCU's.

       - But if it's cached by DAX, then you've just solved that problem.

       - So let's have an example, we DynamoDB it's made of tables and our application is trying to access these tables.

       - Now in between, we're going to create a DAX cluster, which is made of cache nodes, and we have to provision them in advance.

       - Now the application will directly interact with the DAX cluster, and the DAX cluster will fetch the data from the DynamoDB tables.

       - So by default, that means that some data is going to be cached, and if you think cached, you need to think TTL.


• So by default, every cache data will live for 5 minutes in your DAX cluster.

• Up to 10 nodes in the cluster

• Multi-AZ (3 nodes minimum recommended for production) , So one in each AZ. And DAX is fully secure,

• Secure (Encryption at rest with KMS,VPC, IAM, CloudTrail, ...)

-- So remember DAX is here to help you cache the most popular items or queries from DynamoDB.





---------------------------------------------------------- DynamoDB Accelerator (DAX) vs. ElastiCache




-- So with DAX, what you're going to do is that you're going to have a cache for individual objects or for your queries, or your scans.


-- This is very, very handy, and this is what I call simple types of queries. So your objects, your queries and your scans.



-- But if you're doing some kind of logic application wise, 

-- you know you're like doing a scan and then you're doing the sum, and then you're filtering out some data, and so on.

-- you don't want to do this every single time, because this is computationally expensive.

-- What you can do, is you can store the results of whatever your application just did in Amazon ElastiCache. And retrieve the data from ElastiCache directly,

-- instead of re-querying DAX and re-performing the aggregation client side. 

-- So this could be a good way to actually use them both together in the architecture.






                                              Store Aggregation Result
                                    | -----------------------------------------> Amazon ElastiCache
                                    |
                                    |
                                    |
                                    |
                                    |
Application -------------------------- 
                                    |
                                    |
                                    |
                                    |    - Individual objects cache 
                                    |    - Query & Scan cache                      
                                    |
                                    |
                                    | ------------------- (DynamoDB Accelerator (DAX)) ----------> Amazon DynamoDB







---------------------------------------------------------- DAX Hands On 

-- DAX is not free 


-- open dynamodb --> in the left navigation panel --> DAX --> cluster 

-- node family = It could be t-types or r-types.

-- So r is memory(this is for always-ready capacity,) 

-- and t is for bursting (so this is recommended for use case regarding a lower throughput,),

-- node type = dax.t2.small

-- choose all families

-- choose vpc and subnets 

-- ask to create new role for you 

-- Parameter group : So this is going to basically tell how long to cache the items, thanks to the time-to-live, and what's the query time-to-live as well.

      - But by default, the 1.0 is giving you five minutes of TTL on both item time and query time.

  
-- create cluster , it will take 10 min to create 


-- here there is a cluster endpoint, and this is the endpoint your application should leverage to just leverage the DAX feature. 





---------------------------------------------------------- DynamoDB Streams


• Ordered stream of item-level modifications (create/update/delete) in a table

     - So whenever you will insert an item or modify it or delete it, then that modification would be visible in the stream.

     - the stream will represent the list of all the modifications over time in your table.


• Stream records can be sent to multiple places:

     • Sent to Kinesis Data Streams

     • Read by AWS Lambda

     • Read by Kinesis Client Library applications


• Data Retention for up to 24 hours

      - so you need to make sure to either persist it somewhere like Kinesis Data Stream where you can have a longer retention or use whatever Lambda or KCL application to persist it somewhere more durable.


• Use cases:

     • react to changes in real-time (welcome email to users)
     • Analytics
     • Insert into derivative tables
     • Insert into OpenSearch Service
     • Implement cross-region replication



• Ability to choose the information that will be written to the stream: 

     • KEYS_ONLY – only the key attributes of the modified item

     • NEW_IMAGE – the entire item, as it appears after it was modified

     • OLD_IMAGE – the entire item, as it appeared before it was modified

     • NEW_AND_OLD_IMAGES – both the new and the old images of the item


• DynamoDB Streams are made of shards, just like Kinesis Data Streams

• You don’t provision shards, this is automated by AWS

• Records are not retroactively populated in a stream after enabling it    

        - So once you enable the stream, only then will you receive these updates based on the changes that are appearing in your DynamoDB table.





---------------------------------------------------------- Architecture of Dynaobd streams




                                                                                     |-------------------------
                                                                                     |  Processing Layer      |    messaging, notifications
                                                                 | ----------------->|                        |    ---------------------------->  Amazon SNS
                                                                 |                   |   KCL App              |
                                                                 |                   |                        |      filtering, transforming, ...
                                                                 |                   |    Lambda              |    -----------------------------> DDB Table
                                                                 |                   |                        |
                                                                 |                    -------------------------
                                                                 |                          |
              create/update/delete                               |                          |
Application -----------------------------> Table --------> DynamoDB Streams                 |
                   
                                                                 |                          |
                                                                 |
                                                                 |
                                                                 |
                                                                 |                           |                   analytics
                                                                 |                           |              ----------------------------> Amazon Redshift
                                                                 |                           |             |
                                                                 |                           |             |
                                                                                             |                    archiving
                                                         Kinesis Data Streams  ------------------------> KDF -------------------------->  Amazon S3  
                                                                                              |           |
                                                                                              |           |
                                                                                              |           |      indexing
                                                                                              | ---------------------------------------> OpenSearch Service







-- so we have our application, which does create, update, and deletes operations on our table,

-- any of these changes is going to appear in a DynamoDB Stream.

-- So from there, Kinesis Data Streams can be a receiver of your DynamoDB stream. 

-- because we're using KDS, Kinesis Data Streams, then we can have Kinesis Data Firehose as a result, 

-- and then maybe send it to Amazon Redshift to perform some analytics queries on top of your data in DynamoDB,

-- or sending to Amazon S3 for archival of all these changes, in case we need to,

-- or sending it to OpenSearch Service,to index it and to create a search capability on top of your DynamoDB table.

-- The cool thing about this architecture is that it's pretty much everything is managed by AWS.




-- If you wanted to add your own custom logic, you could use a processing layer in which you would create either a Kinesis Client Library App, maybe running on EC2, or a Lambda function that will be reading from DynamoDB streams.

-- So for example, you can have messaging or send notifications using Amazon SNS. You could do some filtering and transformation and then reinsert the data into a DynamoDB table,

-- or for example, you can also use Lambda to send data into OpenSearch, if you wanted to,  






---------------------------------------------------------- DynamoDB Streams & AWS Lambda


• You need to define an Event Source Mapping to read from a DynamoDB Streams

• You need to ensure the Lambda function has the appropriate permissions

• Your Lambda function is invoked synchronously






---------------------------------------------------------- DynamoDB Streams Hands On



Step 1 : create DynamoDb table

-- create table and give partiton key

-- once u create table , add some items into the table

-- go to table --> exports and streams --> turn on DynamoDB stream --> New and old images --> Turn on streams 


Step 2 : create lambda function 

--  in the same page , choose create trigger

-- create new function --> Use Blue print --> search for DynamoDB and select Process updates made to table with python --> give function name --> choose create a new role --> give DynamoDb details in trigger section --> create function

-- once ur function got created , u will get some error 

--  function --> configuration --> permissions --> c.o role --> attach permissions (policies) --> DynamoDBfullaccess --> do refresh the screen the error will disappear

-- ince ur  function is created go back to the trigger page in DynamoDB table add this function with batch size is 100 for this eg 


Step 3 : Test 

-- go back to the DynamoDB table and update and add some items into the table 

-- do some chganges as you want 

-- the changes will trigger to our lambda function '

-- now go to function --> monitor--> view cloudwatch logs --> now you will able to see all the changes , INSERT,UPDATE,DELETE all the modifications that are captured through the DynamoDB Stream APi and trigger through the lambda and you will able to see in the cloud watch logs what has modification occurs 






---------------------------------------------------------- DynamoDB –TimeTo Live (TTL)


• Automatically delete items after an expiry timestamp

• Doesn’t consume any WCUs (i.e., no extra cost)

• The TTL attribute must be a “Number” data type with “Unix Epoch timestamp” value

• Expired items deleted within 48 hours of expiration

    - Now expired items will not be expired right away, there's the guarantee that they will be expired within 48 hours of the expiration, 


• Expired items, that haven’t been deleted, appears in reads/queries/scans (if you don’t want them, filter them out)

• Expired items are deleted from both LSIs and GSIs

• A delete operation for each expired item enters the DynamoDB Streams (can help recover expired items)

• Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations, ...

    - SessionData it is a perfect use case of TTL.








---------------------------------------------------------- DynamoDB –TimeTo Live (TTL) Hands On




-- create one table in dynamoDB

-- partition_key = user_id with string type 

-- choose customize settings 

-- choose provisioned

-- read capacity --> off --> 1

-- write capacity --> off --> 1 

-- create table 

-- now go to table --> explore items --> create items 

-- noe add some data like 

-- userid(string), name(string), Expireson(Number)

-- now go to google and search for https://www.epochconverter.com/

-- so, this is how we can translate a timestamp of soon into an Epoch timestamp,

-- add epoch time to the items u want 

-- now additional settings --> turn on TTL --> provide attribute name from table(Expireson) 

-- after some time , it will delete the items 

-- TTL turnoff will take upto 1 hr 


 



---------------------------------------------------------- DynamoDB CLI – Good to Know


• --projection-expression: one or more attributes to retrieve

         -The idea is that we don't want to retrieve all the columns, all the attributes. We just want to retrieve a subset to maybe have less data come to us, or only the data we need.


• --filter-expression: filter items before returned to you


• General AWS CLI Pagination options (e.g., DynamoDB, S3, ...)

        • --page-size: specify that AWS CLI retrieves the full list of items but with a larger number of API calls instead of one API call (default: 1000 items)

                - So page-size is saying that we still want to retrieve the entire dataset, but this time, each sub-API call that we'll make to AWS will be smaller.

                - And the idea is your API call won't time out.

                - For example, if you have a table of 10,000 items and you do one API call, maybe you will retrieve 10,000 items at a time and you will time out.

                - But if you specify a page-size of 100, what's going to happen is that there's going to be 100 API calls of size 100,

                - that are going to be made behind the scenes, to make sure that the complete API call succeeds.

                - So page-size is an optimization in that sense, in that it allows you to do more API calls, but also avoid timeouts.


        • --max-items: max. number of items to show in the CLI (returns NextToken)

                - max-items works in combination with NextToken, or starting-token,        

                - which is to say that now that you've received, say, 25 items, we want to get the next 25, and so therefore, 
                
                - you need to use that NextToken to retrieve the next 25 items.




----------------------------------------------------------  DynamoDB CLI Hands On




# Demo Projection Expression
aws dynamodb scan --table-name UserPosts --projection-expression "user_id, content"

# Demo Filter Expression
aws dynamodb scan --table-name UserPosts --filter-expression "user_id = :u" --expression-attribute-values '{ ":u": {"S":"john123"}}'

# Page Size demo: will do 1 API call if you have 3 Items
aws dynamodb scan --table-name UserPosts 

# Will do 3 API calls if you have 3 Items
aws dynamodb scan --table-name UserPosts --page-size 1

# Max Item demo:
aws dynamodb scan --table-name UserPosts --max-items 1

# Fetch the next item
aws dynamodb scan --table-name UserPosts --max-items 1 --starting-token eyJFeGNsdXNpdmVTdGFydEtleSI6IG51bGwsICJib3RvX3RydW5jYXRlX2Ftb3VudCI6IDF9

# Fetch the next item
aws dynamodb scan --table-name UserPosts --max-items 1 --starting-token eyJFeGNsdXNpdmVTdGFydEtleSI6IG51bGwsICJib3RvX3RydW5jYXRlX2Ftb3VudCI6IDJ9




-- with the above cmnds we are working on CLI demo

-- take previous table as exmaple , add saome data into it 

-- open cloushell --> explore with the cmnds 



1 Demo Projection Expression

     - in the given table , we have three attributes (userid, roll , name )

     - we are going to retrive some attributes 

     - aws dynamodb scan --table-name demoTTL --projection-expression "userid, roll"

     - by this u will get only required attributes 


2 Demo Filter Expression

     - So we want to scan this table. But this time, we're going to filter all the results using a filter-expression,

     - aws dynamodb scan --table-name demoTTL --filter-expression "userid = :u" --expression-attribute-values '{ ":u": {"S":"123"}}'

     - userid = u , And then u has to be a string that has the value 123.

     -  u will get the values which as userid (string type) 



3 Page Size demo

     - aws dynamodb scan --table-name demoTTL

     - So if you are doing this API call to scan this entire table, then it's going to be one API call in the background,

     - this API call is small enough, of course, but we retrieve three items.

     - if you wanted to be efficient, say there was like 10,000 items, you could specify a page-size 1.



4 Will do 3 API calls if you have 3 Items

     - aws dynamodb scan --table-name demoTTL --page-size 1

     - it gives all table data 

     - But what happened in the background is that there were three API calls made to DynamoDB. So this is the page-size optimization to avoid timeouts.




5 Max Item demo:

      - aws dynamodb scan --table-name demoTTL --max-items 1

      - if you wanted to retrieve one item at a time, you would use the max-items and then a number,

      - so max-items 1 should return only one item. As you can see, we have this item being returned to us.

      - the count is 3. But we have now a NextToken available to us.

      - So this NextToken, we have to use using this command.

      - So we're going to use the same max-items 1, but now we have the starting-token that we have to enter. 
      
      - And the value of the starting-token is what is in your CLI.



6  Fetch the next item

      - aws dynamodb scan --table-name UserPosts --max-items 1 --starting-token eyJFeGNsdXNpdmVTdGFydEtleSI6IG51bGwsICJib3RvX3RydW5jYXRlX2Ftb3VudCI6IDF9
      





---------------------------------------------------------- DynamoDB Transactions



• Coordinated, all-or-nothing operations (add/update/delete) to multiple items across one or more tables

       - So for example, we want to update, delete or add items, not for just for one table, but across multiple items. 
       
       - And either all the transactions, all the writes work, or none of them work. This is why it's called a transaction.


• Provides Atomicity, Consistency, Isolation, and Durability (ACID)

• Read Modes – Eventual Consistency, Strong Consistency,Transactional

• Write Modes – Standard,Transactional

• Now to do a transaction, it will consume Consumes 2x WCUs & RCUs

        • DynamoDB performs 2 operations for every item (prepare & commit)

• Now, you need to know Two operations: 

        • TransactGetItems – one or more GetItem operations

        • TransactWriteItems – one or more PutItem, UpdateItem, and DeleteItem operations



• Use cases: financial transactions, managing orders, multiplayer games, ...




                                                                  Application

                                                                      |
                                                                      |
                                                 one transaction      |
                                                                      |
                                                                      |              A Transaction is written to both tables, or none!
                                                                      |
                                                                      |
                                                                      |
                                                                      |
                                          ----------------------------| -------------------------------
                                          |                                                           |
                                          |                                                           |
                                          |                                                           |
                           UpdateItem     |                                                           |   PutItem
                                          |                                                           |
                                          |                                                           | 
                                          |                                                           |
                                          
             Account_ID          Balance           Last_Tx_ts                        Tx_ID         Tx_ts          From_Acc            To_Acc        Amount   

            acc_759692           230              1631188571                        75969242       230             acc_759692        acc_315972     45

            acc_315972           120              1631274971                        31597232...    120             acc_315972        acc_617055     100
  
            acc_315234           125              1234274971                        61705584...    125             acc_315234        acc_759692      150


               AccountBalance (Table)                                                                 BankTransactions (Table)






-- So let's take an example. So we have AccountBalance, which presents the account ID, the balance, 

-- so how many dollars they are on each accounts, and the last time a transaction was made onto this account.

-- then we have another table called BankTransactions, which represents all the transactions that happened to the bank,

-- so the idea is that we want to, obviously, add a transactions and modify the AccountBalance at the same time.

-- So our application is going to do one transaction.

-- as part of this one transaction, is going to do an UpdateItem on the AccountBalance, and a PutItem on the BankTransactions.

-- Now with the DynamoDB transaction, either the transaction is returned to both tables at the same time or to none.

-- especially in a financial setting like this, that it is really, really important to have these level of transaction guarantees.






---------------------------------------------------------- DynamoDB Transactions Capacity Computations (IMP)


• Important for the exam!


• Example1: 3 Transactional writes per second, with item size 5 KB


ANS : we need 3 * (5 KB / 1 KB) * 2 (𝑡𝑟𝑎𝑛𝑠𝑎𝑐𝑡𝑖𝑜𝑛𝑎𝑙𝑐𝑜𝑠𝑡) = 30 WCU'S


• Example 2: 5 Transaction reads per second , with item size 5 KB

ANS : we need 5 * (8 KB / 4 KB) * 2 (𝑡𝑟𝑎𝑛𝑠𝑎𝑐𝑡𝑖𝑜𝑛𝑎𝑙𝑐𝑜𝑠𝑡) = 20 RCU's 
        
        • (5 gets rounded to the upper 4 KB)







---------------------------------------------------------- DynamoDB as Session State Cache


• It’s common to use DynamoDB to store session states


-- So we know that dynamoDB can be used to store data, but it can also be used to store the session state, as a cache.

-- so this is something that your web application can use. And therefore, they can retrieve or store the session states on demand


• vs. ElastiCache

       • ElastiCache is in-memory, but DynamoDB is serverless

       • Both are key/value stores

       - So if the exam is saying, okay we need a session state store, that's going to be in memory, it probably means ElastiCache.

       - if it talks about automatic scaling and so on, then DynamoDB is probably the right way.


• vs. EFS

       • EFS must be attached to EC2 instances as a network drive

       - But so EFS is a file system and then DynamoDB is a database.



• vs. EBS & Instance Store


       • EBS & Instance Store can only be used for local caching, not shared caching

       - Because your EBS drives and your instance store are attached to only one EC2 instance. So very important, 
       
       - this can be used for caching a local dataset, but not for sharing it across many instances.


• vs. S3
       
        - S3 could be used for session states, but 

       • S3 is higher latency, and not meant for small objects , It's meant for big files, not for small objects.

       - So S3 as a station state cache is not a great tool.




-- preferably DynamoDB and ElastiCache.




      

---------------------------------------------------------- DynamodB partitioning  strategies (DynamoDB Write Sharding)


• Imagine we have a voting application with two candidates, candidate A and candidate B

• If Partition Key is “Candidate_ID”, this results into two partitions, which will generate issues (e.g., Hot Partition)

      - Now, if we used the partition key to be candidate ID, the problem is that all the results will go into two partitions only

      - because, well, the data is going to be partitioned by either candidate A or candidate B, which will generate issues for writes and for reads so we're gonna get hot partition issues.

      - So, how do we solve this issue?


• A strategy that allows better distribution of items evenly across partitions

• Add a suffix to Partition Key value

      - for example, the number 11. So candidate a 11, and add a row, and then we're going to have candidate b 17, candidate b 18, candidate a 20,

      - the idea is that now the partition key, is a lot more distributed, because it takes more unique values and therefore your data set is going to be fully written and fully readable from your Dynamo DB table.



• Two methods:

     • Sharding Using Random Suffix

     • Sharding Using Calculated Suffix using a hashing algorithm.





Partition Key       Sort Key       Attributes

Candidate_ID        Vote_ts        Voter_ID

Candidate_A-11      1631188571      3524
Candidate_B-17      1631102171      7659
Candidate_B-80      1631102171      09987
Candidate_A-20      1631274971      5432
    
    |
    |
    |
    |

Candidate_ID + Random Suffix



-- Both of these things work. The idea is that you want to get a very, very distributed partition key








---------------------------------------------------------- DynamoDB – Write Types (Concurrent Writes , Atomic Writes , Conditional Writes , Batch Writes)




1 Concurrent Writes



           user 1                             user 2
           |                                    | 
           |                                    |
           | Update: value = 1                  |
           |                                    |
           | ---------------------------------  |     Update: value = 2
                                                |
                                                |
                                                |
                                                |

              7791a3d6-...    Michael           0         (The second write overwrites the first write )



-- So the first one is concurrent writes, which we have an item and say to the users, "Want to update the item."

-- So the first one say, "Hey, update this item to value equals one." And the second one says, "Hey, I want you to update the item with value equals two."

-- Now what happens? Well, they're both going to succeed.

-- So the first one will maybe update to value equals one. The second updates to value equals two.

-- And so maybe the second write will overwrite the first writes,

-- But one of them will be overwritten. And this is not a very desired behavior because two of them try to update an item and both get success,

-- but obviously only one of them really succeeded. So that's why it's called concurrent writes.


So how can we solve this?



2 Conditional Writes






           user 1                             user 2
           |                                    | 
           |                                    |
           | Update: value = 1                  |
           | only if value = 0                  |
           | ---------------------------------  |     Update: value = 2 , only if value = 0
                                                |
                                                |
                                                |
                                                |

              7791a3d6-...    Michael           0         (The first write is accepted,the second write fails )



-- In conditional writes, the idea is that the user says, "Hey, I want to get this item with value equals one, but only if the value is currently zero."

-- the second person says,  "Hey, I want you to do the value of this item with value equals two, but only if the value is zero."

-- What's going to happen now is that, for example, the first write is going to be accepted. So the value will be equals one.

-- And the second write is going to fail because at the time of evaluating the condition value equals zero,

-- it turns out that then it would to be able say, "Hey, you know what? Actually, the value now is one." So we shouldn't do your writes.

-- So this is a way to solve the concurrency problems. And that's why it's called optimistic locking.





3 Atomic Writes




   user 1                             user 2
           |                                    | 
           |                                    |
           | Update:INCREASE value by 1         |
           |                                    |
           | ---------------------------------  |     Update: INCREASE value by 2
                                                |
                                                |
                                                |
                                                |

              7791a3d6-...    Michael           0         (Both writes succeed, the value is increased by 3 )



--  So in this example, the user say, "Hey, I want you to increase the value by one," and the user two say, "I want to increase the value by two."             

-- Now both writes are going to succeed. And the value in total will be increased by 3 (1 + 2 = 3)



4 Batch Writes


-- nothing to do with these kind of concurrency, but just batch writes is when a user writes or updates many items at a time,







---------------------------------------------------------- DynamoDB Patterens with S3 


-- let's look at two ways we can use DynamoDB with Amazon S3



---------------------- 1 DynamoDB – Large Objects Pattern


-- the first one is how to store large objects in DynamoDB.

-- Well, it turns out that as you know, in your tables in DynamoDB, you can only store up to 400 kilobytes of data.

-- So obviously if you want to start storing some images, some videos, all that kind of stuff, DynamoDB is not the best place for it.

-- So instead, what we're going to do is that we are going to have an Amazon S3 bucket, that will contain our large objects.



---- So what is the process to upload a large object?




                  617055.jpg                                        617055.jpg
Application    --------------->  S3 Bucket (media-assets-bucket) --------------------->  Application

    |              upload                                             download              |
    |                                                                                       |
    |
    |  store metadata                                                                       |      get metadata
    |
    |                                                                                       |
    |
    |                                                                                       |
    |
    | ---------------> Product_ID        Product_Name          Image_URL  -------------------

                       59692               Jeans        https://media-assets-bucket.us-east-1.amazonaws.com/759692.jpg

                       315972              Coat         https://media-assets-bucket.us-east-1.amazonaws.com/315972.jpg


                               Products (Table)




--  say we upload an image into Amazon S3. We're going to get back objects key.

-- what we going to do is that we're going to store this metadata from the application into DynamoDB.

-- So we'll have a product ID, a product name, and then an image URL, which is a pointer directly into Amazon S3.

-- what we've done is that we've effectively stored a very small amount of data in our products table in DynamoDB.  we store the large item in Amazon S3.

-- Now from the reading perspective and clients who wants to read this data, first gets the metadata from dynamo DB, 

-- and then we'll get the image back from Amazon S3 to reconstruct these large objects.





---------------------- 2 DynamoDB – Indexing S3 Objects Metadata




               upload                     invoke                           store object’s metadata
Application ----------------> S3 Bucket -------------->  Lambda function ----------------------------->   DynamoDB Table

                                                                                                               |
                                                                                                               |
                                                                                                               |
                                                                                                               |
                                                                                                               |
                                                                                                               |   
                                                                                                               |    (path: to dynamoDB table)

                                         Client ---------------------------------------------------> Application (API for objects’ metadata)

                                                    - Search by date
                                                    - Total storage used by a customer
                                                    - List of all objects with certain attributes
                                                    - Find all objects uploaded within a date range



-- So the application is going to upload objects into Amazon S3

-- Amazon S3 we'll have notifications set up, for example, to invoke a Lambda function that Lambda function will store the objects metadata into DynamoDB table, for example, object size, date, who created it,

-- why do we do this?

ANS : because it's much easier for us to build some queries on top of the DynamoDB table.
 
     - Than on top of an S3 bucket, again, A S3 bucket is not meant to be really scanned. It's meant to store large objects, and it's supposed to,

     - you're supposed to have some sort of database that knows what these objects are, their attributes and so on.

     - So by creating an application on top of DynamoDB, we can answer some questions such as, we want you to find objects by a specific timestamp on our S3 buckets. etc..







---------------------------------------------------------- DynamoDB Operations



-- two DynamoDB Operations



1 • Table Cleanup

    • Option 1: Scan + DeleteItem , you can scan all the items in your table and then delete them one by one,

         • Veryslow,consumes RCU & WCU, expensive

    • Option 2: Drop Table + Recreate table , it is much more quick,

         • Fast,efficient,cheap



2  • Copying a DynamoDB Table , across accounts, regions, places,

     • Option 1: Using AWS Data Pipeline

     • Option 2: Backup and restore into a new table

             • Takes some time

     • Option 3: Scan + PutItem or BatchWriteItem

             • Write your own code        

              




 ---------------------------------------------------------- DynamoDB – Security & Other Features


 • Security

       • VPC Endpoints available to access DynamoDB without using the Internet and only keeping all the traffic within your VPC.

       • Access fully controlled by IAM

       • Encryption at rest using AWS KMS and in-transit using SSL/TLS


• Backup and Restore feature available

       • Point-in-time Recovery (PITR) like RDS

       • No performance impact


• GlobalTables

       • Multi-region, multi-active, fully replicated, high performance

       - how to enable it?

       - you need to first enable DynamoDB streams.


• DynamoDB Local

       • Develop and test apps locally without accessing the DynamoDB web service (without Internet)

       - while DynamoDB is a cloud service, It's possible for you to get a simulation of DynamoDB on your local computer, called DynamoDB local,   

       - the idea is that you have, a local DynamoDB database that you can use to develop and test your applications locally without using the DynamoDB web service,




• AWS Database Migration Service (AWS DMS) can be used to migrate to DynamoDB (from MongoDB, Oracle, MySQL, S3, ...)








---------------------------------------------------------- DynamoDB – Users Interact with DynamoDB Directly


-- Now another feature you need to understand around DynamoDB is going to be around fine grain access.

-- So for example, if you have clients and applications, web, or mobile, and they need to access directly our DynamoDB table,

-- then we don't want to give them IAM permissions and IAM roles, users directly from AWS, that will be truly inefficient and a security hole.

-- Instead, we're going to use an identity provider. It could be Amazon Cognito User Pools, Google login, Facebook login, open ID connect, or SAML or whatever.

-- the users will, in the simplified flow, login with these identity providers and they will have the feature to exchange the credentials they just got for temporary AWS credentials.

-- the idea is that because they're temporary, they're more secure. And with them, they can be associated with an IAM role,

-- but this IAM role must be restricted because now that our clients and applications can access our DynamoDB table, you want them to be able to do operations only on the data that they own.

How do this?

-- fine grain access control





---------------------------------------------------------- DynamoDB – Fine-Grained Access Control


• Using Web Identity Federation or Cognito Identity Pools, each user gets AWS credentials

• You can assign an IAM Role to these users with a Condition to limit their API access to DynamoDB


Eg :


{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"FullAccessToUserItems",
         "Effect":"Allow",
         "Action":[
            "dynamodb:GetItem",
            "dynamodb:BatchGetItem",
            "dynamodb:Query",
            "dynamodb:PutItem",
            "dynamodb:UpdateItem",
            "dynamodb:DeleteItem",
            "dynamodb:BatchWriteItem"
         ],
         "Resource":[
            "arn:aws:dynamodb:us-west-2:123456789012:table/GameScores"
         ],
         "Condition":{
            "ForAllValues:StringEquals":{
               "dynamodb:LeadingKeys":[
                  "${www.amazon.com:user_id}"
               ]
            }
         }
      }
   ]
}



-- So in this policy, the user can do get item, batch get item, query, put item, update item, delete item and batch right item on a specific table.

-- But there's a condition here. And the condition is saying,, only amazon userd_id will able to perform

• LeadingKeys – we only limit row-level access for users on the Primary Key , so therefore we make sure that the users can only modify and access their own data.

-- you can also specify conditions on attributes , this would be to limit the specific attributes a user can see in your DynamoDB table,

• Attributes – limit specific attributes the user can see





----- So to summarize, you have fine-grain access control by using a federated login 

-- and by specifying a condition on LeadingKeys,if you want it to limit at the role level

-- attributes, if you want your limits at the column level,






============================================================ Amazon API Gateway =======================================



-- All hands-on in these part are linked with one to one , so do not delete any resources until u finished the whole API Gateway 



-- Build, Deploy and Manage APIs




---------------------------------------------------------- Example: Building a Serverless API




                         Rest Api
             Client <------------------> API Gateway <----(PROXY REQUESTS)------->  Lambda <----------(CRUD)-----------> DynamoDB



-- so the functions can use DynamoDB as a database for our API, and we can do create, read, update, delete on our tables,

-- but we'd like our clients to be able to invoke this Lambda function in some way.

-- So there are multiple of doing it.

-- We can have the client directly invoke the Lambda function, but that means that the client would need IAM permissions,

-- or we've seen we can use an application load balancer to have it in between the client and the Lambda function and that would expose our Lambda function as an HTTP endpoint.

-- There's one last thing we can use. It's called the API Gateway.

-- this is a serverless offering from AWS which allows us to create REST APIs that are going to be public and accessible for our clients.

-- So the client will talk to the API Gateway and the beautiful things that the API Gateway will then proxy the request to our Lambda functions.

-- So it would use an API Gateway because it provides us more than just an HTTP endpoint. It will provide us a lot of features

-- such as authentication, such as usage plans, development stages and all that kind of things.







---------------------------------------------------------- AWS API Gateway


• AWS Lambda + API Gateway: No infrastructure to manage

• Support for the WebSocket Protocol

• API Gateway Handle API versioning (v1, v2...) and not break our clients.

• Handle different environments (dev, test, prod...)

• Handle security (Authentication and Authorization)

• Create API keys, handle request throttling

• Swagger / Open API import to quickly define APIs

• Transform and validate requests and responses in the API Gateway level

• Generate SDK and API specifications

• Cache API responses


-- So lots of features that come with the API Gateway that are not necessarily included when you use something as simple as an application load balancer.





---------------------------------------------------------- API Gateway – Integrations High Level


• Lambda Function

      • Invoke Lambda function

      • Easy way to expose REST API backed by AWS Lambda


• HTTP
 
      • Expose HTTP endpoints in the backend

      • Example: internal HTTP API on premise, Application Load Balancer you have on your cloud environment. ...

      • Why? Add rate limiting, caching, user authentications, API keys, etc...

      - So it's definitely used full to have a layer of API Gateway on top of your HTTP endpoint.


 • AWS Service

       • Expose any AWS API through the API Gateway

       • Example: start an AWS Step Function workflow, post a message to SQS directly from an API Gateway API.

       • Why? Add authentication, deploy publicly, rate control...






---------------------------------------------------------- API Gateway – AWS Service Integration Kinesis Data Streams example





           requests                                  send                                        records                                      store .json files
Client --------------------------> API Gateway --------------------> Kinesis Data Streams ------------------------>  Kinesis Data Firehose ------------------------> Amazon S3





-- So we want to have people send data into a Kinesis Data Streams but in a secure way, without giving them access to AWS credentials.

-- So what we do is that's in between our clients and our Kinesis Data Streams,

-- we're going to have the API Gateway. And the clients will send HTTP request into the API Gateway.

-- it's been configured to then send the messages into a Kinesis Data Streams. as you see in this setting, we don't manage any servers.

-- Then Kinesis Data Stream from there, send the records into a Kinesis Data Firehose and eventually put them into an Amazon S3 bucket in the JSON format.

-- You can actually expose any AWS service to the outside through an API Gateway.








---------------------------------------------------------- API Gateway Deployment Types 


-- There are three ways to deploy your API Gateway. This is called endpoint types.



• Edge-Optimized (default): For global clients, So that means that your API Gateway is going to be accessible from anywhere in the world.

       • Requests are routed through the CloudFront Edge locations (improves latency)

       • The API Gateway still lives in only one region where you created it, 
       
       - but it's accessible efficiently from every CloudFormation Edge location.



• Regional:
  
       • For clients within the same region , So this is when we don't want to use CloudFront Edge locations.

       - So it's when we expect all our users to be within the same region where we created our API Gateway.

       • if you wanted to, Could manually combine with CloudFront (more control over the caching strategies and the distribution)



• Private: 
       
       • Can only be accessed from your VPC using an interface VPC endpoint (ENI)

       • to define access to your API Gateway, Use a resource policy to define access







---------------------------------------------------------- API Gateway – Security


-- So you can identify users on the API Gateway in multiple ways.


1 • User Authentication through

          • IAM Roles (useful for internal applications) , for example, running on EC2 instances, and they want to access an API on API Gateway, they can just use IAM roles.

          • Cognito (identity for external users – example mobile users)

          • Custom Authorizer (your own logic) - This is a Lambda function.


2 • Custom Domain Name HTTPS 

          - security through integration with AWS Certificate Manager (ACM)

          • If using Edge-Optimized endpoint, then the certificate must be in us-east-1

          • If using Regional endpoint, the certificate must be in the API Gateway region

          • Must setup CNAME or A-alias record in Route 53 to point to your domain and API Gateway.





---------------------------------------------------------- API Gateway Hands On



STEP 1 : 


-- create Lambda function with python 3.11

-- now generate code like 



import json

def lambda_handler(event, context):
    body = "Hello from Lambda!"
    statusCode = 200
    return {
        "statusCode": statusCode,
        "body": json.dumps(body),
        "headers": {
            "Content-Type": "application/json"
        }
    }



-- deploy ur changes and test the function 


-- So everything is working fine right now and this is perfect. And what I'm going to do now is to integrate, this Lambda function to my API gateway.



STEP 2 :


-- open API gateway in the amazon console 

-- we have different types of API , choose rest API public 

-- create API with regional endpoint 

-- now we are here, we are going to create our first method in this API.

-- c.o create method --> choose any method (for eg Get) --> choose lambda function --> choose lambda function --> create method

--  we can also integrate with any service in any region. So we choose a region and then we choose a service. 

-- And what it does that you can expose some of your AWS services as an API through API Gateway.

-- So now we have the Lambda function integrated and because we want to see the full requests being passed to Lambda and being sent back from Lambda,

-- enable lambda proxy integration 

-- even though a Lambda function can have a long timeout, for example, five minutes or 15 minutes,

-- API Gateway has a limited timeout and the default is 29 seconds. 

-- You can customize it and have it less than 29 seconds but the default timeout is 29 seconds regardless of how long your Lambda function takes to execute.

-- once u create method , this is going to automatically grant API gateway, the right to invoke our Lambda function.

-- go n check in the lambda function , do refresh 

-- So let's test out this API. , go to test --> do not give any values here , just c.o test --> u will get " hello from lambda"

-- So you can see from the API gateway, the status is 200. That comes right from our code right here which says status is 200.

-- So we have done our first execution of an API gateway on top of a Lambda function.




STEP 3 :


-- So now let's actually debug this and see what is being sent to our Lambda function.

import json

def lambda_handler(event, context):
    print(event)
    body = "Hello from Lambda!"
    statusCode = 200
    return {
        "statusCode": statusCode,
        "body": json.dumps(body),
        "headers": {
            "Content-Type": "application/json"
        }
    }



-- add print(event) --> deploy 

-- we are going to invoke now our Lambda function again from our API gateway.

-- do test again in the API gateway , which still says hello from Lambda, but this time we are printing the events.

-- So if we go to the CloudWatch logs we should be able to find it. 

-- lambda function --> monitor ---> cloudwatch --> recent log

     - u will see like this  {'resource': '/', 'path': '/', 'httpMethod': 'GET', 'headers': None, 'multiValueHeaders': None, ............

-- our Lambda function can use this information to forge a response and send it back to the API gateway.   

-- So now we can go ahead and actually create a new resource.

-- api gateway --> ur api --> create resource --> houses (u can give any name)

-- go to lambda --> create function with py 3.11


import json

def lambda_handler(event, context):
    body = "Hello from myhouse!"
    statusCode = 200
    return {
        "statusCode": statusCode,
        "body": json.dumps(body),
        "headers": {
            "Content-Type": "application/json"
        }
    }



-- in / houses --> create method --> get --> select new lambda arn --> enable lambda proxy --> create method 

-- do test in api without values , u will get o/p 

-- We have the root, GET and we have the new lambda function have GET, which invoke two different Lambda functions.

-- But we want to be able to access this from a web browser, not just testing it from the API Gateway UI.

-- so for this , u can deploy API --> create stage prod --> copy url and do test in browser 

-- eg : prod/houses 

-- if u did any wrong , it will gives u {"message":"Missing Authentication Token"} 

-- We have deployed our API to API Gateway and this is pretty cool because, well, we have two Lambda functions behind the scenes actually replying to it.







---------------------------------------------------------- API Gateway – Deployment Stages


-- So we have deployed our first API through the API Gateway in a deployment stage.

-- whenever we made changes to the API Gateway, they were not effective until we did a deployment.

• Making changes in the API Gateway does not mean they’re effective

• You need to make a “deployment” for them to be in effect

• It’s a common source of confusion , Some people will go and change how the API works, but forget to do a deployment and therefore the API will not be live.

• Changes are deployed to “Stages” (as many as you want)

• Use the naming you like for stages (dev, test, prod)

• Each stage has its own configuration parameters

• Stages can be rolled back as a history of deployments is kept



EG : API Gateway – Stages v1 and v2 API breaking change



               https://api.example.com/v1                              v1 Stage 
V1 Client ------------------------------------------> API GAteway --------------------------------> V1





New URL 
              https://api.example.com/v2                              v2 Stage 
V2 Client ------------------------------------------> API GAteway --------------------------------> V2



-- So let's take the example of we have two stages and we're creating a breaking API change.

-- So our Lambda function backing our API Gateway is going to change.

-- So we have the v1 stage and it invokes the v1 function and we've deployed it so our v1 clients are going to be able to access our API through this URL.

-- But we are programming a new version of our Lambda function called v2. 

-- this v2 does not respect the same format of data and so on and so we know that if we deploy the changes on the v1 stage, it will break our v1 clients.

-- So what we do is then instead we decide to create a new stage, call it the v2 stage, pointing to the v2 function. And this will create a new URL.

-- So now the api.example.com/v2, which represents the v2 stage.

-- our clients can now be updated to version two and access this new URL, and we can maintain compatibility.

-- So for a while, the v1 and the v2 can coexist together as two different stages. 

-- We can migrate the clients from the version one to the version two and then when no more clients are using the version one,


-- So this is a very common use cases for stages. Now, how do we expand this? : Stage Variables



---------------------------------------------------------- API Gateway – Stage Variables


• Stage variables are like environment variables for API Gateway but for the API Gateway stages.

-- you use them to change configuration values without redeploying your API.

• They can be used in:

      • Lambda function ARN

      • HTTP Endpoint

      • Parameter mapping templates


• Use cases: 

      • Configure HTTP endpoints your stages talk to (dev, test, prod...)

      • Pass configuration parameters to AWS Lambda through mapping templates or to point to the right Lambda function,

• Stage variables are passed to the ”context” object in AWS Lambda

      - so we can log it and get access to the value of the stage variables directly from our Lambda functions.


-- the format to access the value of a stage variable within API Gateway is  ${stageVariables.variableName}







---------------------------------------------------------- API Gateway Stage Variables & Lambda Aliases


-- So here is a very, very common use cases for your stage variables.

• We create a stage variable to indicate the corresponding Lambda alias that our API Gateway should invoke.

• So Our API gateway will automatically invoke the right Lambda function!





---------------------------------------------------------- API Gateway – Stages and deployment Hands On


-- create lambda fucntion with py 3.11 

-- What we're going to do is that we're going to create code for it, and other versions, version one, version two, et cetera. We're going to create aliases,



import json

def lambda_handler(event, context):
    body = "Hello from lambda v1"
    statusCode = 200
    return {
        "statusCode": statusCode,
        "body": json.dumps(body),
        "headers": {
            "Content-Type": "application/json"
        }
    }


-- deploy and test this code 

-- now , lambda function --> actions -> publish versions --> make as v1 verison --> publish

-- now we are in version 1 

-- now go to main function --> change to v2 --> deploy and do test --> publish as V2 

-- now go back to main code and make it as body = Hello from lambda DEV , now this is the latetst one 

-- now we have 3 versions (v1, v2 , Latest)

-- now create DEV Alias pointing to $Latest

-- now create TEST ALias pointing to V2 

-- now create PROD ALias pointing to v1

-- So now that we have our function with multiple aliases, it's time to map it to the API gateway.

-- API console --> rest APi --> create resource(stage-variables) --> create method for the resource (GET for lambda function , enabled proxy integration ) --> in lambda function add :${stageVariables.lambdaAlias} --> save 


EG : arn:aws:lambda:ap-south-1:298132369629:function:stage-varibales-get:${stageVariables.lambdaAlias}


-- now u have to add some permissions for this to create and invoke with various stages 



aws lambda add-permission \
--function-name "arn:aws:lambda:ap-south-1:298132369629:function:stage-varibales-get:${stageVariables.lambdaAlias}" \
--source-arn "arn:aws:execute-api:ap-south-1:298132369629:air12uoa3g/*/GET/stage-variables" \
--principal apigateway.amazonaws.com \
--statement-id 261ba472-4270-4a6f-884b-51b6fc6a5537 \
--action lambda:InvokeFunction



-- in the above permissions --> ${stageVariables.lambdaAlias}" change this value for DEV , TEST and PROD 

-- open cloudshell , invoke for DEV  , So here we will allow our API gateway to invoke the DEV alias of our Lambda function.


aws lambda add-permission \
--function-name "arn:aws:lambda:ap-south-1:298132369629:function:stage-varibales-get:DEV" \
--source-arn "arn:aws:execute-api:ap-south-1:298132369629:air12uoa3g/*/GET/stage-variables" \
--principal apigateway.amazonaws.com \
--statement-id 261ba472-4270-4a6f-884b-51b6fc6a5537 \
--action lambda:InvokeFunction



-- invoke for TEST  , So here we will allow our API gateway to invoke the TEST alias of our Lambda function.



aws lambda add-permission \
--function-name "arn:aws:lambda:ap-south-1:298132369629:function:stage-varibales-get:TEST" \
--source-arn "arn:aws:execute-api:ap-south-1:298132369629:air12uoa3g/*/GET/stage-variables" \
--principal apigateway.amazonaws.com \
--statement-id 261ba472-4270-4a6f-884b-51b6fc6a5537 \
--action lambda:InvokeFunction



-- invoke for PROD , So here we will allow our API gateway to invoke the PROD alias of our Lambda function.




aws lambda add-permission \
--function-name "arn:aws:lambda:ap-south-1:298132369629:function:stage-varibales-get:PROD" \
--source-arn "arn:aws:execute-api:ap-south-1:298132369629:air12uoa3g/*/GET/stage-variables" \
--principal apigateway.amazonaws.com \
--statement-id 261ba472-4270-4a6f-884b-51b6fc6a5537 \
--action lambda:InvokeFunction 



-- now create method , go to test for this method --> give Alias (DEV, PROD , TEST) for testing purpose 

-- u will get 200 status code , do test for other stage variables 

-- So this is showing that this stage variable is now impacting which Lambda alias we are invoking.

-- So now let's deploy our API to DEV, and to other stages afterwards, to show you where to use the stage variables.

-- give dev as stage name , once u create this go to below u will find stage varibale --> edit --> lambdaAlias = DEV

-- give test as stage name , once u create this go to below u will find stage varibale --> edit --> lambdaAlias = TEST

-- give prod as stage name , once u create this go to below u will find stage varibale --> edit --> lambdaAlias = PROD

-- So once we have done that, we can test our deployment of the API. copy URL and paste in browser 


EG : 

https://air12uoa3g.execute-api.ap-south-1.amazonaws.com/dev/stage-variables

https://air12uoa3g.execute-api.ap-south-1.amazonaws.com/test/stage-variables

https://air12uoa3g.execute-api.ap-south-1.amazonaws.com/prod/stage-variables




-- So as you can see, each stage is now linked to a specific LambdaAlias, through the usage of stage variables.









---------------------------------------------------------- API Gateway – Stages Configurations Hands On


-- So let's have a look at all the configuration options for your stage.

-- select stage as u want (dev,test,prod) --> edit --> explore 




---------------------------------------------------------- API Gateway –  Canary Deployment


• Possibility to enable canary deployments for any stage (usually prod)

        - So the idea is that you want to enable a way to test a small amount of traffic on the change you have done on your API gateway. So usually it's done in production.


• Choose the % of traffic the canary channel receives

• Metrics & Logs are separate (for better monitoring)

• Possibility to override stage variables for canary

        - you can override any stage variables you wanted for your canary stage and 
        
        
• this is equivalent of performing blue/green deployment with Lambda and API gateway.





---------------------------------------------------------- API Gateway –  Canary Deployment Hands On


-- create resource (canary-demo) --> create method (GET ,lambda function , enable lambda proxy integration , lambdaARN:1) --> create method 

eg : arn:aws:lambda:ap-south-1:298132369629:function:stage-varibales-get:1


-- we're going to invoke version one of this Lambda function.

-- now try to test in API console , u will get o/P

-- now deploy this API --> give stage = canary   --> copy url and test it 

https://air12uoa3g.execute-api.ap-south-1.amazonaws.com/canary/canary-demo

-- u will get "Hello from lambda v1"

-- But now what we're going to do is that we're going to set up a canary.

-- go to canary resource --> go down and select canary --> make 50% for canary and 50% for current stage --> create a canary 

-- So once we have created a canary, then we are ready to deploy a new update to our stage.

-- So for this, I go back to my resources, I go to my method, canary-demo, GET, and I will change the integration request.

-- invoke 2nd version  , arn:aws:lambda:ap-south-1:298132369629:function:stage-varibales-get:2

-- do test u will get "Hello from lambda v2"

-- go to browser and test ur url , here my Canary is switching between v1 and v2. (50-50% traffic )

-- i am done , everything is working fine , so promote the canary 

-- go to canary in below --> promote --> save

-- do refresh then link you only get "Hello from Lambda v2." So effectively, the canary has been promoted.

-- That's how you use Canary in API Gateway.

      - So you first modify the resource,

      - you create a canary in your stage

      - then set up canary in down (set how much traffic )

      - you deploy to this canary, and then when you're ready, 
      
      - you promote the canary to the final destination with 100% of the request going to the new version now.

       




---------------------------------------------------------- API Gateway - IntegrationTypes


-- now let's talk about the different ways we can integrate our API Gateway with our backend.


1 • Integration Type MOCK

        • API Gateway returns a response without sending the request to the backend

        - So this is useful when you are programming your API Gateway and configuring it and you don't want to have any work on the backend just yet.

        - So as you can expect, this is not something for production.
        
        - This is something for just development and testing.



2 • Integration Type HTTP / AWS (Lambda & AWS Ser vices)

        • you must configure both the integration request and integration response

        - the API Gateway will forward our request, but we can modify it.

        • Setup data mapping using mapping templates for the request & response

        - that means that we will be able to change the request made to the backend and to change the response back from the backend before sending it to our clients.



          REST API                                                  AWS Service Integration
Client <---------------->    API Gateway + Mapping Templates    <--------------------------------> SQS Queue




-- For example, if we create a REST API and our API Gateway and we add mapping templates and 

-- then we're going to map this REST API call to an API call on the SQS Queue by changing, renaming, reordering, all these kind of things such as the SQS Queue can understand the API call made by the API Gateway.
 
-- So here, the API Gateway has the power to change the request and the response.





3 • IntegrationType AWS_PROXY (Lambda Proxy):

       • incoming request from the client is the input to Lambda , we haven't modified any request or response because we cannot, it's a proxy.

       • The function is responsible for the logic of request / response

       • No mapping template, headers, query string parameters... All these things are passed as arguments to our function directly.

       - So here in this Proxy Integration, all the work is on the backend and the API Gateway is just here to proxy requests through.

      


4 • IntegrationType HTTP_PROXY

       • No mapping template

       - In which case, again because this is a proxy, we don't have any mapping templates

       • The HTTP request  itself is passed directly to the backend , 

       • The HTTP response from the backend is forwarded by API Gateway , then when the backend replies, then the response is again proxied by the API to going back into the clients.

       • Possibility to add HTTP Headers if need be (ex:API key) , So for example, you can add an API key between your API Gateway and your backend, but your users don't see it.




           HTTP Request                              HTTP_PROXY , Request/Responses are proxied
Client <-------------------------> API Gateway <----------------------------------------------------->  Application Load Balancer

                                                       + (optional)
                                                      HTTP Header API Key: asjdh2j3jh3j...
       



-- So let's take an example, a client makes an HTTP request into your API Gateway.

-- You're going to proxy that request to your backend, for example an Application Load Balancer.

-- But again, as I said, you can add optional HTTP headers, such as an API key, that really guarantees that your API call is going to work.

-- So the client does not need to know about the secret API key, but yet your backend will see it coming from the API Gateway.







---------------------------------------------------------- Mapping Templates (AWS & HTTP Integration)


-- So mapping templates to modify the request and the response,

-- they are only applicable if we integrate within AWS or HTTP, but without using the proxy methods.

• Mapping templates can be used to modify request / responses

• Rename / Modify query string parameters

• Modify body content

• Add headers

• to do this , Uses Velocity Template Language (VTL): for loop, if etc... , It's a scripting language used to modify some requests.

• Filter output results (remove unnecessary data)

• you must set the Content-Type to be application/json or application/xml.





---------------------------------------------------------- Mapping Example 1: JSON to XML with SOAP


-- how to integrate a client with a SOAP API through an API Gateway.

-- So what is a SOAP API?

• SOAP API are XML based, whereas REST API are JSON based , so we need to have something in between to convert the JSON into XML and back.




          RESTful, JSON Payload                                           XML Payload
Client <--------------------------> API Gateway + Mapping Template  <-----------------------------> SOAP API



-- so the idea is that you would create your API Gateway with a mapping template

-- the client would interact with JSON in your API Gateway.

-- But your API Gateway, thanks to the mapping template, can transform this payload into an XML payload, into the SOAP API and back.

• In this case, API Gateway should:

      • Extract data from the request: either path, payload or header

      • Build SOAP message based on request data (mapping template) 

      - necessary for the backend to understand, so this is where we use the mapping template, then call the SOAP service and receive the XML response

      - and finally transform the XML response back into a desired format to respond to your users.







---------------------------------------------------------- Mapping Example 2 : Query String parameters





             HTTP                                                                                          JSON 
             http://example.com/path?name=foo&other=bar                                                    { “my_variable”: “foo”, “other_variable”: “bar” }
Client <-------------------------------------------------------------> API Gateway + MAPPING TEMPLATE <------------------------------------------------------------->  Lambda 

                                                                                                         You can rename variables! (map them to anything you want)




-- Here is another example where we use the mapping template to use query string parameters.                                                                                                         

-- So our client is integrated with API Gateway which is using Lambda directly, not as a proxy.

-- And so we want to be able to map the query string parameters, so if we send a request like this

        -  http://example.com/path?name=foo&other=bar  

        - we want to be able to rename these query string parameters before passing into the Lambda


-- so we can create a mapping template and then the JSON that gets passed into the Lambda function could be my variable foo and other variable bar.

-- so in this example what I've done is that I've renamed variables so you can map them to anything you want.







---------------------------------------------------------- Mapping Templates hands ON


-- create API if u do not have any , or if u have API created then create resource give name as "mapping"

-- create lambda function with py 3.11 , It's going to return a JSON document,

import json

def lambda_handler(event, context):
    # TODO implement
    return {
        'example' : 'hello world!'
    }



-- in API we choose get method and choosinf lambda function but we are not enabled lambda proxy  This time, we're going to integrate directly with the Lambda function.

-- do test in lambda function and in API console also 

-- Now, we didn't need to have the status and the body in here in our Lambda function because we don't have the proxy integration.

-- This time, we're just returning whatever the Lambda returns.

-- Now we want to change the response body to add keys and to rename it and so on.

-- So to do so, we're going to create a mapping template on the integration response.

-- integration response ---> edit ---> create mapping template 

-- we want to remanme "example" to something else

- application/json 

- for the template body, I'm going to create a new response.

{
  "my-key" : "my-value",
  "renamed-key" : $input.json('$.example')
}


-- we want to remanme "example" to something else

-- So renamed key is my renamed key, and then the value of that must be the value, "hello world".

-- So to access it, we must take the value corresponding to the key example.

-- So to do so, you do $ inputs, which corresponds to the JSON received as an input.

-- create template and save

-- what's going to happen is that now, the integration response is going to go through this template and change the outcome.

-- now do test in API console , u will get different o/p as lambda function 

-- So the output of the Lambda function, was being sent to our mapping template, which created this final response body.






---------------------------------------------------------- API Gateway - Open API spec



-- So the API Gateway has a tight integration with the OpenAPI specification.

-- So what is the OpenAPI specification?

       - it's a very common way to define REST APIs and the API definition itself is code.


• Common way of defining REST APIs, using API definition as code


• Import existing OpenAPI 3.0 spec to API Gateway So you define the

    • Method

    • Method Request

    • Integration Request

    • Method Response

    • + AWS extensions for API gateway and setup every single option of these extensions directly from within the API spec.



• Can export current API as OpenAPI spec

   - Similarly, instead of importing stuff into the API Gateway, you can take an existing API in the API Gateway and export it as an OpenAPI spec.

   - Why? Well, because this specification can be used to, for example, generate client code.


• OpenAPI specs can be written inYAML or JSON

• Using OpenAPI we can generate SDK for our applications






---------------------------------------------------------- REST API – Request Validation


-- you can use the OpenAPI spec to perform request validation within your API gateway.

• You can configure API Gateway to perform basic validation of an API request before proceeding with the integration request

-- instead of just sending a payload as is to your backend, API Gateway can verify if it corresponds to a proper schema.

• When the validation fails, API Gateway immediately fails the request

    • Returns a 400-error response to the caller


• This reduces unnecessary calls to the backend

• Checks:

     • The required request parameters in the URI, query string, and headers of an incoming request are included and non-blank

     • The applicable request payload adheres to the configured JSON Schema request model of the method






---------------------------------------------------------- Caching API responses


• Caching reduces the number of calls made to the backend

     - So that means that our clients, when they talk to the API gateway, 
     
     - the API gateway will first check the cache and if a result is already in cache, return that result.
     
     - Otherwise, if there's a cache miss, then it will go and talk to the backend to get the response. 
     
     - So the idea is that we really leverage a lot of the cache to reduce the amount of pressure on our backend.


• Default TTL (time to live) is 300 seconds (min: 0s, max: 3600s)

• Caches are defined per stage

• Possible to override cache settings per method

• Cache encryption option

-- So if you want a method to not use a cache, you can specify it as well at the method level. The cache can be encrypted

• Cache capacity between 0.5GB to 237GB

• Cache is expensive, makes sense in production, may not make sense in dev / test 

-- So caching is really something you wanna use in production or in pre-production.





---------------------------------------------------------- API Gateway Cache Invalidation


• Able to flush the entire cache (invalidate it) immediately

• Clients can invalidate the cache with header: Cache- Control: max-age=0 (with proper IAM authorization)




{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "execute-api:InvalidateCache"
      ],
      "Resource": [
        "arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier"
      ]
    }
  ]
}            
        


-- So this is an IAM policy that would allow a client to invalidate the cache on a specific resource.


• If you don't impose an InvalidateCache policy (or choose the Require authorization check box in the console), any client can invalidate the API cache, and that could be a disaster.


-- select ny stage --> edit --> provision API Cache --> save 

-- the cache is there for Entire API , so once u paste link in browser , for the first time it will come from backend , then for nxt time it will come from CACHE 

-- make sure to turn off the API cache , 'coz it is not a feature , it will get charge for U






---------------------------------------------------------- API Gateway – Usage Plans & API Keys




-- we have now created our API and it is time to make it available for our customers

-- and we may wanna charge them some money for it.



• If you want to make an API available as an offering ($) to your customers


1 • Usage Plan:

     • who can access one or more deployed API stages and methods

     • how much and how fast they can access them

     • uses API keys to identify API clients and meter access

     • configure throttling limits and quota limits that are enforced on individual client



2 • API Keys:

    • alphanumeric string values to distribute to your customers

    • Ex:WBjHxNtoAb4WPKBC7cGm64CBibIb24b4jt8jJHo9

    • Can use with usage plans to control access

    • Throttling limits are applied to the API keys

    • Quotas limits is the overall number of maximum requests





---------------------------------------------------------- API Gateway – Correct Order for API keys


• To configure a usage plan

    1. Create one or more APIs, configure the methods to require an API key, and deploy the APIs to stages.

    2. Generate or import API keys to distribute to application developers (your customers) who will be using your API.

    3. Create the usage plan with the desired throttle and quota limits.

    4. Associate API stages and API keys with the usage plan.


• Callers of the API must supply an assigned API key in the x-api-key header in requests to the API.







---------------------------------------------------------- API Gateway – Logging & Tracing



• CloudWatch Logs

     • Log contains information about request/response body

     • Enable CloudWatch logging at the Stage level (with Log Level - ERROR, DEBUG, INFO) , obviously the DEBUG is going to give you the most amount of information,

     • Can override settings on a per API basis





                   request                                                 request 
      -------------------------------------->                    -------------------------------> 
User                                               API Gateway                                          lambda (backend)
      <-------------------------------------         |     |       <------------------------------ 
                                                     |     |
                 response                            |     |                response
                                                     |     |
                                                     |     |
                                     request         |     |    response
                                                     |     |

                                                   CloudWatch Logs



-- we're going to get a user that makes a request into the API gateway, and automatically that request is going to be logged into CloudWatch Logs.

-- Then the request is making it to your backend, your backend will then give a response to the API gateway, and the response again, will be sent to CloudWatch Logs

-- So it's very helpful to get the request and the response, but be careful if you do enable this, then you may get a lot of sensitive information into CloudWatch Logs.






• X-Ray :

    • Enable tracing to get extra information about requests in API Gateway

    • X-Ray API Gateway + AWS Lambda gives you the full picture





---------------------------------------------------------- API Gateway - CloudWatch Metrics


• Metrics are by stage, Possibility to enable detailed metrics

• The first one is called  CacheHitCount & the other one is called CacheMissCount:  which gives you some information about the efficiency of the cache

    - So if your cache is very efficient, the cache hit will be very high.

    - If it's not efficient, the cache miss will be very high.


• Count:The total number API requests in a given period.

• IntegrationLatency: The time between when API Gateway relays a request to the backend and when it receives a response from the backend.


• Latency: The time between when API Gateway receives a request from a client and when it returns a response to the client.

         - The latency includes the integration latency and other API Gateway overhead(checking the authorization and authentication, checking the cache, doing some mapping templates, and so on.)


-- So the Latency is always going to be a bit higher than the IntegrationLatency.

-- so you should note, that the maximum amount of time that an API gateway can perform any request is 29 seconds.

-- So if your Latency or your IntegrationLatency is over 29 seconds, that means you will see a timeout from your API gateway.

• 4XXError (client-side) & 5XXError (server-side)

      - So server-side means backend, and client-side means the clients using your API gateway.






---------------------------------------------------------- API Gateway Throttling


• Account Limit

       • API Gateway throttles requests at10000 request per sec across all API

       • Soft limit that can be increased upon request


-- So that means that if one of your API is under heavy use the other APIs can also be throttled. So in case you see a throttling, what will you see?

      • In case of throttling => 429 Too Many Requests (retriable error)


• Can set Stage limit & Method limits to improve performance , to make sure that each stage does not use all the quotas of the request, if it's under attack.

• Or you can define Usage Plans to throttle per customer



• Just like Lambda Concurrency, one API that is overloaded, if not limited, can cause the other APIs to be throttled




---------------------------------------------------------- API Gateway - Errors


• 4xx means Client errors

     • 400: Bad Request

     • 403: Access Denied,WAF filtered

     • 429: Quota exceeded,Throttle


• 5xx means Server errors

     • 502: Bad Gateway Exception, usually for an incompatible output returned from a Lambda proxy integration backend and occasionally for out-of-order invocations due to heavy loads.

     • 503: Service Unavailable Exception

     • 504: Integration Failure – ex Endpoint Request Timed-out Exception (29 sec)






---------------------------------------------------------- AWS API Gateway - CORS



-- API Gateway supports the browser security cross-origin resource sharing.

• CORS must be enabled when you receive API calls from another domain.

• The OPTIONS pre-flight request must contain the following headers:

     • Access-Control-Allow-Methods

     • Access-Control-Allow-Headers

     • Access-Control-Allow-Origin


• CORS can be enabled through the console



-- see fig for better understanding   




        


---------------------------------------------------------- API Gateway – Security



------------------- 1 IAM Permissions

    • Create an IAM policy authorization and attach to User / Role , we're able to invoke our API gateway.

    • in this case Authentication = IAM AND  Authorization = IAM Policy
        
         - So this is the optimal way to protect your API gateway if it's being accessed within your AWS accounts,

         
     • Good to provide access within AWS (EC2, Lambda, IAM users...)

        - so that includes if you're using EC2 instances, Lambda functions, or IAM users, etc...

    • Leverages “Sig v4” capability where IAM credential are in headers





          REST API w/ Sig v4
Client <------------------------>  API Gateway  <---------------------------> Backend

                                       |
                                       |
                                       |    IAM Policy check
                                       |
                                       |

                                      IAM    
                  

-- the API gateway is deployed into a stage and secured with IAM permissions,

-- there's a backend of Lambda functions.

-- So first our clients is going to do a rest API call and we'll pass the Sig v4 headers,

-- then API gateway knows how to decrypt those, so it will check with IAM if the user is authorized and so there will be an IAM policy check,

-- so as soon as this is done the API gateway will, if it's authorized, talk to the Lambda function in the backend and returned a result to the clients.

-- So there's is a first permission mode and it is very simple. Now this can be combined with resource policies.



---------------------------------API Gateway – Resource Policies


• Resource policies (similar to Lambda Resource Policy) , they allow you to set JSON policy on your API gateway to define who and what can access your API gateway.

• So the main use case of resource policies Allow for Cross Account Access (combined with IAM Security) , we are able to give access of users or roles in other accounts directly into our API gateway.



{
   "Version": "2012-10-17",
   "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": [
                    "arn:aws:iam::account-id-2:role/developer",
                    "arn:aws:iam::account-id-2:role/Admin"
                ]
            },
            "Action": "execute-api:Invoke",
            "Resource": [
                "execute-api:/stage/GET/pets"
            ]
        }
    ]
}




• Allow for a specific source IP address or  Allow for a VPC Endpoint





----------------- 2 Cognito User Pools


     • Cognito fully manages user lifecycle, token expires automatically

     • API gateway verifies identity automatically from AWS Cognito

     • No custom implementation required

     • Authentication = Cognito User Pools |   Authorization = API Gateway Methods




       Authenticate Retrieve token
    | ------------------------->    Cognito User Pools
    |                                   |  
    |                                   |
    |                                   |      Evaluate Cognito Token
    |                                   |
    |                                   |
    |     REST API + Pass Token     
Client <------------------------>  API Gateway  <---------------------------> Backend



-- here our API gateway is talking to Lambda function in our backend,

-- then, our client first authenticates with the Cognito User Pool to retrieve a connection token.

-- So the users are already restarting user pool, then our client is authenticated they received a token and it will pass the token in the API call to our API gateway.

-- Now what will happen is that the API gateway has a direct integration with Cognito User Pools, and it will evaluate the Cognito token with the Cognito User Pool

-- if the token is correct, it's going to allow access to your backend.






----------------- 3 Lambda Authorizer (formerly Custom Authorizers)


-- this one is the most flexible but requires the most involvement on your end.


• Token-based authorizer (bearer token) – ex JWT (JSON Web Token) or Oauth

• A request parameter-based Lambda authorizer (headers, query string, stage var)

     - so the idea is that, we can pass a request based parameters with headers or query strings into a Lambda authorizer,


• Lambda must return an IAM policy for the user, result policy is cached

• Authentication = External | Authorization = Lambda function



see fig for better xplanation 






---------------------------------------------------------- API Gateway – Security  – Summary


• IAM:

    • Great for users / roles already within your AWS account, + resource policy for cross account

    • Handle authentication + authorization

    • Leverages Signature v4


• Custom Authorizer:

     • Great for 3rd party tokens

     • Very flexible in terms of what IAM policy is returned

     • Handle Authentication verification + Authorization in the Lambdafunction    

     • Pay per Lambda invocation,results are cached


• Cognito User Pool:

     • You manage your own user pool (can be backed by Facebook, Google login etc...)

     • No need to write any custom code

     • Must implement authorization in the backend





---------------------------------------------------------- API Gateway – HTTP API vs REST API


• HTTP APIs

     • low-latency, cost-effective AWS Lambda proxy, HTTP proxy APIs and private integration (no data mapping)

     • support OIDC and OAuth 2.0 authorization, and built-in support for CORS

     • No usage plans and API keys

     - HTTP API's are going to be much cheaper than REST APIs.


• REST APIs

     • All features (except Native OpenID Connect / OAuth 2.0)




 
 ---------------------------------------------------------- API Gateway – WebSocket API – Overview


 • What’s WebSocket?

     • Two-way interactive communication between a user’s browser and a server

     •  it's two-way because Server can push information to the client without having the clients making a request to the server,

     • This enables stateful application use cases


• WebSocket APIs are often used in real- time applications such as chat applications, collaboration platforms, multiplayer games, and financial trading platforms.

• Works with AWS Services (Lambda, DynamoDB) or HTTP endpoints




------------------------- Connecting to the API


WebSocket URL


wss://[some-uniqueid].execute-api.[region].amazonaws.com/[stage-name]


-- starts with wss , it's an encrypted WebSocket URL.





            connect                                                             invoke 
Clients ------------------------------>  Amazon API Gateway WebSocket API  ---------------------------->  Lambda function ((onConnect)) ---------------->  Amazon DynamoDB
    
                                                                                 connectionId                                              connectionId
                                   




-- So the client is going to connect to your WebSocket API gateway and establish a persistent connection into it

-- which is going to invoke the Lambda function and perform a connection ID.

-- the connection ID is going to remain persistent as long as the client is connected to the API gateway.

-- Then the connection ID can be for example, persistent into Amazon DynamoDB to store some metadata




------------------------- Client to Server Messaging , ConnectionID is re-used



WebSocket URL

     wss://abcdef.execute-api.us-west-1.amazonaws.com/dev






              send message                                                            invoke 
Clients ------------------------------>  Amazon API Gateway WebSocket API  ---------------------------->  Lambda function ((sendMessage)) ---------------->  Amazon DynamoDB
        ------------------------------>
        ------------------------------>                                          connectionId                                              connectionId
                                   


-- the client wants to send some messages to the server. So is going to use the same URL I just gave you,

-- is going to send messages over the persistent connection into the WebSocket API and these messages are called frames.

-- So these frames are going to invoke a new Lambda function 

-- So the Lambda function can interact with DynamoDB to retrieve the user information based on the connection ID,

-- so if the clients chooses to send more messages through the API gateway on the WebSocket API, s just going to send it more frames over the same connection.

-- therefore the connection ID is not going to change and it's going to be reused when the Lambda function is invoked.





----------------------------- Now, how does the server, our API gateway communicate back to the clients without the client making a request?



Server to Client Messaging



WebSocket URL

     wss://abcdef.execute-api.us-west-1.amazonaws.com/dev


-- So again, we use the same WebSocket URL and we do the same thing as sending messages but now we want to get some data back.

-- there is when you have a API gateway, a connection URL callback and it looks like this, wss://abcdef.execute-api.us-west-1.amazonaws.com/dev/@connections/connectionId

-- if a Lambda function or whatever, makes an HTTP post that is signed using IAM Sig v4, to this connection URL callback by specifying the connection ID of the clients,

-- it is going to send back a message from the API gateway into the client. And this allows for two way communication








----------------------------- Connection URL Operations

see fig for better explanation


Connection URL

     wss://abcdef.execute-api.us-west-1.amazonaws.com/dev/@connections/connectionId


Operation                          Action

POST               Sends a message from the Server to the connected WS Client

GET                Gets the latest connection status of the connected WS Client

DELETE             Disconnect the connected Client from the WS connection







---------------------------------------------------------- API Gateway – WebSocket API – Routing


--  how does the client know which Lambda function to invoke or which backend to invoke?

--  In WebSocket, there's a concept of routing.


• Incoming JSON messages are routed to different backend

• If no routes => sent to $default

• You request a route selection expression to select the field on JSON to route from

• Sample expression: $request.body.action

• The result is evaluated against the route keys available in your API Gateway

• The route is then connected to the backend you’ve setup through API Gateway ,  that could be a Lambda function or anything else that API gateway integrates with.




IMP to know : 


-- The routing is used to make sure that you can route to a specific backend based on the routing expression

-- because the connection is already open and the routing should be part of the incoming data message.






---------------------------------------------------------- API Gateway - Microservice Architecture 


see fig for better explanation


• Create a single interface for all the microservices in your company

• Use API endpoints with various resources

• Apply a simple domain name and SSL certificates

• Can apply forwarding and transformation rules at the API Gateway level










=================================================================== AWS CICD (CodeCommit, CodePipeline, CodeBuild, CodeDeploy, ...) ============================================================




----------------------------------------------------------------- CICD – Introduction


• We have learned how to:

     • Create AWS resources, manually (fundamentals)

     • Interact with AWS programmatically (AWS CLI)

     • Deploy code to AWS using Elastic Beanstalk


• All these manual steps make it very likely for us to do mistakes!


• We would like our code “in a repository” and have it deployed onto AWS

   • Automatically

   • The right way

   • Making sure it’s tested before being deployed

   • With possibility to go into different stages (dev, test, staging, prod)

   • With manual approval where needed



• To be a proper AWS developer... we need to learn AWS CICD


• This is all about automating the deployment we’ve done so far while adding increased safety

• We’ll learn about:

    • AWS CodeCommit – storing our code

    • AWS CodePipeline – automating our pipeline from code to Elastic Beanstalk

    • AWS CodeBuild – building and testing our code

    • AWS CodeDeploy – deploying the code to EC2 instances (not Elastic Beanstalk)

    • AWS CodeStar – manage software development activities in one place , And that regroups CodeCommit, CodePipeline, CodeBuild, and CodeDeploy in one tool.

    • AWS CodeArtifact – store, publish, and share software packages

    • AWS CodeGuru – automated code reviews using Machine Learning





----------------------------------------------------------------- Continuous Integration (CI)



• Developers push the code to a code repository often (e.g., GitHub, CodeCommit, Bitbucket...)

• A testing / build server checks the code as soon as it’s pushed (CodeBuild, Jenkins CI, ...)

• The developer gets feedback about the tests and checks that have passed / failed

• Find bugs early, then fix bugs , because we are testing the code as soon as it is pushed in a code repository. So the developer doesn't need to test the code on their machine.

• Therefore, the code is going to be delivered faster, because it's going to be tested.

• Deploy often






----------------------------------------------------------------- Continuous Delivery (CD)


• Ensures that the software can be released reliably whenever needed

• Ensures deployments happen often and are quick

• Shift away from “one release every 3 months” to ”5 releases a day” , Because every time we push the code into the code repo, the code is going to be live onto some application servers.

• to do CD we need That usually means automated deployment (e.g., CodeDeploy, Jenkins CD, Spinnaker, ...)




----------------------------------------------------------------- Technology Stack for CICD



Code :

    AWS : CodeCommit

    other : GitHub , Bitbucket


Build and Test :

     AWS : AWS CodeBuild

     others : Jenkins CI





----------------------------------------------------------------- 1 AWS CodeCommit 



• Version control is the ability to understand the various changes that happened to the code over time (and possibly roll back)

    - So to have version control, that means that you can see what happened in the past, who committed some code, what changed, what was added was removed and so on, and then roll back.



• All these are enabled by using a version control system such as Git

• A Git repository can be synchronized on your computer, but it usually is uploaded on a central online repository

• GIT Benefits are:

     • Collaborate with other developers , So it allows, you know, organizations up to maybe hundreds of thousands developers to work on the same code at the same time,

     • Make sure the code is backed-up somewhere , So the code lives on the cloud and not on just someone's computer,

     • Make sure it’s fully viewable and auditable , we can see who committed to what line of code when, and we can revert them. You can roll back. You can do a lot of good things with code repositories




-------- And so with Code Commit, well, So why do we want to use Code Commit?


• Git repositories can be expensive

• The industry includes GitHub, GitLab, Bitbucket, ...

• And with the use of AWS CodeCommit:

    • Private Git repositories , that's because your code actually lives and stays within your VPC on AWS cloud.

    • No size limit on repositories (scale seamlessly)

    • Fully managed, highly available

    • Code only in AWS Cloud account => increased security and compliance

    • Security (encrypted, access control, ...)

    • Integrated with Jenkins, AWS CodeBuild, and other CI tools




-- now there are two ways for CodeCommit to be tracked, there is the CloudWatch Events which is the recommended way and a CloudWatch Event rule will be created automatically 

-- such as whenever the CodeCommits repository is updated automatically it will trigger a CodePipeline.

-- Alternatively you can use CodePipeline to check periodically for changes but it won't be as fast and it's not the recommended way.








 ----------------------------------------------------- CodeCommit – Security



-- Code Commit as very, very good security. 


• Interactions are done using Git (standard) , but then you have authentication on top of it.


• Authentication

     • SSH Keys – AWS Users can configure SSH keys in their IAM Console , to be able to go into a Git repo

     • HTTPS – with AWS CLI Credential helper or Git Credentials for IAM user , If you wanted to get access using standard login and password to get repo.



• Authorization

     • IAM policies to manage users/roles permissions to repositories


• Encryption

     • Repositories are automatically encrypted at rest using AWS KMS

     • Encrypted in transit (can only use HTTPS or SSH – both secure) while you push your code to code commit,


• Cross-account Access

     • Do NOT share your SSH keys or your AWS credentials

     • Use an IAM Role in your AWS account and use AWS STS (AssumeRole API)





----------------------------------------------------- CodeCommit vs. GitHub



CodeCommit 


- Support Code Review (Pull Requests) 

- Integration with AWS CodeBuild

- Authentication (SSH & HTTPS)

- Security : IAM Users & Roles

- Hosting : Managed & hosted by AWS

- UI : Minimal



GitHub



- Support Code Review (Pull Requests) 

- Integration with AWS CodeBuild

- Authentication (SSH & HTTPS)

- GitHub Users

- Hosting : Hosted by GitHub , GitHub Enterprise: self , hosted on your servers

- UI : Fully Featured





----------------------------------------------------------------- AWS CodeCommit Hands On 


-- do login as IAM user for this demo , not with the Root Account 


-- open CodeCommit in console 

-- create one repo like my-nodejs-app

-- if u did not see SSH , u have to login with the IAM user 

-- now create one simple index.html file and upload that file 


<!DOCTYPE html>
<html>
  <head>
    <title>Elastic Beanstalk</title>
    <style>
      body {
        color: #ffffff;
        font-family: Arial, sans-serif;
        font-size:14px;
        -moz-transition-property: text-shadow;
        -moz-transition-duration: 4s;
        -webkit-transition-property: text-shadow;
        -webkit-transition-duration: 4s;
        text-shadow: none;
      }
      body.blurry {
        -moz-transition-property: text-shadow;
        -moz-transition-duration: 4s;
        -webkit-transition-property: text-shadow;
        -webkit-transition-duration: 4s;
        text-shadow: #fff 0px 0px 25px;
      }
      a {
        color: #55aaff;
      }
      .textColumn, .linksColumn {
        padding: 2em;
      }
      .textColumn {
        position: absolute;
        top: 0px;
        right: 50%;
        bottom: 0px;
        left: 0px;

        text-align: right;
        padding-top: 11em;
        background-color: blue;

      }
      .textColumn p {
        width: 75%;
        float:right;
      }
      .linksColumn {
        position: absolute;
        top:0px;
        right: 0px;
        bottom: 0px;
        left: 50%;

        background-color: #33342D;
      }

      h1 {
        color: #33342D;
        font-size: 500%;
        font-weight: normal;
        margin-bottom: 0em;
      }
      h2 {
        font-size: 200%;
        font-weight: normal;
        margin-bottom: 0em;
      }
      ul {
        padding-left: 1em;
        margin: 0px;
      }
      li {
        margin: 1em 0em;
      }
    </style>
  </head>
  <body>
    <div class="textColumn">
      <h1>Congratulations</h1>
      <p>Your first AWS Elastic Beanstalk Node.js application is now running on your own dedicated environment in the AWS Cloud</p>
    </div>
    <div class="linksColumn">
      <h2>What's Next?</h2>
      <ul>
        <li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html">AWS Elastic Beanstalk overview</a></li>
        <li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html">AWS Elastic Beanstalk concepts</a></li>
        <li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs_express.html">Deploy an Express Application to AWS Elastic Beanstalk</a></li>
        <li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs_express_elasticache.html">Deploy an Express Application with Amazon ElastiCache to AWS Elastic Beanstalk</a></li>
        <li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs_geddy_elasticache.html">Deploy a Geddy Application with Amazon ElastiCache to AWS Elastic Beanstalk </a></li>
        <li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs_custom_container.html">Customizing and Configuring a Node.js Container </a></li>
        <li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.loggingS3.title.html">Working with Logs</a></li>
      </ul>
    </div>
  </body>
</html>





-- this will create my first AWS CodeCommit commit.

-- it is commited to default branch called as main , But in CodeCommit, you can go ahead and create more branches, if you want developers to collaborate together.

-- We have a repository my-nodejs-app, and it contains an index.html.

-- let's go over the options in CodeCommits.



1 Pull requests : 

      - pull request is to allow developers to merge their changes from a different branch, into ur main brach 

      - pull requests are here to help you merge your changes, from other branches into your main branch.


2 Commits :

      - all ur commits will save here 


3 Branches :

      - all ur branches will show here , u can create new brach if u want 

4 settings :

      - go to notificatin --> This is where we will create notification rules, for our code repository.

      - create notification settings as u want 

      - So we can create a trigger, triggers are more on specific actual code events.

      - select event as u want , and choose service details 




-- you how we can push our code directly into Code Commit without using the add file or upload file UI.?

-- go to IAM --> users --> select ur user --> security credentials --> create HTTPS Git credentials for AWS CodeCommit 

-- now go to CodeCommit --> cloneURL --> clone HTTPS --> copy link 

-- now u have to install git in your local system , open git in cmd --> git clone  < paste url of CodeCommit > , u have enter the credentails which u have created HTTPS for user just now for codecommit

-- this is proof that we just established a GIT connectivity between our computer and the repo in codecommit using the HTPS credentials we setup IAM.







----------------------------------------------------------------- AWS CodePipeline


• Visual Workflow to orchestrate your CICD within AWS

• Source – my code is in CodeCommit, ECR, S3, Bitbucket, GitHub

• Build – CodeBuild, Jenkins, CloudBees,TeamCity

• Test – CodeBuild,AWS Device Farm,3rd party tools,...

• Deploy – CodeDeploy, Elastic Beanstalk, CloudFormation, ECS, S3, ...

• Invoke – Lambda, Step Functions

• Consists of stages:

    • Each stage can have sequential actions and/or parallel actions

    • Example:Build ----> Test -----> Deploy -----> LoadTesting...

    • Manual approval can be defined at any stage
      





----------------------------------------------------------------- CodePipeline – Artifacts



see Fig for better understand 


• Each pipeline stage can create artifacts

• Artifacts stored in an S3 bucket and passed on to the next stage this is how the next stage is going to be able to do what it needs to do.


-- for Eg , 

-- Developer is going to push some code into CodeCommit,

-- then CodeCommit is going to be orchestrated by CodePipeline, which is going to extract all the code and create an artifact out of it and place that artifact into an S3 bucket.

-- Now, when CodeBuild is invoked, the same artifacts that were extracted are going to be inputted directly into CodeBuild, and that's why CodeBuild doesn't need to have straight access into CodeCommit.

-- Actually, it's CodePipeline that will be pushing the code to CodeBuild through Amazon S3.

-- Then when CodeBuild is building the code, it's going to create some deployment artifacts. 

-- So these artifacts are going to be stored yet again in your S3 bucket by CodePipeline,

-- CodePipeline will push these artifacts yet again to CodeDeploy and CodeDeploy says hey, I have these artifacts, I need to deploy them. Let's go ahead and deploy them.

-- So as you can see, these stages interact with each other through Amazon S3, and this is why we have artifacts in CodePipeline.






----------------------------------------------------------------- CodePipeline – Troubleshooting


• For CodePipeline Pipeline/Action/Stage Execution State Changes

• Use CloudWatch Events (Amazon EventBridge). Example:

       • You can create events for failed pipelines

       • You can create events for cancelled stages

• If CodePipeline fails a stage, your pipeline stops, and you can get information in the console

• If pipeline can’t perform an action, make sure the “IAM Service Role” attached does have enough IAM permissions (IAM Policy)

• AWS CloudTrail can be used to audit AWS API calls







 

----------------------------------------------------------------- CodePipeline Hands ON


-- open elastic beanstak create 2 sample application one is for dev env and one is for prod env with nodejs , as we did previously

--  so CodePipeline is going to allow us to link sources, builds and deploy stages and we're going to create a very first simple Pipeline.

-- open pipeline in console --> create pipeline --> source = codecommit --> choose repo and brach ---> Output artifact format = default ---------> skip build stage ----> deploy provide = beanstak --> choose app and dev env --> create 

-- before the deployment of CodePipeline , the beanstak is in green , but once the pipeline is finished successfully , it will turn to blue ('coz in code it it blue clour thats why)

-- So we've just done our first CodePipeline where we hooked the CodeCommit repository and the code went all the way through CodePipeline into Elastic Beanstalk and updated our first environment to something blue.

-- if u go to beanstak and check there is CodePipeline version is there 

-- So CodePipeline automatically created a new version for us 

-- in CodePipeline --> edit --> choose add stage and create stage (deploytoProd) ---> c.o action group of new stage --> choose manual approval ---> choose 2nd A.G as beanstak , select artifact as source , env = prod ----> create Action group

-- So you have to remember that's action groups go within stages. stages have multiple action group.

-- now in the stage we have two action groups.

-- And the first one is a ManualApproval and only then after the manual approval stage action group is done then we will move on to the other action group that deploy to prod Beanstalk.

-- So we can add sequentially action groups. We can also add parallel actions  

-- sequential = down , parallel = beside 

-- you can really be creative so you can have sequential and parallel actions within action groups and multiple action groups within a stage.

-- on the right top and c.o save

------------  so how are we going to run this new pipeline?

-- go to code commit and chage the colour from blue to red in index.html file and do commit changes 

-- so what we hope to happen is that this code change will be propagated across our entire CodePipeline 

-- and moved into dev and then we'll be asked for a manual approval and then deploy it into prod.

-- now go to beanstak and check the url it changed to red clour previously it was blue 

-- once u check the update and looks fine and now do approve in CodePipeline 

-- once u approved this will deploy prod env , check prod env url and this will change to red colour 

-- so that's a pretty solid pipeline we here; Source, DeployToDev and then DeplyToProd that has a ManualApproval in it.

-- u can check history to check how it happened









----------------------------------------------------------------- AWS CodeBuild


• A fully managed continuous integration (CI) service

• Continuous scaling (no servers to manage or provision – no build queue)

• Compile source code, run tests, produce software packages, ...

• Alternative to other build tools (e.g., Jenkins)

• Charged per minute for compute resources (time it takes to complete the builds)

• Leverages Docker under the hood for reproducible builds

• Use prepackaged Docker images or create your own custom Docker image

• Security:

     • Integration with KMS for encryption of build artifacts

     • IAM for CodeBuild permissions,and VPC for network security

     • AWS CloudTrail for API calls logging



• Source – CodeCommit, S3, Bitbucket, GitHub

      - allows you to take a source of code, then in that source there will be some build instructions.


• Build instructions: Code file buildspec.yml or insert manually in Console

       -  buildspec.yml : that file needs to live at the root of your code.


• Output logs can be stored in Amazon S3 & CloudWatch Logs

• Use CloudWatch Metrics to monitor build statistics

• Use EventBridge to detect failed builds and trigger notifications

• Use CloudWatch Alarms to notify if you need “thresholds” for failures



• Build Projects can be defined within CodePipeline or CodeBuild


-- CodeBuild containers are deleted at the end of their execution (success or failure). You can't SSH into them, even while they're running.






----------------------------------------------------------------- CodeBuild – Supported Environments


• Java
• Ruby
• Python
• Go
• Node.js
• Android
• .NET Core
• PHP
• Docker – extend any environment you like



----------------------------------------------------------------- CodeBuild – buildspec.yml


• buildspec.yml file must be at the root of your code


EG :


version: 0.2

phases: 
    install:
        runtime-versions:
            nodejs: latest
        commands:
            - echo "installing something"
    pre_build:
        commands: 
            - echo "we are in the pre build phase"
    build:
        commands:
            - echo "we are in the build block"
            - echo "we will run some tests"
            - grep -Fq "Congratulations" index.html
    post_build:
        commands:
            - echo "we are in the post build phase"


if congratulations is there in index.html it will succeed             
            

• env – define environment variables 

     • variables – plaintext variables

     • parameter-store – variables stored in SSM Parameter Store

     • secrets-manager – variables stored in AWS Secrets Manager


• phases – specify commands to run:

     • install – install dependencies you may need for your build

     • pre_build – final commands to execute before build

     • Build – actual build commands

     • post_build – finishing touches (e.g., zip output)






----------------------------------------------------------------- AWS CodeBuild Hands On


-- see how we can use CodeBuild not to build this time but to test our code in our CodePipeline.

-- So what do we want to test?

    - if you look at our Beanstalk environment, we really, really like that congratulations message. I think we should have it,

    - and we want to test for this not ever leaving the screen, so we never want congratulations to go away, this is what we want to test for.


-- go to codebuild in console --> source = codecommit --> OS = ubuntu ----> choose buildspec.yml ---> remaing default ---> create 

-- now c.o start build , this will give u error 'coz u did not have buildspec.tml file in repo  so add that file in codecommit repo

-- now do start buid again in codebuild console , this time it will get succeed





----------------- Now how do we integrate this build project into our code deploy ?


-- go to pipeline , i want test b/w source and deploy stage 

-- edit pipeline --> add stage b/w source and deploy name as Test  ----> edit ActionGroup , provider = codebuild , source artifact , output artifacts = outfortest--> save

-- We have added our CodeBuild into this entire pipeline. 

-- so now we have a new pipeline. And so we should be testing it, right?

-- to do test --> go to index.html and change congratulations tp some other words 'coz we want to know how this work for testing 

-- u will get error message in pipeline what exactly we think 

-- now go to index.html and add "congratulations codebuid" and save 

-- now u will get success and do test beanstak url and see u will get  congratulations codebuid in browser 

-- once u ok with update , u can give manual approve 

-- seeing the power of CICD right here, how easy it is to change things, test for them, and then push them eventually to whatever environment you need to push them into.






----------------------------------------------------------------- AWS CodeDeploy


• Deployment service that automates application deployment

- What does that mean?

     - you have a version one of an application running and somehow, you want to upgrade it to version two.

     - Now where can we deploy version one and version two?

     - Well, these application versions can be deployed either to EC2 instances, on-premises servers or Lambda functions or ECS services



• Deploy new applications versions to EC2 Instances, On-premises servers, Lambda functions, ECS Services


• Automated Rollback capability in case of failed deployments, or trigger CloudWatch Alarm

• Gradual deployment control (we can say one instance at a time or all at a time or HalfAtATime or blue/green.)

• A file named appspec.yml defines how the deployment happens






----------------------------------------------------------------- CodeDeploy – EC2/On-premises Platform


• Can deploy to EC2 Instances & on-premises servers

• you can perform two kind of deployments, in-place deployments or blue/green deployments

• Must run the CodeDeploy Agent on the target instances , because the agent is the thing that will be performing the updates on the instances.

• Define deployment speed

      • AllAtOnce: most downtime

      • HalfAtATime: reduced capacity by 50%

      • OneAtATime: slowest, lowest availability impact

      • Custom: define your %






----------------------------------------------------------------- CodeDeploy – In-Place Deployment



 Half At A Time


-- So here we have the v1 running on four EC2 instances.

-- Because we're in a HalfAtATime setting in in-place deployment, two instances are going to be taken down for maintenance.

-- So the agent will stop the application on two instances which then are going to be upgraded again by the agent to version two.

-- once it is done, then the other half is going to be taken down for install and then upgrade it to version two.

-- so this is the process of in-place deployment for HalfAtATime setting.




----------------------------------------------------------------- CodeDeploy – Blue-Green Deployment


-- blue/green is the idea that you have, for example, an Application Load Balancer pointing to version one

-- What's going to happen is that we're going to create a new Auto Scaling group so this can be done either manually or automatically by CodeDeploy.

-- CodeDeploy is going to create as many EC2 instances in this new Auto Scaling group.

-- So we have v1 and v2 now running in parallel and the load balancer through some process is going to be pointing now to the v2 Auto Scaling group and the v1 will be taken down.

-- blue/green because new instances are taken up and the old ones are terminated.





----------------------------------------------------------------- CodeDeploy Agent



• The CodeDeploy Agent must be running on the EC2 instances as a pre- requisites

     - you can install it directly using some Linux commands to install any software or 
     
     - if you wanted to have some kind of automation.



• It can be installed and updated automatically if you’re using Systems Manager

• The EC2 Instances must have sufficient permissions to access Amazon S3 to get deployment bundles

       - WHY? because in Amazon S3 will be stored the application revision.

       - So when we want to update from version one to version two, then the CodeDeploy agent with the proper IAM permissions attached to the EC2 instance will download the application revision from the S3 bucket.







----------------------------------------------------------------- CodeDeploy – Lambda Platform



• CodeDeploy can help you automate traffic shift for Lambda aliases

• Feature is integrated within the SAM framework

• Linear: grow traffic every N minutes until 100%

      • LambdaLinear10PercentEvery3Minutes

      • LambdaLinear10PercentEvery10Minutes


• Canary: try X percent then 100%

      • LambdaCanary10Percent5Minutes

      • LambdaCanary10Percent30Minutes


• AllAtOnce: immediate






----------------------------------------------------------------- CodeDeploy – ECS Platform


• CodeDeploy can help you automate the deployment of a new ECS Task Definition


• this only works for Blue/Green Deployments

• Linear: grow traffic every N minutes until 100%

      • ECSLinear10PercentEvery3Minutes

      • ECSLinear10PercentEvery10Minutes


• Canary: try X percent then 100%

      • ECSCanary10Percent5Minutes

      • ECSCanary10Percent30Minutes


• AllAtOnce: immediate







----------------------------------------------------------------- CodeDeploy  Hands On


-- go to IAM and u have create some 2 Roles 

-- IAM --> ROLES --> choose service = codedeploy ---> already permissions added --> create role 

-- IAM --> ROLES --> choose service = ec2 --->add s3fullaccess --> create role 

-- now go to codedeploy --> choose ec2/onpremises --> create appn

-- Now, to create application we need to create a deployment group.

-- to have a deployment group it should be better to start an EC2 instance.

-- open ec2 and create one instance without key pair , make sure SSH and HTTP are allowed inbound in SG of ur instance , give tag environment = development for ec2 , and attach role u created 

-- connect instance , we have to install codedeploy agent , so follow the cmnds 

       - sudo yum update

       - sudo yum install ruby     , which is necessary to install codedeploy Agent

       - wget https://aws-codedeploy-ap-south-1.s3.ap-south-1.amazonaws.com/latest/install           , change region of urs

       - chmod +x ./install

       - sudo ./install auto

       - sudo service codedeploy-agent status



-- So now that our EC2 instance is ready, let's go into CodeDeploy and create our first deployment group. 

-- codedeploy --> name --> select role --> choose In-place --> choose EC2 --> enter tags key and pair value (if u did not find jst refresh page and give all details again) --> no never --> uncheck LB box --> create 

-- create s3 bucket as same region as ec2 and upload ur appn 

u have create some files in it  and make it as ZIP and upload in s3 bucket


appspec.yml


version: 0.0
os: linux
files:
  - source: /index.html
    destination: /var/www/html/
hooks:
  BeforeInstall:
    - location: scripts/install_dependencies
      timeout: 300
      runas: root
    - location: scripts/start_server
      timeout: 300
      runas: root
  ApplicationStop:
    - location: scripts/stop_server
      timeout: 300
      runas: root


index.html - any index.html page 




stop_server

#!/bin/bash
isExistApp = `pgrep httpd`
if [[ -n  $isExistApp ]]; then
    service httpd stop        
fi


install_dependices

#!/bin/bash
yum install -y httpd



start_server

#!/bin/bash
service httpd start


-- make all files into zip and upload it 

-- now create deploayment and give URI of S3 bucket 

-- start deployment


-- http:// <publicip> to test 






----------------------------------------------------------------- CodeDeploy – Deployment to EC2


• Define how to deploy the application using appspec.yml + Deployment Strategy

• Will do In-place update to your fleet of EC2 instances

• Can use hooks to verify the deployment after each deployment phase




----------------------------------------------------------------- CodeDeploy – Deploy to an ASG


• In-place Deployment

     • Updates existing EC2 instances

     • Newly created EC2 instances by an ASG will also get automated deployments


• Blue/Green Deployment

     • A new Auto-Scaling Group is created (settings are copied)

     • Choose how long to keep the old EC2 instances (old ASG)

     • Must be using an ELB

 


----------------------------------------------------------------- CodeDeploy – Redeploy & Rollbacks


• Rollback = redeploy a previously deployed revision of your application

• Deployments can be rolled back:
 
    1 Automatically – rollback when a deployment fails or rollback when a CloudWatch Alarm thresholds are met

    2 • Manually


• Disable Rollbacks — do not perform rollbacks for this deployment

• If a roll back happens, CodeDeploy redeploys the last known good revision as a new deployment (not a restored version)







----------------------------------------------------------------- AWS CodeStar ---------------------------


-- it will discontinued by 2024 July 

-- it has a replacement called "CodeCatalyst"


• An integrated solution that groups: GitHub, CodeCommit, CodeBuild, CodeDeploy, CloudFormation, CodePipeline, CloudWatch, ...

• Quickly create “CICD-ready” projects for EC2, Lambda, Elastic Beanstalk

• Supported languages: C#, Go, HTML 5, Java, Node.js, PHP, Python, Ruby

• Supported languages: C#, Go, HTML 5, Java, Node.js, PHP, Python, Ruby

• Ability to integrate with Cloud9 to obtain a web IDE (not all regions)

• One dashboard to view all your components

• Free service, pay only for the underlying usage of other services

• Limited Customization





----------------------------------------------------------------- AWS CodeArtifact ------------------


-- So the idea is that when you build software, you use other software that your own software depends on. This is called code dependencies.

-- And every time you build software it's very common to push the software into a repository, and then other software can build onto it.

• Software packages depend on each other to be built (also called code dependencies), and new ones are created

• Storing and retrieving these dependencies is called "artifact management"

• Traditionally you need to setup your own artifact management system it is so complicated

• CodeAr tifact is a secure, scalable, and cost-effective artifact management for software development

• Works with common dependency management tools such as Maven, Gradle, npm, yarn, twine, pip, and NuGet

• Developers and CodeBuild can then retrieve dependencies straight from CodeArtifact



-- see fig 







----------------------------------------------------------------- CodeArtifact – EventBridge Integration



-- CodeArtifact events such as when a package is created or modified or deleted will emit events into what's called EventBridge.

-- EventBridge is like a repository of events within AWS.

-- from EventBridge, you can pretty much integrate and trigger many different AWS services, such as Lambda functions, Step functions, SNS, SQS.

-- it is possible for CodeArtifact to trigger CodePipeline through EventBridge, whenever a package version is updated.

-- so why would we wanna do this?

-- CodePipeline could have CodeCommit so that we know that the dependency has been updated. (codecommit)

-- Then triggering CodeBuild to rebuild your application with the updated dependency maybe for security reasons. (codebuild)

-- then finally deploy the new application to your production environment using CodeDeploy.





----------------------------------------------------------------- CodeArtifact – Resource Policy


-- So any artifact repository within your accounts can be easily accessed by your users of your accounts or your roles with your accounts with an IAM policy.

-- But if you want to authorize another account, and users and roles of that account to access CodeArtifact then you need to use a resource policy.

• Can be used to authorize another account to access CodeArtifact

-- So when you grant someone access to a CodeArtifact repository, what's going to happen is that you either give them access to all packages or none of them.

• A given principal can either read all the packages in a repository or none of them


EG :


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "codeartifact:DescribePackageVersion",
                "codeartifact:DescribeRepository",
                "codeartifact:GetPackageVersionReadme",
                "codeartifact:GetRepositoryEndpoint",
                "codeartifact:ListPackages",
                "codeartifact:ListPackageVersions",
                "codeartifact:ListPackageVersionAssets",
                "codeartifact:ListPackageVersionDependencies",
                "codeartifact:ReadFromRepository"
            ],
            "Effect": "Allow",
            "Principal": {
                 "AWS": [
                      "arn:aws:iam::123456789012:root"
                      "arn:aws:iam::123456789012:user/subbu"
                 ]    
            },
            "Resource": "*"
        }
    ]
}






----------------------------------------------------------------- CodeArtifact Hands On


-- CodeArtifact --> create repo --> pypi-store as ----> our account ---> give domain name (any) --> create 

-- u can see in repositories --> 2 repo's are created --> 

-- But how do we actually now leverage this demo repository.?

-- go to demo repo --> view connection --> choose linux --> copy cmnd --> paste in cloudshell 

-- once u connected to repo then enter this cmnd 
 
         - pip3 install response 

         - This is actually going to connect to code artifact and it's going to pull this response dependency from it.

         - if this package depends on other things then it's going to download the rest as well which is why you see a lot of information in here.


-- now go to codeartifact , refresh , now you see as you can see all the packages that were pulled.         

-- u can also see in another repo , it will there also 

-- So because we keep one copy on the repo that has an external connection, and one copy on the repo that we connected to which is the demo repository.

-- if u want to give permissions then check at "Repository policy" .






----------------------------------------------------------------- Amazon CodeGuru -----------------



• An ML-powered service used for "automated code reviews" and "application performance recommendations"

• Provides two functionalities

     • CodeGuru Reviewer: automated code reviews for static code analysis (development)

     • CodeGuru Profiler: visibility/recommendations about application performance during runtime (production)





 
----------------------------------------------------------------- Amazon CodeGuru Reviewer


• Identify critical issues, security vulnerabilities, and hard-to-find bugs

• Example: common coding best practices, resource leaks, security detection, input validation

• Uses Machine Learning and automated reasoning

• Hard-learned lessons across millions of code reviews on 1000s of open-source and Amazon repositories

• Supports Java and Python

• Integrates with GitHub, Bitbucket, and AWS CodeCommit





----------------------------------------------------------------- Amazon CodeGuru Profiler


• Helps understand the runtime behavior of your application

• Example: identify if your application is consuming excessive CPU capacity on a logging routine

• Features:

     • Identify and remove code inefficiencies

     • Improve application performance (e.g.,reduceCPU utilization)

     • Decrease compute costs

     • Provides heap summary (identify which objects using up memory)

     • Anomaly Detection


• Support applications running on AWS or on- premise

• Minimal overhead on application





----------------------------------------------------------------- Amazon CodeGuru – Agent Configuration


• MaxStackDepth – the maximum depth of the stacks in the code that is represented in the profile

      • Example: if CodeGuru Profiler finds a method A, which calls method B, which calls method C, which calls method D, then the depth is 4

      • If the MaxStackDepth is set to 2, then the profiler evaluates A and B


• MemoryUsageLimitPercent – the memory percentage used by the profiler

• MinimumTimeForReportingInMilliseconds – the minimum time between sending reports (milliseconds)

• ReportingIntervalInMilliseconds – the reporting interval used to report profiles (milliseconds)

• SamplingIntervalInMilliseconds – the sampling interval that is used to profile samples (milliseconds) 

      • Reduce to have a higher sampling rate






----------------------------------------------------------------- AWS Cloud9 ----------------------------


• Cloud-based Integrated Development Environment (IDE)

• Code editor, debugger, terminal in a browser

• Work on your projects from anywhere with an Internet connection

• Prepackaged with essential tools for popular programming languages (JavaScript, Python, PHP,...)

• Share your development environment with your team (pair programming)

• Fully integrated with AWS SAM & Lambda to easily build serverless applications







Q : You would like to automatically deploy a Single Page Application (SPA) to the S3 bucket, after generating the static files from a set of markdown files. Which AWS services should you use for this?

ANS : codepipeline + CodeBuild

EXP : CodeBuild can run any commands, so you can use it to run commands including build a static website and copy your static web files to an S3 bucket.














================================================================================ AWS Serverless Application Model (SAM) ==================================================


Taking your Serverless Development to the next level


-- try in ur personal lap for Hands On 


----------------------------------------------------------------- AWS SAM


-- this is really cool because we are going to view how SAM regroups all the serverless development

• Framework used for developing and deploying serverless applications

• All the configuration is YAML code

• Generate complex CloudFormation from simple SAMYAML file

• Supports anything from CloudFormation: Outputs, Mappings, Parameters, Resources...

• Only two commands to deploy to AWS

• SAM can use CodeDeploy to deploy Lambda functions

• SAM can help you to run Lambda, API Gateway, DynamoDB locally







----------------------------------------------------------------- AWS SAM – Recipe


•  SAM has a YAML template and at the top of it, you will find this transform header.

         • Transform: 'AWS::Serverless-2016-10-31'

         - Well, it means is that this is a SAM template and the CloudFormation will be smart enough to know how to transform these templates into a CloudFormation template.


• Write Code

         • AWS::Serverless::Function

         • AWS::Serverless::Api

         • AWS::Serverless::SimpleTable

         - So these are meant to help you go faster at creating functions, databases and API gateways.




• Package & Deploy:

         - 2 cmnds 

         • aws cloudformation package / sam package

         • aws cloudformation deploy / sam deploy
 



----------------------------------------------------------------- Deep Dive into SAM Deployment





                              Build the application locally (sam build)            Package the application(sam package OR aws cloudformation package)        Deploy the application(sam deploy OR aws cloudformation deploy)


SAM Template                           transform                                                                 zip & upload                 create/execute ChangeSet
(YAML)        + Application Code   -----------------------> CloudFormation Template (YAML) + Application Code ------------------> S3 Bucket ----------------------------->    CloudFormation --------> CloudFormation Stack(API Gateway ,lambda, DynamoDB) 




-- So we have a SAM template, which is written in YAML and our application code.

-- we're going to do a sam build to build the application locally and it's going to create a transformed CloudFormation template and our application code.

-- then we're going to run sam package or aws cloudformation package to zip and upload our function and have it uploaded into an S3 bucket.

-- So both the CloudFormation template and our application code is going to be uploaded and then using the sam deploy command, we're going to deploy our application from the S3 bucket.

-- It's going to create and execute a change set onto CloudFormation to start deploying our application.

-- And then CloudFormation will deploy our Lambda functions, our API gateways, and DynamoDB.

-- So SAM templates gets transformed into CloudFormation with sam build, then packaged into Amazon S3 using the sam package command, and then deployed onto CloudFormation using the sam deploy command.




-- we have a question? how can you run Lambda locally? 

AMS : to run Lambda functions locally, you can use the SAM CLI




----------------------------------------------------------------- SAM – CLI Debugging



• Locally build, test, and debug your serverless applications that are defined using AWS SAM templates

• Provides a lambda-like execution environment locally

• SAM CLI + AWS Toolkits => step-through and debug your code

          - So instead of deploying a Lambda function on the cloud and then stepping through it or trying to find where it failed, 
          
          - you can do all the development that you used to do directly on your local computer without deploying to AWS.


• Supported IDEs:AWS Cloud9,Visual Studio Code, JetBrains, PyCharm, IntelliJ, ...

• AWS Toolkits: IDE plugins which allows you to build, test, debug, deploy, and invoke Lambda functions built using AWS SAM





----------------------------------------------------------------- SAM Policy Templates


• List of templates to apply permissions to your Lambda Functions

https://docs.aws.amazon.com/serverless- application- model/latest/developerguide/serverless- policy-templates.html#serverless-policy- template-table


• Important examples:

     • S3ReadPolicy: Gives read only permissions to objects in S3

     • SQSPollerPolicy: Allows to poll an SQS queue

     • DynamoDBCrudPolicy: CRUD = create read update delete







----------------------------------------------------------------- SAM and CodeDeploy


• SAM framework natively uses CodeDeploy to update Lambda functions

• Traffic Shifting feature

• Pre and Post traffic hooks features to validate deployment (before the traffic shift starts and after it ends)

• Easy & automated rollback using CloudWatch Alarms




• AutoPublishAlias

      • Detects when new code is being deployed

      • Creates and publishes an updated version of that function with the latest code

      • Points the alias to the updated version of the Lambda function


• DeploymentPreference

      • Canary,Linear,AllAtOnce


• Alarms      

      • Alarms that can trigger a rollback


• Hooks

      • Pre and post traffic shifting Lambda functions to test your deployment









----------------------------------------------------------------- SAM – Local Capabilities



• by using SAM FrameWork , u can Locally start AWS Lambda

        • sam local start-lambda

        • you will have your Lambda function available as a local endpoint on your computer with an emulation of the Lambda framework.

        • Can run automated tests against this local endpoint



• Locally Invoke Lambda Function

        • sam local invoke

        • Invoke Lambda function with payload once and quit after invocation completes

        • Helpful for generating test cases

        • If the function make API calls to AWS, make sure you are using the correct --profile option



• Locally Start an API Gateway Endpoint

        • sam local start-api

        • Starts a local HTTP server that hosts all your functions

        • Changes to functions are automatically reloaded


• Generate AWS Events for Lambda Functions


        • sam local generate-event

        • Generate sample payloads for event sources

        • S3, API Gateway, SNS, Kinesis, DynamoDB...






----------------------------------------------------------------- SAM – Exam Summary


• SAM is built on CloudFormation

• SAM requires the Transform and Resources sections

     - SAM needs to work with two things; 

     - 1 the transform header or YAML indication which present the fact that it is a SAM templates

     - 2 the resources section which is necessary in any CloudFormation templates.



• Commands to know:

     • sam build: fetch dependencies and create local deployment artifacts

     • sam package: package and upload to Amazon S3, generate CF template

     • sam deploy: deploy to CloudFormation


• SAM Policy templates for easy IAM policy definition

• SAM is integrated with CodeDeploy to do deploy to Lambda aliases








================================================================================ AWS Cloud Development Kit ========================



• Define your cloud infrastructure using a familiar language:

       • JavaScript/TypeScript,Python,Java,and.NET


• Contains high level components called "constructs"

• The code is “compiled” into a CloudFormation template (JSON/YAML)

• You can therefore deploy infrastructure and application runtime code together

    • Great for Lambda functions

    • Great for Docker containers in ECS / EKS

        



----------------------------------------------------------------- CDK vs SAM


• SAM:

     • Serverless focused

     • Write your template declaratively in JSON orYAML

     • Great for quickly getting started with Lambda

     • Leverages CloudFormation


• CDK:     
 
    • All AWS services

    • Write infra in a programming language JavaScript/TypeScript, Python, Java, and .net

    • Leverages CloudFormation





----------------------------------------------------------------- CDK + SAM


• You can use SAM CLI to locally test your CDK apps

• You must first run cdk synth







----------------------------------------------------------------- CDK Hands on 


-- u have to follow some cmnds to do this DEMO 




# 1. install the CDK
sudo npm install -g aws-cdk-lib

# directory name must be cdk-app/ to go with the rest of the tutorial, changing it will cause an error
mkdir cdk-app
cd cdk-app/

# initialize the application
cdk init app --language javascript
# verify it works correctly
cdk ls

# 2. copy the content of cdk-app-stack.js into lib/cdk-app-stack.js

ls lib 
cd lib
rm  cdk-app-stack.js
touch cdk-app-stack.js
nano cdl-app-stack.js (paste this content )



const cdk = require("aws-cdk-lib");

const s3 = require("aws-cdk-lib/aws-s3");
const iam = require("aws-cdk-lib/aws-iam");
const lambda = require("aws-cdk-lib/aws-lambda");
const lambdaEventSource = require("aws-cdk-lib/aws-lambda-event-sources");
const dynamodb = require("aws-cdk-lib/aws-dynamodb");

const imageBucket = "cdk-rekn-imagebucket";

class CdkAppStack extends cdk.Stack {
    /**
     *
     * @param {cdk.Construct} scope
     * @param {string} id
     * @param {cdk.StackProps=} props
     */
    constructor(scope, id, props) {
        super(scope, id, props);

        // The code that defines your stack goes here

        // ========================================
        // Bucket for storing images
        // ========================================
        const bucket = new s3.Bucket(this, imageBucket, {
            removalPolicy: cdk.RemovalPolicy.DESTROY,
        });
        new cdk.CfnOutput(this, "Bucket", { value: bucket.bucketName });

        // ========================================
        // Role for AWS Lambda
        // ========================================
        const role = new iam.Role(this, "cdk-rekn-lambdarole", {
            assumedBy: new iam.ServicePrincipal("lambda.amazonaws.com"),
        });
        role.addToPolicy(
            new iam.PolicyStatement({
                effect: iam.Effect.ALLOW,
                actions: [
                    "rekognition:*",
                    "logs:CreateLogGroup",
                    "logs:CreateLogStream",
                    "logs:PutLogEvents",
                ],
                resources: ["*"],
            })
        );

        // ========================================
        // DynamoDB table for storing image labels
        // ========================================
        const table = new dynamodb.Table(this, "cdk-rekn-imagetable", {
            partitionKey: { name: "Image", type: dynamodb.AttributeType.STRING },
            removalPolicy: cdk.RemovalPolicy.DESTROY,
        });
        new cdk.CfnOutput(this, "Table", { value: table.tableName });

        // ========================================
        // AWS Lambda function
        // ========================================
        const lambdaFn = new lambda.Function(this, "cdk-rekn-function", {
            code: lambda.AssetCode.fromAsset("lambda"),
            runtime: lambda.Runtime.PYTHON_3_9,
            handler: "index.handler",
            role: role,
            environment: {
                TABLE: table.tableName,
                BUCKET: bucket.bucketName,
            },
        });
        lambdaFn.addEventSource(
            new lambdaEventSource.S3EventSource(bucket, {
                events: [s3.EventType.OBJECT_CREATED],
            })
        );

        bucket.grantReadWrite(lambdaFn);
        table.grantFullAccess(lambdaFn);
    }
}

module.exports = { CdkAppStack };


-- crtl and y and enter to exit 

cd ..


# 3. setup the Lambda function

mkdir lambda && cd lambda
touch lambda.py
nano lambda.py



#
# Lambda function detect labels in image using Amazon Rekognition
#

from __future__ import print_function
import boto3
import json
import os
from boto3.dynamodb.conditions import Key, Attr

minCofidence = 60


def handler(event, context):
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']

    rekFunction(bucket, key)


def rekFunction(bucket, key):
    print("Detected the following image in S3")
    print("Bucket: " + bucket + " key name: " + key)

    client = boto3.client("rekognition")

    response = client.detect_labels(Image={"S3Object": {"Bucket": bucket, "Name": key}},
                                    MaxLabels=10, MinConfidence=minCofidence)

    # Get the service resource
    dynamodb = boto3.resource("dynamodb")

    # Instantiate a table resource object
    imageLabelsTable = os.environ["TABLE"]
    table = dynamodb.Table(imageLabelsTable)

    # Put item into table
    table.put_item(
        Item={"Image": key}
    )

    objectsDetected = []

    for label in response["Labels"]:
        newItem = label["Name"]
        objectsDetected.append(newItem)
        objectNum = len(objectsDetected)
        itemAtt = f"object{objectNum}"
        response = table.update_item(
            Key={"Image": key},
            UpdateExpression=f"set {itemAtt} = :r",
            ExpressionAttributeValues={":r": f"{newItem}"},
            ReturnValues="UPDATED_NEW"
        )



-- cd ..


# 4. bootstrap the CDK application
cdk bootstrap
      
      - this will create all the stuff for u , go n check in CF


# 5. (optional) synthesize as a CloudFormation template
cdk synth


# 6. deploy the CDK stack
cdk deploy

     - So let's go into the S3 bucket right here, open it in a new tab, and then the DynamoDB table as well.

     - So the idea is that whenever we upload a file into this S3 bucket, the Lambda function that get deployed will be run and then we will have some data in our DynamoDB table.

     - upload any image , see in DynamoDB , u will see that what ever the image has it will have the data 'coz Lambda function detect labels in image using Amazon Rekognition

# 7. empty the s3 bucket
# 8. destroy the stack
cdk destroy




-- open cloudshell and perform this cmnds 

-- 





----------------------------------------------------------------- CDK Constructs



• CDK Construct is a component that encapsulates everything CDK needs to create the final CloudFormation stack

• Can represent a single AWS resource (e.g., S3 bucket) or multiple related resources (e.g., worker queue with compute)

• how do we get these Constructs? 

   - AWS Construct Library

         • A collection of Constructs included in AWS CDK which contains Constructs for every AWS resource

         • Contains 3 different levels of Constructs available (L1, L2, L3)


    - • Construct Hub

         - contains additional Constructs from AWS, 3rd parties, and open-source CDK community         





----------------------------------------------------------------- CDK Constructs – Layer 1 Constructs (L1)


• Can be called CFN Resources which represents all resources directly available in CloudFormation

• Constructs are periodically generated from CloudFormation Resource Specification

• Construct names start with Cfn (e.g., CfnBucket)

• You must explicitly configure all resource properties



const bucket = new s3.CfnBucket(this, "MyBucket", {
  bucketName: "MyBucket"
});




----------------------------------------------------------------- CDK Constructs – Layer 2 Constructs (L2)


• Represents AWS resources but with a higher level (intent-based API)

• Similar functionality as L1 but with convenient defaults and boilerplate

     • You don’t need to know all the details about the resource properties

• Provide methods that make it simpler to work with the resource

      (e.g., bucket.addLifeCycleRule())



const s3 = require('aws-cdk-lib/aws-s3');

// "this" is HelloCdkStack
new s3.Bucket(this, 'MyFirstBucket', {
  versioned: true
});




----------------------------------------------------------------- CDK Constructs – Layer 3 Constructs (L3)



• Can be called Patterns, which represents multiple related resources

• Helps you complete common tasks in AWS

• Examples:

      • aws-apigateway.LambdaRestApi represents an API Gateway backed by a Lambda function

      • aws-ecs-patterns.ApplicationLoadBalancerFargateService which represents an architecture that includes a Fargate cluster with Application Load Balancer








----------------------------------------------------------------- CDK – Important Commands to know


npm install -g aws-cdk-lib      -    Install the CDK CLI and libraries

cdk init app - Create a new CDK project from a specified template

cdk synth    - Synthesizes and prints the CloudFormation template

cdk bootstrap - Deploys the CDK Toolkit staging Stack

cdk deploy    - Deploy the Stack(s)

cdk diff      - View differences of local CDK and deployed Stack

cdk destroy   - Destroy the Stack(s)




----------------------------------------------------------------- CDK – Bootstrapping


• The process of provisioning resources for CDK before you can deploy CDK apps into an AWS environment

• AWS Environment = account & region , for CDK, it's the combination of an account and a region.

-- So the idea is that before you are able to deploy to an account and a region, 

-- you must deploy a CloudFormation stack called the CDKToolkit that will contain an S3 bucket and it will contain an IAM role.

• CloudFormation Stack called CDKToolkit is created and contains:

     • S3 Bucket – to store files

     • IAM Roles – to grant permissions to perform deployments

     - these are necessary prerequisites to deploying any CDK stack in that specified environment.


• You must run the following command for each new environment:

      • cdk bootstrap aws://<aws_account>/<aws_region>

      - So here is what happens. We have the account and the region, and the user runs the bootstrap. 
      
      - A CloudFormation stack called CDKToolkit is going to be created with what we need.


• Otherwise, you will get an error “Policy contains a statement with one or more invalid principal”      






----------------------------------------------------------------- CDK –Testing


• To test CDK apps, use CDK Asser tions Module combined with popular test frameworks such as Jest (JavaScript) or Pytest (Python)

• Verify we have specific resources, rules, conditions, parameters...

• Two types of tests:

      • Fine-grained Assertions (common) – test specific aspects of the CloudFormation template (e.g., check if a resource has this property with this value)

      • Snapshot Tests – test the synthesized CloudFormation template against a previously stored baseline template


• To import a template

      • Template.fromStack(MyStack) : stack built in CDK

      • Template.fromString(mystring) : stack build outside CDK










============================================================================== Amazon Cognito ==================================================




• Give users an identity to interact with our web or mobile application

-- So these users usually sit outside of our AWS account, hence the name Cognito, because it gives an identity to users that we don't know about yet.

-- So we have two kind of sub-services within Cognito.


1 • Cognito User Pools:

      • Sign in functionality for app users

      • Integrate with API Gateway & Application Load Balancer


2 • Cognito Identity Pools (Federated Identity):

      • Provide AWS temporarily credentials to users so they can access some AWS resources directly

      • Integrate with Cognito User Pools as an identity provider




• Cognito vs IAM: “hundreds of users”, ”mobile users”, “authenticate with SAML” 

       - So if you are asking yourself don't we already have users in IAM?

       - But Cognito is gonna be for your web and mobile application users, which sits outside of AWS.

       - So look for the keywords such as hundreds of users or mobile users, or authentication with whatever mechanisms such as SAML and so on.






----------------------------------------------------------------- Cognito User Pools (CUP) – User Features


• Create a serverless database of user for your web & mobile apps

• Simple login: Username (or email) / password combination

• Password reset

• Email & Phone Number Verification

• Multi-factor authentication (MFA)

• Federated Identities: users from Facebook, Google, SAML...

• Login sends back a JSON Web Token (JWT)

      - when your users log in with the Cognito User Pool, what they get back from the API is a JWT, so a JSON Web Token.






----------------------------------------------------------------- Cognito User Pools (CUP) - Integrations


• CUP integrates with API Gateway and Application Load Balancer





--------------------------------------------------------------- CUP LAB 






 Cognito User Pools (CUP) 

 -- create user pool 

 -- open console and create user pool

 -- Cognito user pool sign-in options --> email 

 --  step 2 Required attributes --> add any one for user to register --> name 

 -- step 3 --> Send email with Cognito 

 -- step 5 --> name of userpool --> enable hosted UI  --> give domain name any thing eg demo-user-pool-cognito

 -- Initial app client --> public client name --> Allowed callback URL = localhost or anything like www.flipkart.comß

 -- once u created usewr pool --> open user pool ,u can see users and groups 

 -- create user --> set password , once u create user pool 

 --  go to google and search for amazon cognito ui --> view signup page in documentation 

 https://<your domain>/oauth2/authorize?response_type=code&client_id=<your app client id>&redirect_uri=<your callback url>

 -- go to user pools --> app integration -->  change ur details in the above link 

 
https://demo-user-pool-66.auth.ap-south-1.amazoncognito.com/oauth2/authorize?response_type=code&client_id=juroqdd4a95dac05aom4dvadj&redirect_uri=https://www.amazon.com

-- u will get domain name  in APP integration section

-- once the url is ready , paste the link in the browser 

-- this will pop-up sign in page , this is called hosted UI , enter ur details 

-- once u enter it will ask u to change password 

-- once u enter details , u will redirected to the link which u have given in call back url 

--  so delete cookies in ur browser and try to login again 

-- now try to sign up as new user and see how it works 

-- u can also do MFA for another layer of security 






----------------------------------------------------------------- Cognito User Pools – Lambda Triggers



• CUP can invoke a Lambda function synchronously on these triggers:


1 Authentication Events : 


        - Pre Authentication Lambda Trigger : Custom validation to accept or deny the sign-in request

        - Post Authentication Lambda Trigger : Event logging for custom analytics

        - Pre Token Generation Lambda Trigger : Augment or suppress token claims


2 Sign-Up

        - Pre Sign-up Lambda Trigger : Custom validation to accept or deny the sign-up request

        - Post Confirmation Lambda Trigger : Custom welcome messages or event logging for custom analytics

        - Migrate User Lambda Trigger : Migrate a user from an existing user directory to user pools


3 Messages : 

        - Custom Message Lambda Trigger : Advanced customization and localization of messages


4 Token Creation : 

        - Pre Token Generation Lambda Trigger : Add or remove attributes in Id tokens







----------------------------------------------------------------- Cognito User Pools – Hosted Authentication UI


• Cognito has a hosted authentication UI that you can add to your app to handle sign-up and sign-in workflows

• Using the hosted UI, you have a foundation for integration with social logins, OIDC or SAML

• Can customize with a custom logo and custom CSS





----------------------------------------------------------------- CUP – Hosted UI Custom Domain


• For custom domains, you must create an ACM certificate in "us-east-1"

         - So the trick you need to know is that if you're using custom domains with Cognito User Pools, regardless of where it is created, 
         
         - you must create an certificate for using HTTPS. That certificate must be in ACM, 


• The custom domain must be defined in the “App Integration” section

         - because this is going to be a configuration for all app claims. This is a general configuration.






----------------------------------------------------------------- CUP – Adaptive Authentication



• Block sign-ins or require MFA if the login appears suspicious

• Cognito examines each sign-in attempt and generates a risk score (low, medium, high) for how likely the sign-in request is to be from a malicious attacker

• Users are prompted for a second MFA only when risk is detected

• Risk score is based on different factors such as if the user has used the same device, location, or IP address

• Checks for compromised credentials, account takeover protection, and phone and email verification

• Integration with CloudWatch Logs (sign-in attempts, risk score, failed challenges...)





----------------------------------------------------------------- Decoding a ID Token; JWT – JSON Web Token


• CUP issues JWT tokens (Base64 encoded): 

   • Header
   • Payload
   • Signature
   - so Base64 is allowing you to transmit that over the network easily,   


• The signature must be verified to ensure the JWT can be trusted

• Libraries can help you verify the validity of JWT tokens issued by Cognito User Pools

• The Payload will contain the user information (sub UUID, given_name, email, phone_number, attributes...)

• From the sub UUID, you can retrieve all users details from Cognito / OIDC



<header>.
{
   "sub":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "device_key": "aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "cognito:groups":[
      "testgroup"
   ],
   "iss":"https://cognito-idp.us-west-2.amazonaws.com/us-west-2_example",
   "version":2,
   "client_id":"xxxxxxxxxxxxexample",
   "origin_jti":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "event_id":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "token_use":"access",
   "scope":"phone openid profile resourceserver.1/appclient2 email",
   "auth_time":1676313851,
   "exp":1676317451,
   "iat":1676313851,
   "jti":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
   "username":"my-test-user"
}
.<token signature>





----------------------------------------------------------------- Application Load Balancer – Authenticate Users



• Your Application Load Balancer can securely authenticate users

    • Offload the work of authenticating users to your load balancer

    • Your applications can focus on their business logic


-- So you can authenticate in multiple ways for your application load balancer.    



• Authenticate users through:

        1 Identity Provider (IdP): OpenID Connect (OIDC) compliant

        2 Cognito User Pools:

              • SocialIdPs,such as Amazon,Facebook,or Google

              • Corporate identities using SAML, LDAP, or MicrosoftAD


-- So the first one, first option is to integrate directly without Cognito user pools              


• Must use an HTTPS listener to set authenticate-oidc & authenticate-cognito rules

• OnUnauthenticatedRequest – authenticate (default), deny, allow






----------------------------------------------------------------- Application Load Balancer – Cognito Auth.







           1. GET /api/data                                                3. GET /api/data
Users ---------------------------------> Application Load Balancer ------------------------------->  Amazon ECS

                                                   |
                                                   |
                                                   |
                          2. authenticate          |            HTTPS listener Action: authenticate-cognito
                                                   |
                                                   |
                                                   |

                                              Amazon Cognito




-- let's say we have an application load balancer connected to Amazon ECS and we want to implement a login through Amazon Cognito

-- the users will do GET/api/data. Then the ALB is going to be set up with HTTPS with the action authenticate-cognito.

-- So Cognito will do its thing and authenticate the user, 

-- and then the payload and the request is passed on to Amazon ECS with the added information of the user that was doing the request from Cognito.







----------------------------------------------------------------- ALB – Auth through Cognito User Pools


• Create Cognito User Pool, Client and Domain

• Make sure an ID token is returned

• Add the social or Corporate IdP if needed

• Several URL redirections are necessary

• Allow your Cognito User Pool Domain on your IdP app's callback URL.  for example 

      • https://domain- prefix.auth.region.amazoncognito.com/saml2/ idpresponse

      • https://user-pool-domain/oauth2/idpresponse





----------------------------------------------------------------- Application Load Balancer – OIDC Auth.


see fig 






----------------------------------------------------------------- ALB – Auth.Through an Identity Provider (IdP) That is OpenID Connect (OIDC) Compliant




• Configure a Client ID & Client Secret

• Allow redirect from OIDC to your Application Load Balancer DNS name (AWS provided) and CNAME (DNS Alias of your app)

• https://DNS/oauth2/idpresponse

• https://CNAME/oauth2/idpresponse





----------------------------------------------------------------- Cognito Identity Pools (Federated Identities) -------------



• Get identities for “users” so they obtain temporary AWS credentials

       - So, our users are outside of our AWS environment, and there can be a web application users, or mobile users, and they want access to stuff within our AWS environments.

       - So for example, they want to have access to DynamoDB table, or an S3 bucket. And so to access these things, they need temporary AWS credentials.

       - So we cannot create normal IAM users for these users because there are too many of them, it doesn't scale, and we don't trust them.

       - So instead, we're going to give these users access to AWS through a Cognito Identity Pool.


-- So this identity pool can allow your users to log in through a trusted third party.       

• Your identity pool (e.g identity source) can include:

      • Public Providers (Login with Amazon, Facebook, Google, Apple)

      • Users in an Amazon Cognito user pool

      • OpenID Connect Providers & SAML Identity Providers

      • Developer Authenticated Identities (custom login server)

      • Cognito Identity Pools allow for unauthenticated (guest) access



• Users can then access AWS services directly or through API Gateway

      - So, once the users have obtained these AWS credentials, then they can access the AWS services directly,

      - with an API call using an SDK, or through the API gateway.

      • The IAM policies applied to the credentials are defined in Cognito

      • They can be customized based on the user_id for fine grained control






----------------------------------------------------------------- Cognito Identity Pools – IAM Roles


• Default IAM roles for authenticated and guest users

       - So that means the guest users will have one IAM role, and the other will have another one's IAM role.


• Define rules to choose the role for each user based on the user’s ID       

• You can partition your users’ access using policy variables

        - hen we can customize the IAM policy, thanks to using policy variables, and 
        
        - this will be allowing us to make sure that the users will only get access to what they need in DynamoDB or Amazon S3, 


• IAM credentials are obtained by Cognito Identity Pools through STS

• The roles must have a “trust” policy of Cognito Identity Pools








----------------------------------------------------------------- Cognito Identity Pools – Guest User example






{
  "Version": "2012-10-17",
  "Statement": [
    {
    
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:region:accountID:map/ExampleMap"
    }
  ]
}



-- So we want to give our guest users access to AWS, so we want to create an IAM policy,

-- that would allow any guest user to do a get object on a bucket for my picture, 

-- So this would give us, for the guest users, access to AWS with a very simple, and obviously restricted, IAM policy.






----------------------------------------------------------------- Cognito Identity Pools – Policy variable on S3



{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ListYourObjects",
      "Effect": "Allow",
      "Action": "s3:ListBucket",
      "Resource": [
        "arn:aws:s3:::bucket-name"
      ],
      "Condition": {
        "StringLike": {
          "s3:prefix": [
            "cognito/application-name/${cognito-identity.amazonaws.com:sub}/*"
          ]
        }
      }
    },
    {
      "Sid": "ReadWriteDeleteYourObjects",
      "Effect": "Allow",
      "Action": [
        "s3:DeleteObject",
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-name/cognito/application-name/${cognito-identity.amazonaws.com:sub}/*"
      ]
    }
  ]
}



-- for the authenticated users, you can define a policy variable on Amazon S3,

-- so our users are connected, but we want to make sure that they only have access to a prefix in your S3 bucket,






----------------------------------------------------------------- Cognito User Pools vs Identity Pools



• Cognito User Pools (for authentication = identity verification)

    • Database of users for your web and mobile application

    • Allows to federate logins through Public Social, OIDC, SAML...

    • Can customize the hosted UI for authentication (including the logo)

    • Has triggers with AWS Lambda during the authentication flow

    • Adapt the sign-in experience to different risk levels (MFA, adaptive authentication, etc...)


• Cognito Identity Pools (for authorization = access control) 

    • Obtain AWS credentials for your users

    • Users can login through Public Social, OIDC, SAML & Cognito User Pools

    • Users can be unauthenticated (guests)

    • Users are mapped to IAM roles & policies, can leverage policy variables


• CUP + CIP = authentication + authorization    











============================================================================== Other Serverless ==================================================



================================ 1 AWS Step Functions =================




• Model your workflows as state machines (one per workflow)

      • So this could be used if you want to have an Order fulfillment, Data processing

      • Web applications, Any workflow


• Written in JSON

• Visualization of the workflow and the execution of the workflow, as well as history

• Start workflow with SDK call, API Gateway, Event Bridge (CloudWatch Event)


-- AWS Step Functions is a low-code visual workflow service used to orchestrate AWS services, automate business processes, and build Serverless applications. It manages failures, retries, parallelization, service integrations, ...


-------------------------------------- Step Function –Task States


- So step functions have a bunch of boxes and these boxes are called tasks.

• so the task state is used to Do some work in your state machine

• Invoke one AWS service

     • Can invoke a Lambda function
     • Run an AWS Batch job
     • Run an ECS task and wait for it to complete
     • Insert an item from DynamoDB
     • Publish message to SNS, SQS
     • Launch another Step Function workflow...


• Run an one Activity

    • EC2, Amazon ECS, on-premises

    • Activities poll the Step functions for work

    • Activities send results back to Step Functions






-------------------------------------- Example – Invoke Lambda Function




{  
   "StartAt":"CallLambda",
   "States":{  
      "CallLambda":{  
         "Type":"Task",
         "Resource":"arn:aws:states:::lambda:invoke",
         "Parameters":{  
            "FunctionName":"arn:aws:lambda:us-east-1:123456789012:function:MyFunction"
         },
         "End":true
      }
   }
}






------------------------------------------- Step Function - States



• Choice State - Test for a condition to send to a branch (or default branch)

• Fail or Succeed State - Stop execution with failure or success

• Pass State - Simply pass its input to its output or inject some fixed data, without performing work.

• Wait State - Provide a delay for a certain amount of time or until a specified time/date.

• Map State - Dynamically iterate steps.’

• Parallel State - Begin parallel branches of execution.




------------------------------------------- Step Function - hands ON


-- open step functions in console 

-- run Hello World and observe the things 

--  ASL (Amazon State Language)

 "Result": {
        "IsHelloWorldExample": true
      },

-- remove the aboe lines from the hello world code 

-- c.o create 

{
        "IsHelloWorldExample": true
      },

-- give above lines in input and start execution 

-- try with  "IsHelloWorldExample": false and observe the things 







------------------------------------------- Step Function - Invoke Lambda Function hands ON


-- c.o create state Machine 

-- choose blank format and start 

-- drag and paste the lambda 

-- now add the code 




{
  "Comment": "A Hello World example of the Amazon States Language using Pass states",
  "StartAt": "Lambda Invoke",
  "States": {
    "Lambda Invoke": {
      "Type": "Task",
      "Resource": "arn:aws:states:::lambda:invoke",
      "OutputPath": "$.Payload",
      "Parameters": {
        "Payload.$": "$",
        "FunctionName": "arn:aws:lambda:ap-south-1:298132369629:function:hello"
      },
      "Retry": [
        {
          "ErrorEquals": [
            "Lambda.ServiceException",
            "Lambda.AWSLambdaException",
            "Lambda.SdkClientException",
            "Lambda.TooManyRequestsException"
          ],
          "IntervalSeconds": 1,
          "MaxAttempts": 3,
          "BackoffRate": 2
        }
      ],
      "Next": "Choice State"
    },
    "Choice State": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$",
          "StringMatches": "*Subbu*",
          "Next": "Is Teacher"
        }
      ],
      "Default": "Not Teacher"
    },
    "Is Teacher": {
      "Type": "Pass",
      "Result": "Woohoo!",
      "End": true
    },
    "Not Teacher": {
      "Type": "Fail",
      "Error": "ErrorCode",
      "Cause": "Subbu the teacher wasn't found in the output of the Lambda Function"
    }
  }
}



--   now go to lambda and create a function 

-- use nodejs latest version , and give function name as "Hello"

-- add this code 


export const handler = async (event) => {
    return "Hello, " + event.who + "!";
  };


-- now deploy and do test with 

"who" : "subbu" in test event and do test 



-- copy ARN of lambda and paste in step functions (API parameters section )

-- now do create funtion , u will see it will invoke lambda finction 

-- c.o confirm and do start execution , givr input as 



{
  "who" : "Subbu"
}








----------------------------------------------------- Error Handling in Step Functions


-- So a step function will execute many small tasks and these tasks will do very little amount of work.

-- For example, talk to an API and so on. But all the error handling should happen outside of these tasks by step functions themselves.


• Any state can encounter runtime errors for various reasons:

     • State machine definition issues (for example, no matching rule in a Choice state)

     • Task failures (for example, an exception in a Lambda function)

     • Transient issues (for example, network partition events)


• Use Retry (to retry failed state) and Catch (transition to failure path) in the State Machine to handle the errors instead of inside the Application Code

• Predefined error codes:

      • States.ALL : matches any error name
      • States.Timeout:Task ran longer thanTimeoutSeconds or no heartbeat received
      • States.TaskFailed: execution failure
      • States.Permissions: insufficient privileges to execute code



• The state may report is own errors






----------------------------------------------------- Step Functions – Retry (Task or Parallel State)


• Evaluated from top to bottom

• ErrorEquals: match a specific kind of error

• IntervalSeconds: initial delay before retrying

• BackoffRate: multiple the delay after each retry

• MaxAttempts: default to 3, set to 0 for never retried

• When max attempts are reached, the Catch kicks in

-- if you were to define all the Retry logic from within the Lambda function, we would make the Lambda function run for a very, very long time, obviously, and maybe would timeout.





{
    "Comment": "A Retry and Catch example of the Amazon States Language using an AWS Lambda Function",
    "StartAt": "InvokeMyFunction",
    "States": {
      "InvokeMyFunction": {
        "Type": "Task",
        "Resource": "<enter resource ARN here>",
        "Retry": [
          {
            "ErrorEquals": [
              "CustomError"
            ],
            "IntervalSeconds": 1,
            "MaxAttempts": 2,
            "BackoffRate": 2
          },
          {
            "ErrorEquals": [
              "States.TaskFailed"
            ],
            "IntervalSeconds": 30,
            "MaxAttempts": 2,
            "BackoffRate": 2
          },
          {
            "ErrorEquals": [
              "States.ALL"
            ],
            "IntervalSeconds": 5,
            "MaxAttempts": 5,
            "BackoffRate": 2
          }
        ],

        "End" : true 
      }       






----------------------------------------------------- Step Functions – Catch (Task or Parallel State)


  "hello world" : {
  "Type": "Task",
  "Resource": "<enter resource ARN here>",


  "Catch": [
          {
            "ErrorEquals": [
              "CustomError"
            ],
            "Next": "CustomErrorFallback"
          },
          {
            "ErrorEquals": [
              "States.TaskFailed"
            ],
            "Next": "ReservedTypeFallback"
          },
          {
            "ErrorEquals": [
              "States.ALL"
            ],
            "Next": "CatchAllFallback"
          }
        ],
        "End": true
      },
      "CustomErrorFallback": {
        "Type": "Pass",
        "Result": "This is a fallback from a custom lambda function exception",
        "End": true
      },
      "ReservedTypeFallback": {
        "Type": "Pass",
        "Result": "This is a fallback from a reserved error code",
        "End": true
      },
      "CatchAllFallback": {
        "Type": "Pass",
        "Result": "This is a fallback from a reserved error code",
        "End": true
      }
    }
  }




• Evaluated from top to bottom

• ErrorEquals: match a specific kind of error

• Next: State to send to

• ResultPath - A path that determines what input is sent to the state specified in the Next field.







----------------------------------------------------- Step Function – ResultPath


• Include the error in the input

-- So the ResultPath is how you pass the errors from the inputs into the output, into the next task.







----------------------------------------------------- Step Function – Error Handling hands ON


-- lambda --> choose blue print --> search for step-function --> create function

-- now do test , u will get error 

-- we're going to handle this error directly from our Step functions.

-- opem step functions create new state machine 




{
    "Comment": "A Retry and Catch example of the Amazon States Language using an AWS Lambda Function",
    "StartAt": "InvokeMyFunction",
    "States": {
      "InvokeMyFunction": {
        "Type": "Task",
        "Resource": "<enter resource ARN here>",
        "Retry": [
          {
            "ErrorEquals": [
              "CustomError"
            ],
            "IntervalSeconds": 1,
            "MaxAttempts": 2,
            "BackoffRate": 2
          },
          {
            "ErrorEquals": [
              "States.TaskFailed"
            ],
            "IntervalSeconds": 30,
            "MaxAttempts": 2,
            "BackoffRate": 2
          },
          {
            "ErrorEquals": [
              "States.ALL"
            ],
            "IntervalSeconds": 5,
            "MaxAttempts": 5,
            "BackoffRate": 2
          }
        ],
        "Catch": [
          {
            "ErrorEquals": [
              "CustomError"
            ],
            "Next": "CustomErrorFallback"
          },
          {
            "ErrorEquals": [
              "States.TaskFailed"
            ],
            "Next": "ReservedTypeFallback"
          },
          {
            "ErrorEquals": [
              "States.ALL"
            ],
            "Next": "CatchAllFallback"
          }
        ],
        "End": true
      },
      "CustomErrorFallback": {
        "Type": "Pass",
        "Result": "This is a fallback from a custom lambda function exception",
        "End": true
      },
      "ReservedTypeFallback": {
        "Type": "Pass",
        "Result": "This is a fallback from a reserved error code",
        "End": true
      },
      "CatchAllFallback": {
        "Type": "Pass",
        "Result": "This is a fallback from a reserved error code",
        "End": true
      }
    }
  }





-- paste ur ARN of lambda in code 

-- start execution 







----------------------------------------------------- Step Functions – Wait for Task Token


• Allows you to pause Step Functions during aTask until aTaskToken is returned

• Task might wait for other AWS services, human approval, 3rd party integration, call legacy systems...

• Append .waitForTaskToken to the Resource field to tell Step Functions to wait for the Task Token to be returned

• Task will pause until it receives thatTaskToken back with a SendTaskSuccess or SendTaskFailure API call





----------------------------------------------------- Step Functions – Activity Tasks


• Enables you to have theTask work performed by an Activity Worker

• Activity Worker apps can be running on EC2, Lambda, mobile device...

• Activity Worker poll for a Task using GetActivityTask API

• After Activity Worker completes its work, it sends a response of its success/failure using SendTaskSuccess or SendTaskFailure

• To keep theTask active:

       • Configure how long a task can wait by setting TimeoutSeconds

       • Periodically send a heartbeat from your Activity Worker using SendTaskHeartBeat within the time you set in HeartBeatSeconds


• By configuring a long TimeoutSeconds and actively sending a heartbeat,ActivityTask can wait up to 1 year







----------------------------------------------------- Step Functions – Standard vs. Express


1 Standard (default):


Max. Duration : 1 yr

Execution Model : Exactly-once Execution

Execution Rate : Over 2000 / second

Execution History : Up to 90 days or using CloudWatch

Pricing : # of State Transitions

Use cases : Non-idempotent actions ( (e.g., processing Payments))




2 Express


Max. Duration : Up to 5 minutes

Execution Rate :Over 100,000 / second

Execution History : CloudWatch Logs

Pricing : # of executions, duration, and memory consumption

Use cases : IoT data ingestion, streaming data, mobile app backends, ...


in Express we have two 

1 Asynchronous (At-least once)

    - Doesn’t wait for Workflow to complete (get results from CW Logs)

    - You don’t need an immediate response (e.g., messaging services)

    - Must manage idempotence


2 Synchronous At-most once    

    - Wait for Workflow to complete

    - You need an immediate response (e.g., orchestrate microservices)

    - Can be invoked from API Gateway or Lambda function








----------------------------------------------------- AWS AppSync - Overview ---------------------



• AppSync is a managed service that uses GraphQL

         - So if we wants to build a GraphQL API on AWS, look no further than AppSync.

• GraphQL makes it easy for applications to get exactly the data they need.

• This includes combining data from one or more sources

      • NoSQL data stores, Relational databases, HTTP APIs...

      • Integrates with DynamoDB, Aurora, OpenSearch & others

      • Custom sources with AWS Lambda


• Retrieve data in real-time with WebSocket or MQTT on WebSocket

• For mobile apps: local data access & data synchronization

• It all starts with uploading one GraphQL schema





----------------------------------------------------- AppSync – Security

• There are four ways you can authorize applications to interact with your AWS AppSync GraphQL API:

    • API_KEY

    • AWS_IAM: IAM users / roles / cross-account access

    • OPENID_CONNECT: OpenID Connect provider / JSON Web Token

    • AMAZON_COGNITO_USER_POOLS


• For custom domain & HTTPS, use CloudFront in front of AppSync






----------------------------------------------------- AppSync Hands On


-- open appsync in console --> GraphQL APIs ---> Design from scratch ---> Create type backed by a DynamoDB table now --> add fiels as u want ---> give table name as u want and Primary key --> Create

-- if u look at schema in left side automatically it creates for u 

-- go to dynamodb and check whether the table is created or not 

-- now in appsync --> queries --> run --> liststudents , u won't get any 'coz u do not have any list of students 

--  so run --> createstudent --> give data in below and c.o run --> createstudent 

-- now run--> liststudent , u will get list of students from the API

-- go to dynamodb and check the users data created or not as items in table 

-- the cool thing about it is that now we have a full GraphQL-compatible API on top of our DynamoDB table.

-- So we can expose it to our mobile clients or any websites that is using GraphQL and API mechanism, which is very, very handy.








----------------------------------------------------- AWS Amplify --------------------



- AWS Amplify Create mobile and web applications

-- it's made of different components.

        1 Amplify Studio : Visually build a full-stack app, both front-end UI and a backend.

        2 Amplify CLI : Configure an Amplify backend With a guided CLI workflow

        3 Amplify Libraries : Connect your app to existing AWS Services (Cognito, S3 and more)

        4 Amplify Hosting : Host secure, reliable, fast web apps or websites via the AWS content delivery network.






-----------------------------------------------------  AWS Amplify


• Set of tools to get started with creating mobile and web applications

• “Elastic Beanstalk for mobile and web applications”

• Must-have features such as data storage, authentication, storage, and machine-learning, all powered by AWS services

• Front-end libraries with ready-to-use components for React.js,Vue, Javascript, iOS, Android, Flutter, etc...

• Incorporates AWS best practices to for reliability, security, scalability

• Build and deploy with the Amplify CLI or Amplify Studio





----------------------------------------------------- AWS Amplify – Important Features



1 AUTHENTICATION

     - For this, we do amplify add auth,

     • Leverages Amazon Cognito

     • User registration, authentication, account recovery & other operations

     • Support MFA, Social Sign-in, etc...

     • Pre-built UI components

     • Fine-grained authorization


2 DATASTORE

     • For this, we do amplify add api

     • Leverages Amazon AppSync and Amazon DynamoDB

     • Work with local data and have automatic synchronization to the cloud without complex code

     • Powered by GraphQL

     • Offline and real-time capabilities

     • Visual data modeling w/ Amplify Studio








----------------------------------------------------- AWS Amplify Hosting


• For this, we do amplify add hosting

• Build and Host Modern Web Apps

• CICD (build, test, deploy)

• Pull Request Previews

• Custom Domains

• Monitoring

• Redirect and Custom Headers

• Password protection






----------------------------------------------------- AWS Amplify – End-to-End (E2E) Testing


- you have two kind of testing, you have unit testing and end-to-end testing.

• Run end-to-end (E2E) tests in the test phase in Amplify

• Catch regressions before pushing code to production

• Use the test step to run any test commands at build time (amplify.yml)

• Integrated with Cypress testing framework

     • Allows you to generate UI report for your tests




Build(run tests while the app is being built) -----------> Test (E2E) (run tests while the app is deployed (staging)) ----------> Deploy








==================================================== Advanced Identity in AWS =========================================



-----------------------------------------------------  AWS STS – Security Token Service


• Allows to grant limited and temporary access to AWS resources (up to 1 hour).

• AssumeRole: Assume roles within your account or cross account

• AssumeRoleWithSAML: return credentials for users logged with SAML

• AssumeRoleWithWebIdentity
  
      • return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible...)

      • AWS recommends against using this, and using Cognito Identity Pools instead


• GetSessionToken: for MFA, from a user or AWS account root user

• GetFederationToken: obtain temporary creds for a federated user

• GetCallerIdentity: return details about the IAM user or role used in the API call

• DecodeAuthorizationMessage: decode error message when an AWS API is denied





----------------------------------------------------- Using STS to Assume a Role



• Define an IAM Role within your account or cross-account

• Define which principals can access this IAM Role and we authorize everything with IAM policies.

• Use AWS STS (Security Token Service) to retrieve credentials and impersonate the IAM Role you have access to (AssumeRole API)

• Temporary credentials can be valid between 15 minutes to 1 hour





----------------------------------------------------- STS with MFA


• Use GetSessionToken from STS

• Appropriate IAM policy using IAM Conditions

• aws:MultiFactorAuthPresent:true

        - For example, this role only allows us to stop instances or terminate instances only if we have MFA on,

        - so MultiFactorAuthPresent:true. And this is how we would use it.

• Reminder, GetSessionToken returns:

      • Access ID
      • Secret Key
      • SessionToken
      • Expiration date






----------------------------------------------------- Advanced IAM - Authorization Model Evaluation of Policies, simplified


1. If there’s an explicit DENY in ut policy , end decision and DENY

2. If there’s an ALLOW, end decision with ALLOW

3. Else DENY





----------------------------------------------------- IAM Policies & S3 Bucket Policies


• IAM Policies are attached to users, roles, groups

• S3 Bucket Policies are attached to buckets

• When evaluating if an IAM Principal can perform an operation X on a bucket, the union of its assigned IAM Policies and S3 Bucket Policies will be evaluated.




IAM Policy + S3 Bucket Policy = Total Policy Evaluated


- So that means that, for example, if you have an EC2 instance and you remove the S3 policies in the IAM policy,

- but there's still an S3 bucket policy authorizing the EC2 instance to do something, then it will still be able to do stuff in S3.







----------------------------------------------------- Example 1


• IAM Role attached to EC2 instance, authorizes RW to “my_bucket”

• No S3 Bucket Policy attached

-- In this case, can the EC2 instance write and read to the bucket?

ANS : • => EC2 instance can read and write to “my_bucket”

      - Because the union of these two things, is that our EC2 instance has the right to read and write to my bucket.




----------------------------------------------------- Example 2


• IAM Role attached to EC2 instance, authorizes RW to “my_bucket”

• S3 Bucket Policy attached, explicit deny to the IAM Role

- In this case, is the EC2 instance allowed or denied to write to the bucket?

ANS : • => EC2 instance cannot read and write to “my_bucket”

      - we've seen that explicit denies have higher priority than explicit allow.





----------------------------------------------------- Example 3


• IAM Role attached to EC2 instance, no S3 bucket permissions

• S3 Bucket Policy attached, explicit RW allow to the IAM Role

-- In this case, can our EC2 instance read and write to our bucket?

ANS : • => EC2 instance can read and write to “my_bucket”

      - because the union policy, again, has the read write allow from the S3 bucket policy

      - so our EC2 instance, even though within its IAM role it's not specified that it can do stuff in S3,

      - the S3 bucket policy will still allow this and so our EC2 instance can read and write to my bucket.



 


----------------------------------------------------- Example 4


• IAM Role attached to EC2 instance, explicit deny S3 bucket permissions

• S3 Bucket Policy attached, explicit RW allow to the IAM Role

• => EC2 instance cannot read and write to “my_bucket”




----------------------------------------------------- Dynamic Policies with IAM


• How do you assign each user a /home/<user> folder in an S3 bucket?


• Option 1:

      • Create an IAM policy allowing georges to have access to /home/georges

      • Create an IAM policy allowing sarah to have access to /home/sarah

      • Create an IAM policy allowing matt to have access to /home/matt

      • ... One policy per user!

      - every time you add a new user, you will have to create an IAM policy allowing them access to their own home directory.

      • This doesn’t scale


• Option 2:

      • Create one dynamic policy with IAM

      • Leverage the special policy variable ${aws:username}






----------------------------------------------------- Inline vs Managed Policies


• AWS Managed Policy

     • Maintained by AWS

     • Good for power users and administrators

     • Updated in case of new services / new APIs


• Customer Managed Policy (u create policies)

     • Best Practice, re-usable, can be applied to many principals

     • Version Controlled + rollback, central change management


• Inline     

     • Strict one-to-one relationship between policy and principal

     • Policy is deleted if you delete the IAM principal

     - They're not version control, you cannot roll them back,









----------------------------------------------------- Granting a User Permissions to Pass a Role to an AWS Service


• To configure many AWS services, you must pass an IAM role to the service (this happens only once during setup)

• The service will later assume the role and perform actions

• Example of passing a role:

    • To an EC2 instance

    • To a Lambda function

    • To an ECS task

    • To CodePipeline to allow it to invoke other services


• For this, you need the IAM permission iam:PassRole    

• It often comes with iam:GetRole to view the role being passed






----------------------------------------------------- IAM PassRole example



{
    "Version": "2012-10-17",
    "Statement": [{
        "Effect": "Allow",
        "Action": [
            "iam:GetRole",
            "iam:PassRole"
        ],
        "Resource": "arn:aws:iam::account-id:role/EC2-roles-for-XYZ-*"
    }]
}



----- Can a role be passed to any service?

ANS : NO  Roles can only be passed to what their trust allows

• A trust policy for the role that allows the service to assume the role


{
    "Version": "2012-10-17",
    "Statement": {
        "Sid": "TrustPolicyStatementThatAllowsEC2ServiceToAssumeTheAttachedRole",
        "Effect": "Allow",
        "Principal": { "Service": "ec2.amazonaws.com" },
       "Action": "sts:AssumeRole"
    }
}                    





----------------------------------------------------- AWS Directory Services --------------


What is Microsoft Active Directory (AD)?


ANS : it is a software that is found on any windows server with AD domain services.


• Found on any Windows Server with AD Domain Services

• Database of objects: User Accounts, Computers, Printers, File Shares, Security Groups

• Centralized security management, create account, assign permissions

• Objects are organized in trees

• A group of trees is a forest





----------------------------------------------------- AWS Directory Services




1 • AWS Managed Microsoft AD

   • Create your own AD in AWS, manage users locally, supports MFA

   • Establish “trust” connections with your on- premise AD



2 • AD Connector

    • Directory Gateway (proxy) to redirect to on-premise AD, supports MFA

    • Users are managed on the on-premise AD


3 • Simple AD

    • AD-compatible managed directory on AWS

    • Cannot be joined with on-premise AD





-- look console for more info 










======================================================= AWS Security & Encryption (KMS, Encryption SDK, SSM Parameter Store) ==============================================



----------------------------------------------- Why encryption? 



1 Encryption in flight (TLS / SSL)



• Data is encrypted before sending and decrypted after receiving

      - So this is for communication between a client and a server over a network.

• TLS certificates help with encryption (HTTPS)

      - that means that the connection between you and the server is going to be encrypted using TLS certificates.


Why do we want encryption in flight?

ANS : Well, because we're sending data over a network and sometimes a public network and the data goes around many different servers.

- We don't want to be having a man in the middle attacks where a middle server receives the data



• Encryption in flight ensures no MITM (man in the middle attack) can happen





2 Server-side encryption at rest


• Data is encrypted after being received by the server

• Data is decrypted before being sent

• It is stored in an encrypted form thanks to a key (usually a data key)

• The encryption / decryption keys must be managed somewhere, and the server must have access to it

- As we can see in this case, there is a server side encryption because all the encryption and decryption happens on the server.



3 Client-side encryption


• Data is encrypted by the client and never decrypted by the server

• Data will be decrypted by a receiving client

• The server should not be able to decrypt the data

• Could leverage Envelope Encryption

- With Client-Side Encryption, the server doesn't need to know any information about the encryption scheme being used, as the server will not perform any encryption or decryption operations.






---------------------------------------------------------- AWS KMS (Key Management Service)


- Limit is 4KB 


• Anytime you hear “encryption” for an AWS service, it’s most likely KMS

• AWS manages encryption keys for us

• Fully integrated with IAM for authorization

• Easy way to control access to your data

• Able to audit KMS Key usage using CloudTrail

• Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM...)

• Never ever store your secrets in plaintext, especially in your code!

     • KMS Key Encryption also available through API calls (SDK, CLI)

     • Encrypted secrets can be stored in the code / environment variables





---------------------------------------------------------- KMS Keys Types


• KMS Keys is the new name of KMS Customer Master Key



1 • Symmetric (AES-256 keys)


        • Single encryption key that is used to Encrypt and Decrypt

        • AWS services that are integrated with KMS use Symmetric CMKs

        • You never get access to the KMS Key unencrypted (must call KMS API to use)



2 • Asymmetric (RSA & ECC key pairs)

        • Public (Encrypt) and Private Key (Decrypt) pair

        • Used for Encrypt/Decrypt, or Sign/Verify operations

        • The public key is downloadable, but you can’t access the Private Key unencrypted

        • Use case: encryption outside of AWS by users who can’t call the KMS API







---------------------------------------------------------- AWS KMS (Key Management Service)


• Types of KMS Keys:

        • AWS Owned Keys (free): SSE-S3, SSE-SQS, SSE-DDB (default key)

        • AWS Managed Key: free (aws/service-name, example: aws/rds or aws/ebs) , but only from within the service that it's assigned to.

        • Customer managed keys created in KMS: $1 / month

        • Customer managed keys imported (must be symmetric key): $1 / month

        • + pay for API call to KMS ($0.03 / 10000 calls)




• Automatic Key rotation:

        • AWS-managed KMS Key: automatic every 1 year

        • Customer-managed KMS Key: (must be enabled) automatic every 1 year

        • Imported KMS Key: only manual rotation possible using alias







---------------------------------------------------------- Copying Snapshots across regions



-- That means that if we have an EBS volume encrypted with KMS key in a region, for example, eu-west-2,

-- then if you want to copy that to a different region we have to do several steps.

-- First of all, we have to take a snapshot of this EBS volume. And if we take a snapshot from an encrypted snapshot then this snapshot itself will also be encrypted with the same KMS key.

-- Then, to copy the snapshot to another region, we need to re-encrypt the snapshot using a different KMS key. And this is something AWS will do for you.

-- But the same KMS key cannot live in two regions. So now we have an EBS snapshot. It's encrypted with KMS with a different key and it lives in another region.

-- Now we restore the snapshot into its own EBS volume with KMS,  and it's KMS key B into the region ap-southeast-2.





---------------------------------------------------------- KMS Key Policies


• Control access to KMS keys, “similar” to S3 bucket policies

• Difference: you cannot control access without them

       - if you don't have a KMS key policy on your KMS key, then no one can access it.



1 • Default KMS Key Policy:

       • Created if you don’t provide a specific KMS Key Policy , the idea is that the default allows everyone in your account to access this key.

       • Complete access to the key to the root user = entire AWS account



2 • Custom KMS Key Policy:

       • Define users, roles that can access the KMS key

       • Define who can administer the key

       • Useful for cross-account access of your KMS key







---------------------------------------------------------- Copying Snapshots across accounts


1. Create a Snapshot, encr ypted with your own KMS Key (Customer Managed Key)

2. Attach a KMS Key Policy to authorize cross-account access

3. Share the encr ypted snapshot with the target accounts.

4. (in target) Create a copy of the Snapshot, encrypt it with a CMK in your account

5. Create a volume from the snapshot








---------------------------------------------------------- KMS Hands ON



-- open KMS in console --> customer - managed Keys --->  Symmetric ----> Encrypt and decrypt ---> KMS - recommended ----> Single-region key --> alias = give anyname ---> nxt ---> nxt ---> create 

-- now do some data encrypt and decrypt 

-- open cloudshell create one folder like kms 

-- cd kms --> create onefile "ExampleSecretFile.txt" and enter some secrets 

-- u have to cmnds 



# 1) encryption
aws kms encrypt --key-id alias/tutorial --plaintext fileb://ExampleSecretFile.txt --output text --query CiphertextBlob  --region ap-south-1 > ExampleSecretFileEncrypted.base64

# base64 decode for Linux or Mac OS 
cat ExampleSecretFileEncrypted.base64 | base64 --decode > ExampleSecretFileEncrypted

# base64 decode for Windows
certutil -decode .\ExampleSecretFileEncrypted.base64 .\ExampleSecretFileEncrypted


# 2) decryption

aws kms decrypt --ciphertext-blob fileb://ExampleSecretFileEncrypted   --output text --query Plaintext > ExampleFileDecrypted.base64  --region ap-south-1

# base64 decode for Linux or Mac OS 
cat ExampleFileDecrypted.base64 | base64 --decode > ExampleFileDecrypted.txt


# base64 decode for Windows
certutil -decode .\ExampleFileDecrypted.base64 .\ExampleFileDecrypted.txt



-- give cmnds without any gaps at the starting point 

--  now perform 1st cmnd , this will create one file , ExampleSecretFileEncrypted.base64 , do cat this , this represents my encrypted file.

        aws kms encrypt --key-id alias/tutorial --plaintext fileb://ExampleSecretFile.txt --output text --query CiphertextBlob  --region ap-south-1 > ExampleSecretFileEncrypted.base64


-- now decode 

       cat ExampleSecretFileEncrypted.base64 | base64 --decode > ExampleSecretFileEncrypted

     - this will create "ExampleSecretFileEncrypted " this file 

     - So this is the kind of secret file that you would share with someone.


-- now decrypt the file 

      aws kms decrypt --ciphertext-blob fileb://ExampleSecretFileEncrypted   --output text --query Plaintext > ExampleFileDecrypted.base64  --region ap-south-1


      - this will create "ExampleFileDecrypted.base64" 

      - cat ExampleFileDecrypted.base64


-- now decode 

      cat ExampleFileDecrypted.base64 | base64 --decode > ExampleFileDecrypted.txt


      - now do cat ExampleFileDecrypted.txt , u will see the secret 







---------------------------------------------------------- Envelope Encryption


• KMS Encrypt API call has a limit of 4 KB

• If you want to encrypt >4 KB, we need to use Envelope Encryption

• The main API that will help us is the GenerateDataKey API



• For the exam: anything over 4 KB of data that needs to be encrypted must use the Envelope Encryption == GenerateDataKey API






---------------------------------------------------------- Encryption SDK


• The AWS Encryption SDK implemented Envelope Encryption for us

• The Encryption SDK also exists as a CLI tool we can install

• Implementations for Java, Python, C, JavaScript


- this encryption SDK has a feature called data key caching.



• Feature - Data Key Caching:

         • re-use data keys instead of creating new ones for each encryption

         • Helps with reducing the number of calls to KMS with a security trade-off

         • Use LocalCryptoMaterialsCache (max age, max bytes, max number of messages)






---------------------------------------------------------- KMS Symmetric – API Summary


• Encrypt: encrypt up to 4 KB of data through KMS

• GenerateDataKey: generates a unique symmetric data key (DEK)

- this API will do two things,

      • returns a plaintext copy of the data key

      • AND a copy that is encrypted under the CMK that you specify



• GenerateDataKeyWithoutPlaintext:

      • Generate a DEK to use at some point (not immediately)

      • DEK that is encrypted under the CMK that you specify (must use Decrypt later)

      - So the exam will try to trick you to perform envelope encryption right now, you need to use GenerateDataKey API not GenerateDataKeyWithoutPlaintext.



• Decrypt: decrypt up to 4 KB of data (including Data Encryption Keys)


• GenerateRandom: Returns a random byte string







---------------------------------------------------------- Encryption SDK CLI Hands ON


https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/crypto-cli-install.html


-- open cloudshell , 

- sudo pip install aws-encryption-sdk-cli

- aws-encryption-cli --version

- now do key="paste ARN of ur Customer key"

          EG : key="arn:aws:kms:ap-south-1:298132369629:key/268efdd6-1429-4f97-93e1-f278df0de909"


- create one file like hello.txt

             vi hello.txt , add some secrets 


- Here it contains some very super secret data, what we want to do is encrypt this,

      sudo aws-encryption-cli --encrypt \
                     --input hello.txt \
                     --wrapping-keys key=$key \
                     --metadata-output /metadata \
                     --output output/


- now do ls and if u go to output folder u will get hello.txt.encrypted file 

- do cat hello.txt.encrypted 

- our file was being encrypted using this KMS key and the generated data key, so it allows the Encryption SDK to decrypt that file in the future.

- now do decrypt this file using SDK 

- cd ..

- mkdir decrypted 

- enter this cmnd 

      
      sudo aws-encryption-cli --decrypt \
                     --input output/hello.txt.encrypted \
                     --wrapping-keys key=$key \
                     --metadata-output /metadata \
                     --output decrypted/

- ls

- ls decrypted/ , u will find "hello.txt.encrypted.decrypted"

- cat decrypted/hello.txt.encrypted.decrypted

- if u cat this , u will get the secret 






---------------------------------------------------------- KMS Request Quotas


• When you exceed a request quota, you get a ThrottlingException:

• To respond, use exponential backoff (backoff and retry)

• For cryptographic operations, they share a quota

• This includes requests made by AWS on your behalf (ex: SSE-KMS)

- So that means that if we use our key too much, then we will get a ThrottlingException.

- So what can we do to solve it?

      1 if we're using the GenerateDataKey API then we can use DEK cachings. So cache the data encryption key locally , so that we reduce the number of API calls done onto AWS and that is a feature of the encryption ID kit itself.

      2 The other thing we can do is to generate a request quota increase in case we are indeed going over that limit way too many times. so, for this, we can request a quota increase through either an API call or by opening a ticket with the AWS support.








---------------------------------------------------------- KMS and Lambda Practice 


-- open lambda in condole and use py 3.8 and create one function 

import json
import os

db_password = os.getenv("DB_PASSWORD")

def lambda_handler(event, context):

    return "great"


-- So here we're going to play with how we can encrypt environment variables within Lambda.

-- for this, we'll be using the tutorial key that we have generated from before in KMS,

-- go to configuraation --> environmentvariables --> create environmentvariables ( DB_PASSWORD = Subbu)

-- go to configuraation --> environmentvariables --> edit --> Enable helpers for encryption in transit ---> use customer key ---> c.o encrypt ---> select customer key --> encrypt

-- if u see the secret value it get encrypted 

-- now c.o encrypt beside secret value -----> Decrypt secrets snippet ----> u will get code 


import boto3
import os

from base64 import b64decode

ENCRYPTED = os.environ['DB_PASSWORD']
# Decrypt code should run once and variables stored outside of the function
# handler so that these are decrypted once per container
DECRYPTED = boto3.client('kms').decrypt(
    CiphertextBlob=b64decode(ENCRYPTED),
    EncryptionContext={'LambdaFunctionName': os.environ['AWS_LAMBDA_FUNCTION_NAME']}
)['Plaintext'].decode('utf-8')

def lambda_handler(event, context):
    print(ENCRYPTED)
    print(DECRYPTED)
    return "great"


-- do deploy and test  , u will get run time error so go to configuration ---> general configuration ---> increate time to execute 

-- now u will get AccessDeniedException  ,'coz  Our Lambda function is trying to run a decrypt call on our KMS environment variable 

-- That we have in the encrypted one but we haven't provided the, IAM role with a right to decrypt, anything using that KMS key.

-- to fix this issue --> configuration --> permission ---> create inline policy for kms --> give all permissions ---> all resources ---> create policy

-- if u test , u will get o/p 

-- if u look at START RequestId: this is the encrypted environment variable that was passed to my Lambda function as an encrypted secrets.

-- So using this method we have a code that doesn't show any information about the environment variable or the password itself.

-- then if we go into the configuration and look at the environment variables anyone who doesn't have access to the KMS key right here will not be able to decrypt this.








---------------------------------------------------------- S3 Bucket Key for SSE-KMS encryption


• New setting to decrease...

      • Number of API calls made to KMS from S3 by 99%

      • Costs of overall KMS encryption with Amazon S3 by 99%


• This leverages data keys

      • A “S3 bucket key” is generated , So how does that work?

      - Well, a customer master key and KMS is going to be used to generate a data key for your Amazon S3 Bucket once in a while and this key will rotates.

      - this key is what's going to be used to encrypt objects in your Amazon S3 Buckets with KMS encryption.


• That key is used to encrypt KMS objects with new data keys

     - So this extra bucket key is going to generate a lot of data keys using envelope encryption which is going to go into encrypting your S3 Buckets.

     - by adding an S3 Bucket key and not using KMS directly to generate these data keys,

     - we are reducing the number of API calls we are making into KMS. And therefore, we are reducing our costs by a lot.



• You will see less KMS CloudTrail events in CloudTrail


--  to enable this in console ---> s3 bucket --> create bucket ---> default encryption (enable) ---> enable bucket key







---------------------------------------------------------- Key Policy – Examples




{
  "Sid": "Enable IAM User Permissions",
  "Effect": "Allow",
  "Principal": {
    "AWS": "arn:aws:iam::111122223333:root"
   },
  "Action": "kms:*",
  "Resource": "*"
}


-- So we know that a Key Policy is used to define who has access to your KMS Key. 

-- And the default KMS Key Policy that you create through the AWS Console allows anyone within your account to access your KMS Key, as long as they have the proper IAM permissions.




------ key policy for federator user 



-- if you wanted to explicitly authorize a specific user, it could be whatever user, it could be a user, an IAM role, it could be a federated user,

-- allow which KMS actions you want, such as encrypt, decrypt, list, describe , And then you explicitly outline the principle.

-- In that case, the federated user in this example, does not need, additionally, an extra IAM policy to use your KMS key

-- because it's been explicitly allowed in the KMS key policy.

-- 


           
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::111122223333:federator-user/user-name"
            },
            "Action": [
                "kms:Create*",
                "kms:Describe*",
                "kms:Enable*",
                "kms:List*",
                "kms:Put*",
                "kms:Update*",
                "kms:Revoke*",
                "kms:Disable*",
                "kms:Get*",
                "kms:Delete*",
                "kms:TagResource",
                "kms:UntagResource",
                "kms:ScheduleKeyDeletion",
                "kms:CancelKeyDeletion",
                "kms:RotateKeyOnDemand"
            ],
            "Resource": "*"
      






---------------------------------------------------------- Principal Options in IAM Policies


-- So what kind of principles can we explicitly allow in KMS key policies, but also in anything IAM?


• AWS Account and Root User

        
       - "Principal": {  "AWS": "1234456778" }

       - "Principal": {  "AWS": "arn:aws:iam::111122223333:root" }
      
       - So when you define something like this, such as principle AWS and the new account number, or the account number, and root

       - i.e you allow every principle within the accounts,



• IAM Roles

       - "Principal": {  "AWS": "arn:aws:iam::111122223333:role/rolename" }

       - you can authorize a specific IAM role by outlining the role arn directly in the principle statements.




• IAM Role Sessions

       - "Principal": {  "AWS": "arn:aws:iam::111122223333:role/rolename/role-session-name" }

       - "Principal": {  "Federated": "arn:aws:iam::111122223333:saml-provider/provider-name" }

       - "Principal": {  "Federated": "cognito-identity.amazonaws.com" }




• IAM Users 

       - "Principal": {  "AWS": "arn:aws:iam::111122223333:user/user-name" }



• Federated User Sessions

       - "Principal": {  "AWS": "arn:aws:iam::111122223333:Fedareated-user/user-name" }








---------------------------------------------------------- CloudHSM


• KMS => AWS manages the software for encryption , and will have control over the encryption keys.

• CloudHSM => AWS provisions encryption hardware , AWS will provision some encryption hardware.

              - It's called an HSM device, so a dedicated hardware which is a hardware security module.

• Dedicated Hardware (HSM = Hardware Security Module)

• You manage your own encryption keys entirely (not AWS)


- The HSM device is going to be set up within the cloud of AWS,

• HSM device is tamper resistant, FIPS 140-2 Level 3 compliance

• Supports both symmetric and asymmetric encryption (SSL/TLS keys)

• No free tier available

• Must use the CloudHSM Client Software

• Redshift supports CloudHSM for database encryption and key management

• Good option to use with SSE-C encryption


- IAM permissions:

       • CRUD an HSM Cluster

- CloudHSM Software:

       • Manage the Keys
       • Manage the Users






---------------------------------------------------------- CloudHSM High Availability


• CloudHSM clusters are spread across Multi AZ (HA)

• Great for availability and durability
 



---------------------------------------------------------- CloudHSM – Integration with AWS Services


• Through integration with AWS KMS

• Configure KMS Custom Key Store with CloudHSM

• Example: EBS, S3, RDS ...

        
       




---------------------------------------------------------- SSM Parameter Store -------------------------------- 



• Secure storage for configuration and secrets

• Optional Seamless Encryption using KMS

• Serverless, scalable, durable, easy SDK

• Version tracking of configurations / secrets

• Security through IAM

• Notifications with Amazon EventBridge

• Integration with CloudFormation





---------------------------------------------------------- SSM Parameter Store Hierarchy


• /my-department/
    • my-app/
        • dev/
            • db-url
            • db-password

        • prod/
            • db-url
            • db-password

• other-app/



- You also have the opportunity to access Secrets of Secrets Manager through the Parameter Store by using this reference right here.

        • /aws/reference/secretsmanager/secret_ID_in_Secrets_Manager


- there are something called Public Parameters that are issued by AWS that you can use.

        • /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 (public)
           







---------------------------------------------------------- SSM Parameter Standard and advanced parameter tiers




Standard :

- Total number of parameters allowed (per AWS account and Region) :  10,000

- Maximum size of a parameter value                               :  4 KB

- Parameter policies available                                    :  No

- Cost                                                            :  No additional charge

- Storage Pricing                                                 :  Free






Advanced :

- Total number of parameters allowed (per AWS account and Region) :  100,000

- Maximum size of a parameter value                               :  8 KB

- Parameter policies available                                    : yes

- Cost                                                            :  charges Apply

- Storage Pricing                                                 :  $0.05 per advanced parameter per month






---------------------------------------------------------- Parameters Policies (for advanced parameters)



• Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as passwords

• Can assign multiple policies at a time




1 Expiration (to delete a parameter) : 

{
    "Type": "Expiration",
    "Version": "1.0",
    "Attributes": {
        "Timestamp": "2018-12-02T21:34:33.000Z"
    }
}





2 ExpirationNotification (EventBridge)



{
    "Type": "ExpirationNotification",
    "Version": "1.0",
    "Attributes": {
        "Before": "15",
        "Unit": "Days"
    }
}


-- So in this example, 15 days before the parameter expires we'll receive notification in EventBridge which gives us enough time to actually update it

-- and make sure the parameter is not getting deleted because of the TTL.





3 NoChangeNotification (EventBridge)


{
    "Type": "NoChangeNotification",
    "Version": "1.0",
    "Attributes": {
        "After": "20",
        "Unit": "Days"
    }
}


-- maybe sometimes you wanna make sure the parameters change once in a while.

-- So you can have a no change notification in EventBridge so that if a parameter has not been updated for 20 days, then you will be notified as well.








---------------------------------------------------------- SSM Parameter Store Hands ON CLI



-- open ssm in console --> choose parameter store on left side panel ---> give path to store value (/my-app/dev/db-url) ---> string --> value = dev.database.subbu.com:3306 --> create parameter

-- dev.database.subbu.com:3306 = u can give any value 

-- now do create dev password
  
-- give path to store value (/my-app/dev/db-password) ---> securestring --> value = give password here --> KMS Key ID = i am using my own key (eg: tutorial) -----> create parameter       

-- now do create for prod environment also same like as Dev

EG : /my-app/prod/db-url , /my-app/prod/db-password


-- So we are going to use this CLI to get the parameters.

-- open cloudshell

       aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password


-- for the password it's a SecureString, and here is the value of it, which is an encrypted value.

-- So for this, you basically need to decrypt it.

-- for this you have a special parameter and it's called with-decryption,

-- so this will check whether or not I have the KMS permission to decrypt this secret that was encrypted with the KMS tutorial key.

         aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password --with-decryption


-- now observe changes 


  aws ssm get-parameters-by-path --path /my-app/dev     =  u will get all parameters from specific path if u want 

   aws ssm get-parameters-by-path --path /my-app/ --recursive  --with-decryption     = u will get all parameters under /my-app/ 









---------------------------------------------------------- SSM Parameter Store Hands ON with LAMBDA 


-- create one function with py 3.8 runtime 




import json

import boto3

ssm = boto3.client('ssm', region_name="ap-south-1")

def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"])
    print(db_password)
    return "Worked!"



-- now go to configuration ---> permissons --->  create inline policy --> system manager ---> give all access --> all resources --> nxt 

-- now do refresht he lambda page 

-- if u get errror , after adding permissions , then wait for 5 min 

-- now u will get this

-- u can see 'SecureString' is encrypted here 

-- So what we'd like to do is now decrypt it,

-- so in code for password decrypt , add 

        db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"], WithDecryption = True)


-- now do test , u will get (AccessDeniedException) error 

-- because we're not allowed to use the customer master key and decrypt our secrets.

-- So it turns out that because having given KMS access to my IAM role we're not allowed to decrypt the secrets,

-- so this is a good proof that even though I have access to this database password, because it's encrypted and I don't have access to KMS I'm not able to decrypt it,

-- and so that DB password is really safe and secure.

-- to fix this add permissions 

     permissons --> create inline policy --> kms --> add all permissions --> all resources--> create 


-- now do test , u will get decrypted values 


-- now to access through the Environment Variables 

-- create  Environment Variable --> DEV_OR_PROD	 = dev

-- add this in code 



import json

import boto3
import os 

ssm = boto3.client('ssm', region_name="ap-south-1")
dev_or_prod = os.environ['DEV_OR_PROD']


def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-password"], WithDecryption = True)
    print(db_password)
    return "Worked!"




-- if u test this u will get dev values 

--  go to env variable , change to prod (DEV_OR_PROD	 = prod )

--  do test again , u will prod values 

        










---------------------------------------------------------- AWS Secrets Manager ------------------------



• Newer service, meant for storing secrets

• Capability to force rotation of secrets every X days

• Automate generation of secrets on rotation (uses Lambda)

• Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)

• Secrets are encrypted using KMS

• Mostly meant for RDS integration






---------------------------------------------------------- AWS Secrets Manager – Multi-Region Secrets


• Replicate Secrets across multiple AWS Regions

• Secrets Manager keeps read replicas in sync with the primary Secret

• Ability to promote a read replica Secret to a standalone Secret

• Use cases: multi-region apps, disaster recovery strategies, multi-region DB...






---------------------------------------------------------- AWS Secrets Manager Hands ON


-- open secret manager --> store a new secret ---> Other type of secret ---> give key and values ----> select ur KMS key or default ----> Secret name = prod/my-key-api --> create 









---------------------------------------------------------- SSM Parameter Store vs Secrets Manager




• Secrets Manager ($$$): 

       • Automatic rotation of secrets with AWS Lambda

       • Lambda function is provided for RDS, Redshift, DocumentDB

       • KMS encryption is mandatory

       • Can integration with CloudFormation


• SSM Parameter Store ($):

       • Simple API

       • No secret rotation (can enable rotation using Lambda triggered by EventBridge)

       • KMS encryption is optional

       • Can integration with CloudFormation

       • Can pull a Secrets Manager secret using the SSM Parameter Store API







---------------------------------------------------------- CloudFormation – Dynamic References


• Reference external values stored in Systems Manager Parameter Store and Secrets Manager within CloudFormation templates

• CloudFormation retrieves the value of the specified reference during create/update/delete operations

• For example: retrieve RDS DB Instance master password from Secrets Manager

• Supports , 3 kinds 

     • ssm – for plaintext values stored in SSM Parameter Store

     • ssm-secure – for secure strings stored in SSM Parameter Store

     • secretsmanager – for secret values stored in Secrets Manager




syntax : ‘{{resolve:service-name:reference-key}}’





1 SSM parameters


  MyS3Bucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      AccessControl: '{{resolve:ssm:S3AccessControl:2}}' 




2 SSM secure string parameters


  MyIAMUser:
    Type: AWS::IAM::User
    Properties:
      UserName: 'MyUserName'
      LoginProfile:
        Password: '{{resolve:ssm-secure:IAMUserPassword:10}}'






3 Secrets Manager secrets



  MyRDSInstance:
    Type: 'AWS::RDS::DBInstance'
    Properties:
      DBName: MyRDSInstance
      AllocatedStorage: '20'
      DBInstanceClass: db.t2.micro
      Engine: mysql
      MasterUsername: '{{resolve:secretsmanager:MyRDSSecret:SecretString:username}}'
      MasterUserPassword: '{{resolve:secretsmanager:MyRDSSecret:SecretString:password}}'







---------------------------------------------------------- CloudFormation, Secrets Manager & RDS Option 1 – ManageMasterUserPassword



• ManageMasterUserPassword – creates admin secret implicitly

• RDS, Aurora will manage the secret in Secrets Manager and its rotation




---------------------------------------------------------- CloudFormation, Secrets Manager & RDS Option 2 – Dynamic Reference


1. secret is generated

2. Reference secret in RDS DB instance

3. link the secret to RDS DB instance (for rotation)





---------------------------------------------------------- CloudWatch Logs - Encryption



• You can encrypt CloudWatch logs with KMS keys

• Encryption is enabled at the log group level, by associating a CMK with a log group, either when you create the log group or after it exists.

• You cannot associate a CMK with a log group using the CloudWatch console.

- you have to use the CloudWatch Logs API for the CLI and the SDK, 


• You must use the CloudWatch Logs API:

         • associate-kms-key : if the log group already exists

         • create-log-group: if the log group doesn’t exist yet





---------------------------------------------------------- CloudWatch Logs - Encryption Hands ON


-- go to cloud watch and select any log group which does not have KMS KEY ID

-- so here we are using our own key (tutorial) and do associate with this log group 

-- this will donw through the CLI only , u can't do this through the console 




# associate with existing log group
aws logs associate-kms-key --log-group-name /aws/lambda/hello --kms-key-id arn:aws:kms:ap-south-1:298132369629:key/268efdd6-1429-4f97-93e1-f278df0de909 --region ap-south-1

# create new log group
aws logs create-log-group --log-group-name /example-encrypted --kms-key-id arn:aws:kms:eu-west-2:387124123361:key/0509dc31-00a4-4ef6-a739-3d77b2e011f5 --region eu-west-2



-- u have 2 options u can use anyone here , i am using existing log group 

-- open cloushell 

       aws logs associate-kms-key --log-group-name /aws/lambda/hello --kms-key-id arn:aws:kms:ap-south-1:298132369629:key/268efdd6-1429-4f97-93e1-f278df0de909 --region ap-south-1


-- once u apply this cmnd u will accessdeniederrror 

-- So, we need to apply a certain key policy.



 {
      "Effect": "Allow",
      "Principal": { "Service": "logs.region.amazonaws.com" },
      "Action": [ 
        "kms:Encrypt*",
        "kms:Decrypt*",
        "kms:ReEncrypt*",
        "kms:GenerateDataKey*",
        "kms:Describe*"
      ],
      "Resource": "*"
    }  



-- go to kms ---> key policy ---> switch to view key policy --> edit 




{
    "Id": "key-consolepolicy-3",
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Enable IAM User Permissions",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::298132369629:root"
            },
            "Action": "kms:*",
            "Resource": "*"
        },
         {
      "Effect": "Allow",
      "Principal": { "Service": "logs.region.amazonaws.com" },
      "Action": [ 
        "kms:Encrypt*",
        "kms:Decrypt*",
        "kms:ReEncrypt*",
        "kms:GenerateDataKey*",
        "kms:Describe*"
      ],
      "Resource": "*"
    }  
    ]
}




--  the overall policy will look like this , in region u have to add ur region 

-- save and try again the command 

-- now go to c.w and do refresh , u will get KMS ID for the log group 

--  so that mean that this log group is going to be fully encrypted with this KMS key ID.




-- now do with the 2nd cmnd to create with new log group , jst change ur values in cmnd n paste in cli 

     


 





---------------------------------------------------------- CodeBuild Security



-- So first of all, we know that CodeBuild is out of your VPC, but you can launch your CodeBuild inside of your VPC to access your VPC resources.

• To access resources in your VPC, make sure you specify a VPC configuration for your CodeBuild


• Secrets in CodeBuild:

• Don’t store them as plaintext in environment variables

• Instead...

       • Environment variables can reference parameter store parameters

       • Environment variables can reference secrets manager secrets






---------------------------------------------------------- AWS Nitro Enclaves


•  sometimes in the cloud you wanna Process highly sensitive data in an isolated compute environment

        • Personally Identifiable Information (PII), healthcare, financial, ...


-- historically, if you wanted to create this very isolated compute environment, 

-- you would create a new VPC, you would restrict access to it, 

-- you would restrict the networking, and so on, and that would be cumbersome.        


-- So instead, what you can use is Nitro Enclaves.

      • Fully isolated virtual machines, hardened, and highly constrained

             • Not a container, not persistent storage, no interactive access, no external networking


• Helps reduce the attack surface for sensitive data processing apps

     • Cryptographic Attestation – only authorized code can be running in your Enclave

     • Only Enclaves can access sensitive data (integration with KMS)


• Use cases: securing private keys, processing credit cards, secure multi-party computation...







AWS Nitro Enclave 


1. Launch a compatible Nitro-based EC2 instance with the ‘EnclaveOptions’ parameter set to ‘true’

2. Use the Nitro CLI to convert your app to an Enclave Image File (EIF)

3. Using the EIF file as an input, use the Nitro CLI to create an Enclave

4. The Enclave is a separate virtual machine with its own kernel, memory, and CPU









=============================================================== Other Services ==============================






---------------------------------------------------------- Amazon Simple Email Service (Amazon SES)



• Fully managed service to send emails securely, globally and at scale

• Allows inbound/outbound emails

• Reputation dashboard, performance insights, anti-spam feedback

• Provides statistics such as email deliveries, bounces, feedback loop results, email open

• Supports DomainKeys Identified Mail (DKIM) and Sender Policy Framework (SPF)

• Flexible IP deployment: shared, dedicated, and customer-owned IPs

• Send emails using your application using AWS Console, APIs, or SMTP

• Use cases: transactional, marketing and bulk email communications








---------------------------------------------------------- Amazon OpenSearch Service


• Amazon OpenSearch is successor to Amazon ElasticSearch

• In DynamoDB, queries only exist by primary key or indexes...

• With OpenSearch, you can search any field, even par tially matches

• It’s common to use OpenSearch as a complement to another database

• Two modes: managed cluster or serverless cluster

• Does not natively support SQL (can be enabled via a plugin)

• Ingestion from Kinesis Data Firehose, AWS IoT, and CloudWatch Logs

• Security through Cognito & IAM, KMS encryption,TLS

• Comes with OpenSearch Dashboards (visualization)








---------------------------------------------------------- Amazon Athena


• Serverless query service to analyze data stored in Amazon S3

• Uses standard SQL language to query the files (built on Presto)

• Supports CSV, JSON, ORC, Avro,and Parquet

• Pricing: $5.00 per TB of data scanned

• Commonly used with Amazon Quicksight for repor ting/dashboards


• Use cases: Business intelligence / analytics / reporting, analyze & queryVPC Flow Logs,ELB Logs,CloudTrail trails,etc...

• Exam Tip: analyze data in S3 using serverless SQL, use Athena







---------------------------------------------------------- Amazon Athena – Performance Improvement


• Use columnar data for cost-savings (less scan)
 
         • Apache Parquet or  ORC (Optimized Row Columnar)  is recommended 

         • Huge performance improvement

         • Use Glue to convert your data to Parquet or ORC


• Compress data for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd...)         

• Partition datasets in S3 for easy querying on virtual columns

      • s3://yourBucket/pathToTable

                /<PARTITION_COLUMN_NAME>=<VALUE>
                /<PARTITION_COLUMN_NAME>=<VALUE>
                /<PARTITION_COLUMN_NAME>=<VALUE>


• Example:s3://athena-examples/flight/parquet/year=1991/month=1/day=1/

• Use larger files (> 128 MB) to minimize overhead








---------------------------------------------------------- Amazon Athena – Federated Query



• Allows you to run SQL queries across data stored in relational, non-relational, object, and custom data sources (AWS or on-premises)


• Uses Data Source Connectors that run on AWS Lambda to run Federated Queries (e.g., CloudWatch Logs, DynamoDB, RDS, ...)

       - It's a Lambda function, and that Lambda function is going to run the Federated Queries in other services. So that could be, for example, CloudWatch Logs, DynamoDB, RDS, and so on.


• Store the results back in Amazon S3






---------------------------------------------------------- Athena Hands ON 



Step 1 : create s3 bucket and insert some .csv data 

--  insert some  data in the bucket 

--   create one folder inside bucket and go to google and search for sample .csv files downlaod and upload in the s3 bucket 

Step 2 : Glue

-- open glue in console

-- glue --> from left panel data Catalog --> crawlers --> add crawlers --> give name of ur crawler --> not yet --> add a data source --> choose s3 --> copy uri of ur bucket and paste in S3 path --> click on create new role , give name of ur role and select nxt --> click on create new database --> give name of ur database , nxt --> frequency = on-demand --> create crawler

-- now try to run the crawler , 

--  u can also check the logs in cloudwatch 

-- once u done with the running part , a table will added to the database , table name is same as the folder name of s3 that u have uploaded 

--  if ur data is not match , then u can also do edit ur schema 


step 3 : Athena

-- open Athena in the console

-- in the left panel u will see the database and table , u can check all the coloums of ur table

-- now do queries , before that u should add destination to store our outputs in the s3 bucket 

-- go to s3 buckets create one folder in the bucket to store the outputs

-- now try to query in athena editor

-- SELECT * FROM "weather-database"."weather_csv" limit 10;

above is eg query , u can replace with ur details 

weather-database = database name 

weather_csv = table name


NOTE : if u r getting zero records but query is successfull then do 

ur s3 location is like this 

s3://doc-example-bucket/table1.csv 

-- to get avoid from this error , jst create one sub-folder and upload .csv file in that subfolder 

s3://doc-example-bucket/table1/table1.csv



-- in the crawler location u have to give like this , then u won't get any error

s3://doc-example-bucket/table1/







---------------------------------------------------------- Amazon Managed Streaming for Apache Kafka,(Amazon MSK)


• Alternative to Amazon Kinesis , Kafka and Kinesis both allow you to stream data.

• Fully managed Apache Kafka on AWS

       • Allow you to create, update, delete clusters

       • MSK creates & manages Kafka brokers nodes & Zookeeper nodes for you

       • Deploy the MSK cluster in your VPC, multi-AZ (up to 3 for HA)

       • Automatic recovery from common Apache Kafka failures

       • Data is stored on EBS volumes for as long as you want


• MSK Serverless

      • Run Apache Kafka on MSK without managing the capacity

      • MSK automatically provisions resources and scales compute & storage






---------- So what is Apache Kafka then?


-- Apache Kafka is a way for you to stream data and a Kafka cluster is made of multiple brokers

-- then you will have producers that will produce data and so they will have to ingest data from places, such as Kinesis, IoT RDS, et cetera,

-- they will send the data directly into a Kafka topic that is going to be fully replicated into other brokers.

-- Now, this Kafka topic is having real-time streaming of data and consumers will pull from the topic to consume the data itself

-- then your consumer can do whatever he wants, process it or send it to various destinations, such as EMR, S3, SageMaker, Kinesis and RDS.

-- So the idea is that Kafka is quite similar to Kinesis, but there are differences to look 







---------------------------------------------------------- Kinesis Data Streams vs. Amazon MSK



Kinesis Data Streams

     • 1 MB message size limit

     • Data Streams with Shards

     • Shard Splitting & Merging

     • TLS In-flight encryption

     • KMS at-rest encryption



Amazon MSK

     • 1MB default, configure for higher (ex: 10MB)

     • KafkaTopics with Partitions

     • Can only add partitions to a topic

     • PLAINTEXT or TLS In-flight Encryption

     • KMS at-rest encryption











---------------------------------------------------------- AWS Certificate Manager (ACM)



• Let’s you easily provision, manage, and deploy SSL/TLS Certificates

• Used to provide in-flight encryption for websites (HTTPS)

• Supports both public and privateTLS certificates

• Free of charge for publicTLS certificates

• AutomaticTLS certificate renewal


• Integrations with (loadTLS certificates on)

     • Elastic Load Balancers
     • CloudFront Distributions
     • APIs on API Gateway







---------------------------------------------------------- AWS Private Certificate Authority (CA)


• Managed service allows you to create private Certificate Authorities (CA), including root and subordinaries CAs

• Can issue and deploy end-entity X.509 certificates

• Certificates are trusted only by your Organization (not the public Internet)

• Works for AWS services that are integrated with ACM

• Use cases:

     • Encrypted TLS communication, Cryptographically signingcode

     • Enterprise customers building a Public Key Infrastructure (PKI)

     • Authenticateusers , computers, APIendpoints, and IoTdevices






---------------------------------------------------------- AWS Macie



• Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.

• Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)






---------------------------------------------------------- AWS AppConfig 



• Configure, validate, and deploy dynamic configurations

• Deploy dynamic configuration changes to your applications independently of any code deployments

      • You don’t need to restart the application


• Feature flags, application tuning, allow/block listing...

• Use with apps on EC2 instances, Lambda, ECS, EKS...

• Gradually deploy the configuration changes and rollback if issues occur

• Validate configuration changes before deployment using:

     • JSON Schema (syntactic check) or

     • Lambda Function – run code to perform validation (semantic check)







---------------------------------------------------------- CloudWatch Evidently



• Safely validate new features by serving them to a specified % of your users

     • Reduce risk and identify unintended consequences

     • Collect experiment data, analyze using stats, monitor performance


• Launches (= feature flags): enable and disable features for a subset of users

• Experiments (= A/B testing): compare multiple versions of the same feature

• Overrides: pre-define a variation for a specific user

• Store evaluation events in CloudWatch Logs or S3





