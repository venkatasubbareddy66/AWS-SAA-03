

IMP NOTE : ALL the Topics have covered in the Solution architect Associate , here I have prepared the Most advanced topics for Associate Developer which was not covered in the solution architect associate



=========================================================== CloudFront =========================================


-------------------- CloudFront Caching 

• The cache lives at each CloudFront Edge Location

• CloudFront identifies each object in the cache using the Cache Key 

• You want to maximize the Cache Hit ratio to minimize requests to the origin

• You can invalidate part of the cache using the CreateInvalidation API


-------------------- What is CloudFront Cache Key?

• A unique identifier for every object in the cache

• By default, consists of hostname + resource portion of the URL

• If you have an application that serves up content that varies based on user, device, language, location...

• You can add other elements (HTTP headers, cookies, query strings) to the Cache Key using "CloudFront Cache Policies"


-------------------- CloudFront Policies – Cache Policy

• Cache based on:

    • HTTP Headers: None – Whitelist

    • Cookies: None – Whitelist – Include All-Except – All

    • Query Strings: None – Whitelist – Include All-Except – All


• Control the TTL (0 seconds to 1 year), can be set by the origin using the Cache-Control header, Expires header...

• Create your own policy or use Predefined Managed Policies

IMP : • All HTTP headers, cookies, and query strings that you include in the Cache Key are automatically included and forward to ur  origin requests



------------------ 1 CloudFront Caching – Cache Policy HTTP Headers

- let's say we have an example request in french language


GET /blogs/myblog.html HTTP/1.1
Host: mywebsite.com
User-Agent: Mozilla/5.0 (Mac OS X 10_15_2....) 
Date: Tue, 28 Jan 2021 17:01:57 GMT 
Authorization: SAPISIDHASH fdd00ecee39fe.... Keep-Alive: 300
Language: fr-fr


-- so if we define none : 

• None:
   
    • Don’t include any headers in the Cache Key (except default)
    • Headers are not forwarded (except default)
    • Best caching performance

• Whitelist: 

    • only specified headers included in the Cache Key
      
       -     if u want to whitelist specific Headers and that may mean necessary because while you want to have the language as a Cache Key, then you specify which headers you want to include in the Cache Key,

    • Specified headers are also forwarded to Origin

       - so that the origin can actually respond to the request and give you the blog in the correct language.




---------------------------- 2 CloudFront Cache – Cache Policy Query Strings


- for example we have a request like 

 GET /image/cat.jpg?border=red&size=large HTTP/1.1


-- So query strings are what happens in the URL after a question mark. 

-- So for example, border equals red and size equals large. So here, we want a cat image. But apparently, it's going to be customized a little bit by the origin.

-- if you have 

• None : 

    • Don’t include any query strings in the Cache Key

    • Query strings are not forwarded

• Whitelist

    • Only specified query strings included in the Cache Key

    • Only specified query strings are forwarded


• Include All-Except : you specify which ones you don't want but the rest passes

    • Include all query strings in the Cache Key except the specified list

    • All query strings are forwarded except the specified list


• All

    • Include all query strings in the Cache Key

    • All query strings are forwarded

    • Worst caching performance




------------------------------ CloudFront Policies – Origin Request Policy 


• Specify values that you want to include in origin requests without including them in the Cache Key (no duplicated cached content)

• You can include extra :
      
      • HTTP headers: None – Whitelist – All viewer headers options
      • Cookies: None – Whitelist – All
      • Query Strings: None – Whitelist – All

    - but they will be forwarded to the origin but they're not going to be used in the Cache Key.


• Ability to add CloudFront HTTP headers and Custom Headers to an origin request that were not included in the viewer request (eg :  if you wanted to pass an API key or a secret header.)

• Create your own policy or use Predefined Managed Policies



----------------------------- Cache Policy vs. Origin Request Policy


GET /blogs/myblog.html HTTP/1.1
Host: mywebsite.com
User-Agent: Mozilla/5.0 (Mac OS X 10_15_2....) 
Date: Tue, 28 Jan 2021 17:01:57 GMT 
Authorization: SAPISIDHASH fdd00ecee39fe.... Keep-Alive: 300
Language: fr-fr


--  so the request will come with some query strings, some cookies, some headers, and then we will cache based on the cache policy.

-- For example, we want to cache here on the host name, the resource, and a header called authorization.



-- But then, your origin may need more than these three things to actually work and to actually serve properly the request.

-- So you may want to add the user agents,the session ID and the ref query string as part of your request to the origin.

-- So in this case, the request to the origin is going to be enhanced but then the caching will not happen based on what we forwarded to the origin request policy.

-- It's only going to be based on the cache policy.







Client ------------------------------(request)----------------------------> C.F --------------------------(forward)--------------------------------> Origin (EC2 instance)
                                                                             |

                   Cache Policy                                           EDGE Location                   Origin Request Policy (whitelist)

                Cache Key (cache based on)                                  |                                  Type                        Value
                                                                            |                               HTTP Headers         User-Agent, Authorization
                 - mywebsite.com 
                 - /content/stories/example-story.html                     CACHE                            Cookies               session_id
                 - Header: Authorization                                                                    Query Strings          ref


    


----------------------------------- CloudFront – Cache Invalidations


• In case you update the back-end origin, CloudFront doesn’t know about it and will only get the refreshed content after the TTL has expired

• However, you can force an entire or partial cache refresh (thus bypassing the TTL) by performing a CloudFront Invalidation

• You can invalidate all files (*) or a special path (/images/*)


------------------------------------ CloudFront – Cache Behaviors


• Configure different settings for a given URL path pattern

• Example: one specific cache behavior to images/*.jpg files on your origin web server

• Route to different kind of origins/origin groups based on the content type or path pattern
  
      • /images/*
      • /api/*  --  EG : Load balancer 
      • /* (default cache behavior) EG : S3 


• When adding additional Cache Behaviors, the Default Cache Behavior is always the last to be processed and is always /*



------ use case for cache behavior  EG : CloudFront – Cache Behaviors – Sign In Page


                                                                

                                                                        cache behaviours                                                 origins 
           Signed Cookies
       <------------------------->                                    
Users                                           <--------------------------->  /* (default)      -------------------->                      S3
                                          CF Distribution                         
                                                <--------------------------->  /login             ------------------->                EC2 instance (generated signed cookies)
       <------------------------->                                                                <--------------------              
            authenticate                                                                    Signed Cookies              



-- So the way we do it is that we define a cache behavior ,for /login and so the users who hit the /login page will be redirected to our EC2 instance.

-- And the role of our EC2 instance is to generate CloudFront signed cookies.

-- So these signed cookies are sent back to the user and the user will then use the signed cookies to be able to access our default cache behavior, which is any other URL, then /login and then access our S3 bucket files.

-- if the users are trying to access the default cache behavior without doing a login first, what we can do is that we can set up the cache behavior to only accept the request if signed cookies are present.

-- Therefore, we can redirect to the /login page and we're good to go.



---------------------------------------- CloudFront – Maximize cache hits by separating static and dynamic distributions



Static requests --------------> CDN Layer CloudFront (No headers / session caching rules Required for maximizing cache hits) ------(Static content)-----------> S3

Dynamic         --------------> CDN Layer CloudFront (Cache based on correct headers and cookie)------------------(Dynamic Content (REST, HTTP server))----------------> ALB + EC2 





--------------------------------------- CloudFront Signed URL / Signed Cookies


• You want to distribute paid shared content to premium users over the world

• We can use CloudFront Signed URL / Cookie. We attach a policy with:

      • Includes URL expiration
      • Includes IP ranges to access the data from
      • Trusted signers (which AWS accounts can create signed URLs)

• How long should the URL be valid for?

     • Shared content (movie, music): make it short (a few minutes)
     • Private content (private to the user): you can make it last for years


• Signed URL = access to individual files (one signed URL per file)

• Signed Cookies = access to multiple files (one signed cookie for many files)



------------------------------------------ CloudFront Signed URL vs S3 Pre-Signed URL


          • CloudFront Signed URL:                                                                                                • S3 Pre-Signed URL:


• Allow access to a path, no matter the origin  (not only s3 , but HTTP, backend ....)                           • Issue a request as the person who pre-signed the URL

• Account wide key-pair, only the root can manage it                                                             • Uses the IAM key of the signing IAM principal

• Can filter by IP, path, date, expiration                                                                       • Limited lifetime

• Can leverage caching features



------------------------------------------ CloudFront Signed URL Process

• Two types of signers:

    1 • Either a trusted key group (recommended)
        
        • Can leverage APIs to create and rotate keys (and IAM for API security)


    2 • An AWS Account that contains a CloudFront Key Pair

        • Need to manage keys using the root account and the AWS console
        • Not recommended because you shouldn’t use the root account for this



• In your CloudFront distribution, create one or more trusted key groups

• You generate your own public / private key
       
       • The private key is used by your applications (e.g. EC2) to sign URLs
       • The public key (uploaded) is used by CloudFront to verify URLs




---------------------------------------------------------------- LAB Demo 

------------ Type 1 


-- open S3 and create private Bucket 

-- as u create private bucket , no one can access ur url through the S3 

-- Only access through by Cloud-Front directly coz, it is privtae bucket and we did not enable Static hosting also 

-- go to CF in console 

-- create ditrubtion on CF 

-- Origina Domain = load balancer / S3 -- these are the places wher u can host ur applications 

-- OAC --> Create control settings --> do not change any n create 

-- it is created access from S3 

-- Compress objects automatically : CloudFront can automatically compress certain files that it receives from the origin before delivering them to the viewer. CloudFront compresses files only when the viewer supports it, as specified in the Accept-Encoding header in the viewer request.


-- Default root object - optional = index.html  -----> must and should u have to give this , otherwise u won’t get o/p 


----- once u create distrubtion , the S3 bucket policy wil gwt generated copy that policy and paste in bucket policy 

-- now ur appn is getting deployed all over the world 

-- through the CF url customers will able to connect ur webiste through the edge locations 

-- once u change the content of ur website and do uploud again n if u do refresh u won't get new content 

-- u have too do "invalidate the Cache" 

-- go to CF and create invalidation for /index.html or /* , do upload again the file to s3 ,it will get latest file from the S3 and give latest content to customers 

-- By default, CloudFront caches files in edge locations for 24 hours. 


------------ Sign URL System 

--  now i want to make this private , and want to give access to only prime customers 

--  so do create Signed URL'S

-- for this first u have to create public key and do store this key in the Key group 

-- go to google and search for "RSA Key Generator" create 2048 bit key , copy public key and do paste in public key , create public key

-- now create key group and add this public key to this group , u can store upto 5 pulic keys in one group 

-- now go to distribution --> behaviour --> edit --> Restrict viewer access = yes --> save changes 

-- it will get create one URL




TYPE 2 

-- go to security credentials --> create cloudfront key pair --> download both public and private files 

-- now open terminals and enter the command like this 

      aws cloudfront sign --url https://dgw7w0gfg0nkb.cloudfront.net --key-pair-id APKAUK2QS6DOWVAB3OG5 --private-key file://pk-APKAUK2QS6DOWVAB3OG5.pem --date-less-than 2024-03-31


      -- it will generate one url 

      EG :    https://dgw7w0gfg0nkb.cloudfront.net?Expires=1711843200&Signature=Pa7-jzCpDyXKgwS6bqPh75zqiNnCHryUCWDgcQheZLwX7g0wyzLrBSiUmD2KNFtdnx-OKnYLO2zJSiLsORIQO1yDs5RBTCqW6y5BTGqE0-CUdQ5clls4LY4KKdwZWmRs2VyJtMMDNqiwjsID2nTHO8nRUkgWBB0Nx9FShrhsmMoqVYo2JnDmIWnLb8KE4r8vSxbKPmMKByRkqmUmHPSbR6ODct0njHdDbcDJuANZLh3NKXVPvfYNMGre1ipjwfPhz7neEbcZoMq3AYuXce83DzSQd2BN~P8lPKDyDOWy8C3kAHoUKUg~2tneTa9~Ksh2hFHHyOnIthSysoFKmsW5ug__&Key-Pair-Id=APKAUK2QS6DOWVAB3OG5%                                    





--------------------------------------- CloudFront – Multiple Origin

• To route to different kind of origins based on the content type

• Based on path pattern:

      • /images/*
      • /api/* 
      • /*



-------------------------------------- CloudFront – Origin Groups

• To increase high-availability and do failover

• Origin Group: one primary and one secondary origin

• If the primary origin fails, the second one is used


-------------------------------------- CloudFront – Field Level Encryption


• Protect user sensitive information through application stack

• Adds an additional layer of security along with HTTPS

• Sensitive information encrypted at the edge close to user

• Uses asymmetric encryption

• Usage: 

     • Specify set of fields in POST requests that you want to be encrypted (up to 10 fields)
     • Specify the public key to encrypt them


                                      
                                                Encrypt using Public Key

Client -----------(HTTPS)----------------------> EDGE Location ------------(HTTPS)--------------------> C.F -------------(HTTPS)-----------> ALB --------(HTTPS)------------------> WEB Servers

         POST /submit HTTP/1.1 
         Host: www.example.com                                             POST /submit HTTP/1.1                                                                               Decrypt using Private Key
                                                                           Host: www.example.com 



EG : 


LAB : https://325b057e.isolation.zscaler.com/profile/1233fc6e-6618-4022-a03b-96afce7da312/zia-session/?controls_id=5363df57-072a-41ec-8342-91ef13f84e51&region=bom&tenant=462f064b6b51&user=a88c06b13d4badbc3b1628a94e955f23c843810a6a0110d09b7d0213d4fe17aa&original_url=https%3A%2F%2Faws.amazon.com%2Fblogs%2Fsecurity%2Fhow-to-enhance-the-security-of-sensitive-customer-data-by-using-amazon-cloudfront-field-level-encryption%2F&key=sh-1&hmac=dfa6e28bf1bc65f977e7e1b8fb8cd99b505583c03072c2c6e52c6e286b86f799




------------------------------------------ CloudFront – Real Time Logs


• Get real-time requests received by CloudFront sent to Kinesis Data Streams

• Monitor, analyze, and take actions based on content delivery performance

• Allows you to choose:
  
      • Sampling Rate – percentage of requests for which you want to receive

      • Specific fields and specific Cache Behaviors (path patterns)



Real-time Processing  =          Users -----(Requests)--------> C.F -----(LOGS)-------> Kinesis Data Streams ------(records)--------------> Lambda


Near Real-time Processing  =       Users -----(Requests)--------> C.F -----(LOGS)-------> Kinesis Data Streams ------(records)--------------> Kinesis Data Firehose






===================================================== Containers on AWS ========================================

• Docker is a software development platform to deploy apps

• Apps are packaged in containers that can be run on any OS

• Apps run the same, regardless of where they’re run

  • Any machine
  • No compatibility issues
  • Predictable behavior
  • Less work
  • Easier to maintain and deploy
  • Works with any language, any OS, any technology


• Use cases: microservices architecture, lift-and-shift apps from on- premises to the AWS cloud, ...



---------------------------- Where are Docker images stored?

• Docker images are stored in Docker Repositories

• Docker Hub (https://hub.docker.com)

    • Public repository
    • Find base images for many technologies or OS (e.g., Ubuntu, MySQL, ...)


• Amazon ECR (Amazon Elastic Container Registry)

    • Private repository
    • Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)


-- Jfrog also we can store images





-------------------------- Docker vs.Virtual Machines

• Docker is ”sort of ” a virtualization technology, but not exactly

• Resources are shared with the host => many containers on one server

-- see pics in google for better understanding





-------------------------------- Getting Started with Docker



Dockerfile -------(Build)----------------> Docker Image ------------(Run)------------> Docker Container (Eg :python)
                                            |      |
                                            |      |
                                Push        |      |    Pull
                                            |      |
                                            |      |
              
                                        Docker Repositories

                                        Eg : DockerHub , ECR



-- Dockerfile  : which is defining how your Docker container will look. So we have a base Docker image , and we add some files and then we're going to build it.

-- DockerImage : And this will become a Docker image. And that Docker image, you can store it on a Docker repository , it's called a Push and you push it to either Docker hub which is a public repository, or Amazon is ECR
   
                 - Then you can pull back these images from these repositories and then you would run them.


-- Docker Container : And when you run a Docker image , it becomes a Docker container, which runs your code hat you had built from your Docker build.


--- That is the whole process with Docker.





---------------------------------------------- Docker Containers Management on AWS

• Amazon Elastic Container Service (Amazon ECS)
     
     • Amazon’s own container platform


• Amazon Elastic Kubernetes Service (Amazon EKS)

     • Amazon’s managed Kubernetes (open source)


• AWS Fargate
  
     • Amazon’s own Serverless container platform
     • Works with ECS and with EKS


• Amazon ECR:
  
     • Store container images





--------------------------------------------------- 1 Amazon ECS - EC2 Launch Type


• ECS = Elastic Container Service

• Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters 
    
        - an ECS Cluster is made of things.And with the EC2 Launch Type, well these things are EC2 instances.

        - group of servers is called "clusters"


• EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances)

• Each EC2 Instance must run the ECS Agent to register in the ECS Cluster

• AWS takes care of starting / stopping containers

     - AWS is going to be starting or stopping the containers.
     - That means that whenever we have a new Docker container it's going to be placed accordingly on each EC2 Instance.
     





----------------------------------------------------  2 Amazon ECS – Fargate LaunchType


• Launch Docker containers on AWS

• You do not provision the infrastructure (no EC2 instances to manage)

• It’s all Serverless!, because we don't manage servers (there are servers behind., but we are not managing the servers)

• if we have an ECS Cluster , we just create task definition to define our ECS tasks.

• AWS just runs ECSTasks for you based on the CPU / RAM you need

   - So when we want to run a new Docker container, simple as that, it's going to be run, without us knowing where it's run and without an EC2 Instance to be created in the backend in our accounts for it to work.
   - So it's a little bit magic.


• To scale, just increase the number of tasks. Simple - no more EC2 instances





------------------------------------------------------- Amazon ECS – IAM Roles for ECS


 -- So let's take an example of the EC2 Launch Type in which we have an EC2 Instance running the ECS Agent on Docker.

 -- So in this case, we can create an EC2 Instance Profile which is only valued of course if you use EC2 Launch Type.

 1  EC2 Instance Profile (EC2 Launch Type only):

       • Used by the ECS agent
       • Makes API calls to ECS service
       • Send container logs to CloudWatch Logs
       • Pull Docker image from ECR
       • Reference sensitive data in Secrets Manager or SSM Parameter Store



 2 ECSTask Role:

-- our ECS tasks are going to get ECS Task Roles. And so this is valued for both EC2 Launch Type and Fargate.

-- And so here I have two tasks.And we can create a specific role per task.

           TASK A -----------(EC2 Task A Role)-------> s3 

           TASK B -----------(EC2 Task B Role)-------> DynamoDB

-- Well, why do we have different roles? 

ANS : Because each role allows you to be linked to different ECS services.

1  so, for example, the ECS Task A Role allows you to have your Task A, runs some API calls against Amazon S3

2  so, for example, the ECS Task B Role allows you to have your Task B, runs some API calls against Dynamodb


NOTE : you define the Task Role in the task definition of your ECS service.



• Allows each task to have a specific role

• Use different roles for the different ECS Services you run

• Task Role is defined in the task definition 

     -- task definition : The Task definitions view lists each task definition family you've created.

     -- You can perform the following actions:
        
         - Deploy the task definision as a service or a task.
         - Create a new revision






---------------------------------------------- Amazon ECS – Load Balancer Integrations


1  Application Load Balancer 

    -- supported and works for most use cases

    -- multiple ECS Tasks running. It's all in the ECS Cluster.

    -- And we want to expose these tasks as a HTP or HTTPS endpoint.

    -- Therefore we can run an Application Load Balancer in front of it and then our users will be going to the ALB and in the back end to the ECS tasks directly.

    -- the ALB is supported and will support most use cases, and that's a good choice.


2   Network Load Balancer

    -- recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link


3   Classic Load Balancer supported but not recommended (no advanced features – no Fargate)

    - you cannot link your Elastic Load Balancer to Fargate.






----------------------------------------------------- Amazon ECS – Data Volumes (EFS)


• Mount EFS file systems onto ECS tasks

   - So say you have an ECS cluster and in this case are represented both the EC2 Instance , as well as the Fargate Launch Type for my ECS Cluster.

   - And we want to mount a file system onto the ECS task to share some data. In that case, we use an Amazon EFS file system,


• Works for both EC2 and Fargate launch types

   - because it's a network file system is going to be compatible with both EC2 and the Fargate launch types. And it allows us to mount the file system directly onto our ECS tasks.


• Tasks running in any AZ will share the same data in the EFS file system

• Fargate + EFS = Serverless


• Use cases: persistent multi-AZ shared storage for your containers


IMP NOTE : 

    • Amazon S3 cannot be mounted as a file system ( S3 isn't a file system, it is object storage. )



--------------------------------------------------------- Capacity providers ECS 

-- The capacity provider view provides details about your Fargate and EC2 capacity providers.

-- For Amazon ECS on AWS Fargate, the Capacity Provider is FARGATE and a FARGATE_SPOT and the Type is FargateProvider. When you select AWS Fargate, these providers are added automatically. You cannot update or delete them.

-- For Amazon ECS on Amazon EC2, the Capacity Provider is the Auto Scaling group name and the Type is ASGProvider.

-- For Amazon ECS on Amazon EC2, you can create, update, and delete the capacity provider.

--  When you delete the capacity provide the capacity provider association is removed. You must go the Amazon EC2 console to delete the Auto Scaling group.






---------------------------------------------- LAB for ECS 

-- open ECS , give name for ur cluster

--  do check Fargate and ec2 instances 

-- now it create new ASG automatically for u , go with amazon linux 2 (os) , t2.micro (instance type) 

-- if u want to deploy these in ur vpc then select ur vpc , otherwise go with default vpc 

-- remaining all are default 

-- once the cluster is get created , automatically one ASG get created for u 

-- FARGATE / FARGATE_SPOT / Our ASG  = these are capacity providers 

-- go to democluster --> infrastructure 

        - we can launch the ec2 instances directly in the cluster through an ASG 

-- if u change desired capacity is 1 , it will create 1 ec2 for u 

-- democluster --> infrastructure --> the created instance is show in Container instances , these instance can be create by FARGATE / FARGATE_SPOT / Our ASG 



---- create an ECS service 

-- u need to create task definition, create new task definition

-- create TD with the name "nginxdemos-hello"

     - nginxdemos-hello : this is from the official dockerhub page from internet ,we are using this in this demo

-- Infrastructure requirements = AWS Fargate 

     - u can also do select ec2 , but here i want to be serverless

-- OS, Architecture, Network mode = Linux/x86_64

-- Task size = CPU = .5 vcpu and memory = 1 GB

-- IN Container – 1 

     - Name = nginxdemos-hello   and Image URI = nginxdemos/hello : this will directly pull image from the dockerhub repo

-- keep remaining are default

-- create TD with this configuration , 

-- let's launch this task definition as a service.

-- democluster --> create service 

-- Compute options = Launch type 

-- Launch type = Fargate

-- Application type = Service : Launch a group of tasks handling a long-running computing work that can be stopped and restarted. For example, a web application

             - Task : Launch a standalone task that runs and terminates. For example, a batch job.

-- for this demo choose service

-- choose family and revision

-- Service name = same as family name

-- create one new sg in Network field , allow HTTP from anywhere and public ip turned On

-- and also create NEW ALB for this demo in load balcer section

-- create service , wait for few minutes to create 

-- once the service is get created do observe the service 

-- the service is linked to the target group and this target group is present infront of LB , 

-- in the target , u can see ip address , this is ip of ur container 

-- now go to load balancer and copy the DNS , paste in browser , u will get nginx welcome page  , /hello with dns in browser , ---> get o/p as /hello 

-- now here we have one service , u can also launch some more 

-- cluster --> service --> democluster --> update service

-- put desired = 3 1 per AZ , now we have two more tasks being provisioned and they are provisioned on the Fargate engine.

-- So that means that behind the scenes, AWS is going to provision automatically the resource that it needs to launch these tasks.

-- now do refresh in the browser , it will refresh for 3 services , ALB is distributing the load equally

-- to avoid charges , make sure our desired capacity = 0 in services and ASG both 






-------------------------------------------------- ECS Service Auto Scaling


• Automatically increase/decrease the desired number of ECS tasks

• Amazon ECS Auto Scaling uses "AWS Application Auto Scaling" , we have three metrics we can scale on using the service.

    • ECS Service Average CPU Utilization
    • ECS Service Average Memory Utilization-Scale on RAM
    • ALB Request Count Per Target–metric coming from the ALB


• Target Tracking – scale based on target value for a specific CloudWatch metric

• Step Scaling – scale based on a specified CloudWatch Alarm

• Scheduled Scaling – scale based on a specified date/time (predictable changes)

• ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level)

    - IMP NOTE : Remember, that scaling your service, your ECS Service, at the task level is not equal to scaling your cluster of EC2 instances if you are in the EC2 launch type.

• Fargate Auto Scaling is much easier to setup (because Serverless)



------------------------------------------------ EC2 Launch Type – Auto Scaling EC2 Instances

• Accommodate ECS Service Scaling by adding underlying EC2 Instances

-- We have 2 types


1 • Auto Scaling Group Scaling

    • Scale your ASG based on CPU Utilization 
    • Add EC2 instances over time


2 • ECS Cluster Capacity Provider
 
     • Used to automatically provision and scale the infrastructure for your ECSTasks
     • Capacity Provider paired with an Auto Scaling Group
     • Add EC2 Instances when you’re missing capacity (CPU, RAM...)



IMP NOTE : So if you have to choose between Auto Scaling Group Scaling and ECS Cluster Capacity Provider, please use ECS Cluster Capacity Provider for your EC2 launch type.


USer -------------> CloudWatch Metric (ECS Service CPU Usage) ---------------(Trigger)-------------->CloudWatch Alarm -----------(scale)-------> ASG / Capacity providers





------------------------------------------------ ECS Rolling Updates


• When updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order

-- you will have two settings, the minimum healthy percent and the maximum percent.

-- So by default they're 100 and 200

-- EG : So your ECS service, for example, this one is running nine tasks represents an actual running capacity of 100%. 
    
         - And then if you set a minimum healthy percent of less than 100, this is going to say, "Hey you're allowed to terminate all the tasks on the right hand side, as long as we have enough tasks to have a percentage over the minimum healthy percent."

         - And in the maximum percent shows you how many new tasks you can create of the version two, to basically roll updates your service.

         - So this is how these two settings would impact your updates. 

         - so you will go ahead, create new tasks, then terminate all tasks and so on. All to make sure that all your tasks are going to be terminated and then updated to a newer version.


-- lets discuss 2 scenarios


------------------------------------------- 1 ECS Rolling Update – Min 50%, Max 100% 


 • Starting number of tasks: 4

   - In this case, we're going to lose four tasks to be terminated , so that we're running at 50% capacity.

   - Then two new tasks are going to be created, Now we're back at 100% capacity.

   - Then two old tasks are going to be terminated, we're back at 50% capacity.And two new tasks are going to be created, we're back at 100 capacity.

   - we have done a rolling updates.

   - In this case, we have been terminating tasks because we set the minimum to 50% and the maximum to 100%.



------------------------------------------- 2 ECS Rolling Update – Min 100%, Max 150%


• Starting number of tasks: 4

   - We cannot terminate a task because the minimum is 100%.

   - Therefore we can go into create two new tasks and this will bring our capacity to 150%. (total 6)

   - Then because we are above the minimum 100% we can terminate two old task and we're back at 100%.

   - Then we will create two new tasks and finally, terminates two old tasks. And this will have performed our rolling updates for our ECS service.




----------------------------------------- Amazon ECS – Task Definitions

• Task definitions are metadata in JSON form to tell ECS how to run a Docker container

• It contains crucial information, such as:
    
    • Image Name
    • Port Binding for Container and Host • Memory and CPU required
    • Environment variables
    • Networking information
    • IAM Role
    • Logging configuration (ex CloudWatch)

• Can define up to 10 containers in a Task Definition


-- Amazon ECS allows you to run and maintain a specified number of instances of a task definition simultaneously in an Amazon ECS cluster. This is called a service.



-------------------------------------- Amazon ECS – Load Balancing (EC2 Launch Type)


• We get a Dynamic Host Port Mapping if you define only the container port in the task definition

    - So if you have load balancing and you're using the EC2 launch type, then you're going to get what's called a Dynamic Host Port Mapping. If you define only the container port and the task definition.

Explanation : 

    - So we are running for example, an ECS task, and all of them have the container port set to 80 but the host port set to zero,meaning not set.

    - the host port is going to be random, is going to be dynamic.

    - so, each ECS task from within the EC2 instance, is going to be accessible from a different port on the host,the EC2 instance.

    - therefore, if you define an application load balancer , then you may say, well, it is difficult for the ALB to connect to the ECS test because the port is changing.

    - But the ALB when linked to an ECS service knows how to find the right port, thanks to the Dynamic Host Port Mapping feature.

NOTE : but it does not work with a classic load balancer because it is older generation.


• You must allow on the EC2 instance’s Security Group any port from the ALB’s Security Group







-------------------------------------- Amazon ECS – Load Balancing (Fargate)


• Each task has a unique private IP

• Only define the container port (host port is not applicable)

   -  because this is Fargate, there is no host


-- for example, with four tasks each task is going to get its own private IP through an Elastic Network Interface or ENI. And then each ENI is going to get the same container ports.

-- And therefore, when you have an ALB, then to connect to the Fargate task, it's just going to connect to all all of them on the same port on port 80.



• Example

   • ECS ENI Security Group

         • Allow port 80 from the ALB


   • ALB Security Group

         • Allow port 80/443 from web





------------------------------------------------------ Amazon ECS One IAM Role per Task Definition


-- you should know that IAM roles are assigned per task definition.

-- you have a task definition and then you assign an ECS task role. And this will allow you, for example, for your ECS tasks out of your task definition, to access the Amazon S3 service.

-- And therefore when you create an ECS service from this task definition then each ECS task automatically is going to assume and inherit this ECS task role.

NOTE :  you should know that the role is defined at the task definition level, not at this service level.so, therefore all the tasks within your service, are going to get access to Amazon S3.





----------------------------------------------------- Amazon ECS – Environment Variables


• Environment Variable

    • Hardcoded – e.g., URLs
    • SSM Parameter Store – sensitive variables (e.g., API keys, shared configs)
    • Secrets Manager – sensitive variables (e.g., DB passwords)


• Environment Files (bulk) – Amazon S3



---------------------------------------------------- Amazon ECS – Data Volumes (Bind Mounts)


Bind Mount :

-- With bind mounts, a file or directory on a host, such as an Amazon EC2 instance, is mounted into a container. Bind mounts are supported for tasks that are hosted on both Fargate and Amazon EC2 instances.

-- Bind mounts are tied to the lifecycle of the container that uses them. After all of the containers that use a bind mount are stopped, such as when a task is stopped, the data is removed.

-- For tasks that are hosted on Amazon EC2 instances, the data can be tied to the lifecycle of the host Amazon EC2 instance by specifying a host and optional sourcePath value in your task definition. 

The following are common use cases for bind mounts.

  - To provide an empty data volume to mount in one or more containers.
  
  - To mount a host data volume in one or more containers.
  
  - To share a data volume from a source container with other containers in the same task.

  - To expose a path and its contents from a Dockerfile to one or more containers.




• Share data between multiple containers in the same Task Definition

• Works for both EC2 and Fargate tasks

• EC2 Tasks – using EC2 instance storage

     • Data are tied to the lifecycle of the EC2 instance

• Fargate Tasks – using ephemeral storage
  
     • Data are tied to the container(s) using them
     • 20 GiB – 200 GiB (default 20 GiB)


• Use cases:

   • Share ephemeral data between multiple containers

   • “Sidecar”container pattern, where the “sidecar” container used to send metrics/logs to other destinations (separation of conerns)







----------------------------------------------------  Amazon ECS – Task Placement


• When an ECS task is started with EC2 Launch Type, ECS must determine where to place it, with the constraints of CPU and memory (RAM) and available port

• Similarly, when a service scales in, ECS needs to determine which task to terminate

• You can define:
  
    • Task Placement Strategy
    • Task Placement Constraints


• Note: only for ECS Tasks with EC2 LaunchType (Fargate not supported)



----------------------------------------------------  Amazon ECS – Task Placement Process


• Task Placement Strategies are a best effort


• When Amazon ECS places a task, it uses the following process to select the appropriate EC2 Container instance:

   1. Identify which instances that satisfy the CPU, memory, and port requirements

   2. Identify which instances that satisfy the Task Placement Constraints

   3. Identify which instances that satisfy the Task Placement Strategies

   4. Select the instances for task placement




---------------------------------------------------- Amazon ECS –Task Placement Strategies


• Binpack

    • Tasks are placed on the least available amount of CPU and Memory

    • Minimizes the number of EC2 instances in use (cost savings)


JSON :


"placementStrategy": [
    {
        "field": "memory",
        "type": "binpack"
    }
]



• Random

    • Tasks are placed randomly


JSON : 



"placementStrategy": [
    {
       
        "type": "random"
    }
]



• Spread


  • Tasks are placed evenly based on the specified value

  • Example: instanceId, attribute:ecs.availability-zone, ...


  JSON :

  "placementStrategy": [
    {
       
        "type": "spread",
        "field": "attribute:ecs.availability-zone"
    }
]



-------------------- You can mix them together


1 we can have a spread on availability zone and then a spread on instance ID

   
EG :

 "placementStrategy": [
    {

         "field": "attribute:ecs.availability-zone"
         "type": "spread",
       
    },

    {
          "field": "instanceid"
         "type": "spread",
    }
]




2  we can have a spread on availability zone and then a binpack on memory.

    
EG :

"placementStrategy": [
    {

         "field": "attribute:ecs.availability-zone"
         "type": "spread",
       
    },

    {
          "field": "memory"
         "type": "binpack",
    }
]






---------------------------------------------------- Amazon ECS –Task Placement Constraints


1  distinctInstance

     • Tasks are placed on a different EC2 instance

     - So you will never have two tasks on the same instance.

    
 "placementStrategy": [
    {
       
        "type": " distinctInstance"
        
    }
]


2   memberOf

    • Tasks are placed on EC2 instances that satisfy a specified expression

    • Uses the Cluster Query Language (advanced)

EG: 1 

    "placementStrategy": [
    {
       
        "type": "memberOf"
        "expression": "attribute:ecs.availability-zone in [ap-south-1,ap-south-2]"
        
    }
]

EG : 2 


"placementStrategy": [
    {
       
        "type": "memberOf"
        "expression": "attribute:ecs.instance-type =~ t2.*"
        
    }
]






---------------------------------------------------- Amazon ECR

• ECR = Elastic Container Registry

• Store and manage Docker images on AWS

• Private and Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)

• Fully integrated with ECS, backed by Amazon S3

• Access is controlled through IAM (permission errors => policy)

• Supports image vulnerability scanning, versioning, image tags, image lifecycle, ...




----------------------------------------------------  Amazon ECR – Using AWS CLI

• Login Command

   • AWS CLI v2

        aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com



• Docker Commands
 
   • Push

       docker push aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest


   • Pull
     
       docker pull aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest



• In case an EC2 instance (or you) can’t pull a Docker image, check IAM permissions





---------------------------------------------------- AWS Copilot


• CLI tool to build, release, and operate production-ready containerized apps
      
      - So Copilot is not a service,


• The idea is that we want to remove the difficulty of running apps on AppRunner, ECS and Fargate, by just using a CLI tool to deploy to these environments.

• Helps you focus on building apps rather than setting up infrastructure

• Provisions all required infrastructure for containerized apps (ECS,VPC, ELB, ECR...) is done for you by Copilot.

• Automated deployments with one command using CodePipeline

• Deploy to multiple environments

• Troubleshooting, logs, health status...



Microservices Architecture
Use CLI or YAML to describe the architecture of your applications   --------------------------------> AWS Copilot CLI for containerized applications (Well-architected infrastructure setup, Deployment Pipeline, Effective Operations and Troubleshooting)----------------> Amazon ECS / AWS Fargate / AWS App Runner



------------------------------------- LAB for Copilot

-- open cloud9 , create environment

-- all are default , create environment

sudo curl -Lo /usr/local/bin/copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux \
   && sudo chmod +x /usr/local/bin/copilot \
   && copilot --help


-- by using the above command we can install the copilot in the Identify

-- make sure that docker should be installed in the IDE , check by typing docker

-- clone the url from the github
  
      git clone https://github.com/aws-samples/aws-copilot-sample-service example

-- do cd example

-- this example folder contains docker file , and index.html files 

-- do copilot init

-- So this is the Copilot CLI and it's going to give us some questions. And with it we're going to be able to set up a containerized application on AWS.

-- Application name: copilot-guide

 Which workload type best represents your architecture?  [Use arrows to move, type to filter, ? for more help]
  > Request-Driven Web Service  (App Runner)
    Load Balanced Web Service   (Public. ALB by default. Internet to ECS on Fargate)
    Backend Service             (Private. ALB optional. ECS on Fargate)
    Worker Service              (Events to SQS to ECS on Fargate)
    Static Site                 (Internet to CDN to S3 bucket)
    Scheduled Job               (Scheduled event to State Machine to Fargate)


-- u will get this once u give ur application load balancer 

-- here we have LB service web app

-- Service name: web-app
 
   Which Dockerfile would you like to use for web-app?  [Use arrows to move, type to filter, ? for more help]
  > ./Dockerfile
    Enter custom path for your Dockerfile
    Use an existing image instead

-- choose 1st option

-- select N 

-- in web-app folder one manifest file will get created here 

-- now create an environment to run our application in.

    copilot env init --name prod --profile default --app copilot-guide\

-- choose default 

-- now now it's going to update all the resources 

-- now go to cloudformation and check StackSet-copilot-guide-infrastructure --> resource , it will create ecr repo , s3 bucket and bucket policy and KMS created for us

-- next what we have to do is to actually go ahead and provision our application.

-- once u run copilot env deploy --name prod  

-- it will gives u InvalidClientTokenId error , so to avoid this , u can do  click on cloud9 logo --> preferences --> aws settings --> disable temporary credentials 

-- now go to IAM --> create user --> with administration access policy --> generate Access key 

-- now do aws configure and give details and o/p format is JSON

-- now it is working , see what it is created for u 

-- check in ecs cluster got vreated but no services are there 

-- so now run copilot deploy

-- so it will search for our appn "web-app" and choose environment to run our application

-- So now it goes ahead and uses Docker to actually build our final Docker image.

-- Then it pushes that Docker image into ECR, and then from ECR is going to create an ECS service that will be referencing that image, and will be started on our ECS cluster.

-- now wait for 5-7 minutes , it will get created the whole process for u  do not close the window unitl it get created 

- Creating the infrastructure for stack copilot-guide-prod-web-app                [create complete]  [351.8s]
  - Service discovery for your services to communicate within the VPC             [create complete]  [0.0s]
  - Update your environment's shared resources                                    [update complete]  [172.8s]
    - A security group for your load balancer allowing HTTP traffic               [create complete]  [3.6s]
    - An Application Load Balancer to distribute public traffic to your services  [create complete]  [151.8s]
    - A load balancer listener to route HTTP traffic                              [create complete]  [1.1s]
  - An IAM role to update your environment stack                                  [create complete]  [16.3s]
  - An IAM Role for the Fargate agent to make AWS API calls on your behalf        [create complete]  [16.3s]
  - An HTTP listener rule for path `/` that forwards HTTP traffic to your tasks   [create complete]  [0.0s]
  - A custom resource assigning priority for HTTP listener rules                  [create complete]  [3.0s]
  - A CloudWatch log group to hold your service logs                              [create complete]  [7.3s]
  - An IAM Role to describe load balancer rules for assigning a priority          [create complete]  [16.3s]
  - An ECS service to run and maintain your tasks in the environment cluster      [create complete]  [122.5s]
    Deployments                                                                                      
               Revision  Rollout      Desired  Running  Failed  Pending                                       
      PRIMARY  1         [completed]  1        1        0       0                                             
  - A target group to connect the load balancer to your service on port 80        [create complete]  [15.3s]
  - An ECS task definition to group your containers and run them on ECS           [create complete]  [0.0s]
  - An IAM role to control permissions for the containers in your tasks


-- these are things that it will create all the complexity of thinking about what you need to actually create when you run an application on ECS is taken away by Copilot.

-- now do check in ecs , the service is get created 

-- now copy the link and paste in the broswer , u will get o/p

-- all the complexity will tAKEN BY Copilot for u
    
-- now delete resource , do run copilot app delete

-- wait for some time it will get deleted 

-- delete user in iam also 


---------------------------------------------------- Amazon EKS(Elastic Kubernetes service) Overview

• Amazon EKS = Amazon Elastic Kubernetes Service

• It is a way to launch managed Kubernetes clusters on AWS

• Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) application

• It’s an alternative to ECS, similar goal but different API

• EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers

• Use case: if your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes

• Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP...)

• For multiple regions, deploy one EKS cluster per region

• Collect logs and metrics using CloudWatch Container Insights



----------------------- Amazon EKS – Node Types

• Managed Node Groups

    • Creates and manages Nodes (EC2 instances) for you
    • Nodes are part of an ASG managed by EKS
    • Supports On-Demand or Spot Instances

• Self-Managed Nodes

    • Nodes created by you and registered to the EKS cluster and managed by an ASG
    • You can use prebuilt AMI - Amazon EKS Optimized AMI
    • Supports On-Demand or Spot Instances


• AWS Fargate

    • No maintenance required; no nodes managed


------------------ Amazon EKS – Data Volumes

• Need to specify StorageClass manifest on your EKS cluster

• Leverages a Container Storage Interface (CSI) compliant driver

• Support for...
   
   • Amazon EBS
   • Amazon EFS (works with Fargate) 
   • Amazon FSx for Lustre
   • Amazon FSx for NetApp ONTAP





================================================= AWS Elastic Beanstalk ===============================================


Developer problems on AWS

  • Managing infrastructure
  • Deploying Code
  • Configuring all the databases, load balancers, etc
  • Scaling concerns


• Most web apps have the same architecture (ALB + ASG)

• All the developers want is for their code to run!

• Possibly, consistently across different applications and environments



------------------------------------- Elastic Beanstalk – Overview

• Elastic Beanstalk is a developer centric view of deploying an application on AWS

• It uses all the component’s we’ve seen before: EC2, ASG, ELB, RDS, ...

• Managed service

     • Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration, ...

     • Just the application code is the responsibility of the developer

• We still have full control over the configuration

• Beanstalk is free but you pay for the underlying instances



------------------------------------- Elastic Beanstalk – Components

• Application: collection of Elastic Beanstalk components (environments, versions, configurations, ...)

• Application Version: an iteration of your application code (v1, v2 , v3)

• Environment 

      • Collection of AWS resources running an application version (only one application version at a time in an environment),  where we can see we can actually update an application version within an environment from version one to version two.
   
      • Tiers: Web Server Environment Tier & Worker Environment Tier

      • You can create multiple environments (dev, test, prod, ...)



------------------------------------- Elastic Beanstalk – Supported Platforms

• Go
• Java SE
• Java withTomcat
• .NET Core on Linux
• .NET on Windows Server 
• Node.js
• PHP
• Python
• Ruby
• Packer Builder
• Single Container Docker 
• Multi-container Docker 
• Preconfigured Docker


------------------------------------- Web ServerTier vs. WorkerTier (environments)


 Web ServerTier :

 -- This is the traditional architecture that we know, where we have a load balancer and then it's sending traffic to an auto scaling group that has multiple EC2 instances that are going to be your web server.


 WorkerTier :

 -- So this time there is no clients accessing directly your EC2 instances. We're going to use an SQS queue, which is a message queue and the message will be sent into the SQS queue and the EC2 instances are going to be workers,

 -- because they're going to pull messages from the SQS queue to process them.

 • Scale based on the number of SQS messages

 • Can push messages to SQS queue from anotherWeb ServerTier




------------------------------------ Elastic Beanstalk Deployment Modes

1 Single Instance Great for dev

  -- In this case, you'll have one EC2 instance which will have an Elastic IP, potentially it can also launch an RDS database and so on,

  -- but it's all based on one instance with an Elastic IP. It's great for development purposes,


NOTE : but then if you wanted to scale a real Elastic Beanstalk mode, then you would go for high available with a load balancer,


2  High Availability with Load Balancer Great for prod

   -- which is great for production environments, in which case, you can have a load balancer distributing the loads across multiple EC2 instances that are managed for an auto scaling group and multiple available zones.

   -- And finally, you may have an RDS database that's also multi AZ with a master and a standby.





----------------------- 1st environment LAB 

Single Instance Great for dev

-- open Elasticbeanstalk --> Web server environment

-- choose nodejs

-- sample application --> single instance

-- choose Create and use new service role

-- sometimes the EC2 instance profile is not getting by default , so create ROLE for this 

    - IAM --> ROLES --> create role --> aws service --> ec2 --> attach permissions --> create role 

          AWSElasticBeanstalkMulticontainerDocker
          AWSElasticBeanstalkWebTier
          AWSElasticBeanstalkWorkerTier


-- now go directly to skip review

-- submit --> this will create our first environment




----------------------- 2nd environment LAB 



-- name = prod

-- sample application 

-- Presets  = high availability

-- select role , skip for review then submit , it will create for u 

-- it will take almsot 10 min

-- for this environment , the loadbalancer will created , SG with port 80 , and ASG 


NOTE : use the above method to create environment , in the new console there might be chance of getting error , so follow this only 





---------------------------------------- Beanstalk Deployment Options for Updates

1 • All at once (deploy all in one go) – fastest, but instances aren’t available to serve traffic for a bit (downtime)

2 • Rolling: update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy

3 • Rolling with additional batches: like rolling, but spins up new instances to move the batch (so that the old application is still available)

4 • Immutable: spins up new instances in a new ASG, deploys version to these instances, and then swaps all the instances when everything is healthy

5 • Blue Green: create a new environment and switch over when ready

6 • Traffic Splitting: canary testing – send a small % of traffic to new deployment



Explanation for Deployment Options



---------- 1 All at once (deploy all in one go)

-- Here is our four EC2 instances, and they all run the version one, which is blue, of our application.

-- Then we are going to do an all at once deployment. So we want deploy v2.

-- And what happens, that first Elastic Beanstalk will just stop the applications on all our EC2 instances.

-- So then I put it as gray, as in they don't run anything. for sometime

-- And then we will be running the new V2, because Elastic Beanstalk will deploy V2 to these instances.

-- So what do we notice? 

     • Fastest deployment

     • Application has downtime

     • Great for quick iterations in development environment

     • No additional cost



------------ 2 Elastic Beanstalk Deployment Rolling


• Application is running below capacity

• Can set the bucket size

• No additional cost

-- We have four instances running v1,and the bucket size will be two for the example.

-- So what happens is that the application on the instances will be stopped, and so they're gray.

-- But we still have the other two instances running v1, we have maybe half capacity here.

-- Then these first two instances will be updated, so they'll be running v2, and then we will roll on to the next bucket, or to the next batch.And so that's why it's called rolling.

-- first 2 will updated to 2nd version and then nxt 2 instancs updated to 2nd version , the application, at some point during the deployment, is running both versions simultaneously.

-- No additional cost

-- so if you set a very small bucket size and you have hundreds and hundreds of instances, it may be a very long deployment,

-- Right now, in this example, we have a bucket size of two and four instances, but we can have a bucket size of 2 and 100 instances. It will just take a very long time to upgrade everything.

-- Zero downtime



---------------- 3 Elastic Beanstalk Deployment Rolling with additional batches

-- so in this case, the application is not running under capacity, just like before.

-- Before, at some point, we were only running two instances out of four. So that was below capacity.

-- In this mode, we run at capacity,and we can also set the bucket size.

-- basically our application will still be running both versions simultaneously,

-- but at a small additional cost.

-- That additional batch, that we'll see in a second, will be removed at the end of the deployment.

-- the deployment is going to be long.

-- It's honestly a good way to deal with prod.

-- Zero downtime

EG : 

-- We have our four v1 EC2 instances, and the first thing we're going to do is deploy new EC2 instances, and they will have the v2 version on it. (now total they are 6 instances)

-- you can see that the additional two are running, already, the newer version.

-- Now we take the first batch to the first bucket of two and they get stopped, the application gets stopped,and the application gets updated to v2,

-- Then the process repeats again, just like in rolling.So the application running v1 gets stopped, and then the application is updated to v2.

-- so at the end, you can see, we have six EC2 instances running v2.

-- so at the end of it, the additional batch gets terminated and taken away.(now we have 4 insatnces)




---------------------------------- 4 Elastic Beanstalk Deployment Immutable


--  Zero downtime

--  New Code is deployed to new instances on a temporary ASG

• High cost, double capacity

• Longest deployment

• Quick rollback in case of failures (just terminate new ASG)

• Great for prod


EG : 

-- We have a current ASG with three applications v1 running on three instances.

-- And then we're going to have a new temporary ASG being created. At first, Beanstalk will launch one instance on it, just to make sure that one works.And if it works and it passes the health checks, it's going to launch all the remaining ones.

-- it's going to sort of merge the ASG with a temporary ASG. So it's going to move all the temporary ASG instances to the current ASG.

-- So now, in the current ASG, we have six instances, And when all of this is done and the temporary ASG is empty, then we have the current ASG that will terminate all the v1 applications, while the v2 applications are still there. And then, finally, the temporary ASG will just be removed.




--------------------------------- 5 Elastic Beanstalk Deployment Blue / Green

• Not a “direct feature” of Elastic Beanstalk

• Zero downtime and release facility

• Create a new “stage” environment and deploy v2 there

• The new environment (green) can be validated independently and roll back if issues

• Route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage environment

• Using Beanstalk, “swap URLs” when done with the environment test



-------------------------------- 6 Elastic Beanstalk - Traffic Splitting (Canary)

• Canary Testing

• New application version is deployed to a temporary ASG with the same capacity

• A small % of traffic is sent to the temporary ASG for a configurable amount of time

• Deployment health is monitored

• If there’s a deployment failure, this triggers an automated rollback (very quick)

• No application downtime

• New instances are migrated from the temporary to the original ASG

• Old application version is then terminated

-- this could be a big improvement on top of the blue/green technique



REF : https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html





--------------------------------- Deployment models LAB 


-- prod environment --> configuraations --> Updates, monitoring, and logging --> edit  --> select immutable --> apply 

-- it will take some time to update 

-- in the meanwhile go to google --> sample application node js beanstalk --> downlaod zip file for nodejs app

-- in this code --> index.html --> text coloumn --> change to blue colour

-- now upload and deploy zip file of this code

-- let's observe how it is creating in the events 

-- once it is create , check in brower , it will turn to blue 

-- here emv = green and prod = blue

-- now do environment swapping , that means prod become env and env become prod

-- And the reason we would do so is, for example, you take this environment, for example, prod. First, you're going to clone it.

-- So you're going to create a copy of prod,then you would deploy your new application to the new environment where you can perform some extensive testing. Call it prod number two.

-- in prod environment --> do swap environment domain --> choose env 

-- it will take sometime to update 

-- now check in browser , the pro become green and env become blue 





----------------------------------------------------------- Elastic Beanstalk CLI 


• We can install an additional CLI called the “EB cli” which makes working with Beanstalk from the CLI easier

• Basic commands are:
 
   • eb create
   • eb status 
   • eb health 
   • eb events 
   • eb logs
   • eb open
   • eb deploy
   • eb config
   • eb terminate

• It’s helpful for your automated deployment pipelines!


----------------------------------------------- Elastic Beanstalk Deployment Process


• Describe dependencies
  
      (requirements.txt for Python, package.json for Node.js)


• Package code as zip, and describe dependencies

      • Python: requirements.txt
      • Node.js: package.json

• Console: upload zip file (creates new app version), and then deploy

• CLI: create new app version using CLI (uploads zip), and then deploy


• Elastic Beanstalk will deploy the zip on each EC2 instance, resolve dependencies and start the application




------------------------------------------------------- Beanstalk Lifecycle Policy 


• Elastic Beanstalk can store at most 1000 application versions

• If you don’t remove old versions, you won’t be able to deploy anymore

• To phase out old application versions, use a lifecycle policy

   • Based on time (old versions are removed)
   • Based on space (when you have too many versions)

• Versions that are currently used won’t be deleted

• Option not to delete the source bundle in S3 to prevent data loss


-- All the applications are being registed in the amazon s3 bucket 

--  go to Elasticbean stalk --> appn versions --> activate lifecycle policy --> choose any one b/w the 2 options 




------------------------------------------------------- Elastic Beanstalk Extensions


• A zip file containing our code must be deployed to Elastic Beanstalk

• All the parameters set in the UI can be configured with code using files

• Requirements:

    • in the .ebextensions/ directory in the root of source code

    • YAML / JSON format

    • .config extensions (example: logging.config)

    • Able to modify some default settings using: option_settings

    • Ability to add resources such as RDS, ElastiCache, DynamoDB, etc...


• Resources managed by .ebextensions get deleted if the environment goes away


---- now add environmentvariables.config file  in the zip to the env environment (took from stphene codes)

-- u will see the environment variables in configuration section





------------------------------------------------------- Elastic Beanstalk Under the Hood


 Under the Hood : (The underlying implementation of a product (hardware, software, or idea)).doing work on something that isn't visible or apparent to someone else, or of something that is complex and not easily understood.


• Under the hood, Elastic Beanstalk relies on CloudFormation

• CloudFormation is used to provision other AWS services (we’ll see later)

• Use case: you can define CloudFormation resources in your .ebextensions to provision ElastiCache, an S3 bucket, anything you want!


-- go to CF and check in resource , that it has created for u 




------------------------------------------------------- Elastic Beanstalk Cloning

• Clone an environment with the exact same configuration

• Useful for deploying a “test” version of your application

• All resources and configuration are preserved:

    • Load Balancer type and configuration

    • RDS database type (but the data is not preserved)

    • Environment variables

• After cloning an environment, you can change settings 

-- we did during the lab 



------------------------------------------------------- Elastic Beanstalk Migration: Load Balancer


----------------------- 1 Elastic Beanstalk Migration: Load Balancer

• After creating an Elastic Beanstalk environment, you cannot change the Elastic Load Balancer type (only the configuration)

  -- if you want it to somehow upgraded from a Classic Load Balancer to an Application Load Balancer or from an Application Load Balancer to a Network Load Balancer. You will need to perform a migration and the steps are as such.

• To migrate:

1. create a new environment with the same configuration except LB (can’t clone) -- u can't use clone 'coz it uses same configuration.

2. deploy your application onto the new environment

3. perform a CNAME swap or Route 53 update -- to shift traffic



----------------------- 2 RDS with Elastic Beanstalk

• RDS can be provisioned with Beanstalk, which is great for dev / test

• This is not great for prod as the database lifecycle is tied to the Beanstalk environment lifecycle

• The best for prod is to separately create an RDS database and provide our EB application with the connection string (for example using an environment variable.)


-------- Elastic Beanstalk Migration: Decouple RDS

1. Create a snapshot of RDS DB (as a safeguard)

2. Go to the RDS console and protect the RDS database from deletion

3. Create a new Elastic Beanstalk environment, without RDS, point your application to existing RDS (for example, using an environment variable.)

4. perform a CNAME swap (blue/green) or Route 53 update, confirm working

5. Terminate the old environment (RDS won’t be deleted)

6. Delete CloudFormation stack (in DELETE_FAILED state) (so we need to just go in CloudFormation and delete that CloudFormation stack manually.)








============================================================================= AWS CloudFormation ========================================================================

Managing your infrastructure as code


• CloudFormation is a declarative way of outlining your AWS Infrastructure, for any resources (most of them are supported)

• For example, within a CloudFormation template, you say:

    • I want a security group
    • I want two EC2 instances using this security group
    • I want two Elastic IPs for these EC2 instances
    • I want an S3 bucket
    • I want a load balancer (ELB) in front of these EC2 instances


• Then CloudFormation creates those for you, in the right order, with the exact configuration that you specify




---------------------------------------------- Benefits of AWS CloudFormation 

• Infrastructure as code

    • No resources are manually created, which is excellent for control

    • The code can be version controlled for example using Git

    • Changes to the infrastructure are reviewed through code



• Cost

    • Each resources within the stack is tagged with an identifier so you can easily see how much a stack costs you

    • You can estimate the costs of your resources using the CloudFormation template

    • Savings strategy: In Dev, you could automation deletion of templates at 5 PM and recreated at 8 AM, safely




• Productivity


   • Ability to destroy and re-create an infrastructure on the cloud on the fly

   • Automated generation of Diagram for your templates!

   • Declarative programming (no need to figure out ordering and orchestration)



• Separation of concern: create many stacks for many apps, and many layers. Ex:

   • VPC stacks
   • Network stacks 
   • App stacks



• Don’t re-invent the wheel


    • Leverage existing templates on the web!

    • Leverage the documentation





---------------------------------------------------------- How CloudFormation Works


• Templates must be uploaded in S3 and then referenced in CloudFormation



   Template ------(upload)-------------> S3 bucket ------------- (reference)-----------> AWS CloudFormation ---------------(create)-----------> Stack ----------(create)----> AWS Resources



• To update a template, we can’t edit previous ones. We have to re- upload a new version of the template to AWS


• Stacks are identified by a name within a region 

• Deleting a stack deletes every single artifact that was created by CloudFormation.





---------------------------------------------------------- Deploying CloudFormation Templates


1 • Manual way

     • Editing templates in CloudFormation Designer or code editor

     • Using the console to input parameters, etc...

     • We’ll mostly do this way in the course for learning purposes


2 • Automated way

     • Editing templates in a YAML file

     • Using the AWS CLI (Command Line Interface) to deploy the templates, or using a Continuous Delivery (CD) tool

     • Recommended way when you fully want to automate your flow



NOTE : there is no order in which CloudFormation should create your resources. When you write a CloudFormation template





---------------------------------------------------------- CloudFormation – Building Blocks


• Template’s Components

    • AWSTemplateFormatVersion – identifies the capabilities of the template “2010-09-09”

    • Description – comments about the template

    • Resources (MANDATORY) – your AWS resources declared in the template

    • Parameters – the dynamic inputs for your template

    • Mappings – the static variables for your template

    • Outputs – references to what has been created

    • Conditionals – list of conditions to perform resource creation


• Template’s Helpers

    • References
    • Functions




---------------------------------------------------------- LAB for Stacks


-------------- 1 creating source


-- make sure to use us-east-1 region only for creating stacks , 'coz all the templates have been designed for that region, especially when talking about AMI IDs, which are region specific.

-- open cloudformation --> choose yaml files for easy purpose --> upload a yaml file which has template for ec2 creation --> do submit 
 
EG for ec2 
     Resources:
     MyInstance:
       Type: AWS::EC2::Instance
       Properties:
          AvailabilityZone: us-east-1a
          ImageId: ami-0a3c3a20c09d6f377
          InstanceType: t2.micro



-- our first stack is being created , do refresh it will created for u 

-- do explore in CF Properties





------------ 2 Update and Delete Stack 


-- here only way to update is to upload a new version of template to update 

-- upload updated template yaml file 

EG for updated template 



---
Parameters:
  SecurityGroupDescription:
    Description: Security Group Description
    Type: String

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
      SecurityGroups:
        - !Ref SSHSecurityGroup
        - !Ref ServerSecurityGroup

  # an elastic IP for our instance
  MyEIP:
    Type: AWS::EC2::EIP
    Properties:
      InstanceId: !Ref MyInstance

  # our EC2 security group
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  # our second EC2 security group
  ServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Ref SecurityGroupDescription
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 192.168.1.1/32



-- in above template we are attaching 2 security groups and EIP to the instance

-- choose nxt --> SecurityGroupDescription , u will get this option 'coz in this code u have written so give Description --> nxt 

-- in the summary page in step 2 u have stack deatils with parameters and in Changeset preview , u can able to see the changes and modifiers 

-- a "change set" represents a list of things that is going to change as part of your CloudFormation updates.

-- do submit , u can see that in the same stack the update is happening , go n check all the resources are created through the template 

-- to delete stack ,jst do delete stack 






------------------------------------------------------------------  YAML Crash Course

EG for YAML code 

Parameters:
  KeyName:
    Description: The EC2 Key Pair to allow SSH access to the instance
    Type: 'AWS::EC2::KeyPair::KeyName'
Resources:
  Ec2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
      SecurityGroups:
        - !Ref InstanceSecurityGroup
        - MyExistingSecurityGroup
      KeyName: !Ref KeyName
      ImageId: ami-7a11e213
  InstanceSecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0




 • YAML and JSON are the languages you can use for CloudFormation

 • JSON is horrible for CF , 'coz of of many string interpolations and so on,

 • YAML is great in so many ways

 • Key value Pairs

 • Nested objects  (SG is nested object, resource , preperties etc )

 • Support Arrays ( - symbol means which represents an array)

 • Multi line strings 





------------------------------------------------------------------ 1 CloudFormation – Resources


• Resources are the core of your CloudFormation template (MANDATORY)

• They represent the different AWS Components that will be created and configured

• Resources are declared and can reference each other

• AWS figures out creation, updates and deletes of resources for us

• There are over 700 types of resources (!)

• Resource types identifiers are of the form:

          service-provider::service-name::data-type-name




--------------------------- How do I find Resources documentation?

• All the resources can be found here:

      https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/a ws-template-resource-type-ref.html


• Then, we just read the docs 

• Example here (for an EC2 instance):

     https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/a ws-resource-ec2-instance.html




--------------------------------CloudFormation – Resources FAQ

• Can I create a dynamic number of resources?

  - Yes, you can by using CloudFormation Macros and Transform
  


• Is every AWS Service supported?

   - Almost. Only a select few niches are not there yet

   - You can work around that using CloudFormation Custom Resources



------------------------------------------------------------------ 2 CloudFormation – Parameters


• Parameters are a way to provide inputs to your AWS CloudFormation template

• They’re important to know about if:

     • You want to reuse your templates across the company

     • Some inputs can not be determined ahead of time


• Parameters are extremely powerful, controlled, and can prevent errors from happening in your templates, thanks to types    -- IMP to know




------------------------------- When should you use a Parameter?


EG : 

Parameters:
  KeyName:
    Description: The EC2 Key Pair to allow SSH access to the instance
    Type: 'AWS::EC2::KeyPair::KeyName'


-- EG for the parameter 

• Ask yourself this:

       • Is this CloudFormation resource configuration likely to change in the future?

       • If so, make it a parameter


• You won’t have to re-upload a template to change its content





------------------------------- CloudFormation – Parameters Settings


• Parameters can be controlled by all these settings:

   • Type:
       
       • String
       • Number
       • CommaDelimitedList
       • List<Number>
       • AWS-Specific Parameter (to help catch invalid values – match against existing values in the AWS account)
       • List<AWS-Specific Parameter>
       • SSM Parameter (get parameter value from SSM Parameter store)
       • Description
       • ConstraintDescription (String)
       • Min/MaxLength
       • Min/MaxValue
       • Default
       • AllowedValues (array)
       • AllowedPattern (regex)
       • NoEcho (Boolean)


-- So you don't have to remember all of those, but what you need remember is that the parameters are not just strings. You can have constraints and validation, allowing you to make sure they are safe to use.
 





------------------------------- CloudFormation – Parameters Example

     1 • AllowedValues (array)

Parameters:
 InstanceType:
   Description: Choose an ec2 type
   Type: String
   AllowedValues:
      - t2.micro
      - t3.micro
      - t2.small


Resources:
  Ec2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
       InstanceType: !Ref InstanceType
       ImageId: xxxxxxxxxxxxxx


-- Here , we have a parameter called InstanceType. To choose an EC2 InstanceType of Type: String.

-- But we have defined AllowedValues being t2.micro, t2.small, or t2.medium with a Default being t2.micro. And this parameter is reused in the EC2Instance.

-- So thanks to it, we'll have a dropdown and the user can only select one of these three values, hence giving them choice while giving you control.




     2 NoEcho (Boolean)


Parameters:
 DBPassword:
   Description: db password
   Type: String
   NoEcho: true

Resources:
  MyDBInstance:
   Type: 'AWS::RDS::DBInstance'
    Properties:
      DBInstanceClass: db.t2.micro
      AllocatedStorage: 20
      Engine: mysql 
      MasterUsername: administration
      MasterUserpassword: !Ref DBPassword
      DBInstanceIdentifier: MydbInstance


-- So for example, say we want as a parameter to put in the database password, but of course it is a password so we have to keep it secret.

-- So we want to remove it from the logs and so on. So we'll have NoEcho: true , so that the password is not displayed anywhere.






------------------------------- How to Reference a Parameter?

• The Fn::Ref function can be leveraged to reference parameters

• Parameters can be used anywhere in a template

• The shorthand for this inYAML is !Ref

• The function can also reference other elements within the template


IMP : you need to make sure that your resources don't have the same name as your parameters.



------------------------------- CloudFormation – Pseudo Parameters

• AWS offers us Pseudo Parameters in any CloudFormation template

• These can be used at any time and are enabled by default

• Important pseudo parameters:

    EG ;

    AWS::AccountId

    AWS::Region

    AWS::StackId

    AWS::StackName

    AWS::NotificationARNs

    AWS::NoValue





------------------------------------------------------------------ 3 CloudFormation – Mappings



• Mappings are fixed variables within your CloudFormation template

• They’re very handy to differentiate between different environments (dev vs prod), regions (AWS regions), AMI types...

• All the values are hardcoded within the template


EG : 

Mappings: 
  Mapping01: 
    Key01: 
      Name: Value01
    Key02: 
      Name: Value02
    Key03: 
      Name: Value03



RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309


-- So here, based on the region you have, so us-east-1, us-west-1, or eu-west-1, and based on the architecture you're using, for example, HVM64 or HVMG2, this is going to give you a different AMI ID every time.

-- Well, we know that the AMIs are specific based on the region, so of course it makes sense to have a different AMI per region.



--------------------------------- Accessing Mapping Values (Fn::FindInMap)



• We use Fn::FindInMap to return a named value from a specific key

• !FindInMap [ MapName,TopLevelKey, SecondLevelKey ]


EG :

AWSTemplateFormatVersion: "2010-09-09"
Mappings: 
  RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309
Resources: 
  myEC2Instance: 
    Type: "AWS::EC2::Instance"
    Properties: 
      ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", HVM64]
      InstanceType: m1.small


-- we have an EC2 instance that is using an ImageId. And this ImageId is using the FindInMap function.

-- to use this FindInMap function, we first need to use a map name. So here we have the RegionMap. Then we have the top level key.

-- So what we want to use in here, we have a reference to the pseudo parameter AWS::Region. So if you launch this template in us-east-1, it's going to be us-east-1, and if you launch it to us-west-1, automatically this pseudo parameter is going to resolve to us-west-1.
 
-- And then finally, the type of architecture you want, for example, HVM64. And this works great because, well, AMIs are region specific, and so you want to make sure you have the right AMI for the right region and the right architecture.



--------------------------------- When would you use Mappings vs. Parameters?



• Mappings are great when you know in advance all the values that can be taken and that they can be deduced from variables such as 

   • Region
   • Availability Zone
   • AWS Account
   • Environment (dev vs prod) 
   • etc...

• They allow safer control over the template

• Use parameters when the values are really user specific





------------------------------------------------------------------ 4  CloudFormation –  Outputs



• The Outputs section declares optional outputs values that we can import into other stacks (if you export them first)!

• You can also view the outputs in the AWS Console or in using the AWS CLI

• They’re very useful for example if you define a network CloudFormation, and output the variables such as VPC ID and your Subnet IDs

• It’s the best way to perform some collaboration cross stack, as you let expert handle their own part of the stack



-------------------------- CloudFormation – Outputs Example

• Creating a SSH Security Group as part of one template

• We create an output that references that security group


Outputs:
  StackSSHSecurityGroup:
    Description: The SSH SG
    Value: !Ref MycompanywideSShSG
    Export:
      Name: SSH SecurityGroup




-------------------------- CloudFormation – Outputs Cross-Stack Reference

• We then create a second template that leverages that security group

• For this, we use the Fn::ImportValue function

• You can’t delete the underlying stack until all the references are deleted





------------------------------------------------------------------ 5 CloudFormation –  Conditions


• Conditions are used to control the creation of resources or outputs based on a condition

• Conditions can be whatever you want them to be, but common ones are:
   • Environment (dev / test / prod) 
   • AWS Region
   • Any parameter value

• Each condition can reference another condition, parameter value or mapping


-------------------------- How to define a Condition

  Conditions:
     CreateprodResource: !Equals [!Ref EnvType, prod]
  

• The logical ID is for you to choose. It’s how you name condition

• The intrinsic function (logical) can be any of the following:

   • Fn::And
   • Fn::Equals
   • Fn::If
   • Fn::Not
   • Fn::Or


----------------- How to use a Condition

• Conditions can be applied to resources / outputs / etc...

   




-------------------------------------------------------------------- CloudFormation – Intrinsic Functions


mk = must know 



• Ref              mk
• Fn::GetAtt       mk
• Fn::FindInMap    mk
• Fn::ImportValue   mk
• Fn::Join
• Fn::Sub
• Fn::ForEach
• Fn::ToJsonString
• Condition Functions (Fn::If, Fn::Not, Fn::Equals, etc...)     mk
• Fn::Base64        mk
• Fn::Cidr
• Fn::GetAZs 
• Fn::Select
• Fn::Split
• Fn::Transform 
• Fn::Length



--------------------------- 1  Intrinsic Functions – Fn::Ref

• The Fn::Ref function can be leveraged to reference

     • Parameters – returns the value of the parameter

     • Resources – returns the physical ID of the underlying resource (e.g., EC2 ID)

     - it won’t work for conditions 
     

• The shorthand for this inYAML is !Ref

MyEIP:
  Type: "AWS::EC2::EIP"
  Properties:
    InstanceId: !Ref MyEC2Instance



--------------------------- 2 Intrinsic Functions – Fn::GetAtt


• Attributes are attached to any resources you create

• To know the attributes of your resources, the best place to look at is the documentation



-- The following example template returns the SourceSecurityGroup.OwnerAlias and SourceSecurityGroup.GroupName of the load balancer with the logical name myELB.


EG 

AWSTemplateFormatVersion: 2010-09-09
Resources:
  myELB:
    Type: AWS::ElasticLoadBalancing::LoadBalancer
    Properties:
      AvailabilityZones:
        - eu-west-1a
      Listeners:
        - LoadBalancerPort: '80'
          InstancePort: '80'
          Protocol: HTTP
  myELBIngressGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: ELB ingress group
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          SourceSecurityGroupOwnerId: !GetAtt myELB.SourceSecurityGroup.OwnerAlias
          SourceSecurityGroupName: !GetAtt myELB.SourceSecurityGroup.GroupName



-- in general So anytime we use Ref, we're going to get the reference ID out of it.

-- But if we use GetAtt to get an attribute, we have the option to get more information out of this EC2 instance. So here we have the AvailabilityZone. So for example, we can know in which AZ an instance was launched, for example, us-east-1b. You get the Id again,

-- you could get the PrivateDNSName, the PrivateIp, the PublicDNSName, and the PublicIp. So while the Ref gives you usually a reference to the ID of the resource you have created, the GetAtt allows you to get more out of the resource and you can only get what CloudFormation supports in terms of attributes that are defined in documentation







--------------------------- 3 Intrinsic Functions – Fn::FindInMap

• We use Fn::FindInMap to return a named value from a specific key

• !FindInMap [ MapName,TopLevelKey, SecondLevelKey ]


TopLevelKey: The top-level key name. Its value is a list of key-value pairs.

SecondLevelKey: The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.



Return value:

  The value that's assigned to SecondLevelKey.


EG 

Mappings: 
  RegionMap: 
    us-east-1: 
      HVM64: "ami-0ff8a91507f77f867"
      HVMG2: "ami-0a584ac55a7631c0c"
    us-west-1: 
      HVM64: "ami-0bdb828fd58c52235"
      HVMG2: "ami-066ee5fd4a9ef77f1"
    eu-west-1: 
      HVM64: "ami-047bb4163c506cd98"
      HVMG2: "ami-31c2f645"
    ap-southeast-1: 
      HVM64: "ami-08569b978cc4dfa10"
      HVMG2: "ami-0be9df32ae9f92309"
    ap-northeast-1: 
      HVM64: "ami-06cd52961ce9f0d85"
      HVMG2: "ami-053cdd503598e4a9d"
Resources: 
  myEC2Instance: 
    Type: "AWS::EC2::Instance"
    Properties: 
      ImageId: !FindInMap
        - RegionMap
        - !Ref 'AWS::Region'
        - HVM64
      InstanceType: m1.small






--------------------------- 4 Intrinsic Functions – Fn::ImportValue


• Import values that are exported in other stacks

• For this, we use the Fn::ImportValue function

-- The intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references. 

-- In the following example template snippets, Stack A exports VPC security group values and Stack B imports them.

EG :

Stack A Export


Outputs:
  PublicSubnet:
    Description: The subnet ID to use for public web servers
    Value:
      Ref: PublicSubnet
    Export:
      Name:
        'Fn::Sub': '${AWS::StackName}-SubnetID'
  WebServerSecurityGroup:
    Description: The security group ID to use for public web servers
    Value:
      'Fn::GetAtt':
        - WebServerSecurityGroup
        - GroupId
    Export:
      Name:
        'Fn::Sub': '${AWS::StackName}-SecurityGroupID'



Stack B Import


Resources:
  WebServerInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.micro
      ImageId: ami-a1b23456
      NetworkInterfaces:
        - GroupSet:
            - Fn::ImportValue: 
              'Fn::Sub': '${NetworkStackNameParameter}-SecurityGroupID'
          AssociatePublicIpAddress: 'true'
          DeviceIndex: '0'
          DeleteOnTermination: 'true'
          SubnetId: Fn::ImportValue: 
            'Fn::Sub': '${NetworkStackNameParameter}-SubnetID'



-- Declaration

   Fn::ImportValue: sharedValueToImport


-- Alternatively, you can use the short form:

   !ImportValue sharedValueToImport




------- Important to know : 

 - You can't use the short form of !ImportValue when it contains the short form of !Sub.

 # do not use
!ImportValue
  !Sub '${NetworkStack}-SubnetID' 


- Instead, you must use the full function name, for example:

Fn::ImportValue:
  !Sub "${NetworkStack}-SubnetID"







--------------------------- 5 Intrinsic Functions – Fn::Base64


• Convert String to it’s Base64 representation

     !Base64 "valueToEncode"


• Example: pass encoded data to EC2 Instance’s UserData property


-- The intrinsic function Fn::Base64 returns the Base64 representation of the input string. This function is typically used to pass encoded data to Amazon EC2 instances by way of the UserData property.


EG:

Resources:
  WebServerInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.micro
      ImageId: ami-a1b23456
        UserData:
          Fn::Base64: |
            #!/bin/bash
            # Use this for your user data (script from top to bottom)
            # install httpd (Linux 2 version)
            yum update -y
            yum install -y httpd
            systemctl start httpd





--------------------------- 6 Intrinsic Functions – Condition Functions


Conditions:
     CreateprodResource: !Equals [!Ref EnvType, prod]
  

• The logical ID is for you to choose. It’s how you name condition

• The intrinsic function (logical) can be any of the following:

   • Fn::And
   • Fn::Equals
   • Fn::If
   • Fn::Not
   • Fn::Or




Ref for other functions  : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-base64.html







-------------------------------------------------------------------- CloudFormation – Rollbacks


-- they're very important to understand at the exam.


• If Stack Creation Fails: you have two options.

     • Default: everything rolls back (gets deleted). We can look at the log

     • Option to disable rollback and troubleshoot what happened



• If it is a problem with Stack Update Fails:

     • The stack automatically rolls back to the previous known working state, again, deleting anything that was newly created.

     • Ability to see in the log what happened and error messages



• Rollback Failure? Fix resources manually then issue ContinueUpdateRollback API from Console

    • Or from the CLI using continue-update-rollback API call




----------------- LAB for failuers 


-- create one yaml file , make some wrong configurations in it , for eg : ImageId


ami-0a3c3a20c09d6f377


-- create stack with the yaml file which has wrong ami-id, it has 2 Security Groups


subbu1.yaml


---
Parameters:
  SecurityGroupDescription:
    Description: Security Group Description
    Type: String

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-123345
      InstanceType: t2.micro
      SecurityGroups:
        - !Ref SSHSecurityGroup
        - !Ref ServerSecurityGroup

  # an elastic IP for our instance
  MyEIP:
    Type: AWS::EC2::EIP
    Properties:
      InstanceId: !Ref MyInstance

  # our EC2 security group
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  # our second EC2 security group
  ServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Ref SecurityGroupDescription
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 192.168.1.1/32



-- choose Stack failure options = Preserve successfully provisioned resources , create stack

-- here it is failed to create resource 'coz , i have not given description for security group

-- but i did not give description for "ServerSecurityGroup" but the SSHSecurityGroup is created 

-- now u cannot update the stack , so delete the stack , when there is create failure , u should delete the stack and troubleshoot and upload again



now create another yaml file with correct configuration 


subbu2.yaml 


---
Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro


-- now upload this file , 


-- choose  Stack failure options = Roll back all stack resources

-- create stack , now the resource got created 

-- now update the stack by uploading the 1st example yaml file (subbu1.yaml) file 

-- here u will get update option , 'coz u have created resources successfully 

-- upload the 1st yaml file 

-- now here provide group description

-- choose Stack failure options = Roll back all stack resources

-- create stack 

-- here the securitygropus were created but due to wrong ami-id , it is failed to create , so as per the condition that we ahve given (rollback), so, that means that my server security group and SSH security groups should disappear.




-- now update the stack with the same yaml file subbu1.yaml , bit choose Stack failure options = Preserve successfully provisioned resources

-- This would create SSH and server security groups, but it would not roll them back in case there is a rollback happening, a stack failure.

-- So, this is up to you to choose what you want, but both the behaviors can be desirable based on what you're trying to do.









-------------------------------------------------------------------- CloudFormation – Service Role


-- So CloudFormation can use service roles. What are they?


• IAM role that allows CloudFormation to create/update/delete stack resources on your behalf

   - Well, they are iam roles that you create and they're dedicated to CloudFormation and they allow CloudFormation to actually create update and delete stack resources on your behalf.


• Give ability to users to create/update/delete the stack resources even if they don’t have permissions to work with the resources in the stack


• Use cases:

    • You want to achieve the least privilege principle

    • But you don’t want to give the user all the required permissions to create the stack resources


• User must have iam:PassRole permissions which is a necessary permission to give a role to a specific service in AWS.




----------- LAB IAm roles with CF


-- open IAM --> create new role --> aws service --> CF --> give s3 full access --> create role 


-- now create stack , upload subbu2.yaml file for demo and in the permission sections = select IAM role that we have created now ,

-- So if I don't specify, then iam role is going to use my own personal permissions. 

-- But if I want to specify an iam role, I can look at this DemoRole for CFN with S3 capabilities. AWS CloudFormation will use this role for all stack operations. Other users that have permissions to operate on this stack will be able to use this role, even if they don't have permission to pass it. Ensure that this role grants the least privilege.

-- but this one, and because this one is just powered with Amazon S3 permissions,then actually my stack will fail because my stack is actually creating an EC2 instance.






-------------------------------------------------------------------- CloudFormation – Capabilities


• We have  CAPABILITY_NAMED_IAM and CAPABILITY_IAM 
       
       - So they are capabilities you need to give to CloudFormation whenever your CloudFormation template is going to create or update IAM resources, such as when you create a IAM user, a role, a group, a policy, and so on, through your CloudFormation templates.

      • Necessary to enable when you CloudFormation template is creating or updating IAM resources (IAM User, Role, Group, Policy, Access Keys, Instance Profile...)

      • Specify CAPABILITY_NAMED_IAM if the resources are named, otherwise, just CAPABILITY_IAM.

      - And the reason we do so is that we want to explicitly acknowledge the fact that CloudFormation is going to create IAM resources.


• CAPABILITY_AUTO_EXPAND

     - which is when your CloudFormation template is including macro and nested stacks, so stacks within stacks, to perform dynamic transformations.

     • Necessary when your CloudFormation template includes Macros or Nested Stacks (stacks within stacks) to perform dynamic transformations

     • You’re acknowledging that your template may change before deploying



• InsufficientCapabilitiesException

     • Exception that will be thrown by CloudFormation if the capabilities haven’t been acknowledged when deploying a template (security measure)

     - while launching a template, that means that the CloudFormation templates was requiring capabilities, but you haven't acknowledged them.

     - So, as a security measure, you need to redo the templates, upload, and launch using, this time, these capabilities. It's just an extra argument in your API call, or a box to tick on your AWS console.



-------------- lab

-- create one capabilities.yaml file 



AWSTemplateFormatVersion: '2010-09-09'
Description: An example CloudFormation that requires CAPABILITY_IAM and CAPABILITY_NAMED_IAM

Resources:
  MyCustomNamedRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: MyCustomRoleName
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: [ec2.amazonaws.com]
            Action: ['sts:AssumeRole']
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2FullAccess
      Policies:
        - PolicyName: MyPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 's3:*'
                Resource: '*'

Outputs:
  RoleArn:
    Description: The ARN of the created IAM Role
    Value: !GetAtt MyCustomNamedRole.Arn



-- here CloudFormation template, which actually has an IAM role being created. And it's named, so there's a role name, MyCustomRoleName. And it's using a managed policy, which is the AmazonEC2FullAccess. So we're dealing with IAM, and so as such, we are creating some IAM roles.


-- now try to create this stack in console with this file 

-- u will have a checkbox at last while u try to create this stack , do check and submit , then only u will able to create this stack 

-- the role has been created . check in console 







--------------------------------------------------------------------  1 CloudFormation – DeletionPolicy Delete


• DeletionPolicy:
  
     •  DeletionPolicy is a setting you can apply to resources on your CloudFormation templates, which allows you to Control what happens when the CloudFormation template is deleted or when a resource is removed from a CloudFormation template

     • Extra safety measure to preserve and backup resources

• Default DeletionPolicy=Delete 

-- So by default, we've seen that when we delete a CloudFormation template, all the resources within are also deleted. That means that the default DeletionPolicy is delete, so you don't have to specify it because it is the default.

EG:

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
    DeletionPolicy: Delete  




EG 2 :

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  myS3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete


-- in the above example , We have an S3 buckets and we have DeletionPolicy=Delete.

-- It will work only if the S3 bucket is empty. If it's not empty, then the delete will fail. So the way to fix this if you wanted to would be to either manually delete everything within the S3 bucket and then continue with the deletion of your CloudFormation templates.

-- Or it would be for you to implement a custom resource to actually delete everything within the S3 bucket before automatically having the S3 bucket go away.






----------------------------- 2 CloudFormation – DeletionPolicy Retain

• DeletionPolicy=Retain: 

     • Specify on resources to preserve in case of CloudFormation deletes

     • Works with any resources




EG :

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  myS3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain



-- So example here, we have a S3, and we know that by default, it would be deleted when I delete my CloudFormation template, 

-- but maybe we actually wanna keep it, keep the data within because we care about the data of this bucket. And so we would specify at the bottom DeletionPolicy retain.

-- And so even if I delete my CloudFormation templates, this S3 Bucket would stay, and this works with any resources.





----------------------------- 3 CloudFormation – DeletionPolicy Snapshot


• DeletionPolicy=Snapshot

• Create one final snapshot before deleting the resource

• Examples of supported resources:

    • EBS Volume, ElastiCache Cluster, ElastiCache ReplicationGroup

    • RDS DBInstance, RDS DBCluster, Redshift Cluster, Neptune DBCluster, DocumentDB DBCluster



----------- LAB 

-- create one yaml file that has DeletionPolicy


Resources:
  MySG:
    Type: AWS::EC2::SecurityGroup
    DeletionPolicy: Retain
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  MyEBS:
    Type: AWS::EC2::Volume
    DeletionPolicy: Snapshot
    Properties:
      AvailabilityZone: us-east-1a
      Size: 1
      VolumeType: gp2



-- in above example , there is a security group and the DeletionPolicy is retained. So that means that if I delete my transformation stack, this security group should stay.

-- And there is an EBS volume, and the DeletionPolicy is a snapshot. So that means that upon deleting the stack, the volume should go away, but a snapshot should be created first.

-- let's see how it is working 

-- upload yaml file and create one stack 

-- once the resource is created , do delete the stack

-- u can observe that MySG = DELETE_SKIPPED , 'coz we have given retain 

-- ebs volume is deleted but the snapshot is already created for u go n check in console 

-- u can delete manually through console , if u want to delete 







--------------------------------------------------------------------  CloudFormation – Stack Policies


• During a CloudFormation Stack update, all update actions are allowed on all resources (default)


-- but sometimes, you may want to protect your stack against updates, or part of your stack against updates. This is where Stack policies come in.


• A Stack Policy is a JSON document that defines the update actions that are allowed on specific resources during Stack updates


EG:

{
  "Statement" : [
  {
    "Effect" : "Allow",
    "Principal" : "*",
    "Action" : "update:*",
    "Resource" : "*",
   },

  {
    "Effect" : "Deny",
    "Principal" : "*",
    "Action" : "Update:*",
    "Resource" : "LogicalResourceId/ProductionDatabase"
  }
  ]
}


-- So here, we have an example where the first statement is saying "Allow update*" on everything, meaning that everything in your CloudFormation Stack can be updated,

-- second part is saying "Deny update*" on Resource Production Database. That means that whatever is named "Production Database" in your CloudFormation Stack is going to be protected against any kind of updates,



• Protect resources from unintentional updates

• When you set a Stack Policy, all resources in the Stack are protected by default

• Specify an explicit ALLOW for the resources you want to be allowed to be updated






--------------------------------------------------------------------  CloudFormation – Termination Protection


• To prevent accidental deletes of CloudFormation Stacks, use TerminationProtection


-- once u create stack --> stack actions --> set TerminationProtection





--------------------------------------------------------------------  CloudFormation – Custom Resources


• Used to

     • define resources not yet supported by CloudFormation

     • define custom provisioning logic for resources can that be outside of CloudFormation (on-premises resources, 3rd party resources...)

     • have custom scripts run during create / update / delete through Lambda functions (Eg : running a Lambda function to empty an S3 bucket before being deleted)


• Defined in the template using

      - AWS::CloudFormation::CustomResource or 
      
      - Custom::MyCustomResourceTypeName (recommended)


• Backed by a Lambda function (most common) or an SNS topic




------------------------------- How to define a Custom Resource?


• Ser viceToken specifies where CloudFormation sends requests to, such as Lambda ARN or SNS ARN (required & must be in the same region)

• Input data parameters (optional)


EG :

Resources:
  MyCustomResourceUsingLambda:
   Type: Custom::MylambdaResource
   properties:
      ServiceToken: arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME
      #input values (optional)
      ExamplePreperty: "exampleValue"




------------------------------- Use Case – Delete content from an S3 bucket


• You can’t delete a non-empty S3 bucket

• To delete a non-empty S3 bucket, you must first delete all the objects inside it

• We can use a custom resource to empty an S3 bucket before it gets deleted by CloudFormation


EG : 

-- Whenever we run delete stack on CloudFormation, then your custom resource backed by Lambda function is going to run API calls to empty your S3 bucket.

-- when your S3 bucket is emptied, then CloudFormation is going to actually attempt to delete your S3 bucket, and everything will work.





---------------- LAB 


-- in this demo , we are going to create lambda function , s3 bucket , lambdaexecutionrole, and custom resource 

-- as per the documentation we can not delete the s3 bucket directly which has objects in it , but through the custom resources with lambda function , we can delete the s3 bucket directly

-- create one yaml files which will create an s3 bucket , CustomResourceLambdaFunction , CustomResourceLambdaExecutionRole , CustomResource



EG :  delete-resource.yaml


----
Resources:

    CustomResourceLambdaExecutionRole:
        Type: 'AWS::IAM::Role'
        Properties:
            AssumeRolePolicyDocument:
                Version: 2012-10-17
                Statement:
                    - Effect: Allow
                      Principal:
                          Service: lambda.amazonaws.com
                      Action:
                          - 'sts:AssumeRole'
            Policies:
                - PolicyName: LoggingPolicy
                  PolicyDocument:
                      Version: 2012-10-17
                      Statement:
                          - Effect: Allow
                            Action:
                                - logs:CreateLogGroup
                                - logs:CreateLogStream
                                - logs:PutLogEvents
                            Resource: '*'
                - PolicyName: S3Policy
                  PolicyDocument:
                      Version: 2012-10-17
                      Statement:
                          - Effect: Allow
                            Action:
                                - s3:List*
                                - s3:DeleteObject
                            Resource: '*'

    CustomResourceLambdaFunction:
        Type: 'AWS::Lambda::Function'
        Properties:
            Code:
                ZipFile: |
                    import cfnresponse
                    import boto3

                    def handler(event, context):
                        print(event)
                        print('boto version ' + boto3.__version__)

                        # Globals
                        responseData = {}
                        ResponseStatus = cfnresponse.SUCCESS
                        s3bucketName = event['ResourceProperties']['s3bucketName']

                        if event['RequestType'] == 'Create':
                            responseData['Message'] = "Resource creation successful!"

                        elif event['RequestType'] == 'Update':
                            responseData['Message'] = "Resource update successful!"

                        elif event['RequestType'] == 'Delete':
                            # Need to empty the S3 bucket before it is deleted
                            s3 = boto3.resource('s3')
                            bucket = s3.Bucket(s3bucketName)
                            bucket.objects.all().delete()

                            responseData['Message'] = "Resource deletion successful!"

                        cfnresponse.send(event, context, ResponseStatus, responseData)

            Handler: index.handler
            Runtime: python3.12
            Role: !GetAtt CustomResourceLambdaExecutionRole.Arn

    CustomResource:
        Type: Custom::CustomResource
        Properties:
            ServiceToken: !GetAtt CustomResourceLambdaFunction.Arn
            s3bucketName: !Ref S3Bucket
        DependsOn: S3Bucket

    S3Bucket:
      Type: AWS::S3::Bucket
      Properties:
        AccessControl: Private
        BucketName: custom-resource-s3-bucket1


-- once u upload this file this will create s3 bucket , CustomResourceLambdaFunction , CustomResourceLambdaExecutionRole , CustomResource 

-- now go to bucket and upoad some files in it , now do dleete the stack u can able to delete the s3 bucket (in this process , the custom resource will delete first and then remaing will delete after this )



-- suppose u have s3 bucket already and have files in it so , u have to change 2 items in the 'CustomResource' resource:

     1 - Change the `DependsOn` property to the logical name of the S3 bucket in your template

     2  - change the parameter passed to the custom resource to reference your S3 bucket.  So you should change this line:

                `s3bucketName: !Ref S3Bucket`

        - to this:

                `s3bucketName: !Ref <your s3 bucket logical ID>`


--  DependsOn: < logical name of ur bucket >

--  s3bucketName: !Ref <your s3 bucket logical ID>

-- this is how u can automate the process


-- it will take almost 30 min to give status 

-- once u delete the stack , u will 2 cases 

1 Delete a custom resource that's stuck in DELETE_FAILED status

2 Delete a custom resource that's stuck in DELETE_IN_PROGRESS status




1 Delete a custom resource that's stuck in DELETE_FAILED status


-- To delete your stack, complete the following steps:

1.    Open the CloudFormation console.

2.    Choose the stack that contains your custom resource that's stuck in DELETE_FAILED status.

3.    Choose Actions, and then choose Delete Stack.

4.    In the pop-up window that provides a list of resources to retain, choose the custom resource that's stuck in DELETE_FAILED status. Then, choose Delete.

5.    Choose Actions, and then choose Delete Stack.

The status of your stack changes to DELETE_COMPLETE.

Note: Your custom resource isn't a physical resource, so you don't have to clean up your custom resource after stack deletion.




2 Delete a custom resource that's stuck in DELETE_IN_PROGRESS status


-- To force the stack to delete, you must manually send a SUCCESS signal. The signal requires the ResponseURL and RequestId values, which are both included in the event that's sent from CloudFormation to Lambda.

-- open lambda --> monitoring --> choose log it has values

Received event: {
  "RequestType": "Delete",
  "ServiceToken": "arn:aws:lambda:us-east-1:111122223333:function:awsexamplelambdafunction",
  "ResponseURL": "https://cloudformation-custom-resource-response-useast1.s3.us-east-1.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A111122223333%3Astack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2%7CMyCustomResource%7Ce2fc8f5c-0391-4a65-a645-7c695646739?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170313T0212304Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=QWERTYUIOLASDFGBHNZCV%2F20190415%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=dgvg36bh23mk44nj454bjb54689bg43r8v011uerehiubrjrug5689ghg94hb",
  "StackId": "arn:aws:cloudformation:us-east-1:111122223333:stack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2",
  "RequestId": "e2fc8f5c-0391-4a65-a645-7c695646739",
  "LogicalResourceId": "MyCustomResource",
  "PhysicalResourceId": "test-MyCustomResource-1URTEVUHSKSKDFF",
  "ResourceType": "Custom::PingTester"

}


--  To send a SUCCESS response signal in the response object to the delete request, run the following command in your local command-line interface. Be sure to include the values that you copied from above values 

curl -H 'Content-Type: ''' -X PUT -d '{
    "Status": "SUCCESS",
    "PhysicalResourceId": "test-CloudWatchtrigger-1URTEVUHSKSKDFF",
    "StackId": "arn:aws:cloudformation:us-east-1:111122223333:stack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2
  ",
    "RequestId": "e2fc8f5c-0391-4a65-a645-7c695646739",
    "LogicalResourceId": "CloudWatchtrigger"
  }' 'https://cloudformation-custom-resource-response-useast1.s3.us-east-1.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A111122223333%3Astack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2%7CMyCustomResource%7Ce2fc8f5c-0391-4a65-a645-7c695646739?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170313T0212304Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=QWERTYUIOLASDFGBHNZCV%2F20190415%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=dgvg36bh23mk44nj454bjb54689bg43r8v011uerehiubrjrug5689ghg94hb
  '



-- u will able to delete successfully 





--------------------------------------------------------------------  CloudFormation – StackSets


• Create, update, or delete stacks across multiple accounts and regions with a single operation/template

• Target accounts to create, update, delete stack instances from StackSets

• When you update a stack set, all associated stack instances are updated throughout all accounts and regions

• Can be applied into all accounts of an AWS Organization

• Only Administrator account (or Delegated Administrator) can create StackSets


        https://www.youtube.com/watch?v=SC6o6FnVt-M







======================================================================= AWS Integration & Messaging =======================================================================

SQS, SNS & Kinesis




---------------------------- Section Introduction


• When we start deploying multiple applications, they will inevitably need to communicate with one another

• There are two patterns of application communication

     1) Synchronous communications  (application to application) 

     2) Asynchronous / Event based  (applicahon to queue to application)


• Synchronous between applications can be problematic if there are sudden spikes of traffic

• What if you need to suddenly encode 1000 videos but usually it’s 10?


• In that case, it’s better to decouple your applications,

   • using SQS: queue model
   • using SNS: pub/sub model
   • using Kinesis: real-time streaming model


• These services can scale independently from our application!





---------------------------------------------------------------------- 1 Amazon SQS What’s a queue?


-- SQS is a simple queuing service.

-- So we have an SQS queue, and it is going to contain messages. And to contain messages, well, something needs to send messages into our SQS queue and whatever sends a message into our SQS queue is called a producer.

-- So it's possible for us to have one producer, but also have more. You can have multiple producers sending many messages into an SQS queue. And the message could be whatever he wants. For example, it could be process this order, or process this video. Whatever message you create goes into the queue.

-- Then something needs to process the messages from the queue and receive them, and it's called consumers.

-- So consumers will poll the messages from the queue, so that means that they will ask the queue, do you have any message for me? And the queue says, yes, here it is. And the consumer will poll these messages and get some information.

-- And then with that message, it will process it and then delete it back from the queue. And you may have multiple consumers consuming messages from an SQS queue.

-- So a queuing service is here to be a buffer to decouple between your producers and your consumers.



               Producer -----------------(send messagaes)-----------------> SQS Queue -----------------(Poll messages)---------------> Consumer




--------------------------------------- Amazon SQS – Standard Queue



• Oldest offering (over 10 years old)

• Fully managed service, used to decouple applications


• Attributes:

     • Unlimited throughput, unlimited number of messages in queue

     • Default retention of messages: 4 days, maximum of 14 days

     • Low latency (<10 ms on publish and receive)

     • Limitation of 256KB per message sent


• Can have duplicate messages (at least once delivery, occasionally)

• Can have out of order messages (best effort ordering)





--------------------------------------- SQS – Producing Messages


-- So, messages that are up to 256 kilobytes are sent into SQS by producers. how does this happen?

-- Well, the producers will send the messages to SQS using an SDK, software development kits. And the API to send a message to SQS is called SendMessage.

• Produced to SQS using the SDK (SendMessage API)

• The message, it will be written, it will be is persisted in SQS until a consumer deletes it , which signifies that the message has been processed.

• Message retention: default 4 days, up to 14 days


• Example: send an order to be processed

    • Order id
    • Customer id
    • Any attributes you want


-- so you will send a message into the SQS queue with maybe some information, such as the order ID, the customer ID, and any attributes you may want. For example, the address, and so on.

-- And then your consumer, that is in application rights, will have to deal with that message itself.

• SQS standard: unlimited throughput




--------------------------------------- SQS – Consuming Messages


• Consumers (running on EC2 instances, servers, or AWS Lambda)...

• Poll SQS for messages (receive up to 10 messages at a time)

• Process the messages (example: insert the message into an RDS database)

• Delete the messages using the DeleteMessage API



 


--------------------------------------- SQS – Multiple EC2 Instances Consumers


• Consumers receive and process messages in parallel

--  so each consumer will receive a different set of messages by calling the poll function.

-- if somehow a message is not processed fast enough by a consumer, it will be received by other consumers, and so this is why we have at least once delivery.

• At least once delivery

• Best-effort message ordering

• Consumers delete messages after processing them

     -- when the consumers are done with the messages, they will have to delete them, otherwise, other consumers will see these messages.

-- if we need to increase the throughputs because we have more messages, then we can add consumers and do horizontal scaling to improve the throughput of processing.

• We can scale consumers horizontally to improve throughput of processing




--------------------------------------- SQS with Auto Scaling Group (ASG)


-- Well, that means that your consumers will be running on EC2 instances inside of an Auto Scaling group and they will be polling for messages from the SQS queue.

-- But now your Auto Scaling group has to be scaling on some kind of metric, and a metric that is available to us is the Queue Length. It's called "ApproximateNumberOfMessages."

-- It is a CloudWatch Metric that's available in any SQS queue. And we could set up an alarm, such as whenever the queue length go over a certain level, then please set up a CloudWatch Alarm, and this alarm should increase the capacity of my Auto Scaling group by X amount.

-- this will guarantee that the more messages you have in your SQS queue, maybe because there's a surge of orders on your websites, the more EC2 instances will be provided by your Auto Scaling group, and you will accordingly process these messages at a higher throughputs.





--------------------------------------- SQS to decouple between application tiers


-- the use case is to decouple between applications, so application tiers.

-- So, for example, let's take an example of an application that processes videos. 

-- We could have just one big application that's called a front-end that will take the request and whenever a video needs to be processed, it will do the processing and then insert that into an S3 bucket.

-- But the problem is that processing may be very, very long to do and it may just slow down your websites if you do this in the front-end here.

-- So instead, you can decouple your application here the request of processing a file and the actual processing of a file can happen in two different applications.

-- therefore, whenever you take a request to process a file, you will send a message into an SQS queue.

-- Now, when you do the request to process, that file will be in the SQS queue and you can create a second processing tier called the back-end processing application that will be in its own Auto-Scaling group to receive these messages, process these videos, and insert them into an S3 bucket.

-- So as we can see here with this architecture, we can scale the front-end accordingly, and we can scale the back-end accordingly as well, but independently.

-- because the SQS queue has unlimited throughputs and it has unlimited number of messages in terms of the queue, then you are really safe, and this is a robust and scalable type of architecture.




   requests ------------ fronted ---------- (SendMessage)----------> SQS Queue -----------(ReceiveMessages) ----------> Back-end processing Application(video processing) -------------(insert) -----> S3





--------------------------------------- Amazon SQS - Security


• Encryption:

    • So we have encryption in-flight encryption by sending and producing messages  HTTPS API

    • At-rest encryption using KMS keys

    • Client-side encryption if the client wants to perform encryption/decryption itself (It's not something that's supported by SQS)


• Access Controls: IAM policies to regulate access to the SQS API

• SQS Access Policies (similar to S3 bucket policies)

     • Useful for cross-account access to SQS queues

     • Useful for allowing other services (SNS, S3...) to write to an SQS queue




--------------------------------------- LAB for SQS standard queue


-- refer to Solution Architect course




-----------------------------------------------------  SQS Queue Access Policy


-- SQS Queue Access Policies. And there are two good use cases for SQS Queue Access Policies.

-- They're similar to S3 Bucket policies in terms that they are resource policies, so JSON IAM policies that you're going to add directly onto your SQS Queue.


----------------- 1st use case : Cross Account Access




Account 444455556666 (SQS Queue (queue1))---------------(poll for messages)-------> Account 111122223333(ec2 insances)




-- Say you have a queue in an account and another account needs to access that queue. Maybe it has an EC2 Instance.

-- So for that EC2 Instance to be able to pull message across accounts, what you need to do is to create a Queue Access Policy look like this , and you attach it to the SQS Queue in the first account.


{
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_Send_Receive",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "111122223333"
         ]
      },
      "Action": [
         "sqs:SendMessage",
         "sqs:ReceiveMessage"
      ],
      "Resource": "arn:aws:sqs:*:444455556666:queue1"
   }]
}


-- What this Queue access policy will do 

-- it will allow the principle of AWS to be 111122223333, which represents the account on the right hand side on the sqs ReceiveMessage on this resource right here.

-- so this Queue Access Policy is really what will allow your EC2 Instance to pull from the SQS Queue in another account.





----------------------- 2nd use case : Publish S3 Event Notifications To SQS Queue


-- for example, when you have an S3 Bucket, and it will publish event notifications to an SQS Queue.

-- for example, you upload an object into an S3 Bucket, and what you want is to get automatically a message sent to the SQS Queue.



      Upload object ------------------>  S3 Bucket (bucket1) ---------(Send message)-------------> SQS Queue (queue1)


-- As you can see the SQS Queue, wIll need to give permission to the S3 Bucket to write a message to it.



{
"Version": "2012-10-17", "Statement" : [{
"Effect": "Allow",
"Principal": { "AWS": "*"},
"Action": [ "sqs:SendMessage" ],
"Resource": "arn:aws:sqs:us-east-1:444455556666:queue1", "Condilon": {
"ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:bucket1" },
"StringEquals": { "aws:SourceAccount": "<bucket1_owner_account_id>" }, }
}] }


-- look at the details for example, the action is sqs:SendMessage,

-- the condition is that the sourceArn of the bucket represents the S3 Bucket named bucket1, 

-- and that the source accounts needs to be the account owner of the S3 buckets.

-- So once you have this, then the S3 bucket is allowed to write to an SQS Queue.




---------------------------------------------- LAB for s3 event notifications 


-- create standard queue , IMP : encryption = disable

-- now create one s3 bucket --> properties --> create event notificatin for all prefix and suffix  --> choose destination is SQS queue , try to create event notificatin

              link : https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html#step1-create-sqs-queue-for-notification

-- here , u will get an Error "Unable to validate the following destination configurations"

-- to resolve this error , we need to modify it to allow our S3 Bucket to write into our SQS Queue.

-- go to queue --> Access policy --> c.o info (Access policy (Permissions) ) --> in the last right corner down side u will get documentation link --> open that Walkthrough: Configuring SNS or SQS 

-- the SQS queue will look like this 



{
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
        {
            "Sid": "example-statement-ID",
            "Effect": "Allow",
            "Principal": {
                "Service": "s3.amazonaws.com"
            },
            "Action": [
                "SQS:SendMessage"
            ],
            "Resource": "SQS-queue-ARN",
            "Condition": {
                "ArnLike": {
                    "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1"
                },
                "StringEquals": {
                    "aws:SourceAccount": "bucket-owner-account-id"
                }
            }
        }
    ]
}



-- paste this sqs queue policy in the sqs standard queue that u have created

-- u have modify some items in this policy like   "Resource": "SQS-queue-ARN",  "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1" ,  "aws:SourceAccount": "bucket-owner-account-id"

-- once u change these , save policy  , now go to s3 and try to create event notification , now u will able to create successfully

-- now do upload any file in the s3 and check in SQS , u will get details of that object 










----------------------------------------------  SQS – Message Visibility Timeout


-- we have a consumer doing a ReceiveMessage request, and therefore, a message will be returned from the queue. Now the visibility timeout begins.

• After a message is polled by a consumer, it becomes invisible to other consumers

• By default, the “message visibility timeout” is 30 seconds

-- That means that during these 30 seconds, the message has to be processed, if you do so, that means that if the same or other consumers do a message request API call, then the message will not be returned.

• After the message visibility timeout is over, the message is “visible” in SQS

-- So effectively, during the visibility timeout, that message is invisible to other consumers.

-- But after the visibility timeout is elapsed and if the message has not been deleted, then the message will be put back into the queue and therefore, another consumer or the same consumer doing a receive message API call will receive the message again, the same message as before.



IMP to know 

• If a message is not processed within the visibility timeout, it will be processed twice Because it will be received by two different consumers, or twice by the same consumer.

-- And so, if a consumer is actively processing a message but knows that it needs a bit more time to process the message because otherwise, it will go out of the visibility timeout window, there is an API called "ChangeMessageVisibility."

• A consumer could call the ChangeMessageVisibility API to get more time

-- so if a consumer knows that a message needs a bit more time to be processed and you don't want to process that message twice, then the consumer should call the ChangeMessageVisibility API to tell SQS, hey, do not make that message visible for now, I just need a bit more time to process this message.

-- how do you set this message visibility timeout ?
   
   • If visibility timeout is high (hours), and consumer crashes, re-processing will take time

   - if you set it to a really, really high value by default, say, hours, and the consumer crashes, then it will take hours until this message reappears, re-becomes visible in your SQS queue, and that will take a lot of time.

   • If visibility timeout is too low (seconds), we may get duplicates

   - If you set it to something really, really low, like a few seconds, what happens that if the consumer doesn't end up having enough time to process that message for whatever reason, then it will be read many times by different consumers and you may get duplicate processing.


-- So the idea is that the visibility timeout should be set to something reasonable for your application and your consumer should be programmed that if they know they need a bit more time, then they should call the ChangeMessageVisibility API to get more time and increase the timeout of that visibility window.





-------------- LAB to observe Visibility Timeout 

-- create one std queue

-- open 2 windows , in 1st window send some message and go to 2nd window try to refresh ,

-- in 2nd window u can't get any messages 'coz it is in visibility timeout period , after 30 sec it will visible to U

-- the more u poll , the receive count will increase for same message 

--  u can also change visibility time , but default is better 






---------------------------------------------- Amazon SQS – Dead Letter Queue (DLQ)


• If a consumer fails to process a message within the Visibility Timeout...the message goes back to the queue!

• We can set a threshold of how many times a message can go back to the queue

• After the "MaximumReceives" threshold is exceeded, the message goes into a Dead Letter Queue (DLQ)

• Useful for debugging!

• DLQ of a FIFO queue must also be a FIFO queue

• DLQ of a Standard queue must also be a Standard queue

• Make sure to process the messages in the DLQ before they expire:

      • Good to set a retention of 14 days in the DLQ





---------------------------------------------- SQS DLQ – Redrive to Source 


• Feature to help consume messages in the DLQ to understand what is wrong with them

-- So you have your messages now you know they haven't been processed in the source queue and therefore they are in the Dead Letter Queue and you're going to do a manual inspection and debugging of these messages.

-- then you're going to fix your consumer code understand why the message wasn't processed in case the message was correct.

-- And then what you can do is re drive that message from the dead letter queue into the source SQS queue

-- what's going to happen with this is that the consumer can now reprocess that message without even knowing that the message went into the Dead Letter Queue and then the message processing has happened and as a cool feature. 

• When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches without writing custom code






---------------------------------------------- LAB DLQ 


-- create one std queue and one dead letter queue with all default values 

-- now go to std queue --> change visibility time = 5 , and add DLQ , Maximum receives = 3 (keep below 5 ) , do save this queue

-- go to std queue --> send message --> poll message 

-- So here we're receiving the messages once and after five seconds, we will receive it a second time. And after, again, five seconds, we're going to receive a third time,

-- stop polling now , try to poll for messages again , u can not see any messages 

-- go to DLQ --> poll for messages --> u will see that message 

-- let me show you how to redrive that message from the DLQ into this first queue.

-- go to DLQ queue --> start DLQ redrive 

-- go to main Std queue --> start polling for messagaes , u will get message from the DLQ 






---------------------------------------------- Amazon SQS – Delay Queue


• Delay a message (consumers don’t see it immediately) up to 15 minutes

• Default is 0 seconds (message is available right away), That means that as soon as you send the message into an SQS queue, the message will be available right away to be read 

• Can set a default at queue level , to say all the messages should be delayed by X number of seconds

• Can override the default on send using the DelaySeconds parameter 

    - every time you send a message you can set a per-message delay if you wanted to using the DelaySeconds parameter.


-- SQS Delay Queues is a period of time during which Amazon SQS keeps new SQS messages invisible to consumers. In SQS Delay Queues, a message is hidden when it is first added to the queue. (default: 0 mins, max.: 15 mins)



Producer--------------------(Send messages)-----> SQS queue ----------(Poll messages) -------------> Consumer



----------------------------- LAB for Delay Queue


-- create new Delayqueue 

-- set Delivery delay = 10 sec , so that messages will wait 10 seconds before being read by a consumer.

-- create Queue , now type some message and poll first , now c.o send message , u will get message after 10 sec 

-- so, as you can see, there was a delay between the send and the actual delivery of that message.





-------------------------------------------------------------------------------  Certified Developer concepts


---------------------------------------------- 1 Amazon SQS - Long Polling


• When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue

• This is called Long Polling

-- So why do we do long polling ?

• LongPolling decreases the number of API calls made to SQS while increasing the efficiency and decreasing the latency of your application.

-- we do long polling because we are doing less API calls into the SQS queue. And on top of it, we know that as soon as the message will arrive in the SQS queue, then the SQS queue will send it back to the consumer. 

-- So we are increasing the efficiency, because we do less API calls, so less CPU cycles are used. And we are also decreasing the latency because as soon as a message will be received by your SQS queue, it will be received by your consumer.

• The wait time can be between 1 sec to 20 sec (20 sec preferable)

• Long Polling is preferable to Short Polling

• Long polling can be enabled at the queue level or at the API level using "ReceiveMessageWaitTimeSeconds"


--------important thing that you need to understand the difference is in short polling Amazon SQS sends the response right away even if the query found is no message 

------- long pooling Amazon SQS asked you sends and empty response only if the polling wait time expires and that is why we use long polling if we want to save API call costs.





---------------------------------------------- 2 SQS Extended Client


• Message size limit is 256KB, how to send large messages, e.g. 1GB?


• Using the SQS Extended Client (Java Library) , which does something very simple that you could implement in any other language, but the idea is that it will use an Amazon S3 buckets as a repository for the large data.


IMP :


-- for example , your producer wants to send a large message into SQS, but first what's going to happen is that the actual large message will end up in Amazon S3,

-- what will be sent into your SQS queue is it will be a small metadata message that has a pointer to the larger message in your Amazon S3 buckets.

-- So the SQS queue will contain small messages, and your Amazon S3 bucket will contain large objects.

-- your consumer when it reads from the SQS queue using this library, the SQS extended clients, then it will consume this small metadata message,

-- which will say to the consumer, "Hey, go read "that bigger message out of Amazon S3," and the consumer will be able to read and retrieve large messages from S3.

-- So a typical use case for this is if you're processing video files, you don't send the entire video file into your SQS queue node, you upload that video file into your Amazon S3 bucket, 

-- and you send a small message with a pointer to that video file into your SQS queue. And that allows you to accommodate any message size really through this pattern.



----------- WORKFLOW Structure


        Producer -------> Small metadata message ------------> SQS Queue ------------> Small metadata message -----------> Consumer
           |                                                                                                                 |
           |                                                                                                                 |
           |                                                                                                                 |
           |                                                                                                                 |
            -------------(Send large message to S3)-----------> S3 Bucket ------------(Retrieve large message from S3)--------
   
                         



---------------------------------------------- 3 SQS – Must know API


-- we'll, you will maybe see some API calls given through by the exam. And so it's just normal API calls


• CreateQueue (MessageRetentionPeriod), DeleteQueue

    - So CreateQueue is used to create a queue, and you can use the argument "MessageRetentionPeriod" to set how long a message should be kept in a queue before being discarded.

    - DeleteQueue is used to delete a queue and delete all the messages in the queue at the same time.


• PurgeQueue: delete all the messages in queue 


• SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage

    - when we are sending messages, we use to SendMessage API, and if we want to send messages with a delay, we can use the DelaySeconds parameter.

    - ReceiveMessage is to do polling, and DeleteMessage is to delete a message once it has been processed by a consumer.

• MaxNumberOfMessages: default 1, max 10 (for ReceiveMessage API)

    - So when you receive a message, by default, the parameter MaxNumberOfMessages is set to 1. That means that you receive one message at a time, but you can receive up to 10 messages in a time in SQS. So you can set the max number of messages parameter for the ReceiveMessage API to 10 to receive a batch of messages from SQS.


• ReceiveMessageWaitTimeSeconds: Long Polling

    - telling your consumer how long to wait before getting a response for from the queue. And this is equivalent of enabling long polling. 
    

• ChangeMessageVisibility: change the message timeout

    - And the ChangeMessageVisibility, it is used to change the message timeout in case you need more time to process a message.


• if you want to use Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility this helps decrease the number of API calls you're doing into API and therefore helps decrease your costs



--------------------------- LAB for Long Polling in AWS 


-- go to std queue --> edit --> Receive message wait time = 20 (0 - 20 sec ) --> save 

-- this is saying that you should wait up to 20 seconds to receive a message if the queue is empty.

-- send and receive messagaes , here I'm gonna go ahead and start a consumer. Now this consumer is doing long polling, and so that means that only one API call is happening, and it's waiting for a message coming from the SQS queue because right now there is none.

-- But if I say say hello world and just press Send, as soon as I press Send, the message was received by my consumer.

-- This was extremely low latency, because my consumer was in long polling mode, and it was waiting for a message from SQS thanks to the WaitMessageTime setting that we set from before.





------------------------------------------------------------------------ Amazon SQS – FIFO Queue


• FIFO = First In First Out (ordering of messages in the queue)



    Producer --------(Send messages 4,3,2,1) ---------> SQS Queue -----------(Poll messages 4,3,2,1)------------> Consumer


-- Now, because we have such a constraint and guarantee about ordering, this SQS queue has limited throughput.

• Limited throughput: 300 msg/s without batching, 3000 msg/s with Batching

• Exactly-once send capability (by removing duplicates) 

• Messages are processed in order by the consumer

-- So FIFO queue is something you should see whenever you have decoupling, but also the need to maintain the ordering of messages, and also make sure that you are on this throughput's constraints that you are not sending too many messages into SQS,

-- the FIFO queue is ends with .fifo only while u create any FIFO queue 




---------------- LAB 


-- create our first FIFO Queue , with all default settings 

-- send message and observe the things 




---------------------------------------------- SQS FIFO – Deduplication


• De-duplication interval is 5 minutes

• Two de-duplication methods:

      • Content-based deduplication: will do a SHA-256 hash of the message body , And if the same message body is encountered twice, the same hash will be the same twice. And so therefore, the second message will be refused.

      • Explicitly provide a Message Deduplication ID directly while sending a message. And if the same deduplication ID is encountered twice then the message will be gone.



---------------------------------------------- SQS FIFO – Message Grouping


-- The second advanced concept we need to look at is message grouping.

• If you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order

• To get ordering at the level of a subset of messages, specify different values for MessageGroupID

      • The idea is that the Messages that share a common Message Group ID will be in order within the group

      • Each Group ID can have a different consumer ( So you can enable parallel processing!) on your SQS FIFO queue.


• Ordering across groups is not guaranteed


for example , And we're grouping messages into three groups: A, B, and C. 

-- Say we have message A1, A2, A3. Then we can have a consumer for the group A.

-- Then we have another group of messages: B1, B2, B3, B4. We can have another consumer for that group B.

--  for consumer group C , we have C1 and C2.

-- The idea is that, for example, maybe sometimes you don't need the total ordering of all the messages. But you want ordering of all the messages for a specific customer ID.

-- for that one specific customer ID, you can use this as your message group ID.
     
      

                   ----------------(A3, A2 , A1) --------------------> Consumer for Group “A”

      SQS FIFO     ----------------(b4, b3, b2 , b1) --------------------> Consumer for Group “B” 
                  
                   ----------------(c2 , c1) --------------------> Consumer for Group “C”





--------------------------- LAB 


-- go to FIFO queue --> edit --> enable contenct based deduplication --Save 

-- now try to send message with group ID , u willl receive message , now try to send message again u can't send 'coz the deduplication is occuring here , now change the contect with same group id , u will able to send the message 'coz we have changed our content 


IMP to know 

-- here during the polling period only u can able to delete the messages once it gets polled after u cannot delete in the FIFO queue(The receipt handle has expired.)

-- where u can delete in std queue

-- now try to delete during the polling time , u can able to delete the message 







---------------- the differences are , IMP to Know

1  Message Order

-- Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they are sent. Occasionally (because of the highly-distributed architecture that allows high throughput), more than one copy of a message might be delivered out of order.

-- FIFO queues offer first-in-first-out delivery and exactly-once processing: the order in which messages are sent and received is strictly preserved.

2  Delivery

-- Standard queues guarantee that a message is delivered at least once and duplicates can be introduced into the queue.

-- FIFO queues ensure a message is delivered exactly once and remains available until a consumer processes and deletes it; duplicates are not introduced into the queue.

3  Transactions Per Second (TPS)

-- Standard queues allow nearly-unlimited number of transactions per second.

-- FIFO queues allow to process up to 3000 messages per second per API action.

4  Regions

-- Standard queues are available in all the regions.

-- FIFO queues are currently available in limited regions only.

5  AWS Services Supported

-- Standard Queues are supported by all AWS services.

-- FIFO Queues are currently not supported by all AWS services like: CloudWatch Events, S3 Event Notifications, SNS Topic Subscriptions, Auto Scaling Lifecycle Hooks, AWS IoT Rule Actions, AWS Lambda Dead Letter Queues.






----------------------------------------------------------------- Amazon Simple Queue Service (Amazon SQS) temporary queues


-- Temporary queues help you save development time and deployment costs when using common message patterns such as request-response.

-- You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.

-- The following are the benefits of temporary queues:

   - They serve as lightweight communication channels for specific threads or processes.
   - They can be created and deleted without incurring additional costs.
   - They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.

-- To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client.

-- This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. 

-- The key concept behind the client is the virtual queue. 

-- Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. 

-- Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.





----------------------------------------------------------------- Amazon Simple Notification Service (SNS) --------------------------------------------------------------


• What if you want to send one message to many receivers?


Method 1 : Direct integration 

   - So you could have a direct integration where for example a buying service application could send an email notification, then send a message to a fraud service send a message to a shipping service and maybe even send a message into an SQS Queue.

   - This is cumbersome because every time you have to add a new receiving service. You need to create and write that integration.


Method 2 :  Pub / Sub (Publish-subscribe.)
    
   - The idea is that the buying service will send a message into an SNS topic which is publishing a message into a topic. And that topic will have many subscribers.

   - And each of the subscriber will be able to receive that message from the SNS topic and have it for their own.



---------------------------------------- Amazon SNS


• The “event producer” only sends message to one SNS topic

• As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications 

• Each subscriber to the topic will get all the messages (note: new feature to filter messages), except if you're using a feature to filter messages and it is possible as well.

• Up to 12,500,000 subscriptions per topic

• 100,000 topics limit , and you can increase that limit as well.


        SNS -----------------(publish)------------> SQS / Lambda / Kinesis Data Firehose / HTTP(S) Endpoints / Emails / SMS and Mobile Nonficanons 


Note: Kinesis Data Firehose is now supported, but not Kinesis Data Streams.




------------------------ SNS integrates with a lot of AWS services


• Many AWS services can send data directly to SNS for notifications

CloudWatch Alarms / AWS Budgets / Lambda / DynamoDB / S3 Bucket (Events) / Auto Scaling Group (Notifications) / RDS Events / AWS DMS (New Replic) / CloudFormation (State Changes) ------------(publish)--------> SNS



----------------------- Amazon SNS – How to publish


• Topic Publish (using the SDK)
  • Create a topic
  • Create a subscription (or many)
  • Publish to the topic

• Direct Publish (for mobile apps SDK)
  • Create a platform application
  • Create a platform endpoint
  • Publish to the platform endpoint
  • Works with Google GCM, Apple APNS, Amazon ADM...



----------------------  Amazon SNS – Security

• Encryption:
  • In-flight encryption using HTTPS API
  • At-rest encryption using KMS keys
  • Client-side encryption if the client wants to perform encryption/decryption itself

• Access Controls: IAM policies to regulate access to the SNS API

• SNS Access Policies (similar to S3 bucket policies)
  • Useful for cross-account access to SNS topics
  • Useful for allowing other services ( S3...) to write to an SNS topic 




-------------------------------- SNS + SQS: Fan Out


-- The idea is that you want a message to be sent to multiple SQS queues, but if you send them individually to every SQS queue, there can be problems associated with it.

-- For example, if your application crashes in between, if there are delivery failures, or if you add more SQS queues down the road. Therefore we want to use the fan-out pattern.

• The idea is that you're going to Push once in SNS, receive in all SQS queues that are subscribers

    - The idea is that you're going to push once an SNS topic and then you're going to subscribe as many SQS queues as you want to the SNS topic. These queues are subscribers and they will all receive the messages sent into SNS.

-- So for example, we have a buying service that wants to send messages to two SQS queues. What it will do instead, it will send one message into an SNS topic, 

-- and the queues are subscribers of that SNS topic so that the fraud service and the shipping service can read all messages from their own SQS queues.

• Fully decoupled, no data loss 

• SQS allows for: data persistence, delayed processing and retries of work

• Ability to add more SQS subscribers over time

• Make sure your SQS queue access policy allows for SNS to write

• Cross-Region Delivery: works with SQS Queues in other regions

    - we have cross-region delivery. So it's definitely possible for an SNS topic in one region to send messages to SQS queues in other regions if the security allows it.






------------------------------------------- Next, so how do we use this pattern for other purposes?



------------------------------------- 1 Application: S3 Events to multiple queues


• For the same combination of: event type (e.g. object create) and prefix (e.g. images/) you can only have one S3 Event rule

   - So there is a limitation of S3 event rules is that for a combination of event type, for example, an object is being created and a prefix, for example, images/, you can only have one S3 event rule.


----------- But what if you want to send the same S3 event notification to multiple SQS queues?


• If you want to send the same S3 event to many SQS queues, use fan-out

    

       S3 object created -----------(events)----------> A S3 ----------------> SNS Topic -----(FAN-OUT)------> SQS Queues / Lambda function 




------------------------------------- 2 Application: SNS to Amazon S3 through Kinesis Data Firehose


-- Another architecture is that you can send data directly from SNS to Amazon S3 through Kinesis Data Firehose.

-- So because SNS has a direct integration with KDF

• SNS can send to Kinesis and therefore we can have the following solutions architecture:

     

          Buying Service ----------------------> SNS Topic --------------- Kinesis Data Firehose ------------- Amazon S3 / Any supported KDF Destination





------------------------------------- 3 Amazon SNS – FIFO Topic


-- So we can apply the fan-out pattern to FIFO topics as well.

• FIFO = First In First Out (ordering of messages in the topic)

 
     Producer -----------(Send messages 4,3,2,1)-------------- SNS TOPIC --------------(Receive messages 4,3,2,1)-----------------> Subscribers SQS FIFO


• Similar features as SQS FIFO:

     • Ordering by Message Group ID (all messages in the same group are ordered)

     • Deduplication using a Deduplication ID or Content Based Deduplication


• Can have SQS Standard and FIFO queues as subscribers

• Limited throughput (same throughput as SQS FIFO)




----------------- SNS FIFO + SQS FIFO: Fan Out


• In case you need fan out + ordering + deduplication







------------------------------------- SNS – Message Filtering


• JSON policy used to filter messages sent to SNS topic’s subscriptions

• If a subscription doesn’t have a filter policy, it receives every message





---------------------------------------------------------- Kinesis Overview -------------------------------------------------


• Makes it easy to collect, process, and analyze streaming data in real-time

• Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data...

-- As long as the data is generated fast and in real time, this counts as a real time stream of data.



------------------ So there are four services that comprise Kinesis.


• Kinesis Data Streams: capture, process, and store data streams

• Kinesis Data Firehose: load data streams into AWS data stores

• Kinesis Data Analytics: analyze data streams with SQL or Apache Flink

• Kinesis Video Streams: capture, process, and store video streams




-------------------------------- 1 Kinesis Data Streams


-- Kinesis Data Streams is a way for you to stream big data in your systems.

-- So a Kinesis Data Stream is made of multiple shards, and shards are numbered. Number one, number two, all the way to number N.

-- And this is something you have to provision ahead of time. So when you start with Kinesis Data Streams, you're saying, hey, I want a stream with six shards. And so the data is going to be split across all the shards.

-- And the shards are going to be defining your stream capacity in terms of ingestion and consumption rates.

-- then we have producers. So producers send data into Kinesis Data Streams, and producers can be manyfold. like they could be Applications / Client / Kinesis Producer Library (KPL)/ Kinesis Agent

-- So all these producers do the exact same thing. They rely on the SDK at a very, very low level, and they're going to produce records into our Kinesis Data Stream.



---RECORD : So a record, at its fundamental, is made of two things, it's made of a partition key and it is made of the data blob, or the value that is up to one megabytes.

    - So the partition key will define and help determine in which shard will the record go to.       

    - And the data blob is the value itself. (Up to 1 MB)

    - So when you have the producers sending data to Kinesis Data Streams, they can send data at a rate of one megabytes per second, or a thousand messages per second, per shard.
 
    - So if you have six shards, you get six megabytes per second, or 6,000 messages per second, overall,



-- Now, once the data is in Kinesis Data Streams, it can be consumed by many consumers, and these consumers, again, can have many forms like Apps (KCL, SDK) / Lambda / Kinesis Data Firehose / Kinesis Data Analytics


-- So when the consumer receives a record, it receives, again, the partition key, also a sequence number which represents where the record was in the shard, as well as the data blob, so the data itself.


--- Now we have different consumption modes for Kinesis Data Streams.

     - We have two megabytes per second of throughput shared for all the consumers, per shard,

                                 OR

     - you get two megabytes per second, per shard, per consumer if you are enabling the enhanced consumer mode, the enhanced fan-out.



-- So again, producers send data to Kinesis Data Streams. It stays in there for a while, and then it is read by many different consumers.




WORKFLOW : 


  Producers --------------------------------------------(Record , 1 MB/sec or 1000 msg/sec per shard) -----------> KDS (it has shards)-----------------(Record, 2 MB/sec (shared) Per shard all consumers or 2 MB/sec (enhanced) Per shard per consumer)------------------> consumers (Apps (KCL, SDK) / Lambda / Kinesis Data Firehose / Kinesis Data Analytics) 

   Applications / Client                                   Partition Key and Data Blob (upto1MB)                                                                       Partition Key , Sequence no.,  Data Blob 
   / Kinesis Producer Library (KPL)                                                                                                                                            
   / Kinesis Agent     



Question : You have a Kinesis data stream with 6 shards provisioned. This data stream usually receiving 5 MB/s of data and sending out 8 MB/s. Occasionally, your traffic spikes up to 2x and you get a ProvisionedThroughputExceededException exception. What should you do to resolve the issue?

ANS : Add more shards

EXP : The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity.





---------------------------------------------------------------- some properties of Kinesis Data Streams.


• Retention between 1 day to 365 days

• Ability to reprocess (replay) data

• Once data is inserted in Kinesis, it can’t be deleted (immutability)

• Data that shares the same partition goes to the same shard (ordering)

• Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent

• Consumers:

     • Write your own: Kinesis Client Library (KCL), AWS SDK

     • Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics,




---------------------------------------------------------------- Kinesis Data Streams – Capacity Modes


• Provisioned mode:
  • You choose the number of shards provisioned, scale manually or using API
  • Each shard gets 1MB/s in (or 1000 records per second)
  • Each shard gets 2MB/s out (classic or enhanced fan-out consumer) 
  • You pay per shard provisioned per hour

• On-demand mode:
  • No need to provision or manage the capacity
  • Default capacity provisioned (4 MB/s in or 4000 records per second)
  • Scales automatically based on observed throughput peak during the last 30 days
  • Pay per stream per hour & data in/out per GB




---------------------------------------------------------------- Kinesis Data Streams Security 


-- it is deployed within a region. And so you have your shards.

• Control access / authorization using IAM policies

• Encryption in flight using HTTPS endpoints

• Encryption at rest using KMS

• You can implement encryption/decryption of data on client side (harder)

• VPC Endpoints available for Kinesis to access within VPC

• Monitor API calls using CloudTrail




---------------------------------------------------------------- Kinesis Producers 


• Puts data records into data streams

• Data record consists of:

     • Sequence number (unique per partition-key within shard)
     • Partition key (must specify while put records into stream)
     • Data blob (up to 1 MB)

• Producers:

     • AWS SDK: simple producer
     • Kinesis Producer Library (KPL): C++, Java, batch, compression, retries
     • Kinesis Agent: monitor log files


• Write throughput: 1 MB/sec or 1000 records/sec per shard

• PutRecord API : the API to send data into Kinesis is called the PutRecord API.

• Use batching with PutRecords API to reduce costs & increase throughput 



IMP to Know :


-- while you produce to Kinesis data streams with a partition key.

-- if one device is very chatty and sends a lot of data it may overwhelm a shard. Also, you need to make sure your partition key is very well distributed to avoid the concept of a hot partition,

-- because then you will have one shard that's going to have more throughputs than the others and they will bring some imbalance.

-- So you need to think about a way to have a distributed partition key.

-- For example, if you have six shards and 10,000 users the user ID is very distributed.

-- But if you have six shards and you just look at Chrome, Firefox and Safari as web browser and the name of the web browser as a partition key, 

-- then it's going to be very hot maybe for Chrome, because there are many, many Chrome users in the world versus Firefox or Safari.


        Use highly distributed partition key to avoid “hot partition”




-------------- Kinesis - ProvisionedThroughputExceeded


-- So when we produce to Kinesis data streams from our applications, we know that we can produce 1 megabyte per second or 1,000 records per second and so as long as we do so it goes well.

-- But if we start over-producing into a shard we're going to get an exception because we are going over the provision throughput.

-- So we get the ProvisionedThroughputExceeded exception.


-- the solution for this exception is 

      • Use highly distributed partition key

      • Retries with exponential backoff , to ensure that we can retry these exceptions

      • Increase shards (scaling) , you need to scale the shard, maybe it's called shard-splitting to split a shard into and augment the throughputs , by increasing the number of shards this can help address throughput exceptions 






---------------------------------------------------------------- Kinesis Data Streams Consumers


• Get data records from data streams and process them

-- consumers can be 

• AWS Lambda
• Kinesis Data Analytics
• Kinesis Data Firehose
• Custom Consumer (AWS SDK) – Classic or Enhanced Fan-Out
• Kinesis Client Library (KCL): library to simplify reading from data stream




---------------------------------------------------------------- Kinesis Consumers – Custom Consumer



(look picture at pdf for better understanding )



1 Shared (Classic) Fan-out Consumer  (Pull Model)


-- So when you write your consumer in the classic shared throughput way you have your Kinesis Data Stream with a lot of shards and you get two megabytes per second per shard across all consumers.

-- That means that if you look at shard, number one just for this example we can write a Consumer Application A that is going to issue a get records API call to get records from the shard one, 

-- but as possible for us to have many different applications reading from the same Kinesis Data Stream.

-- So consumer Application B will also issue get records API calls, 

-- the Consumer Application C will also issue get records across all consumers.

-- Now, what happens in this instance is that they are all sharing two megabytes per second per shard across all consumers.

-- That means that in this instance we have three consumers sharing, two megabytes per second. That means that each consumer can get a maximum of approximately 666 kilobytes per second of data.

-- So as you can see, there's a limit to how many consumers we can have in the more consumer applications we add onto Kinesis Data Stream. The more we're put limitation we're going to have.




2  Enhanced Fan-out Consumer (Push Model)


-- So in this case we get two megabytes per second, per consumer, per shard. So it's not across all consumers it's per consumer, per shard.

-- So that means that the Consumer Application A will use a new API code called Subscribe to Shard and this will make the shard, send the data push the data into our Consumer Application A at the rate of two megabytes per second.

-- If the second Consumer Application B issues another subscribed to shard then this Consumer Application B as well. Will get the data pushed by the shard into the Consumer Application at the rate of two megabytes per second.

-- So as we can see here we have three consumer applications and we get six megabytes per second of throughputs for this one shard.





----------------------------------------------------------- Kinesis Consumers Types Difference





Shared (Classic) Fan-out Consumer - pull                                                    Enhanced Fan-out Consumer - push


• Low number of consuming applications                                                 • Multiple consuming applications for the same stream

• Read throughput: 2 MB/sec per shard across all consumers                             • 2 MB/sec per consumer per shard

• Max. 5 GetRecords API calls/sec  per shard                                                   

• Latency ~200 ms                                                                      • Latency ~70 ms

• Minimize cost ($)                                                                    • Higher costs ($$$)

• Consumers poll data from Kinesis using GetRecords API call                           • Kinesis pushes data to consumers over HTTP/2 (SubscribeToShard API)

• Returns up to 10 MB (then throttle for 5 seconds) or up to 10000 records             • Soft limit of 5 consumer applications (KCL) per data stream (default)





----------------------------------------------------------- Kinesis Consumers – AWS Lambda


-- it is a way for you to consume data without using servers.

-- So we have Kinesis Data Stream and say has three shards. so we are going to have lambda functions and their role will be to process records and save the record into dynamodb which is a serverless database.

-- So the Lambda functions are going to call get batch onto the Kinesis Data Stream.

-- And the data is going to be sent to lambda the functions by partition key to be processed.

-- The Lambda functions can then send data into dynamodb and we have a way to process our Kinesis Data Stream using a serverless mechanism.


• Supports Classic & Enhanced fan-out consumers

• Read records in batches

• Can configure batch size and batch window

• If error occurs, Lambda retries until succeeds or data expired

• Can process up to 10 batches per shard simultaneously





------------------------------------------------------------- LAB for Kinesis Data Streams 


#!/bin/bash

# get the AWS CLI version
aws --version

# PRODUCER

# CLI v2
aws kinesis put-record --stream-name test --partition-key user1 --data "user signup" --cli-binary-format raw-in-base64-out

# CLI v1
aws kinesis put-record --stream-name test --partition-key user1 --data "user signup"


# CONSUMER 

# describe the stream
aws kinesis describe-stream --stream-name test

# Consume some data
aws kinesis get-shard-iterator --stream-name test --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON

aws kinesis get-records --shard-iterator <>





-- open Kinesis --> choose data streams --> Data stream capacity  = provisioned (1 shard is enough for demo ) ----> create stream

-- in the data streams section , u will able to see producers and consumers and monitoring also 

-- let's keep it simple. We just want to write and read from our stream,

-- so therefore, we're going to use the SDK for producing and for consuming.

-- to do so , open cli in AWS Console 

-- As an alternative, you could be using your own terminal or CLI if it's preconfigured,

-- we have follow some commands to do this LAB 

-- 1st command 

          aws --version

-- u will get version of ur AWS CLI

-- 2nd cmd 

          aws kinesis put-record --stream-name test --partition-key user1 --data "user signup" --cli-binary-format raw-in-base64-out


-- here instead of test (replace ur data stream name ) , press enter , u will get 


-- {
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49651005991449160444837921106981744793678338524213936130"
}


-- u will get this ID's of shard and sequence

-- do same for login and logout messages 

--  aws kinesis put-record --stream-name mystream --partition-key user1 --data "user login" --cli-binary-format raw-in-base64-out


--  aws kinesis put-record --stream-name mystream --partition-key user1 --data "user logout" --cli-binary-format raw-in-base64-out



-- if you waited a little bit, and went into monitoring, and look at the stream metrics,

-- So next we want to be able to consume from Kinesis Data Stream.

-- So to do so, we're going to first describe the stream, to get some information around what this stream is made of, because we need to be able to consume from a specific shard.

--  3 rd cmd # describe the stream 
 
         aws kinesis describe-stream --stream-name test

-- change test , put ur datastream name 

-- as you can see here, we have this StreamDescription. 

-- We have one shard called shardId-0000000000000, and so we need to keep this in our mind. To be able to read from this stream.

IMP : 

-- so when you use the CLI, the SDK at a very, very low level, you need to specify from which shard you are reading from.

-- But if you are using Kinesis Client Library, all of this is handled for you by the library itself.

-- But we are using the CLI, so we have to specify the shard ID.




-- 4th cmd  # Consume some data

        aws kinesis get-shard-iterator --stream-name test --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON


-- "ShardIterator": "AAAAAAAAAAGSYiojmBSCJQpNaQz0qWSwWlj8yGA5j6QASza3PdcI4rAGHwggraCdkYpPqT47zlgorq92fYLSsrhfQuckhtteKHuzZ6O1AYq9P5G1dOyh4Xd4CDxEecYe0YZBiYlrR3/ySB1xt2IkjHIibWcyvyaQBrzHexR0d5jj8sY0dpSPkYUh4YlBkSodUSoIFqbywWwJQRmn1eqSKjZWEIExBGtIWF1WKkNVTB1qZKlUrarQpg=="

-- u will get like this 


-- here , we have the shard-iterator-type is TRIM-HORIZON.this means that you're going to read from the very beginning of the stream, so it will read all the records that were sent for from the beginning.

-- this ShardIterator can be reused to consume records,


-- 5th cmd 

       aws kinesis get-records --shard-iterator <"ShardIterator" content>


eg :  aws kinesis get-records --shard-iterator "AAAAAAAAAAGSYiojmBSCJQpNaQz0qWSwWlj8yGA5j6QASza3PdcI4rAGHwggraCdkYpPqT47zlgorq92fYLSsrhfQuckhtteKHuzZ6O1AYq9P5G1dOyh4Xd4CDxEecYe0YZBiYlrR3/ySB1xt2IkjHIibWcyvyaQBrzHexR0d5jj8sY0dpSPkYUh4YlBkSodUSoIFqbywWwJQRmn1eqSKjZWEIExBGtIWF1WKkNVTB1qZKlUrarQpg=="


-- So this consumption mode I'm doing right now, by using the low level API, describing the stream, getting a ShardIterator, and getting records is using the shared consumption mode.

-- This is not using enhanced Fan-Out,which in my opinion, should be using the Kinesis Client Library, Consumer Library for you to really leverage, and have a nice API to do so.


-- once u enter this u will get all the records like this 


  "Records": [
        {
            "SequenceNumber": "49651005991449160444837921106981744793678338524213936130",
            "ApproximateArrivalTimestamp": "2024-04-11T02:57:35.479000+00:00",
            "Data": "dXNlciBzaWdudXA=",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921106982953719497975830815965186",
            "ApproximateArrivalTimestamp": "2024-04-11T03:03:05.345000+00:00",
            "Data": "dXNlciBsb2dpbg==",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921106984162645317591353343868930",
            "ApproximateArrivalTimestamp": "2024-04-11T03:03:18.207000+00:00",
            "Data": "dXNlciBsb2dvdXQ=",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921107080876710886870538971512834",
            "ApproximateArrivalTimestamp": "2024-04-11T03:29:42.186000+00:00",
            "Data": "dXNlciBzaWdudXA=",
            "PartitionKey": "user1"
        },
        {
            "SequenceNumber": "49651005991449160444837921107082085636706486336377323522",
            "ApproximateArrivalTimestamp": "2024-04-11T03:29:59.749000+00:00",
            "Data": "dXNlciBsb2dpbg==",
            "PartitionKey": "user1"
        },

  ]



-- we get a batch of records out of it. So we used to have record one right here, which is PartitionKey user1.

-- moe open in google , base64 data decode online , decode the data 



IMP : it will expire after 5 min ,  If u get timeout error like this , do this again from step 1 within below 5 min

-- An error occurred (ExpiredIteratorException) when calling the GetRecords operation: Iterator expired. The iterator was created at time Thu Apr 11 03:18:15 UTC 2024 while right now it is Thu Apr 11 03:25:34 UTC 2024 which is further in the future than the tolerated delay of 300000 milliseconds.
         





---------------------------------------------------------------- Kinesis Client Library (KCL)


• A Java library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload

• Each shard is to be read by only one KCL instance

     • 4 shards = max. 4 KCL instances

     • 6 shards = max. 6 KCL instances


IMP : When using Kinesis Client Library, each shard is to be read-only by one KCL instance. So, if you have 10 shards, then the maximum KCL instances you can have is 10.



--------------------------- how the Kinesis Client Library works ?


• Progress is checkpointed into DynamoDB (needs IAM access) 

    - So, the Kinesis Client Library will be reading from our Kinesis Data Stream and the progress of how far it's been reading is going to be checkpointed into DynamoDB, and so your application running KCL will need IAM access to DynamoDB.


• Track other workers and share the work amongst shards using DynamoDB

    - It will be able, thanks to DynamoDB to track the other workers of your KCL application and share the work among shards.


• KCL can run on EC2, Elastic Beanstalk, and on-premises as long as they have correct IAM credentials.

• Records are read in order at the shard level

-- there are two versions of the Kinesis Client Library,

     • KCL 1.x (supports shared consumer)

     • KCL 2.x (supports shared & enhanced fan-out consumer)




------------------------------------ KCL Example: 4 shards


look at picture in pdf for better understanding 



-- if we look at an example of 4 shards into our stream, we can have a DynamoDB table to check on the progress,

-- so we can run two KCL apps of the same coherent application running on two different EC2 instances.

-- in this case, thanks to DynamoDB, they will know how to share the work, so the first KCl app is going to be reading from shard 1 and 2, and the second KCL app is going to be reading from shard 3 and 4.

-- Now, the progress of how far the app has been reading into the Kinesis Data Stream will be checkpointed into DynamoDB.

-- And so, for example, if one of these application goes down, DynamoDB and KCL apps working together, will know that an app will go down, and so reading from the other shards will be resumed from where it was checkpointed.




------------------------------------ KCL Example: 4 shards, Scaling KCL App


-- if you have 4 shards and now you run 4 KCL applications, then it will be each reading from one shard.

-- And therefore the progress will be resumed from DynamoDB and checkpointed again.

-- But we can not have more KCL apps than shards, because well, otherwise one will be doing nothing.




------------------------------ KCL Example: 6 shards, Scaling Kinesis


-- So if you want to read to scale Kinesis, you can scale Kinesis and add 6 shards, so now we still have our 4 KCL applications, but now we have six shards in Kinesis in the streams.

-- And so again, they will detect this change, and working together with DynamoDB, they will again, split the work between each KCL application and the shard assignments.




--------------------------------- KCL Example: 6 shards, Scaling KCL App


-- So that means that once we have 6 shards Kinesis Data Stream then we can have 6 KCL applications reading from them, and checkpointing the progress into DynamoDB.





---------------------------------------------------------------- Kinesis Operation – Shard Splitting 



• Used to increase the Stream capacity (1 MB/s data in per shard)

    - So Shard Splitting is used to split a shard into two. So that means we have more shards and so it's used to increase the Stream capacity.

    - So if we add more shards by splitting a shard we get an extra one megabyte per second of throughput per shard.


• Used to divide a “hot shard”

    - So it's used for example, when you want to divide a hot shard,

    - for example , we have 3 shards into our KDS , here shard 2 is very hot, a lot of data is sent to it.

    - here , We're going to split the shard and we're gonna get shard 4 and 5 out of it

    - and by having the new shards, we've increased our stream throughput from three megabytes per second to four megabytes per second.

    - So we've increased the capacity but because we are being built in terms of how many shards we have in our stream we've also increased the cost of our Kinesis Data Stream .


• The old shard is closed and will be deleted once the data is expired

    - The old shard is going to be closed or Shard two will not be written two anymore and the old data will be expired after some time.

    - So depending on what your retention period is between one to 365 days, you will have to wait this long.
  
    - Then the data is expired and then finally, the shard will be deleted.


• No automatic scaling (manually increase/decrease capacity)

    - There is no auto-scaling in Kinesis Data Streams although you can find a way to create your own auto-scaling and there is a solution architecture for that 
    
    - but there's no setting to do auto-scaling with Kinesis Data Streams and so it is for you to manually increase or decrease capacity.


• Can’t split into more than two shards in a single operation




---------------------------------------------------------------- Kinesis Operation – Merging Shards


-- the opposite operation of Shard Splitting is Merging Shards .

• So you realize that you want to Decrease the Stream capacity and save costs

• Can be used to group two shards with low traffic (cold shards) , you're going to merge them together into a new shard.

-- So in this example, Shard one and four are merged into Shard six and this is used to decrease capacity and the cost. (look at picture for better understanding from pdf)

• Old shards are closed and will be deleted once the data is expired

• Can’t merge more than two shards in a single operation

-- with Shard Splitting and Shards Merging you've seen how to scale up and down Kinesis.







---------------------------------------------------------------------- Kinesis Data Firehose (delivery stream)



-- So it is a very helpful service that can take data from producers. And producers can be everything we've seen for Kinesis Data Stream, so applications, clients, SDK, or the Kinesis agents can all produce into your Kinesis Data Firehose.

-- But also, a Kinesis Data Stream can produce into Kinesis Sata Firehose. Amazon CloudWatch logs and events can produce into Kinesis Data Firehose.

-- all these applications are going to send records into Kinesis Data Firehose, and then Kinesis Data Firehose can optionally choose to transform the data using a Lambda function, but this is optional.

-- once the data is transformed optionally, then it can be written in batches into destinations.

-- So Kinesis Data Firehose take data from sources. Usually the most common is going to be Kinesis Data Streams and it's going to write this data into destinations without you writing any kind of code, because Kinesis Data Firehose knows how to write data.



---------------------- So there are three kinds of destinations with Kinesis Data Firehose.


1 AWS Destinations :

    1 the first one is Amazon S3, so you can write all your data into Amazon S3.

    2 The second one is Amazon Redshift, which is a warehousing database. to do so, it first writes the data into Amazon S3. And then Kinesis Data Firehose will issue a copy command, and this copy command is going to copy data from Amazon S3 into Amazon Redshift.

    3 Amazon OpenSearch


2 There are also some third party partner destinations.

    - So Kinesis Data Firehose can send data into Datadog, Splunk, New Relic, MongoDB. And this list can get bigger and bigger over time,


3 Custom Destinations

    - if you have your own API with an HTTP endpoint, it is for you to send data from Kinesis Data Firehose into a custom destination.



-- so once the data is sent into all these destinations, you have two options.

      - You can also send all the data into an S3 bucket as a backup or just send the data that was failed to be written into these destinations into a failed S3 buckets. 



• Fully Managed Service, no administration, automatic scaling, serverless

    • AWS: Redshift / Amazon S3 / OpenSearch
    • 3rd party partner: Splunk / MongoDB / DataDog / NewRelic / snowflake ...
    • Custom: send to any HTTP endpoint


• Pay for data going through Firehose

• Near Real Time - Why?

     - because we write data in batches from Firehose to the destination.

     • Buffer interval: 0 seconds (no buffering) to 900 seconds

     • Buffer size: minimum 1MB

     - So in case you have buffering, well, that makes Kinesis Data Firehose a near real-time service.

     - And if you don't have buffering, if you have zero seconds for your buffer interval, it's still going to be considered near real-time because going to take a few seconds to deliver your data into your destination.



• Supports many data formats, conversions, transformations, compression

• Supports custom data transformations using AWS Lambda

• Can send failed or all data to a backup S3 bucket 




-------------------------------------------------------------- diff b/w Kinesis Data Streams vs Firehose



          Kinesis Data Streams                                                           Kinesis Data Firehose


• Streaming ser vice for ingest at scale                                      • Load streaming data into S3 / Redshift / OpenSearch / 3rd party / custom HTTP / snowflake


• Write custom code (producer /consumer)                                      • Fully managed


• Real-time (~200 ms)                                                         • Near real-time


• Manage scaling (shard splitting / merging)                                  • Automatic scaling

• Data storage for 1 to 365 days                                              • No data storage

• Supports replay capability                                                  • Doesn’t support replay capability






--------------------------------------------------------------  Kinesis Data Firehose 


-- create KDF ---> source = KDS --> destinations = s3 --> select KDS ARN ---> buffer interval = 60 sec for demo --> create bucket and choose that bucket as destination

-- create KDF,  reamin all are default 

-- here we have our producer is KDS , so go to KDS and create records as we did before like for signup , login and logout

-- so three records have been sent. And what I can do now is go into Amazon S3 and see if they have appeared in Amazon S3.

-- currently, there are zero objects in my bucket. That's because the Kinesis Data Firehose has a buffer of 60 seconds. So we need to wait 60 seconds until the data makes it into Amazon S3.

-- after 60 sec , it will store into s3 bucket , open n see 

-- now delete the resources 




-------------------------------------------------------------- Kinesis Data Analytics (Managed Apache Flink)


-- there are two flavors of it.


--------------- 1 Kinesis Data Analytics for SQL applications


-- So it sits in the center. And the two data sources that it's able to read from are Kinesis Data Streams and Kinesis Data Firehose.

-- So you can read from either of those, and then you can apply SQL statements to perform your real-time analytics.

-- It's also possible for you to join some reference data by referencing it from an Amazon S3 bucket. This will, for example, allow you to enrich the data in real-time.

-- Then you can send data to various destinations, and there are two of them.

      1 The first one is a Kinesis data stream. So you can create a stream out of a Kinesis Data Analytics real-time query, or  
      
      2 you can send it directly into Kinesis Data Firehose, 
      
      
-- each with their own use cases.If you send directly into Kinesis Data Firehose, then you can send into Amazon S3, Amazon Redshift, or Amazon OpenSearch, or any other Firehose destinations.

-- Whereas if you send it into a Kinesis data stream, you can do real-time processing of that stream of data using AWS Lambda or whatever applications you are running on EC2 instances.

-- remember the diagram(structure, get picture from pdf )

-- this is for Kinesis Data Analytics for SQL Applications.


• Real-time analytics on Kinesis Data Streams & Firehose using SQL

• Add reference data from Amazon S3 to enrich streaming data

• Fully managed, no servers to provision

• Automatic scaling

• Pay for actual consumption rate

• Output:

      • Kinesis Data Streams: create streams out of the real-time analytics queries

      • Kinesis Data Firehose: send analytics query results to destinations

• Use cases:

    • Time-series analytics 
    • Real-time dashboards 
    • Real-time metrics




------------------  2 Kinesis Data Analytics for Apache Flink


• Use Flink (Java, Scala or SQL) to process and analyze streaming data

   - So Flink are special applications you need to write as code. And what it allows you is that you can actually run these Flink applications on the cluster that's dedicated to it on Kinesis Data Analytics.


-- But it's all behind the scenes. And with Apache Flink, you can read from two main data sources, you can read from Kinesis Data Streams or Amazon MSK.


• So with this service, you Run any Apache Flink application on a managed cluster on AWS , the idea is that Flink is going to be a lot more powerful than just standard SQL.

     - So if you need advanced querying capability, or to read streaming data from other services such as Kinesis Data Streams or Amazon MSK, which is managed Kafka on AWS, then you would use this service.
     
     • provisioning compute resources, parallel computation, automatic scaling

     • application backups (implemented as checkpoints and snapshots)

     • Use any Apache Flink programming features

     • Flink does not read from Firehose (use Kinesis Analytics for SQL instead)
         
         - You cannot read from Kinesis Data Firehose. If you need to read and do real-time analytics on Kinesis Data Firehose, then you must use Kinesis Data Analytics for SQL.





--------------------------------------------------------------  Ordering data into Kinesis



-- how data is being ordered for Kinesis and SQS FIFO. Because even though these technologies look similar and have some similar capabilities, they're actually very, very, very different.


• Imagine you have 100 trucks (truck_1, truck_2, ... truck_100) on the road sending their GPS positions regularly into AWS.

      - So let's have a little case study. Imagine you have 100 trucks on the road, and each truck will have a truck ID. and they're on the road and they're going to send their GPS positions very regularly into AWS.


• You want to consume the data in order for each truck, so that you can track their movement accurately.

• How should you send that data into Kinesis?

• Answer : send using a “Par tition Key” value of the “truck_id”

--  the value of that partition key is the truck ID. So the truck one will send it for the  partition key truck one and then truck two will send for partition key truck two .....

• The same key will always go to the same shard


------- EG : see picture in pdf for better understanding 


Explanation :


-- So we have our Kinesis Data Stream and it has three shards , to simplify things, I'm not going to show you 100 trucks, but five should be enough. So we have five trucks, and they're on the road and they're sending the data into Kinesis.

-- As I said, we choose the partition key to be truck ID.

-- So that means that my truck one, when it's sending it's GPS data, it will send it to Kinesis with the partition key, truck one and Kinesis will say, okay, partition key truck one, I will hash it I mean we'll do a computation. And in this instance, it figures out that truck one should go into shard number one.

-- So my data will go into shard number one.

-- Then the truck two will be sending its data as well and will send a partition key of truck two. And Kinesis will look at this partition key and say I've hashed it and now it looks like you should go into shard two.

-- Same for truck three so truck three will be on the road. And it will send truck three as the partition key. But this time, the Kinesis Data Stream service will hash that truck three as the key and say you should go to shard one and that's fine.

-- It just says it doesn't have to be shard three it just says, this partition key should go to shard one.

-- Now for truck four, it will go to shard three and for truck five, it will go to shard two.

-- So this is the idea now we have a repartition and it's called partition hence the name partition key of each truck on each shard based on the partition key.

-- because truck one keeps on sending the same partition key which is truck one, then the data will always go to the same shard.

-- Hence so on the next data point for the truck one will be in shard one and the next data point for truck three will be in shard one as well and so on.

-- So anytime the truck one sends data, it will be in shard one and anytime the blue truck, the shard three sends data then it will be in shard one as well, because we are specifying to use the same partition key over time

-- So we see here is that truck one and three will always have the data into shard one.

-- Now if we look at the shard two, then only truck two and five will have the data into shard two.

-- if you look at shard three, in this example, we only have the truck four that will send its data into shard three.

-- So now imagine you have 100 trucks and maybe five shards, then each shard on average will have about 20 trucks.

-- But there is no linkage directly, you can tell between the truck and each shard.

-- Kinesis will have to hash the partition key to determine which shard to go to. 

-- The idea is though that as soon as we have a stable partition key, then each truck will be sending this data to the same shard and therefore we will have the data in order for each truck at the shard level.






-------------------------------------------------------------- Ordering data into SQS


• For SQS standard, there is no ordering.

• For SQS FIFO, if you don’t use a Group ID, messages are consumed in the order they are sent, with only one consumer

• You want to scale the number of consumers, but you want messages to be “grouped” when they are related to each other

• Then you use a Group ID (similar to Partition Key in Kinesis)





-------------------------------------------------------------- Kinesis vs SQS ordering


• Let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO


 • Kinesis Data Streams:

      • On average you’ll have 20 trucks per shard , thanks to the hashing, so each truck will be designated one shard and will stay in that shard forever.

      • Trucks will have their data ordered within each shard

      • The maximum amount of consumers in parallel we can have is 5 , because we have five shards and we need one consumer per shard.

      • Can receive up to 5 MB/s of data , So the Kinesis Data Stream though because it has five shards can receive up to five megabytes per second of data, which is quite a high throughput.


• SQS FIFO

      • You only have one SQS FIFO queue

      • You will have 100 Group ID, because we have 100 trucks, then we can create 100 group ID, each equal to the truck ID.

      • You can have up to 100 Consumers (due to the 100 Group ID) , Each consumer will be hooked to one specific group ID.

      • You have up to 300 messages per second (or 3000 if using batching)



---- conclusion


-- sometimes is going to be better to use SQS FIFO. If you want to have a dynamic number of consumers based on the number of group IDs,

-- sometimes it could be better to use Kinesis Data Stream if you have say 10,000 trucks and you need to send it lot of data, and also have data ordering per shard in your Kinesis Data Stream. 








--------------------------------------------------------------  SQS vs SNS vs Kinesis

SQS:                                                          SNS:                                         Kinesis

• Consumer“pulldata”                                    • Pushdatatomany subscribers                           • Standard:pulldata (2 MB per shard)

• Data is deleted after being consumed                  • Upto 12,500,000subscribers                           • Enhanced-fanout:pushdata (2 MB per shard per consumer)

• Canhaveasmanyworkers (consumers) as we want           • Data is not persisted(lost if not delivered)         • Possibility to replay data

• Noneedtoprovision throughput                          • Pub/Sub                                              • Meantforreal-timebigdata,analytics and ETL

• Ordering guarantees only on FIFO queues               • No need to provision throughput                      • Ordering at the shard level

• Individual message delay capability                   • Integrates with SQS for fan- out architecture        • Data expires after X days
                                                          pattern     

                                                        • FIFO capability for SQS  FIFO                        • Provisioned mode or on- demand capacity mode   









================================================== AWS Monitoring, Troubleshooting & Audit (CloudWatch, X-Ray and CloudTrail) ========================================


-- we have our application it's in the cloud. It's running. And your manager calls you at 2:00 a.m. and say it's not running anymore.

-- What happened?

-- Well we've deployed our application but we forgot to turn on monitoring.

-- Monitoring is so important and it will make sure that your applications are running the right way. 

-- Such as you can see what's happening with the logs with the metrics with tracing and audits who made what's in your AWS infrastructure.





---------------------------------------- Why Monitoring is Important


• We know how to deploy applications

     • Safely
     • Automatically
     • Using Infrastructure as Code
     • Leveraging the best AWS components!


• Our applications are deployed, and our users don’t care how we did it...

• Our users only care that the application is working!

     • Application latency: will it increase over time?

     • Application outages: customer experience should not be degraded

     • Users contacting the IT department or complaining is not a good outcome

     • Troubleshooting and remediation


• Internal monitoring:

     • Can we prevent issues before they happen? 
     • Performance and Cost
     • Trends (scaling patterns)
     • Learning and Improvement





---------------------------------------- Monitoring in AWS


1 • AWS CloudWatch:

      • Metrics: Collect and track key metrics
      • Logs: Collect, monitor, analyze and store log files
      • Events: Send notifications when certain events happen in your AWS 
      • Alarms: React in real-time to metrics / events


2 • AWS X-Ray:

      • Troubleshooting application performance and errors , so we'll see the latency and we'll see the errors just live.
      • Distributed tracing of microservices
      - X-Ray is kind of a new service that is not very popular yet, but I think it is one of the most awesome ones.


3 • AWS CloudTrail:

      • Internal monitoring of API calls being made
      • Audit changes to AWS Resources by your users






---------------------------------------- AWS CloudWatch Metrics


• CloudWatch provides metrics for every services in AWS , you need to understand what the metric means.

• Metric is a variable to monitor (CPUUtilization, NetworkIn...), and then based on how the metric is behaving, it gives you an idea of how the service is behaving, and you can do some troubleshooting based on this.

• Metrics belong to namespaces

• Dimension is an attribute of a metric (instance id, environment, etc...).

• Up to 30 dimensions per metric

• Metrics have timestamps

• Can create CloudWatch dashboards of metrics


-- CloudWatch is all about Alarms,Events and Logs

-- it is regional only 

-- CW is used to monitor performance of all as resource 

-- to monitor the resource , CW need Host level metrics also known as default metrics 

     1 CPU 

     2 Network 

     3 Disk --- volume 

     4 Status Check 

-- memory not comes under HLM 

-- memory is custom metrics 

-- 2 types of moitoring 

     1 Basic and 
     2 Detailed Moitoiring 

-- Basic is free nad it will tke every 5 min data

-- deatiled monitoiring : every 1 min data points , billable


-- u create alaram in CW 

-- alarms can do actions like (terminate , reboot ,stop , recover ) 

-- Alarm has 3 States : 

     1 In Alarm : > 90 

     2 OK : < 90

     3 Insufficient : ec2 stopped due to some reasons



-- we also have concept called " Composite Alarms"

- CW alarms are single metric

- Composite Alarms are monitoring the states of multiple alarms 

 eg: AND or OR conditions 




IMP : In CloudWatch Logs, Log Retention Policy defined at ........................... level.


ANS : Log Groups 


---------------------------------------- some  terminologies to understand for the Cloudwatch 


1  NameSpaces : Containers for monitoring data , it is a way to keep things seperate 

      -- NameSpace has got a name , it can be anythng as long as it stays within the rule set 

      -- All aws data goes to aws namesoace ---> AWS/Service

      -- Namespace contains related metric 


2  Metric : it is a collection of related Data Points , in the time ordered structures 

      Eg: cpu usage network IN/OUT 


3  Data Point : let us say we have metric called CPU Utilization , everytime any server measures its utilixation and send it into cloudwatch that goes into the CPU utilization metrics and each one of those measures so everytime the server reports the cpu that measure is called "Data Point" 


     --it has 2 components 

     1 timestamp 

     2 value 


Note : CPU utilization metric could contain data from many servers ,so how do we seperate data for this? so use " Dimesions"


4  Dimensions : these are Name Value Pairs that seperate data point for different thngs or perspective withn the same metric 


      -- while sending data points to cloud watch ,AWS also send in , these two 

      A  Name = InstanceID , Value =I-xxxx

      B  name = InstanceType , value =t2.micro.......


5  ALARM : CW also allows to take actions based on metrics which is done using Alarms 

      -- two states 

      A  OK --> Everything is working fine 

      B  ALARM --> Something bad has happened 




REF : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html






---------------------------------------- EC2 Detailed monitoring 


• EC2 instance metrics have metrics “every 5 minutes”

• With detailed monitoring (for a cost), you get data “every 1 minute”

• Use detailed monitoring if you want to scale faster for your ASG! , it gives you some benefits for your ASG, if you want to scale out and in faster.

• The AWS Free Tier allows us to have 10 detailed monitoring metrics


• Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)







---------------------------------------- CloudWatch Logs for EC2 


• By default, no logs from your EC2 machine will go to CloudWatch

• You need to run a CloudWatch agent on EC2 to push the log files you want

• Make sure IAM permissions are correct

• The CloudWatch log agent can be setup on-premises too



------------------------------------- Hands On 

--- u ave to install CW Agent 

-- by deafult CW agent do not have permission to push logs to cloudwatch logs 

-- so we should create ROLE here , and give CWFULL access policy and this role is attach to the ec2 

1  create IAM role and give CW permissions 

2  attach The role to the ec2 instance ,make sure ur kernal version is 5.10....

3  login to te ec2 and install CW agent 

4  configure the files 

5  start the CW agent service 

6  see the logs in CW loogs 

-- crtae IAM role 

-- give CLOUDWATCHFILLACES permissions and attach role to the ec2 

-- now loginto the ec2 and enter cmnds 

1 make sure ur kernal version is 5.10....


- login as root user like sudo -s

2 in real time u always do Yum Update 

3 yum install -y awslogs  --- to intall CW Agent

4 once  u install cw packages , u have to create 2 files 

- go to cd /etc/awslogs/

- press enter 

- once u do ls , two files will be created 

-  1  awscli.conf and  2 awslogs.conf

- do sudo cat awscli.conf

- change region as per ur requriemnt , sudo vim awscli.conf (to change the region)

- do cat another file awslogs.conf

- file = var/logs/messages (this is appn log path) , it is the place where it will collects the logs 

- log_stream_name = from here logs coming 

5 now u have to start awslogs , once u start awslogs automatically the log group generated once u start 

6 start the cloudwaatch Agent service

- systemctl start awslogsd 

- press nter 

7 once u pres nter the backend process will run and all the system logs will push to the CW logs 

8 go n check in log groups 

- /var/log/messages  log group created 

- we have given log_stream as Instance _id open n check the log group 

- once u open instnce id u can check all the system logs 

9  u can also push appn logs to CW agent 


- insted of var/log/messages u put log path   




---------------------------------------- CloudWatch Custom Metrics


• Possibility to define and send your own custom metrics to CloudWatch

• Example: memory (RAM) usage, disk space, number of logged in users ...

• Use API call "PutMetricData"

• Ability to use dimensions (attributes) to segment metrics

     • Instance.id
     • Environment.name

IMP • Metric resolution (StorageResolution API parameter – two possible value):

      • Standard: 1 minute (60 seconds)
      • High Resolution: 1/5/10/30 second(s) – Higher cost


• Important: Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly)

     - Something good to know is that with custom metrics, when you push a metric in the past or in the future, this works as well.

     - So this is a very important exam point.

     - So if you are pushing a metric up to two weeks in the past or two hours in the future, you're not going to get an error from CloudWatch,

     - This is going to accept your metric as is. And so that means you need to make sure that your EC2 instance time is currently configured. If you want the metrics to be synchronized with the actual time from AWS.

    - See Timestamp in documentation


-- for more custom metrics examples 

REF : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html





----------------------------------------  LAB for custom metrics (Memory)


Expore topic : till now we only see Host level metrics , now find how to get metrics for memory 

- By default, we cannot monitor Memory metrics on EC2 Instances.

- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-scripts-intro.html

- use this link for the documentation 

or u can do in another process

- create ec2 instance 

- crate one role and attach policies 

-  attach policies cloudwatchfullacces and AmazonSSMFullAccess

-  why SSM? 

ANs: i have to store particular value(json document which is used to fetch the memory utilixation from aws ec2 and send it to AWS cloudwatch ) in ssm

-  create a parameter in the system manager(system manager --> application --> parameter) with the name (u can give any name) 

eg :  /alarm/AWS-CWAgentLinConfig 

- go system manager -->paramter store --> give name --> in the value place copy json script 

{
	"metrics": {
		"append_dimensions": {
			"InstanceId": "${aws:InstanceId}"
		},
		"metrics_collected": {
			"mem": {
				"measurement": [
					"mem_used_percent"
				],
				"metrics_collection_interval": 60
			}
		}
	}
}


-- ceate ec2 and attach role to ec2 and create with userdata with cloudwatch agent to install

userdata: 

#!/bin/bash
wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip
unzip AmazonCloudWatchAgent.zip
sudo ./install.sh
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:/alarm/AWS-CWAgentLinConfig -s


-- check whether clouwtch agent is instaled or not by using 

-- sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status

-- if it is running it is success , after one minute

-- now go to CLoudwatch and --> all metrics --> CW Agent --> give instacne id and chck logs of memory






---------------------------------------- CloudWatch Logs


-- So CloudWatch Logs is the perfect place to store your application logs in AWS.

-- And to do so, you must first define log groups.

• Log groups: arbitrary name, usually representing an application

• Log stream: instances within application / log files / containers

• Can define log expiration policies (never expire, 1 day to 10 years...)

• CloudWatch Logs can send logs to:

     • Amazon S3 (exports)
     • Kinesis Data Streams
     • Kinesis Data Firehose
     • AWS Lambda
     • OpenSearch

• Logs are encrypted by default

• you Can setup KMS-based encryption with your own keys



---------------------------------- what types of logs can go into CloudWatch Logs? (CloudWatch Logs - Sources)


• we can send the logs using the SDK, CloudWatch Logs Agent, CloudWatch Unified Agent

• Elastic Beanstalk: which is used to collect logs from the application directly into CloudWatch.

• ECS: ECS will send the logs directly from the containers into CloudWatch.

• AWS Lambda: collection from function logs , will send logs from the functions themselves.

• VPC Flow Logs: will send logs specific to your VPC metadata network traffic.

• API Gateway : will send all the requests made to the API Gateway into CloudWatch Logs.

• CloudTrail : you can send the logs directly based on the filter.

• Route53: will log all the DNS queries made to its service.




---------------------------------------- what if you wanted to query the logs in CloudWatch Logs? (CloudWatch Logs Insights)


-- So, it's a querying capability within CloudWatch Logs which allows you to write your query.

-- You specify the timeframe you want to apply your query to and then automatically you're going to get a result as a visualization.

-- This visualization can also be exported either as a result or added to a dashboard for being able to rerun it whenever you want.

• Search and analyze log data stored in CloudWatch Logs

• Example: find a specific IP inside a log, count occurrences of “ERROR” in your logs...

      - So there are lots of simple queries provided as part of the console for CloudWatch Logs Insights. 

      - For example, you can find the most 25 most recent events, or you can have a look at how many events had exceptions or errors in your logs, or you can look for a specific IP and so on ..
 

• Provides a purpose-built query language

      • Automatically discovers fields from AWS services and JSON log events

      • Fetch desired event fields, filter based on conditions, calculate aggregate statistics, sort events, limit number of events...

      • Can save queries and add them to CloudWatch Dashboards


• Can query multiple Log Groups in different AWS accounts

• It’s a query engine, not a real-time engine





---------------------------------------- CloudWatch Logs – S3 Export


-- CloudWatch Logs can be exported into several destinations.


1 CloudWatch Logs – S3 Export

     • Log data can take up to 12 hours to become available for export

        - So this is for a batch export to send all your logs into Amazon S3, and this export can take up to 12 hours to be completed.

     • The API call to initiate this export is called CreateExportTask.

     • So because this is a batch export, this is Not near-real time or real-time... use Logs Subscriptions instead




---------------------------------------- CloudWatch Logs Subscriptions


• Get a real-time log events from CloudWatch Logs for processing and analysis

• Send to Kinesis Data Streams, Kinesis Data Firehose, or Lambda

• Subscription Filter – filter which logs are events delivered to your destination

      - So, the subscription filter can send data into Kinesis Data Streams. This would be a great choice if you wanted to use, for example, the integration with Kinesis Data Firehose, Kinesis Data Analytics, or Amazon EC2, or Lambda.

      - You can also directly send it into Kinesis Data Firehose. From there, you can send it in near real-time fashion into Amazon S3 or the OpenSearch Service,

      - So you can write your own custom Lambda function, or you can use a managed Lambda function that is sending data in real-time into the OpenSearch Service.





---------------------------------------- CloudWatch Logs Aggregation Multi-Account & Multi Region


-- thanks to these subscription filters, it is possible for you to aggregate data from different CloudWatch Logs into different accounts and different regions into a common destination such as the Kinesis Data Stream in one specific accounts. And then Kinesis Data Firehose. And then in near real-time into Amazon S3.

-- So that is very possible, and that is a way for you to perform log aggregation.



---------------------------------------- Cross-Account Subscription


• Cross-Account Subscription – send log events to resources in a different AWS account (KDS, KDF)

-- So, let's say you have a sender account and the recipient accounts.

-- So you create a CloudWatch Log subscription filter in sender account , and then this gets sent into a subscription destination, which is a virtual representant of the Kinesis Data Stream in the recipient accounts.

-- Then you attach a destination access policy to allow the first account to actually send data into this destination.

-- Then you create an IAM role in the recipient account, which has the permission to send records into your Kinesis Data Stream, and you make sure that this role can be assumed by the first account.

-- when you have all these things in place, then it is possible for you to send data from CloudWatch Logs in one account into a destination in another account.




----------------------------------------  Live Tail


-- So first let's create a log group

-- go to log group --> create log stream called demostream 

-- go to demostream  , we have option called tailing  , do tailing 

-- select log group as u want and select Select log streams , apply filter 

-- So that means that as events are being posted into Cloudwatch logs, they are going to appear here in our Live Tail,

-- now go to demostream  --> actions --> create log event (Hello world)

-- now check in live tail , it will be there 

-- So this is quite a nice way because if you have log streaming very fast, it can all appear here. And then from this we can get more information around when this happened, the group and so on.

-- So it's just a very cool, very easy feature to debug your CloudWatch logs. And from a pricing perspective,

-- you only get a few hours a day, so maybe one hour a day of free usage of Live Tail. So please make sure to cancel and close your lifetail session so that you don't have any cost, but you have one hour of free every day.




---------------------------------------- CloudWatch Logs Agent & Unified Agent


• For virtual servers (EC2 instances, on-premise servers...)

• CloudWatch Logs Agent

      • Old version of the agent

      • Can only send to CloudWatch Logs


• CloudWatch Unified Agent

      • Collect additional system-level metrics such as RAM, processes, etc...
      • Collect logs to send to CloudWatch Logs
      • Centralized configuration using SSM Parameter Store




---------------------------------------- CloudWatch Unified Agent – Metrics


• Collected directly on your Linux server / EC2 instance

• CPU (active, guest, idle, system, user, steal)

• Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops)

• RAM (free, inactive, used, total, cached)

• Netstat (number of TCP and UDP connections, net packets, bytes)

• Processes (total, dead, bloqued, idle, running, sleep)

• Swap Space (free, used, used %)


• Reminder: out-of-the box metrics for EC2 – disk, CPU, network (high level)


-- If you want more granularity think CloudWatch Unified Agents,





---------------------------------------- CloudWatch Logs Metric Filter


• CloudWatch Logs can use filter expressions 

    • For example, find a specific IP inside of a log
    • Or count occurrences of “ERROR” in your logs 
    • Metric filters can be used to trigger alarms


• So, one thing you should know is that when you create a filter, it  do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.


• Ability to specify up to 3 Dimensions for the Metric Filter (optional)



-- So, let's take an example. We have CloudWatch Logs agent installed on an EC2 instance, which is streaming the logs into CloudWatch Logs.

-- Then an actual metric is going to be created out of it. This is your metric filter.

-- From there, we can, for example, integrate it with a CloudWatch alarm to say that, "Hey, if we count five times error "in less than a minute in your logs, "you may want to know about it "and be alerted in an SNS topic."



 
 CloudWatch Logs Agent (Ec2 instance)--------------(stream)-----------> CW Logs --------------------> Metric Filters ------------------> CW Alarm -----------> SNS 





----------------------------------------  CloudWatch Logs Metric Filter - Hands On


-- see in course 




---------------------------------------- CloudWatch Alarms


• Alarms are used to trigger notifications for any metric

• Various options (sampling, %, max, min, etc...)

• Alarm States:

    • OK :  means that it's not triggered.

    • INSUFFICIENT_DATA  : means that there's not enough data for the alarm to determine a state.

    • ALARM : which is that your threshold has been breached and therefore a notification will be sent.


• Period:

    • Length of time in seconds to evaluate the metric

    • High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec





---------------------------------------- CloudWatch Alarm Targets 


-- alarms have three main targets.


1 • Stop,Terminate, Reboot, or Recover an EC2 Instance

2 • Trigger Auto Scaling Action

3 • Send notification to SNS (from which you can do pretty much anything)




---------------------------------------- CloudWatch Alarms – Composite Alarms


• CloudWatch Alarms are on a single metric

-- but then if you wanted to have multiple metrics, use Composite Alarms

• Composite Alarms are monitoring the states of multiple other alarms

-- So the composite alarm is the action of combining all these other alarms together.

• you can use AND and OR conditions to be able to be very flexible in terms of the condition you're checking for.

• Helpful to reduce “alarm noise” by creating complex composite alarms

   - it's very helpful to reduce alarm noise because you can create complex composite alarms and saying, 
   
   - for example, if the CPU is high and the network is high, then don't alert me because I only wanna know when the CPU is high and the network is low, this kind of things.




---------------------------------------- EC2 Instance Recovery


• Status Check:

     • Instance status = check the EC2 VM

     • System status = check the underlying hardware


• Recovery: Same Private, Public, Elastic IP, metadata, placement group






---------------------------------------- CloudWatch Alarm: good to know


• Alarms can be created based on CloudWatch Logs Metrics Filters

• To test alarms and notifications, set the alarm state to Alarm using CLI

    aws cloudwatch set-alarm-state --alarm-name "myalarm" --state-value ALARM --state-reason "testing purposes"





---------------------------------------- CloudWatch Alarms Hands On


-- create once ec2 instance , it will take 5 minutes to get some metrics 

-- go to cw and go to all alaram --> select metric -->choose ec2 --> paste instance is --> choose CPUUtilization -->  CPUUtilization > 90 --> Additional configuration Datapoints to alarm = 3 of 3 --> choose ec2 action as u want --> create alaram

-- So now this alarm obviously does have insufficient data, so we need to wait 15 minutes for it 

-- we could go into the EC2 instance and launch a way to get the CPU very high for 15 minutes, but this would be a very, very long, 

-- or we can use the API call name, Set alarm state, to really see what would happen if this alarm went into the breach phase.

-- open cloushell , go to goole and search for cloudwatch alarm state , enter this below cmd as ur names 

   -  aws cloudwatch set-alarm-state --alarm-name <value> --state-value <value> --state-reason <value>

   eg : aws cloudwatch set-alarm-state --alarm-name terminateec2 --state-value ALARM --state-reason "Testing"


-- paste this in cli , it will turn to in alarm state , then as per rule it will going to terminate the ec2 instance 






---------------------------------------- CloudWatch Synthetics Canary


• Configurable script that monitor your APIs, URLs, Websites, ...

• Reproduce what your customers do programmatically to find issues before customers are impacted

• Checks the availability and latency of your endpoints and can store load time data and screenshots of the UI

• Integration with CloudWatch Alarms

• Scripts written in Node.js or Python

• Programmatic access to a headless Google Chrome browser

• Can run once or on a regular schedule



---------------------------------------- CloudWatch Synthetics Canary Blueprints


• Heartbeat Monitor – load URL, store screenshot and an HTTP archive file

• API Canary – test basic read and write functions of REST APIs

• Broken Link Checker – check all links inside the URL that you are testing

• Visual Monitoring – compare a screenshot taken during a canary run with a baseline screenshot

• Canary Recorder – used with CloudWatch Synthetics Recorder (record your actions on a website and automatically generates a script for that)

• GUI Workflow Builder – verifies that actions can be taken on your webpage (e.g., test a webpage with a login form)





---------------------------------------- CloudWatch Synthetics Canary Hands On


-- cloudwatch ---> application signals or Application logs ----> Synthetics Canaries 

-- create canary with any application link (amazon,google.....) --> use hearbeat canary --> remaing all are default --> create canary and wait for 2-3 minutes 

-- this will gives the info about the application , it is working fine or not 







----------------------------------------------------- Amazon EventBridge (formerly CloudWatch Events)



-- Amazon EventBridge is a serverless service that uses events to connect application components together, making it easier for developers to build scalable event-driven applications.

-- Event Bridge is an Event Bus that helps in integrating different AWS Services , and custom applications and SAAS Applications 

-- Earlier we have Cloud watch events , the only negative point is that , it did not support SAAS applications and custom applications outside from the AWS 

-- EventBridge was build for the same purpose , u can create effortless event-driven architectures

Eg 1 : u want to receive SNS notification every time production EC2 instance are terminated , this will be done by EventBridge

Eg 2 : automated deployment using code pipelines at 11PM everyday

Eg 3 : if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


-- these all things will done by the event Bridge and u can schedule the events also 

-- It is fully managed service 

-- pay what u use model 


Different Parts in EventBridge:


Working flow -----


Event Producer --> Event --> Amazon EventBridge event Bus --> Rule ---> AWS Lambda , AWS Kinesis Data Firehose, Amazon Simple Notification Service


--  Event producer : AWS Service / Custom applications / Third Party SaaS providers,

--  From the producers the event will generated , this will transform to the bus and it move to rules , in rules there are multiple rules and this rules will send to multiple targets 

-- A single rule may have multiple targets and max target is 5 ,all targets will process in the event in parallel


Important terms in EventBridge :

1 Event  : An Event indicated a change in an environment 

2 Rule   : A rule matches incoming events and routes them to targets for processing  

3 Target : target application of ur rule. A target process events 

Targets can include ec2 , lambda functions , kinesis streams , Ecs tasks,  , the target receive events in JSON format 

4 Event Bus : An event bus receives events. When u create a rule , u associate it with a specific event bus and rules is matched only to events received by that event bus 

- rule can not be create standAlone , it should have it's parent as EventBus

Components of EventBridge :

1 EventBridge Rule : A rule matches incoming events and sends them to targets for processing.

2 EventBridge Pipes : A pipe connects an event source to a target with optional filtering and enrichment.

3 EventBridge Schedule : A schedule invokes a target one-time or at regular intervals defined by a cron or rate expression.

4 EventBridge Schema registry : Schema registries collect and organize schemas.


EventBus : Usecases

if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


In the above example , the working flow will be like 


User deletes his subscription --> custom event is trigged --> rights custom event would be pass into the EventBus --> rule matching --> then it reaches to the targets , once the rule is matched  here then it sends to targets --> 1 the targets are Feedback service 2 Schedule call service 3 offers service 





stepehe note

-- formerly known as CloudWatch Events

• Schedule: Cron jobs (scheduled scripts)

    - Schedule Every hour ----------> Lambda (Trigger script on Lambda function)


-- but not just a schedule like every hour, it can also react to an event pattern. 

• Event Pattern: Event rules to react to a service doing something

    - So there are event rules that can react to a service doing something. For example, you can react to the event of IAM root user sign in in the console. 
    
    - So when that happens, maybe you want to send a message into an SNS topic and receive an email notification, so that if anyone is using the root account, then you will receive an email,

    - which may be a good security feature for your accounts.


          IAM Root User Sign in Event (EVENT)-------------> SNS Topic with Email Notification



• you have different destinations like Trigger Lambda functions, send SQS/SNS messages...


• Event buses can be accessed by other AWS accounts using Resource-based Policies

• You can archive events (all/filter) sent to an event bus (indefinitely or set period)

• Ability to replay archived events ,which is super handy for debugging, super handy for troubleshooting, and for fixing production as well.





----------------------------------------------------- Amazon EventBridge – Schema Registry


-- EventBridge receives a lot of events from different places and so therefore, you need to understand what the events are going to look like and remember, these events are in this adjacent format we just saw.

• EventBridge can analyze the events in your bus and infer the schema

• The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus

• Schema can be versioned



----------------------------------------------------- Amazon EventBridge – Resource-based Policy


• Manage permissions for a specific Event Bus

• Example: allow/deny events from another AWS account or AWS region

• Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region







======================================= practicals for event bridge 


Example 1 

1  Schedule AWS Lambda Functions Using EventBridge


-- lambda scheduling use cases 

- Automated backups at EOD of ur applications 

- backend cleaning (including logs and temp files )

- consolidated reports after business hours , so lambda can trigger  Athena queries and can run any database queries and send to business stack holders through SNS 


- to schedule events u have two options 

1 to schedule  at fixed rate -- for every 1 minute 

2 Cron job (10 * * * * )

-- open console 

-- open lambda --> python 3.9 --> create function 

-- write python code to invoke lambda function

import json
from datetime import datetime

def lambda_handler(event, context):
    # TODO implement
    currentTime = datetime.now()
    print("Time at which Lambda invoked" + str(currentTime))


-- give empty JSON to test {}

-- now go to Amazon eventBridge in console 

-- c.o create rule --> schedule --> continue to create rule --> schedule that run at regular intervals--> select 1 minute --> select lambda function --> create rule 

-- u can check in CloudWatch logs , for invoking is there or not 

--  the logs are generated 


-- if u want to monitor ur invocations --> Metrics --> all Metrics --> query --> choose AWS/events --> Myrule name 

- metric name = COUNT(invocation)

- filter by = Rulename = rule name that u have created in lambda function to test the function 

- choose number in right corner to see the no. of invocations 


-------------------------------------------

Example 2 : EventBridge with SNS 


-- AS we know that EventBridge will use for 2 process 

1 event-event process : when the event has occurred according to our rule then it will trigger to the targets 

2 schedule process : do schedule 

-- in the above example we have seen Schedule event

-- now event to event 

- for example the ec2 is stopped then it will send alert to the subscribers on SNS Topic 

-- open SNS in console --> create one topic --> standard --> create topic --> open topic -->create subscription --> add protocol email or phone number 

-- when ever the instance is getting stooped then I would like send an alert to all the subscribers 

-- this is called event-event process 

-- create one ec2 instance 

-- create rule in event bridge --> name --> rule with an event pattern --> in event pattern = select ec2 --> select event type as u want --> in Target1 choose AWS Service = SNS topic select SNS topic --> create rule 

-- do stop the instance 

-- u will get notification once the instance is get stopped 


--------------- now for schedule event 

-- create another rule , that is based on the time to stop the instance 

-- select schedule pattern option --> A schedule that runs at a regular rate, such as every 10 minutes. --> target = terminateinstanceAPI call --> u can also add another target to get notification 
 

-- the instance will get terminated after the time that u have specified 


-- this is simple basic example for schedule pattern 




-------------------------------------------------------------------- AWS X-Ray


-- when you do debugging production ,  the good old way:

     • Test locally
     • Add log statements everywhere 
     • Re-deploy in production , and from the logs try to figure out what is breaking, what is happening.

-- It's really painful. It's not best practices.

• Log formats differ across applications using CloudWatch and analytics is hard.

• Debugging: monolith “easy”, distributed services “hard”

• No common views of your entire architecture!


-- here comes x-ray 

-- So, X-ray is going to give you a visual analysis of your application.

-- We'll see that basically as a client doing a request to our application we will see how many of these requests fail or don't fail. And then, from the application we'll see what it does.

-- So, it will call other IPs, it will call SNS it will call a DynamoDB Table. And so, as you can see, we'll be able to trace exactly, visually what happens when we talk to our EC2 instance.






-------------------------------------------------------------------- AWS X-Ray advantages


• Troubleshooting performance (bottlenecks)

• You can Understand dependencies in a microservice architecture, because you can visually see what is happening and how all your microservices interact with one another.

• Pinpoint service issues

• Review request behavior

• Find errors and exceptions 

•  We can answer questions Are we meeting time SLA? , in terms of latency or time to process a request?

• Where I am throttled? we can understand which service really slows down, throttles us.

• Identify users that are impacted by our errors 




-------------------------------------------------------------------- X-Ray compatibility


-- So, X-ray has a lot of compatibility.

• AWS Lambda
• Elastic Beanstalk
• ECS
• ELB
• API Gateway
• EC2 Instances or any application server (even on premise)


-- So, they really made X-Ray try to be as wide as possible and as applicable as to any application they can.




-------------------------------------------------------------------- X-Ray, how does it work? (AWS X-Ray Leverages Tracing)


-- it leverages something called tracing.

• Tracing is an end to end way to following a “request”

• So, when I make a request to, for example, my application server,  each component that will deal with a request that could be my database, that could be my gateway, my load balancer, my application server. Each component dealing with the request adds its own “trace” 

• Tracing is made of segments (+ sub segments)

• Annotations can be added to traces to provide extra-information

• So, when all these things are together, Ability to trace:

      • Every request
      • Sample request (as a % for example or a rate per minute)


• X-Ray Security:

       • IAM for authorization
       • KMS for encryption at rest


-- So, once you get all these traces, basically, X-Ray provides its magic and provide this nice little graph




-------------------------------------------------------------------- AWS X-Ray How to enable it?


-- Well, you have two ways 


1 Your code (Java, Python, Go, Node.js, .NET) must import the AWS X-Ray SDK

    • Very little code modification needed

    • The application SDK will then capture:

          • Calls to AWS services
          • HTTP / HTTPS requests
          • Database Calls (MySQL, PostgreSQL, DynamoDB)
          • Queue calls (SQS)


2) Install the X-Ray daemon or enable X-Ray AWS Integration

• X-Ray daemon works as a low level UDP packet interceptor (Linux / Windows / Mac...)

       - The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API.

       - So, if we run on a machine, on-premise server or EC2 instance, we need to install the daemon.

       - And the daemon is basically a little program that works as a low level UDP packet interceptor.

       - It can be running on Linux, Windows, and Mac. , so, you have to install it on your machine.


• AWS Lambda / other AWS services that already have integration with X-Ray then they will run the daemon for you and you don't have to worry about it.

• Each application must have the IAM rights to write data to X-Ray

 

Question : So, a very common question is, my X-Ray application works on my computer when I test locally, but doesn't work on my EC2 machine, why?

 -- the answer is probably because on your machine you're running the X-Ray daemon, but when you deploy to your EC2 instance, it's not running the X-Ray daemon, and therefore, X-Ray doesn't see your calls.



  EC2 Instance (Applicaxon Code + AWS X-Ray SDK ---------> X-Ray Daemon Running on machine) ------------------(Send batch every 1 second to AWS X-Ray)-----------> X-Ray




-------------------------------------------------------------------- The X-Ray magic


• X-Ray service collects data from all the different services

• Service map is computed from all the segments and traces

• X-Ray is graphical, so even non technical people can help troubleshoot



-------------------------------------------------------------------- AWS X-Ray Troubleshooting


• If X-Ray is not working on EC2

       • Ensure the EC2 IAM Role has the proper permissions

       • Ensure the EC2 instance is running the X-Ray Daemon


• To enable on AWS Lambda:

     • Ensure it has an IAM execution role with proper policy (AWSX-RayWriteOnlyAccess)

     • Ensure that X-Ray is imported in the code

     • Enable Lambda X-Ray Active Tracing




-------------------------------------------------------------------- AWS X-Ray Hands On


-- demp app will get errors , so create ur own demo app and we will deploy this on cloudformation

-- so here U r using cloud formation , so make sure that to use us-east-1 region for better o/p


-- create one yaml file for this demo



Eg :



AWSTemplateFormatVersion: '2010-09-09'
Description: >
  AWS CloudFormation template to create a new VPC
  or use an existing VPC for ECS deployment
  in Create Cluster Wizard. Requires exactly 1
  Instance Types for a Spot Request.
Parameters:
  Email:
    Type: String
    Default: UPDATE_ME  # <- change to a valid "abc@def.xyz" email (without quotes)
  FrontendImageUri:
    Type: String
    Default: public.ecr.aws/xray/scorekeep-frontend:latest
  BackendImageUri:
    Type: String
    Default: public.ecr.aws/xray/scorekeep-api:latest
  EcsClusterName:
    Type: String
    Description: >
      Specifies the ECS Cluster Name with which the resources would be
      associated
    Default: scorekeep-cluster
  EcsAmiId:
    Description: AMI ID
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ecs/optimized-ami/amazon-linux-2/recommended/image_id
  EcsInstanceTypeT2:
    Type: CommaDelimitedList
    Description: >
      Specifies the EC2 instance type for your container instances.
      Defaults to t2.micro.
    Default: t2.micro
    ConstraintDescription: must be a valid EC2 instance type.
  EcsInstanceTypeT3:
    Type: CommaDelimitedList
    Description: >
      Specifies the EC2 instance type for your container instances.
      Defaults to t3.micro.
    Default: t3.micro
    ConstraintDescription: must be a valid EC2 instance type.
  KeyName:
    Type: String
    Description: >
      Optional - Specifies the name of an existing Amazon EC2 key pair
      to enable SSH access to the EC2 instances in your cluster.
    Default: ''
  VpcId:
    Type: AWS::EC2::VPC::Id

  SubnetId1:
    Type: AWS::EC2::Subnet::Id
  SubnetId2:
    Type: AWS::EC2::Subnet::Id

  SecurityGroupId:
    Type: String
    Description: >
      Optional - Specifies the Security Group Id of an existing Security
      Group. Leave blank to have a new Security Group created
    Default: ''
  AsgMaxSize:
    Type: Number
    Description: >
      Specifies the number of instances to launch and register to the cluster.
      Defaults to 1.
    Default: '1'
  IamRoleInstanceProfile:
    Type: String
    Description: >
      Specifies the Name or the Amazon Resource Name (ARN) of the instance
      profile associated with the IAM role for the instance
    Default: ScorekeepInstanceProfile
  SecurityIngressFromPort:
    Type: Number
    Description: >
      Optional - Specifies the Start of Security Group port to open on
      ECS instances - defaults to port 0
    Default: '80'
  SecurityIngressToPort:
    Type: Number
    Description: >
      Optional - Specifies the End of Security Group port to open on ECS
      instances - defaults to port 65535
    Default: '80'
  SecurityIngressCidrIp:
    Type: String
    Description: >
      Optional - Specifies the CIDR/IP range for Security Ports - defaults
      to 0.0.0.0/0
    Default: 0.0.0.0/0
  EcsEndpoint:
    Type: String
    Description: >
      Optional - Specifies the ECS Endpoint for the ECS Agent to connect to
    Default: ''
  RootEbsVolumeSize:
    Type: Number
    Description: >
      Optional - Specifies the Size in GBs of the root EBS volume
    Default: 30
  EbsVolumeSize:
    Type: Number
    Description: >
      Optional - Specifies the Size in GBs of the data storage EBS volume used by the Docker in the AL1 ECS-optimized AMI
    Default: 22
  EbsVolumeType:
    Type: String
    Description: Optional - Specifies the Type of (Amazon EBS) volume
    Default: 'gp2'
    AllowedValues:
      - ''
      - standard
      - io1
      - gp2
      - sc1
      - st1
    ConstraintDescription: Must be a valid EC2 volume type.
  RootDeviceName:
    Type: String
    Description: Optional - Specifies the device mapping for the root EBS volume.
    Default: /dev/xvda
  DeviceName:
    Type: String
    Description: Optional - Specifies the device mapping for the EBS volume used for data storage. Only applicable to AL1.
    Default: /dev/xvdcz
  UseSpot:
    Type: String
    Default: 'false'
  IamSpotFleetRoleArn:
    Type: String
    Default: ''
  SpotPrice:
    Type: String
    Default: ''
  SpotAllocationStrategy:
    Type: String
    Default: 'diversified'
    AllowedValues:
      - 'lowestPrice'
      - 'diversified'
  UserData:
    Type: String
    Default: |
      #!/bin/bash
      echo ECS_CLUSTER=scorekeep-cluster >> /etc/ecs/ecs.config;echo ECS_BACKEND_HOST= >> /etc/ecs/ecs.config;
  IsWindows:
    Type: String
    Default: 'false'
  ConfigureRootVolume:
    Type: String
    Description: Optional - Specifies if there should be customization of the root volume
    Default: 'true'
  ConfigureDataVolume:
    Type: String
    Description: Optional - Specifies if there should be customization of the data volume
    Default: 'false'
  AutoAssignPublicIp:
    Type: String
    Default: 'INHERIT'
Conditions:
  UseT2MicroInstance:
    !Or
    - !Or
      - !Equals [!Sub '${AWS::Region}', 'us-east-1']
      - !Equals [!Sub '${AWS::Region}', 'us-east-2']
      - !Equals [!Sub '${AWS::Region}', 'us-west-1']
      - !Equals [!Sub '${AWS::Region}', 'us-west-2']
      - !Equals [!Sub '${AWS::Region}', 'ap-south-1']
      - !Equals [!Sub '${AWS::Region}', 'ap-northeast-3']
      - !Equals [!Sub '${AWS::Region}', 'ap-northeast-2']
      - !Equals [!Sub '${AWS::Region}', 'ap-southeast-1']
      - !Equals [!Sub '${AWS::Region}', 'ap-southeast-2']
      - !Equals [!Sub '${AWS::Region}', 'ap-northeast-1']
    - !Or
      - !Equals [!Sub '${AWS::Region}', 'ca-central-1']
      - !Equals [!Sub '${AWS::Region}', 'eu-central-1']
      - !Equals [!Sub '${AWS::Region}', 'eu-west-1']
      - !Equals [!Sub '${AWS::Region}', 'eu-west-2']
      - !Equals [!Sub '${AWS::Region}', 'eu-west-3']
      - !Equals [!Sub '${AWS::Region}', 'sa-east-1']
      - !Equals [!Sub '${AWS::Region}', 'cn-northwest-1']
  CreateEC2LCWithKeyPair:
    !Not [!Equals [!Ref KeyName, '']]
  SetEndpointToECSAgent:
    !Not [!Equals [!Ref EcsEndpoint, '']]
  CreateNewSecurityGroup:
    !Equals [!Ref SecurityGroupId, '']
  CreateWithSpot: !Equals [!Ref UseSpot, 'true']
  CreateWithASG: !Not [!Condition CreateWithSpot]
  CreateWithSpotPrice: !Not [!Equals [!Ref SpotPrice, '']]
  IsConfiguringRootVolume: !Equals [!Ref ConfigureRootVolume, 'true']
  IsConfiguringDataVolume: !Equals [!Ref ConfigureDataVolume, 'true']
  IsInheritPublicIp: !Equals [!Ref AutoAssignPublicIp, 'INHERIT']
Resources:
  ScorekeepTaskDefinition:
    Type: AWS::ECS::TaskDefinition
    Properties: 
      ContainerDefinitions: 
      - Cpu: '256'
        Image: !Ref FrontendImageUri
        MemoryReservation: '256'
        Name: scorekeep-frontend
        PortMappings: 
          - ContainerPort: '80'
      - Cpu: '512'
        Environment: 
          - Name: AWS_REGION
            Value: !Sub ${AWS::Region}
          - Name: NOTIFICATION_TOPIC
            Value: !Sub arn:aws:sns:${AWS::Region}:${AWS::AccountId}:scorekeep-notifications
          - Name: NOTIFICATION_EMAIL
            Value: !Ref Email
        Image: !Ref BackendImageUri
        MemoryReservation: '512'
        Name: scorekeep-api
        PortMappings: 
          - ContainerPort: '5000'
      - Cpu: '256'
        Essential: true
        Image: amazon/aws-xray-daemon
        MemoryReservation: '128'
        Name: xray-daemon
        PortMappings: 
          - ContainerPort: '2000'
            HostPort: '2000'
            Protocol: udp
      Cpu: '1024'
      ExecutionRoleArn: !Ref ECSExecutionRole
      Family: scorekeep
      Memory: '900'
      NetworkMode: host
      RequiresCompatibilities: 
        - EC2
      TaskRoleArn: scorekeepRole
  UserTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-user"
      KeySchema:
        HashKeyElement: {AttributeName: id, AttributeType: S}
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  SessionTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-session"
      KeySchema:
        HashKeyElement: {AttributeName: id, AttributeType: S}
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  GameTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-game"
      AttributeDefinitions:
        - AttributeName: "id"
          AttributeType: "S"
        - AttributeName: "session"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "id"
          KeyType: "HASH"
      GlobalSecondaryIndexes:
        - IndexName: "session-index"
          KeySchema:
            - AttributeName: "session"
              KeyType: "HASH"
          ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
          Projection: { ProjectionType: ALL }
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  MoveTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-move"
      AttributeDefinitions:
        - AttributeName: "id"
          AttributeType: "S"
        - AttributeName: "game"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "id"
          KeyType: "HASH"
      GlobalSecondaryIndexes:
        - IndexName: "game-index"
          KeySchema:
            - AttributeName: "game"
              KeyType: "HASH"
          ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
          Projection: { ProjectionType: ALL }
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  StateTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: "scorekeep-state"
      AttributeDefinitions:
        - AttributeName: "id"
          AttributeType: "S"
        - AttributeName: "game"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "id"
          KeyType: "HASH"
      GlobalSecondaryIndexes:
        - IndexName: "game-index"
          KeySchema:
            - AttributeName: "game"
              KeyType: "HASH"
          ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
          Projection: { ProjectionType: ALL }
      ProvisionedThroughput: {ReadCapacityUnits: 2, WriteCapacityUnits: 2}
  NotificationTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: "scorekeep-notifications"
  ECSExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement: 
          - 
            Effect: "Allow"
            Principal: 
              Service: 
                - "ecs-tasks.amazonaws.com"
            Action: 
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
      RoleName: "scorekeepExecutionRole"
  ECSTaskRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement: 
          - 
            Effect: "Allow"
            Principal: 
              Service: 
                - "ecs-tasks.amazonaws.com"
            Action: 
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess"
        - "arn:aws:iam::aws:policy/AmazonSNSFullAccess"
        - "arn:aws:iam::aws:policy/AWSXrayFullAccess"
      RoleName: "scorekeepRole"
  ScorekeepECSRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: ScorekeepECSRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Action: "sts:AssumeRole"
          Principal:
            Service: ["ec2.amazonaws.com"]
          Effect: "Allow"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
  ScorekeepInstanceProfile:
    DependsOn: ScorekeepECSRole
    Type: AWS::IAM::InstanceProfile
    Properties: 
      InstanceProfileName: !Ref IamRoleInstanceProfile
      Roles: 
        - ScorekeepECSRole
  ScorekeepECSCluster:
    Type: AWS::ECS::Cluster
    Properties: 
      ClusterName: !Ref EcsClusterName
  EcsSecurityGroup:
    Condition: CreateNewSecurityGroup
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: ECS Allowed Ports
      VpcId: !Ref VpcId
      SecurityGroupIngress:
       - IpProtocol: 'tcp'
         FromPort: '80'
         ToPort: '80'
         CidrIp: 0.0.0.0/0
  EcsInstanceLc:
    DependsOn:
    - ScorekeepECSCluster
    - ScorekeepInstanceProfile
    Type: AWS::AutoScaling::LaunchConfiguration
    Condition: CreateWithASG
    Properties:
      ImageId: !Ref EcsAmiId
      InstanceType: !Select [ 0, !If [ UseT2MicroInstance, !Ref EcsInstanceTypeT2, !Ref EcsInstanceTypeT3 ] ]
      AssociatePublicIpAddress: !If [ IsInheritPublicIp, !Ref "AWS::NoValue", !Ref AutoAssignPublicIp ]
      IamInstanceProfile: !Ref IamRoleInstanceProfile
      KeyName: !If [ CreateEC2LCWithKeyPair, !Ref KeyName, !Ref "AWS::NoValue" ]
      SecurityGroups: [ !If [ CreateNewSecurityGroup, !Ref EcsSecurityGroup, !Ref SecurityGroupId ] ]
      BlockDeviceMappings:
        - !If
          - IsConfiguringRootVolume
          - DeviceName: !Ref RootDeviceName
            Ebs:
              VolumeSize: !Ref RootEbsVolumeSize
              VolumeType: !Ref EbsVolumeType
          - !Ref AWS::NoValue
        - !If
          - IsConfiguringDataVolume
          - DeviceName: !Ref DeviceName
            Ebs:
              VolumeSize: !Ref EbsVolumeSize
              VolumeType: !Ref EbsVolumeType
          - !Ref AWS::NoValue
      UserData:
        Fn::Base64: !Ref UserData
  EcsInstanceAsg:
    DependsOn:
    - ScorekeepTargetGroup
    - EcsInstanceLc
    Type: AWS::AutoScaling::AutoScalingGroup
    Condition: CreateWithASG
    Properties:
      VPCZoneIdentifier:
        - !Ref SubnetId1
        - !Ref SubnetId2
      LaunchConfigurationName: !Ref EcsInstanceLc
      MinSize: '0'
      MaxSize: !Ref AsgMaxSize
      DesiredCapacity: !Ref AsgMaxSize
      Tags:
        -
          Key: Name
          Value: !Sub "ECS Instance - ${AWS::StackName}"
          PropagateAtLaunch: true
        -
          Key: Description
          Value: "This instance is the part of the Auto Scaling group which was created through ECS Console"
          PropagateAtLaunch: true
      TargetGroupARNs:
      - !Ref ScorekeepTargetGroup
  ScorekeepService:
    DependsOn:
    - ScorekeepTaskDefinition
    - UserTable
    - SessionTable
    - GameTable
    - MoveTable
    - StateTable
    - NotificationTopic
    - ECSExecutionRole
    - ECSTaskRole
    - ScorekeepECSCluster
    - EcsInstanceAsg
    Type: AWS::ECS::Service
    Properties:
      Cluster: scorekeep-cluster
      DeploymentConfiguration:   
        MaximumPercent: 100
        MinimumHealthyPercent: 0
      DesiredCount: 1
      LaunchType: EC2
      SchedulingStrategy: REPLICA
      ServiceName: scorekeep-service
      TaskDefinition: !Ref ScorekeepTaskDefinition
  ScorekeepTargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: ScorekeepTargetGroup
      TargetType: instance
      Port: 80
      Protocol: HTTP
      VpcId: !Ref VpcId
  ScorekeepLoadBalancer:
    DependsOn: EcsSecurityGroup
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: scorekeep-lb
      Scheme: internet-facing
      SecurityGroups: 
        - !Ref EcsSecurityGroup
      Subnets: 
        - !Ref SubnetId1
        - !Ref SubnetId2
      Tags:
        - Key: Name
          Value: !Join [_, [!Ref 'AWS::StackName']]
      Type: application
  ScorekeepLoadBalancerListener:
    DependsOn:
    - ScorekeepLoadBalancer
    - ScorekeepTargetGroup
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties: 
      DefaultActions:
        - Type: forward
          TargetGroupArn: !Ref ScorekeepTargetGroup
      LoadBalancerArn: !Ref ScorekeepLoadBalancer
      Port: 80
      Protocol: HTTP
Outputs:
  LoadBalancerUrl:
    Description: The URL of the ALB
    Value: !GetAtt ScorekeepLoadBalancer.DNSName





-- if you have a look at it, this CloudFormation template actually deploys an ECS cluster.

-- then we're going to have a front-end Image a backend API all using X-Ray so that we see data in the X-Ray folder and then it's going to be running on the T2 or T3 micro.

             -  Default: public.ecr.aws/xray/scorekeep-frontend:latest

             -  Default: public.ecr.aws/xray/scorekeep-api:latest



-- open CF , upload this template 

-- now all are default in 2nd step except u have to add 2 subnets and VPC , And the reason we do so is that this is we indicate to the templates where we want to deploy our resources. 

-- all are default , acknowledge then submit 

-- this is going to create a CloudFormation template for us.

-- check the resources that template created for u 

-- add notificatin subscription , if u want 

-- So how do we use this web application. go to Outputs , open link in the new tab 

-- c.o create --> give sample name --> rules (tic tac toe)--> create 

-- c.o View traces for this session , it will show you all the traces in X-ray session

-- play the game , What's going to happen is that as we play the games this is going to send traces into X-Ray.

-- now go to X-ray , So this is my service map, and this shows the dependencies of all my components in AWS and how they relate towards the API calls that have been made.

-- if any error presents , it will highlight in clours 

-- u can debug and find the error and do trouble shoot 

-- delete the stack





-------------------------------------------------------------------- X-Ray Instrumentation in your code


-- some advanced concepts for X-Ray , and the first thing I want to show you is how to instrument your code.


• Instrumentation means the measure of product’s performance, diagnose errors, and to write trace information. So it is a field in Software Engineering to do all these things.

• To instrument your application code, you use the X-Ray SDK

 
- Example app.js - Express


var app = express();

var AWSXRay = require('aws-xray-sdk');
app.use(AWSXRay.express.openSegment('MyApp'));

app.get('/', function (req, res) {
  res.render('index');
});

app.use(AWSXRay.express.closeSegment());



REF : https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs.html


-- So here is an example of how we can instrument our node js code with the X-Ray SDK.

-- so once we add some code, for example here, requiring the X-Ray SDK and using it in our express app, then our code will be instrumented.

-- That means that we will get trace information from our code into the X-Ray service.

• Many SDK require only configuration changes

• You can modify your application code to customize and annotation the data that the SDK sends to X- Ray, using interceptors, filters, handlers, middleware...





-------------------------------------------------------------------- X-Ray Concepts


• Segments: each application / service will send them

• Subsegments: if you need more details in your segment

• Trace: segments collected together to form an end-to-end trace

• Sampling: decrease the amount of requests sent to X-Ray, reduce cost

• Annotations: Key Value pairs used to index traces and use with filters

   - So annotations is are extremely important in X-ray. If you want to be able to search your traces with new indexes.


• Metadata: Key Value pairs, not indexed, not used for searching


• The X-Ray daemon / agent has a config to send traces cross account:
  
    • make sure the IAM permissions are correct – the agent will assume the role

    • This allows to have a central account for all your application tracing






-------------------------------------------------------------------- X-Ray Sampling Rules


• with the sampling rules, we are able to control the amount of data that you send to the X-Ray service and record. the more data you send you X-Ray, the more you're going to pay.

• You can modify sampling rules without changing your code

• By default there is a Sampling Rules, which says that the X-Ray SDK records the first request each second (Blue Part), an five percent(Green Part) of any additional requests.

• One request per second is the reser voir, which ensures that at least one trace is recorded each second as long the service is serving requests.

• Five percent is the rate at which additional requests beyond the reservoir size are sampled.



-------------------------------------------------------------------- X-Ray Custom Sampling Rules


• You can create your own rules with the reservoir and rate


• Example 1 – Higher minimum rate for POSTs


• Rule name: POST minimum

• Priority: 100

• Reservoir: 10

• Rate: 10 (.1 if configured using a JSON document)

• Service name: *

• Service type: *

• Host: *

• HTTP method: POST

• URL path: *

• Resource ARN: *


--  in this example ,  Reservoir: 10 , iThat means that 10 requests per second will be sent into X-Ray and then 10% of the other ones will be sent.

-- So here we have a higher minimum rate and we send more requests into X-Ray.




Example – Debugging rule to trace all requests for a problematic route

         -- A high-priority rule applied temporarily for debugging.

• Rule name: DEBUG – history updates

• Priority: 1

• Reservoir: 1

• Rate: 1

• Service name: Scorekeep

• Service type: *

• Host: *

• HTTP method: PUT

• URL path: /history/*

• Resource ARN: *


-- Whereas here we want you to have debugging. And so we say we want to have all requests. So one a reservoir= 1  and one of rate = 1.

-- That means that all requests will be sent into X-Ray. So we don't want to lose any traces. And this is very helpful when we want you to debug to find what's going on for every single trace.

-- Obviously, in production, this will be very, very expensive because now we are sending a lot of data into X-Ray, 

-- but it's very helpful temporarily to change these custom sampling rules to see what's going on.

-- the cool thing about it is that if you change your sampling rules in the X-Ray console, you don't have to restart your applications. You don't have to do anything with your X-Ray in SDK.

-- Automatically the daemon, the extra daemon knows how to get sampling rules and correctly send the right amount of data into the X-Ray service.





-------------------------------------------------------------------- X-Ray Write APIs (used by the X-Ray daemon)



{
  "Version" : "2012-10-17",
  "Statement" : [
    {
      "Effect" : "Allow",
      "Action" : [
        "xray:PutTraceSegments",
        "xray:PutTelemetryRecords",
        "xray:GetSamplingRules",
        "xray:GetSamplingTargets",
        "xray:GetSamplingStatisticSummaries"
      ],
      "Resource" : [
        "*"
      ]
    }
  ]
}


REF : arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess



-- So this is a managed policy called the X-Ray Write Only Access and you can see it has five line items


1  PutTraceSegments: Uploads segment documents to AWS X-Ray

     - as the name indicates, it uploads a segment documents into AWS X-Ray that's necessary to have if you want to write into X-Ray.


2  • PutTelemetryRecords: Used by the AWS X-Ray daemon to upload some information about how many segments were received, rejected and backend connection errors. So this helps with the metrics.

     - SegmentsReceivedCount, 
     
     - SegmentsRejectedCounts, 
     
     - BackendConnectionErrors...


3  • GetSamplingRules: Retrieve all sampling rules (to know what/when to send)

     - So usually when we write stuff, we have a lot of puts because this was how the APIs are named in AWS whenever you're write it says, "put."

     - So do you know why?

     - we saw that whenever we were changing the sampling rules in the X-Ray console, all the X-Ray daemons were automatically updated to know when to send data into X-Ray.

     - So for your X-Ray daemon to be able to know how the sampling rules are changing, then the GetSamplingRule authorization and permission is necessary.


• GetSamplingRules same applies for  GetSamplingTargets & GetSamplingStatisticSummaries: advanced , 

• The X-Ray daemon needs to have an IAM policy authorizing the correct API calls to function correctly





-------------------------------------------------------------------- X-Ray Read APIs


{
  "Version" : "2012-10-17",
  "Statement" : [
    {
      "Sid" : "AWSXrayReadOnlyAccess",
      "Effect" : "Allow",
      "Action" : [
        "xray:GetSamplingRules",
        "xray:GetSamplingTargets",
        "xray:GetSamplingStatisticSummaries",
        "xray:BatchGetTraces",
        "xray:BatchGetTraceSummaryById",
        "xray:GetDistinctTraceGraphs",
        "xray:GetServiceGraph",
        "xray:GetTraceGraph",
        "xray:GetTraceSummaries",
        "xray:GetGroups",
        "xray:GetGroup",
        "xray:ListTagsForResource",
        "xray:ListResourcePolicies",
        "xray:GetTimeSeriesServiceStatistics",
        "xray:GetInsightSummaries",
        "xray:GetInsight",
        "xray:GetInsightEvents",
        "xray:GetInsightImpactGraph"
      ],
      "Resource" : [
        "*"
      ]
    }
  ]
}



-- this is more complicated, but this is a managed policy for reading.

-- as you can see, all these things as says get, get, get, get, get, and we have one called batch get trace which also says gets.



• GetServiceGraph:is to get the main graph that we saw in the console.

• BatchGetTraces: Retrieves a list of traces specified by ID. Each trace is a collection of segment documents that originates from a single request.

• GetTraceSummaries: Retrieves IDs and annotations for traces available for a specified time frame using an optional filter. To get the full traces, pass the trace IDs to BatchGetTraces.

• GetTraceGraph: Retrieves a service graph for one or more specific trace IDs.





-------------------------------------------------------------------- X-Ray with Elastic Beanstalk


• AWS Elastic Beanstalk platforms include the X-Ray daemon so we don't need to include it

• You can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextensions/xray-daemon.config)

• Make sure to give your instance profile the correct IAM permissions so that the X-Ray daemon can function correctly

• Then make sure your application code is instrumented with the X-Ray SDK to send these traces

• Note:The X-Ray daemon is not provided for Multicontainer Docker



----------------- in the console , u have to add some options to enable X-Ray 


1 in the monitoring stage enable , X-Ray 

2 make sure that the EC2 instances have an IAM role that allow them to connect into X-Ray. --> IAM --> roles --> beanstalk-instancerole --> AWSElasticBeanstalkWebTier  it has x-ray permissions 





-------------------------------------------------------------------- ECS + X-Ray integration options


-- theory part around how to integrate ECS with X-Ray



-------------- 1 ECS Cluster : X-Ray Container as a Daemon


-- you have an ECS Cluster and one way to run the X-Ray Daemon is to use the container as a Daemon itself.



what does that mean?


-- That means we have our two EC2 instance. For example, in our ECS cluster, and remember we manage those EC2 instances and so we're going to run a Daemon task, a Daemon container of the X-Ray Daemon.

-- That means that the X-Ray Daemon Container will be running on every single EC2 instances. If you have 10 EC2 instances in your ECS cluster, then you will have 10 containers, one on each EC2 instance, running as a Daemon Container.

-- so that means that the X-Ray agent is now running on all these EC2 instances. 

-- And so you can launch your App Containers on the EC2 instances, and after matting them correctly from the networking standpoint to hit the X-Ray Daemon with a UDP port. Then you can run all your applications.

-- So, in this case, you will just have one X-Ray Daemon Container per EC2 instance.



-------------- 2 ECS Cluster : X-Ray Container as a “Side Car”


-- That means you still have your EC2 instances, but now you're going to run one X-Ray Daemon Container alongside each application container, and they will connect from a networking stand point.

-- So this is why it's called a Side Car, it's because the X-Ray Daemon now runs side-to-side as our application container, and it's a Side car.

-- if we look at this now, we have one Side Car per App Container. So, if you have 20 App Containers under one EC2 instance then we'll have 20 X-Ray Side Car.




-------------- 3 Fargate Cluster : X-Ray Container as a “Side Car”


-- Now, Fargate Clusters, we don't have control over the EC2 instances, it's just an ECS Cluster that we don't have any control over the underlying instances.

-- So, we can not use the X-Ray Daemon Container, we also have to use the X-Ray Container as a Side Car pattern.

-- So, if you want to launch a Fargate task, it would be the App Container and the X-Ray Side Car, here and there.






---------------------------------------------------------------- AWS Distro for OpenTelemetry


-- So OpenTelemetry is a project and AWS has created a distribution of that project that is AWS supported and they call it Secure and Production Ready.


--------- So what is OpenTelemetry?

ANS : OpenTelemetry is a way to get a single set of APIs, library, agents and collector services to collect distributed traces and metrics for your applications.

-- The idea is that it's very similar to X-Ray, but it's open-source.



-- It can also help you collect metadata from your AWS resources and services.

-- so you have agents and these agents can be auto-instrumented to collect traces without you even changing your code which looks very similar to X-Ray.

-- All these traces and these metrics can be sent to multiple AWS services as well as partner solutions.

     • X-Ray,CloudWatch,Prometheus...


• Instrument your apps running on AWS (e.g., EC2, ECS, EKS, Fargate, Lambda) as well as on-premises

• Migrate from X-Ray to AWS Distro for Temeletry if you want to standardize with open-source APIs from Telemetry or send traces to multiple destinations simultaneously





AWS Distro for OpenTelemetry ----------------(Collect Traces ----> Collect Metrics -------> AWS Resources and Contextual Data) ----------------> AWS X-Ray / Amazon CloudWatch / Amazon Managed Service for Prometheus / Partner Monitoring Solunons






---------------------------------------------------------------- AWS CloudTrail


• Provides governance, compliance and audit for your AWS Account

• CloudTrail is enabled by default!

• Get an history of events / API calls made within your AWS Account by:

• Console
• SDK
• CLI
• AWS Services

• Can put logs from CloudTrail into CloudWatch Logs or S3

• A trail can be applied to All Regions (default) or a single Region.

• If a resource is deleted in AWS, investigate CloudTrail first!

-- By default, CloudTrail trails created via the AWS Management Console will have global service events enabled. It is recommended that you only have one trail allocated to global service events per account in order to reduce duplicate events.


EPV : AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible.






---------------------------------------------------------------- CloudTrail Events



1 • Management Events:

     • Operations that are performed on resources in your AWS account

     • Examples:

          • Configuring security (IAM AttachRolePolicy)

          • Configuring rules for routing data (Amazon EC2 CreateSubnet)

          • Setting up logging (AWS CloudTrail CreateTrail)

          • By default, trails are configured to log management events.

          • Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)



2 • Data Events:

      • By default, data events are not logged (because high volume operations)

      - So what are Data Events?

      • Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject):as you can see, these can be happening a lot on an S3 bucket. so this is why they're not logged by default and u have the option can separate Read and Write Events

      - So a Read Event will be a GetObject whereas a Right Event would be a DeleteObject or a PutObject.

      - Another kind of event you can have in a CloudTrail are AWS Lambda function execution activities.

      • AWS Lambda function execution activity (the Invoke API)

          - So whenever someone uses the Invoke API so you can get insights about how many times your Lambda functions are being evoked.

          - this could be really high volumes, if your Lambda functions are executed a lot.




3  CloudTrail Insights (u have to pay)


-- So when we have so many Management Events across all types of services and so many APIs happening very quickly in your accounts, 

-- it can be quite difficult to understand what looks odd, what looks unusual and what doesn't.

-- so this is where CloudTrail Insights comes in.


-- So with CloudTrail Insights and you have to enable it and you have to pay for it, it will analyze your events and try to detect unusual activity in your accounts.

• Enable CloudTrail Insights to detect unusual activity in your account:

    EG : 
    
    • inaccurate resource provisioning

    • hitting service limits

    • Bursts of AWS IAM actions

    • Gaps in periodic maintenance activity


• CloudTrail Insights analyzes normal management events to create a baseline

• And then continuously analyzes write events to detect unusual patterns

    • Anomalies appear in the CloudTrail console
    • Event is sent to Amazon S3
    • An EventBridge event is generated (for automation needs)






---------------------------------------------------------------- CloudTrail Events Retention


• Events are stored for 90 days in CloudTrail and then afterwards they're deleted,

• To keep events beyond this period, log them to S3 and use Athena



---------------------------------------------------------------- LAB 

-- open cloud trail --> event history u can able to see all the events which u have done in the aws account through the root user or normal user 

-- for lab , lets create a new cloud trail 

-- create new s3 bucket , all the API events that will send to s3 and stored in this bucket 

-- Log file SSE-KMS encryption  and Log file validation = uncheck no need for this demo 

-- management events --> create trail that's it 

-- wait for 5 min atleast to see the current time and date iin the trail 

-- after 5 min go n check in the s3 bucket --> it will create log files 

-- in the mean while create one simple ec2 instance for example purpose\

-- check in the history it will shows u the instance running in the trail

-- this is how u will get all the API calls that U have made in ur AWS Account 






------------------------------------------------------- Amazon EventBridge – Intercept API Calls


-- A very important cloud trail integration you need to know about is the one with Amazon EventBridge to intercept any API calls.

-- So let's say you wanted to receive an SNS notification, anytime a user would delete a table in DynamoDB by using the DeleteTable API Call.

-- So what happens that whenever we do an API call in AWS, as you know, the API call itself is going to be logged in CloudTrail. That's for any API call.

-- But also  these all these API calls will end up as events as well in Amazon EventBridge.

-- so we can look for that very specific delete table API call, and create a rule out of it. And this rule will have a destination the destination being Amazon SNS and therefore, we can create alerts.



 User -----------(DeleteTable API Call)------> DynamoDB -----------(Log API call)-----> CloudTrail (any API call) -----------(event)------> Amazon EventBridge -------(alert) ------------> SNS





-------------- few more examples on how you can integrate Amazon Eventbridge and CloudTrail.


1 

   -- For example, say, you wanted to be notified whenever a user was assuming a role in your accounts.

   -- So the AssumeRole is an API in the IAM service and therefore, is going to be logged by CloudTrail.

   -- And then using EventBridge integration, we can trigger a message into an SNS topic.


   User ------ (AssumeRole) ---------> IAM Role --------(API Call logs) ----------> CloudTrail --------(event)---------> EventBridge -------> SNS





2 

    -- Similarly, we can also intercept API calls that, for example, change the Security Group inbound rules. So the Security Group call is called AuthorizeSecurityGroupIngress, and it's an EC2 API call.

    -- So these are going to be logged again by CloudTrail and then they will appear in EventBridge and then we can trigger a notification in SNS.



    User -------------(AuthorizeSecurityGroupIngress)--------> EC2(Security Group) ------------- (API Call logs) -------------> CloudTrail ------------(event)------> EventBridge ---------> SNS






------------------------------------------------------- CloudTrail vs CloudWatch vs X-Ray


• CloudTrail:

   • Audit API calls made by users / services / AWS console
   • Useful to detect unauthorized calls or root cause of changes


• CloudWatch:

   • CloudWatch Metrics over time for monitoring
   • CloudWatch Logs for storing application log
   • CloudWatch Alarms to send notifications in case of unexpected metrics
   - CloudWatch is really just for overall metrics,


• X-Ray:

   • Automated Trace Analysis & Central Service Map Visualization
   • Latency, Errors and Fault analysis
   • Request tracking across distributed systems
   - X-Ray is a lot more granular, trace-oriented type of service







======================================================================== AWS Lambda ============================================================================


It’s a serverless world


What’s serverless?


• Serverless is a new paradigm in which the developers don’t have to manage servers anymore...

• They just deploy code

• They just deploy... functions !

• Initially... Serverless == FaaS (Function as a Service)

• Serverless was pioneered by AWS Lambda but now also includes anything that’s managed: “databases, messaging, storage, etc.”

• Serverless does not mean there are no servers... it means you just don’t manage / provision / see them





------------------------------------------------------ Serverless in AWS


• AWS Lambda
• DynamoDB
• AWS Cognito
• AWS API Gateway 
• Amazon S3
• AWS SNS & SQS
• AWS Kinesis Data Firehose 
• Aurora Serverless
• Step Functions
• Fargate



------------------------------------------------------ Why AWS Lambda


Amazon EC2 :

  • Virtual Servers in the Cloud
  • Limited by RAM and CPU
  • Continuously running
  • Scaling means intervention to add / remove servers


Amazon Lambda : 

   • Virtual functions – no servers to manage!
   • Limited by time - short executions
   • Run on-demand
   • Scaling is automated!




------------------------------------------------------ Benefits of AWS Lambda


• Easy Pricing:

    • Pay per request and compute time 

    • Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time


• Integrated with the whole AWS suite of services

• Integrated with many programming languages

• Easy monitoring through AWS CloudWatch

• Easy to get more resources per functions (up to 10GB of RAM!)

• Increasing RAM will also improve CPU and network!





------------------------------------------------------ AWS Lambda language support


• Node.js (JavaScript)
• Python
• Java (Java 8 compatible)
• C# (.NET Core)
• Golang
• C# / Powershell
• Ruby
• Custom Runtime API (community supported, example Rust)


• Lambda Container Image : this Lambda container image is quite special.

     • The container image must implement the Lambda Runtime API , so it's not any container image that can run on Lambda. There needs to be some prerequisites about how that container image is built.

     • ECS / Fargate is preferred for running arbitrary Docker images


-- So the exam, if they ask you to run a container on Lambda, unless that container does implement the Lambda runtime API, you will run  that container on ECS or Fargate.





------------------------------------------------------ AWS Lambda Integrations Main ones


1 API Gateway : So API Gateway is to create a REST API, and they will invoke our Lambda functions.


2 Kinesis : Kinesis will be using Lambda to do some data transformations on the fly.


3 DynamoDB :  DynamoDB will be used to create some triggers, so whenever something happens in our database a Lambda function will be triggered.


4 S3 : A Lambda function would be triggered anytime, for example, a file is created in S3.


5 CloudFront :  CloudFront, this will be Lambda@edge,


6 CloudWatch Events : CloudWatch Events or EventBridge. This is whenever things happen in our infrastructure on AWS, and we want to be able to react to things.

             - For example, say we have a cut pipeline, state changes and we want to do some automations based on it, we can use a Lambda function.


7 CloudWatch Logs : CloudWatch Logs, to stream these logs, wherever you want.


8 SNS : SNS to react to notifications and your SNS topics.


9 SQS : SQS to process messages from your SQS queues.


10 Cognito : Cognito to react to whenever, for example, a user login to your database.




-- So, these are just the main ones. There are tons of Lambda integrations.





------------------------------------------------------ Example: Serverless Thumbnail creation


-- So let's say we have an S3 bucket, and we want to create thumbnails on the fly.

-- So there will be an event which is that the new image will be uploaded in Amazon S3.

-- This will trigger, through an S3 event notification, a Lambda function.

-- that Lambda function will have code to generate a thumbnail. That thumbnail maybe pushed and uploaded into another S3 bucket or the same S3 bucket, which would be a smaller version of that image.

-- And also, our Lambda function may want to insert some data into DynamoDB, around some metadata for the image, for example the image, name, size, creation date, etc..




New image in S3 -------------(trigger)----------> AWS Lambda Function Creates a Thumbnail -------------(Push)-------> New thumbnail in S3 / Metadata in DynamoDB




------------------------------------------------------ Example: Serverless CRON Job


-- So CRON is a way on your EC2 instances, for example, to generate jobs every five minutes, or every Monday at 10:00 AM, etc

-- But you need to run CRON on a virtual server. So an EC2 two instance and so on.

-- so while your instance is not running, or at least your CRONs are not doing anything, then your instance time is wasted.

-- so, as such, you can create a CloudWatch Event rule or an EventBridge rule that will be triggered every 1 hour. And every 1 hour it will be integrated with a Lambda function that will perform your task.

-- So this is a way to create a serverless CRON,

-- in this example, CloudWatch Events is serverless and Lambda functions are serverless too.




------------------------------------------------------ AWS Lambda Pricing: example


• You can find overall pricing information here:

     https://aws.amazon.com/lambda/pricing/


• Pay per calls:

     • First 1,000,000 requests are free

     • $0.20 per 1 million requests thereafter ($0.0000002 per request)


• Pay per duration: (in increment of 1 ms)

     • 400,000 GB-seconds of compute time per month for FREE

     • == 400,000 seconds if function is 1GB RAM

     • == 3,200,000 seconds if function is 128 MB RAM

     • After that $1.00 for 600,000 GB-seconds


• It is usually very cheap to run AWS Lambda so it’s very popular





------------------------------------------------------ Lambda Hands On


-- open lambda in console ---> check how it will work on the intro page 

-- c.o scale seamlessly , to observe how this will work and how it will get scale automatically

-- Lambda responds to events : Once you create Lambda functions, you can configure them to respond to events from a variety of sources. Try sending a mobile notification, streaming data to Lambda, or placing a photo in an S3 bucket.


-- Scale seamlessly : Lambda scales up and down automatically to handle your workloads, and you don't pay anything when your code isn't running.

       - * Your first 1 million requests or 400,000 GB-seconds of compute per month are free. Costs in this demo are based on a 128 MB function with a 1 second invocation duration.



-- choose blue print --> hello world python --> this will give lambda function code --> create function


-- create test event and do test and observe the result 



------------------------------------------------------ Lambda – Synchronous Invocations


• Synchronous: CLI, SDK, API Gateway, Application Load Balancer

   -  So you're doing a synchronous invocation when you're using the CLI, and the SDK, the API Gateway, or even an Application Load Balancer.


-- What does that mean by synchronous?


• Results is returned right away : that means that you're waiting for the results, and then the result will be returned to you.

• Any Errors that come backs to you, must be handled on the client side. (retries, exponential backoff, etc...)

-- That means that if my lambda function fails, and I just invoked it from the console, I want to click on the retry button to retry it.

-- So that means any time there is an error on lambda, the client has to figure out what to do. Do you want to retry it, do you want to do an exponential backoff, etc etc.

-- So synchronous means a direct invocation that you wait the result of.



Example 1 :


-- So the CLI and the SDK would just invoke our lambda function, the lambda function would do something, and give us our response.


             ------------------------------------(invoke)---------------------------------->
  SDK/CLI                                                                                        Lambda     (Do something)
             <------------------------------------(Response)---------------------------------



Example 2 :


--  when we'll use the API Gateway ,the API Gateway will proxy its request to the lambda function, so it will invoke the lambda function for you; 

-- the lambda function will give the response to your API Gateway, which will give you the response.

-- so in this schema, we're just waiting for the response that makes it a synchronous type of invocation.


           -------------------------(invoke) ----------->               ------------(proxy)----------------> 
   Client                                                   API Gateway                                        AWS LAmbda  (Do something)
   
           <------------------(Response)------------------              <------------------------------------





------------------------------------------------------ Lambda - Synchronous Invocations - Services


-- Well, first of all, any time it's user-invoked then it's going to be synchronous.


• User Invoked:

      • Elastic Load Balancing (Application Load Balancer)

      • Amazon API Gateway

      • Amazon CloudFront (Lambda@Edge)

      • Amazon S3 Batch


• Service Invoked:

     • Amazon Cognito

     • AWS Step Functions


• Other Services:

     • Amazon Lex

     • Amazon Alexa

     • Amazon Kinesis Data Firehose





------------------------------------------------------ Lambda - Synchronous Invocations Hands On


-- actually when we had our code over function, and we just did test, then this was a synchronous invocation because we are waiting for the results of the invocation, and we get it here in this window.

-- If it took maybe two minutes for this invocation to complete, we would have waited two minutes to get the execution results.

-- The other thing we can test the synchronous invocation with is using the CLI.

-- open cloudshell in console 

-- So let's use this CLI to list our functions.

      aws lambda list-functions


-- if u r using nrml CLI , then add --region "ur region name"

-- So next we want to do a synchronous invocation from the CLI of our lambda function,

-- for this this just go into our code and under a synchronous.sh we have the different commands. So I'm on Linux Mac because this is CloudShell and this is using the CLI V2 versions.




aws lambda list-functions --region eu-west-1


# LINUX / MAC
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --region eu-west-1 response.json

# WINDOWS POWERSHELL
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\" }' --region eu-west-1 response.json

# WINDOWS CMD
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload "{""key1"":""value1"",""key2"":""value2"",""key3"":""value3""}" --region eu-west-1 response.json





-- based on ur OS , u can use the commands 

-- here i am using on linux , so 1st once i choose 

       aws lambda invoke --function-name mydemo --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }'  response.json



-- now do cat response.json

-- I mean, my synchronize invocation has worked.





------------------------------------------------------ Lambda Integration with ALB


-- So, for now, lambda functions, they can be either invoked with CLI or the SDK, But sometimes if you want to expose them to the internet, you want to allow people to use them through an HTTP or an HTTPS endpoint.

-- so as such, you have two ways for doing this. 
   
       - The first way is to use an Application Balancer, or 
       
       - an API Gateway,


• for it to work, The Lambda function must be registered in a target group 



      Client <-----------(HTTP/HTTPS)---------> Application Load Balancer (ALB) <-------------(INVOKE SYNC)--------------> Lambda (Target Group)



-- So your clients will be invoking and sending requests in the form of HTTP or HTTPS to your ALB, which will be invoking synchronously your lambda function in a target group,

-- because synchronously, because we're waiting for the lambda function to get back to the application balancer, which will in turn, return a response to the client.



Question : "How does the load balancer convert an HTTP request into a lambda invocation?"


ANS : 


1 ALB to Lambda: HTTP to JSON

    - the HTTP gets transformed into a JSON document.


-- Example Application Load Balancer request event


{
    "requestContext": {
        "elb": {
            "targetGroupArn": "arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/lambda-279XGJDqGZ5rsrHC2Fjr/49e9d65c45c6791a"
        }
    },
    "httpMethod": "GET",
    "path": "/lambda",
    "queryStringParameters": {
        "query": "1234ABCD"
    },
    "headers": {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "accept-encoding": "gzip",
        "accept-language": "en-US,en;q=0.9",
        "connection": "keep-alive",
        "host": "lambda-alb-123578498.us-east-1.elb.amazonaws.com",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36",
        "x-amzn-trace-id": "Root=1-5c536348-3d683b8b04734faae651f476",
        "x-forwarded-for": "72.12.164.125",
        "x-forwarded-port": "80",
        "x-forwarded-proto": "http",
        "x-imforwards": "20"
    },
    "body": "",
    "isBase64Encoded": false
}




-- elb = there is the ELB information, so which ELB invokes and what is the target group.

-- Then we get some information around the HTTP method, so it was a GET and the path was /lambda.

-- We get the query string parameters as key/value pairs, so each query string will be appearing in the JSON document.

-- We'll get the headers as key/value pairs,

-- we'll get the body for POST, PUT, and the value is base 64 encoded, so do you need to decode it or not.

-- for this information, there is a translation of the entire HTTP request to JSON.




2  Lambda to ALB conversions: JSON to HTTP

-- So, similarly, our lambda function should return something, a JSON document and the ALB will convert this back into HTTP.


Example response document format


{
    "statusCode": 200,
    "statusDescription": "200 OK",
    "isBase64Encoded": False,
    "headers": {
        "Content-Type": "text/html"
    },
    "body": "<h1>Hello from Lambda!</h1>"
}



-- So if we look at the response from the lambda function, it's very simple.

-- It needs to include a status code and a description,

-- as well as the response headers as key/value pairs,

-- finally the body of the response and the flag, whether or not it is base 64 encoded.





------------------------------------------------------ ALB Multi-Header Values


• So, if we have our client talking to our ALB, ALB can support multi header values (ALB setting) which is to have multi-header values.

-- What does that mean?

-- That means that if we pass in multiple headers with the same value or query strings with the same value,


• When you enable multi-value headers, HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects.



EG :

1 HTTP

     http://example.com/path?name=foo&name=bar


2 JSON

     ”queryStringParameters”: {“name”: [“foo”,”bar”] }



-- So, in this example, name=foo&name=bar have the same name, but different values. 

-- Then we can enable the setting and both the headers and the query string parameters will be converted as an array into the lambda function.

-- So that means that when my ALB invokes my lambda function for the queryStringParameters JSON,

-- what I will see is name, and instead of one value, I will see an array of values. So, foo and bar.

-- "How do we support multi-header values?"

       ANS :It's just a setting on the ALB and this is what it does.






------------------------------------------------------ lambda and ALB Hands On


-- open console lambda --> choose scratch --> give name --> select python 3.9 or above --> create function .

IMP -- now i have to create one load balancer for my lambda application , create TG , make sure SG have HTTP inbound rule and target have Lambda function as target and enable Health checks for TG 

-- so now if we look at our lambda code , it is just simple code that just returns the status 200

-- Now let's add a little bit of logging.

import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }


-- once u add print (event) , here we can see that the event that was passed is being printed to the console.

-- this will be helpful, when we want to see the event pass to another function from the load balancer.

-- now go to LB --> copy DNS and paste in browser --> it will downlaod one file in that file u have o/P

-- 'body': json.dumps('Hello from Lambda!') , 'coz of this we r getting o/p as in document form 

--  what we would like to have this response not to be downloaded, but instead to be displayed in my web browser.

-- so u have make changes in the code , the code will look like this 


import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        "statusCode": 200,
        "statusDescription": "200 OK",
        "isBase64Encoded": False,
        "headers": {
            "Content-Type": "text/html"
        },
        "body": "<h1>Hello from Lambda!</h1>"
    }



-- the content type to be text html, and therefore this is deployed in our web browser,

-- So this is how a Lambda function is properly sent in data into our ALB, and then our ALB showing the data directly from the web browser itself.

-- if u want to allow multi head values , u can enable , go to TG --> Attributes --> Multi value headers

-- but if you enable multi-value header, you need to change your response a little bit. ---- Remember

-- delete LB 






------------------------------------------------------ Lambda – Asynchronous Invocations





-- we've seen synchronous invocations,

-- let's go into asynchronous invocation. So they're for service that will invoke another functions behind the scenes,

• for example,  S3, SNS, CloudWatch Events...

-- for example , we have an S3 bucket and an S3 event notification for new files.

-- This will go into the Lambda Service, and because it's asynchronous something will happen. The events are going to be placed in an internal Event Queue.

-- So we have an Event Queue here and your Lambda function is going to be reading off that Event Queue.

-- The Lambda function then will try to process these events, but if somehow things go wrong, the Lambda function will automatically attempt to retry.

-- So that means that it will be three tries in total.

-- The first one will happen right away, then the second one will happen a minute after, and the third one will happen two minutes after the second one.

-- So the idea is that our Lambda function is going to retry three times in total.

-- And then, once the retries happen, that means that our Lambda function is possibly going to process those same events multiple times and so this could be a problem.

-- And so if you lambda function is not "idempotent" this could be big problem, so meaning your Lambda function should be idempotent.

            - Idempotentcy means that, in case of retries,the result is the same.


-- So then, if you have a retry happening, what will happen is that you will see duplicate log entries in CloudWatch Logs because your Lambda function will try over and over and over again.

-- So we can define a DLQ, or dead-letter queue, for after the retries are done.

-- So that means that in case there's a failed processing and we never were able to succeed because of the retries,

-- then the Lambda function can send some events to SQS or SNS for further processing later on.

-- this is the whole idea behind asynchronous invocations.


"Why we will use asynchronous versus synchronous?"

ANS : 

- Well, first of all, some services must use asynchronous, so you have no choice,

- and the second one is, for example, say you need to speed up processing and you don't need to wait for the results, then you can start 1000 files being processed at the same time,

- you just wait at the end for all to be processed in parallel, you don't wait for each individual results, and so that speeds up your processing time.


• Asynchronous invocations allow you to speed up the processing if you don’t need to wait for the result (ex: you need 1000 files processed)




                            
                                                    | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
                                                    |  Lambda Service                                                         |
                                                    |                                                                         |
                                                    |
                                                    |                                            - - - - - - - -              |
                                                    |                                            |              |
                                                    |                                            |   retries    |             |
                                                    |                                            |              |               
                                                    |                                            |              |             |
S3 bucket ----------------(New file event)---------------> Event Queue ----------(Read)-------> Function -------              |

                                                    |                                             |
                                                    |                                             |
                                                    |                                             |  DLQ for failed processing
                                                    |                                             |
                                                    |                                         SQS or SNS 
                                                    |
                                                    |
                                                    | - - - - - - - - - - - - - - - - - - - - - - - - -- - - - - - - - - - - - - 





Summary :


• The events are placed in an Event Queue

• Lambda attempts to retry on errors

     • 3 tries total
     • 1 minute wait after 1st , then 2 minutes wait

• Make sure the processing is idempotent (in case of retries)

• If the function is retried, you will see duplicate logs entries in CloudWatch Logs

• Can define a DLQ (dead-letter queue) – SNS or SQS – for failed processing (need correct IAM permissions)








------------------------------------------------------ Lambda - Asynchronous Invocations - Services


• Amazon Simple Storage Service (S3) , when we have S3 Event Notifications invoking Lambda functions.

• Amazon Simple Notification Service (SNS), so when we receive notifications and we trigger a Lambda function.

• Amazon CloudWatch Events / EventBridge, which will basically have our Lambda functions react to events happening in our AWS infrastructure.

• AWS CodeCommit (CodeCommit Trigger : new branch, new tag, new push)

• AWS CodePipeline (invoke a Lambda function during the pipeline, Lambda must callback)


----- other -----


• Amazon CloudWatch Logs (log processing)

• Amazon Simple Email Service

• AWS CloudFormation

• AWS Config

• AWS IoT

• AWS IoT Events





------------------------------------------------------ Lambda – Asynchronous Invocations Hands On


--  so i have my lambda function 

-- But if we invoke this asynchronously, we cannot do it from the console, we have to do it from the CLI.

-- open cloudshell , based on ur OS , u can use the below cmnds 



# LINUX / MAC
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --invocation-type Event --region eu-west-1 response.json

# WINDOWS POWERSHELL
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\" }' --invocation-type Event --region eu-west-1 response.json

# WINDOWS CMD
aws lambda invoke --function-name demo-lambda --cli-binary-format raw-in-base64-out --payload "{""key1"":""value1"",""key2"":""value2"",""key3"":""value3""}" --invocation-type Event --region eu-west-1 response.json



-- I am using linux 

-- so the whole purpose of an asynchronous invocation is that we do not have the results back to us.


EG : aws lambda invoke --function-name my-demo --cli-binary-format raw-in-base64-out --payload '{"key1": "value1", "key2": "value2", "key3": "value3" }' --invocation-type Event  response.json


-- it will gives u 202 response ,So that means that the lambda function has been successfully invoked but we don't know the results.

-- go to cloud watch --> fucntion log group --> open recent logs --> we can see the function was invoked and the requests was successful.

-- But we don't know this, if it was successful or not, because this was an asynchronous invocation.

-- now go to code and purposefully do make changes in code , for getting errror like this 


 #return event['key1']  # Echo back the first key value
 raise Exception('Something went wrong')



-- now do enter same commands again , but u will get same response and go n check in CW , u will get error in logs regardless of whether or not the function succeeded or failed because we don't want to know the results, due to the nature of asynchronous.

-- So what we can do instead is maybe set up a dead letter queue.

-- now go to sqs in console --> create one std queue 

-- now come to lambda --> refresh the page --> configuration --> open IAM role for lambda --> attch AmazonSQSFullAccess permissions 

-- lambda --> configuration --> asynchronous configuration --> edit --> add SQS 

-- test function , it will fail we know that 

-- So, if we try to go now into our CloudShell, and invoke this function again, we'll know that the invocation itself will fail because the lambda function function was coded to fail.

-- But now what's going to happen is that the DLQ is going to kick into effect, so we're gonna have 2 retry attempts, and after the retry attempts have failed then the message should go into Amazon SQS.

-- now go to sqs and check for message we have one message 





------------------------------------------------------ CloudWatch Events / EventBridge


-- how we can integrate CloudWatch Events or EventBridge with Lambda.

-- 2 ways 


1 CRON or Rate EventBridge Rule ------------------(Trigger Every 1 hour)------------> AWS Lambda Funchon Perform a task


2 CodePipeline EventBridge Rule ------------------(Trigger on State Changes) ------------> AWS Lambda Function Perform a task





------------------------------------------------------  CloudWatch Events / EventBridge (Hands ON)

-- create one function with python 3.9

-- make sure this function is being invoked by EventBridge.

-- Eventbridge --> rule --> create new rule --> Rule type = Schedule --> c.o continue to create rule ---> A schedule that runs at a regular rate, such as every 10 minutes. (1 min)--> choose target as aws lambda fun---> create rule

-- do refresh page of lambda u will see invocation of Eventbridge

-- wait for 1 min --> now check in cloudwatch invocation happens

-- now add print(event) in code and deploy the changes and wait for 1 Min

-- now do check in CW recent logs, u will see the event info

-- do disable our rule 




------------------------------------------------------ lambda and S3 Events Notifications


• S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...

    - So just a reminder on S3 event notifications, it is a way for you to get notified whenever an object is created, removed, restored, when there is a replication happening.

• Object name filtering possible (*.jpg)

    - You can filter by prefix and by suffix. And the use case is, the classic one is to generate thumbnail images of every image uploaded into Amazon S3.
    
    • Use case: generate thumbnails of images uploaded to S3


-- So you have your events into Amazon S3, and S3 can send it to three things,


1 Amazon S3 -------(Events)-------> SNS ---------> SQS

 
EXP : your events into Amazon S3, send to SNS and from an SNS topic, we can do a fan out pattern to send to multiple SQ-Q,  we can sent it into an SQ-Q


2 Amazon S3 -------(Events)-------> SQS ---------> Lambda Function


EXP : we can sent it into an SQ-Q , and have a Lambda function directly read off that SQ-SQ,


3 Amazon S3 -------(Events)-------(async)----------> Lambda Function ----------- (DLQ) ---------> SQS


EXP : we could have an Amazon S3 event notification, directly invoke our Lambda function and this is an asynchronous invocation.

- this Lambda function could do whatever it wants with that data, and then in case things go wrong, we can set up a dead-letter queue, for example an SQS, as we've seen from before.



• S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer

• If two writes are made to a single non- versioned object at the same time, it is possible that only a single event notification will be sent

• If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.




------------------------------------------------------ Simple S3 Event Pattern – Metadata Sync


  S3 bucket -------------(New file event)------------> lambda (Update metadata table) -----------> DynamoDB Table / Table in RDS


EXP : So, here is a simple pattern. An S3 bucket will have a new file event into Lambda. And Lambda function will process that file maybe insert that data into DynamoDB Table or even a table in RDS database.





------------------------------------------------------ lambda and S3 Events Notifications Hands On


-- create one lambda function and one s3 bucket , make sure both are in same region

-- s3 buket --> properties --> event notificatin --> create event notification for all object creation

-- So this event notification is enabled to send data into lambda function.

print(event) in code 


import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }



-- now do upload a file in the s3 bucket

-- now what's going to happen is that this should trigger an event into my Lambda function and to see whether or not this has worked,

-- lambda --> cloudwatch --> u will see that records have created with all the event info 







------------------------------------------------------ Lambda – Event Source Mapping


-- So we have seen asynchronous processing, we have seen synchronous processing, and now we are going to see Event Source Mapping. So this is the last category of how Lambda can process events in AWS.

--  it applies to 


   • Kinesis Data Streams
   • SQS & SQS FIFO queue
   • DynamoDB Streams


• Common denominator: records need to be polled from the source

    - So Lambda needs to ask the service to get some records and then the records will be returned.

    - So that means that Lambda needs to poll from these services.


• Your Lambda function is invoked synchronously



            Poll
         ---------------->
Kinesis                         Inside the lambda function (Event Source Mapping(internal)) ----------------(INVOKE WITH EVENT BATCH)-----------------> Lambda Function
         <----------------
            Return



-- We have Kinesis and the Lambda service, and if we configure Lambda to read from Kinesis, 

-- then they will be internally an Event Source Mapping that will be created. And that is responsible for polling Kinesis, and returning and getting the results back from Kinesis.

-- So Kinesis will return a batch to us.

-- then once this Event Source Mapping has some data for Lambda to process, it's going to invoke our Lambda function synchronously, with an event batch.







------------------------------------------------------ So there are two categories of Event Source Mapper. (Streams and Queues)



------------------------------------------ 1 Streams & Lambda (Kinesis & DynamoDB)


• So in case of streams, they will be an Event Source Mapping. They will create an iterator for each shard,

    - so each Kinesis shard or DynamoDB Stream shard. And the items will be processed in order at the shard level.


• so we can configure where to start to read from. You can read it with just the new items, or from the beginning of the shard or from a specific timestamp.


• Whenever an item is processed from a shard, whether it be from Kinesis or DynamoDB,Processed items aren't removed from the stream (other consumers can read them)


-- So the use case for this is either low traffic or high traffic.


• Low traffic: use batch window to accumulate records before processing , So to make sure that you invoke another function efficiently.

-- then if you have a very high throughput stream and you want to speed up processing,

• You can process multiple batches in parallel at shard level 

    • up to 10 batches per shard
    • in-order processing is still guaranteed for each partition key,




------------------------------------------ Streams & Lambda – Error Handling


• By default, if your function returns an error, the entire batch is reprocessed until the function succeeds, or the items in the batch expire.

    - this is very imp ,Having an error in a batch can block your processing. 


• To ensure in-order processing, processing for the affected shard is paused until the error is resolved

-- so you can manage that in several ways.

• You can configure the event source mapping to:

     • discard old events

     • restrict the number of retries

     • split the batch on error (to work around Lambda timeout issues)


• Discarded events can go to a Destination





------------------------------------------ 2 Lambda – Event Source Mapping SQS & SQS FIFO





            Poll
         ---------------->
SQS                        Inside the lambda function (Event Source Mapping(internal)) ----------------(INVOKE WITH EVENT BATCH)-----------------> Lambda Function
         <----------------
            Return


-- The SQS queue will be polled by a Lambda Event Source Mapping.And then whenever a batch is returned, your Lambda function will be invoked synchronously with the event batch.

-- So in the case of SQS, the Event Source Mapping will poll SQS using Long Polling. So it's going to be efficient.

• Specify batch size (1-10 messages)

• Recommended: Set the queue visibility timeout to 6x the timeout of your Lambda function

• To use a DLQ

      • set-up on the SQS queue, not Lambda (DLQ for Lambda is only for async invocations)

      • Or use a Lambda destination for failures


---- but not on Lambda. Why?

ANS : Because the DLQ for Lambda only works for asynchronous invocations. And this is a synchronous invocations.



• Lambda also supports in-order processing for FIFO (first-in, first-out) queues, And the number of Lambda functions that will be scaling to process your queue will be equal to the number of active message groups.

• For standard queues, items aren't necessarily processed in order.

• for a standard queue, Lambda will scale as fast as possible to read all the messages in your standard queue.

• When an error occurs, batches are returned to the queue as individual items and might be processed in a different grouping than the original batch.

• Occasionally, the event source mapping might receive the same item from the queue twice, even if no function error occurred.

• Lambda deletes items from the queue after they're processed successfully.

• You can configure the source queue to send items to a dead-letter queue if they can't be processed.





------------------------------------------ Lambda Event Mapper Scaling


• Kinesis Data Streams & DynamoDB Streams:

     • One Lambda invocation per stream shard

     • If you use parallelization, up to 10 batches processed per shard simultaneously


• SQS Standard:

     • Lambda adds 60 more instances per minute to scale up

     • Up to 1000 batches of messages processed simultaneously


• SQS FIFO:

     • Messages with the same GroupID will be processed in order

     • The Lambda function scales to the number of active message groups







------------------------------------------------------ Lambda event source mapping Hands On


-- create one function with python 3.8 ,  make sure that lambda role has "AWSLambdaSQSQueueExecutionRole" permisssions 

-- create one SQS std queue ,

-- now go to lambda --> add triggers --> sqs --> create trigger

-- go to code --> make some changes

import json

def lambda_handler(event, context):
    # TODO implement
    print(event)
    return "success"


-- now let's test our Lambda function by simply going into SQS and sending a message.

-- send some message from SQS

--  so, because my Lambda function is continuously pulling from SQS, then it should process this message.

-- open loga in cloudwatch , u will able to see the message in the cloudwatch records

-- now go to queue --> see u have 0 messages 'coz the messages has been processed by our lambda function.

-- now disable the sqs trigger in the lambda 

-- now add 2nd type trigger called kinesis to lambda , do try by using aws documentation 






------------------------------------------------------ Lambda – Event and Context Objects



EventBridge -------------(invoke)----------------> Lambda Funcxon


EVENT Object :


• JSON-formatted document contains data for the function to process

• Contains information from the invoking service (e.g., EventBridge, custom, ...)

• Lambda runtime converts the event to an object (e.g., dict type in Python)

• Example: input arguments, invoking service arguments, ...


-- So let's take an example where your Lambda function is invoked, for example, by a EventBridge rule.

-- So what's going to happen is that EventBridge is going to create an event and that event is going to be passed to your Lambda function

-- your Lambda function will receive that event, and it's called the event object.

-- the event object includes a lot of detail around, well, the event itself. Where it was emitted from and the service itself will include a lot of data related to that event in the event.


Context Objects :

• Provides methods and properties that provide information about the invocation, function, and runtime environment

• Passed to your function by Lambda at runtime

• Example: aws_request_id, function_name, memory_limit_in_mb, ...


-- this is more some metadata around your function, such as the request ID from AWS, your function name, the log group associated with your Lambda function, the memory limit, and so on.

-- So this event object and this context object are very different but very complementary.





------------------------------------------------------ Access Event & Context Objects using Python


import time

def lambda_handler(event, context):   
    print("Lambda function ARN:", context.invoked_function_arn)
    print("CloudWatch log stream name:", context.log_stream_name)
    print("CloudWatch log group name:",  context.log_group_name)
    print("Lambda Request ID:", context.aws_request_id)
    print("Lambda function memory limits in MB:", context.memory_limit_in_mb)
    # We have added a 1 second delay so you can see the time remaining in get_remaining_time_in_millis.
    time.sleep(1) 
    print("Lambda time remaining in MS:", context.get_remaining_time_in_millis())




-- so the handler has an event and has a context, the event will have information, for example, such as the source of the event or the region of the event and so on that we can print to the console 

-- the context will have information, such as the request ID, the function ARN, the function name, the memory limits in megabytes, and then, for example, some information about CloudWatch Logs, such as the stream name or the group name.






------------------------------------------------------ Lambda – Destinations


-- So, the problem is that, when we've been doing asynchronous invocations or event mappers,

-- it was really hard for us to see if it had failed or succeeded.

-- So the idea with destinations is to send the result of an asynchronous invocation or the failure of an event mapper into somewhere.


• So here is a very cool new feature from Nov 2019: Can configure to send result to a destination

• Asynchronous invocations - can define destinations for successful and failed event:

    • Amazon SQS 
    • Amazon SNS
    • AWS Lambda
    • Amazon EventBridge bus

• Note: AWS recommends you use destinations instead of DLQ now (but both can be used at the same time)


• Event Source mapping: This is only used for when you have an event batch that gets discarded because we can't process it.

So we can, instead, send that event batch into Amazon SQS or Amazon SNS,

IMP • Note: that if you have an Event Source mapping reading from SQS, you can set either a failed destination or you can set up a DLQ directly on your SQS Queue.






------------------------------------------------------ Lambda – Destinations


-- for this demo , choose s3 event notificatin invoke funtion , 

-- now go to sqs create two queues 1 for success and 2nd is for failure 

-- go to lambda function --> add destination --> asynchronous invocation --> onfailure --> sqs --> fail sqs --> save

-- go to lambda function --> add destination --> asynchronous invocation --> onsuccess --> sqs --> success sqs --> save

-- So now we need to test the fact that these two destinations are working properly.

-- now upload any file in the s3 bucket , go to sqs refresh the page , u will find one message , u will find the body of the file 

-- So we get both the event source, as well as the event response and some extra information into the body of my SQS queue message,

-- now try for failure , go to code and raise exception

import json

def lambda_handler(event, context):
    print(event)
    # TODO implement
    raise Exception("this is failure sqs")


-- now upload some files in bucket , my lambda function is going to be invoked asynchronously and is going to raise an exception.

-- I should not see any messages right away. in SQS , do you know why?

ANS : because S3 invoking my lambda function is an asynchronous type of invocation. 


    - Then if we go into the configuration remember, and go to the asynchronous invocation, well we have two retry attempts that are going to be running.

    - And then once the retry attempts are run, then the destination will be invoked as a failure.

    - so wait for sometime (3-4 min)


-- now do refresh in SQS , u will find one message for failure and poll that message 

-- So we could debug what happened in here, in our lambda function to make sure that it doesn't happen next time.





------------------------------------------------------ Lambda Execution Role (IAM Role)


• Grants the Lambda function permissions to AWS services / resources

• Sample managed policies for Lambda:
  
      • AWSLambdaBasicExecutionRole – Upload logs to CloudWatch.

      • AWSLambdaKinesisExecutionRole – Read from Kinesis

      • AWSLambdaDynamoDBExecutionRole – Read from DynamoDB Streams

      • AWSLambdaSQSQueueExecutionRole – Read from SQS

      • AWSLambdaVPCAccessExecutionRole – Deploy Lambda function in VPC

      • AWSXRayDaemonWriteAccess – Upload trace data to X-Ray.



• When you use an event source mapping to invoke your function, Lambda uses the execution role to read event data.

• Best practice: create one Lambda Execution Role per function



------------------------------------------------------ Lambda Resource Based Policies


• Use resource-based policies to give other accounts and AWS services permission to use your Lambda resources

• Similar to S3 bucket policies for S3 bucket

• An IAM principal can access Lambda:

      • if the IAM policy attached to the principal authorizes it (e.g. user access and we have full permissions, so we can access our Lambda function, this is what we've been doing so far,)

      • OR if the resource-based policy authorizes (e.g. service access) , this is more helpful when you have a service to service access.



• When an AWS service like Amazon S3 calls your Lambda function, the resource-based policy gives it access.





------------------------------------------------------  Lambda Resource Based Policies Hands On


--  go to IAM --> search for lambda permissions --:> explore those permissions it have 




------------------------------------------------------ Lambda Environment Variables


• Environment variable = key / value pair in “String” form

• Adjust the function behavior without updating code

• The environment variables are available to your code

• Lambda Service adds its own system environment variables as well

• Helpful to store secrets (encrypted by KMS)

• Secrets can be encrypted by the Lambda service key, or your own CMK




------------------------------------------------------ Lambda Environment Variables Hands ON 



-- create one lambda function with python 3.8

--  so we're going to pass environment variables in unencrypted form to learn their function.

-- modify the lambda code first 

import json
import os 

def lambda_handler(event, context):
    # TODO implement
    return os.getenv("ENVIRONMENT_NAME")


-- deploy changes

-- os package  = this is going to allow us to get access to environment variables.

-- So we're going to create this environment variable called "ENVIRONMENT_NAME" and we're going to give it a value and the value of which will be retrieved by the lambda function and return.

-- lambda function --> configuration --> environment variables --> edit --> ENVIRONMENT_NAME = give value ---> save 

-- So for now this environment variable is unencrypted.

-- now do test ur code , change ur variable name and test again 

-- so the code did not change, so here we see the impact that environment variables can have on to the return values and the behavior of our code, 




------------------------------------------------------ Lambda Logging & Monitoring


-- how Lambda does logging, monitoring, and tracing.


• CloudWatch Logs: 

      • AWS Lambda execution logs are stored in AWS CloudWatch Logs

      • Make sure your AWS Lambda function has an execution role with an IAM policy that authorizes writes to CloudWatch Logs , this is included in the Lambda basic execution role as we've seen before.


• CloudWatch Metrics: 
   
      • AWS Lambda metrics are displayed in AWS CloudWatch Metrics UI or the Lambda UI.

      • they will represent information about your Invocations, Durations, Concurrent Executions

      • Error count, Success Rates, Throttles

      • Async Delivery Failures

      • Iterator Age (Kinesis & DynamoDB Streams)



------------------------------------------------------ Lambda Tracing with X-Ray


• Enable in Lambda configuration (Active Tracing)

• It will Runs the X-Ray daemon for you

• Use AWS X-Ray SDK in Code

• Ensure Lambda Function has a correct IAM Execution Role

      • The managed policy is called AWSXRayDaemonWriteAccess


IMP • Environment variables to communicate with X-Ray

        • _X_AMZN_TRACE_ID: contains the tracing header 

        • AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR

 IMP    • AWS_XRAY_DAEMON_ADDRESS: the X-Ray Daemon IP_ADDRESS:PORT

     



------------------------------------------------------ Lambda Tracing with X-Ray Hands On


-- lambda function ---> configuration --> monitor --> enable active tracing 

-- take s3 event notifications examples ---> write code for success --> upload object --> wait for 5 minutes --> go n check in x-ray console 




------------------------------------------------------ Lambda@Edge and CloudFront Functions 


-- Customization At The Edge : Well we know we deploy our functions and our applications in a specific region, but sometimes using, for example, CloudFront, we have the Edge locations are distributing our content.

• Many modern applications execute some form of the logic at the edge, before reaching the application itself. So these are called Edge Functions,


• Edge Function:

      • A code that you write and attach to CloudFront distributions

      • Runs close to your users to minimize latency


• CloudFront provides two types: CloudFront Functions & Lambda@Edge

• You don’t have to manage any servers, deployed globally

• Use case: customize the CDN content

• Pay only for what you use

• Fully serverless



------------------------- CloudFront Functions & Lambda@Edge Use Cases


• Website Security and Privacy
• Dynamic Web Application at the Edge
• Search Engine Optimization (SEO)
• Intelligently Route Across Origins and Data Centers 
• Bot Mitigation at the Edge
• Real-time Image Transformation
• A/B Testing
• User Authentication and Authorization
• User Prioritization
• User Tracking and Analytics



------------------------- CloudFront Functions




              Viewer Request                               Origin Request
          ------------------------>                    -------------------------> 
  Client                                  CloudFront                                      Origin 
          <------------------------                    <------------------------
              Viewer Response                               Origin Response 




-- So the client will do a request into CloudFront, and this is called a viewer request, because the client views it.

-- Then CloudFront will do an origin request into your origin server, the server will reply to CloudFront,

-- so we have an origin response, and finally CloudFront sends that response to the client, so we have a viewer response.




• CloudFront Functions are Lightweight functions written in JavaScript and they modified the viewer request and response.

• They're used  For high-scale, latency-sensitive CDN customizations

• which gives you Sub-ms startup times, millions of requests/second

• Used to change Viewer requests and responses:

       • Viewer Request: after CloudFront receives a request from a viewer we can modify this 

       • Viewer Response: before CloudFront forwards the response to the viewer


• Native feature of CloudFront (manage code entirely within CloudFront)

IMP - Remember, CloudFront Functions high performance, high scale only for the viewer request and response.






------------------------- Lambda@Edge


-- Now Lambda@Edge is a bit more, so you can modify all of them.

• Lambda functions written in NodeJS or Python

• Scales to 1000s of requests/second

• Used to change CloudFront requests and responses:
 
     • Viewer Request – after CloudFront receives a request from a viewer

     • Origin Request – before CloudFront forwards the request to the origin

     • Origin Response – after CloudFront receives the response from the origin

     • Viewer Response – before CloudFront forwards the response to the viewer


• Author your functions in one AWS Region (us-east-1), then CloudFront replicates to its locations





              Viewer Request                               Origin Request
          ------------------------>                    -------------------------> 
  Client                                  CloudFront                                      Origin 
          <------------------------                    <------------------------
              Viewer Response                               Origin Response 




------------------------- CloudFront Functions vs. Lambda@Edge



                                        CloudFront Functions                                               Lambda@Edge         


Runtime Support                          JavaScript                                                         Node.js, Python

# of Requests                            Millions of requests per second                                    Thousands of requests per second

CloudFront Triggers                      - Viewer Request/Response                                          - Viewer Request/Response  and - Origin Request/Response

Max. Execution Time                      <1ms                                                                5 – 10 seconds

Max. Memory                              2MB                                                                 128MB upto 10GB

Total Package Size                       10 KB                                                                 1MB–50MB

Network Access, File System Access         No                                                                   YES

Access to the Request Body               NO                                                                      YES

Pricing                                  Free tier available, 1/6th price of @Edge                             No free tier, charged per request & duration





------------------------- CloudFront Functions vs. Lambda@Edge - Use Cases


CloudFront Functions


• Cache key normalization

      • Transform request attributes (headers, cookies, query strings, URL) to create an optimal Cache Key


• Header manipulation

      • Insert/modify/delete HTTP headers in the request or response


• URL rewrites or redirects


• Request authentication & authorization

      • Create and validate user-generated tokens (e.g., JWT) to allow/deny requests      





Lambda@Edge 


• Longer execution time (several ms)

• Adjustable CPU or memory

• Your code depends on a 3rd libraries (e.g., AWS SDK to access other AWS services)

• Network access to use external services for processing

• File system access or access to the body of HTTP requests





------------------------------------------------------ Lambda in VPC 



Lambda by default


   • By default, your Lambda function is launched outside your own VPC (in an AWS-owned VPC)

   • Therefore it cannot access resources in your VPC (RDS, ElastiCache, internal ELB...)



Lambda in VPC

   • You must define the VPC ID, the Subnets and the Security Groups

   • Lambda will create an ENI (Elastic Network Interface) in your subnets

   • your Lambda function needs a  AWSLambdaVPCAccessExecutionRole




                                     |- - - - - - - - - - - - - - - - 
                                     | Private Subnet 
                                     |
                                     |
                                     |
                                     |
                                     |
                                     |
                                     |
  Lambda Function ------------------ |--------> Elastic Network Interface (ENI) (Lambda SG) ----------------> Amazon RDS In VPC (RDS Security group)
                                     |
                                     |
                                     |- - - - - - - - - - - - - - - - 



-- we have our RDS security group around our Amazon RDS database in our VPC.

-- this Lambda function, we want to give it VPC access. Therefore once we've set it up correctly

-- it will create an ENI, an elastic network interface, alongside the Lambda security group

-- to access your RDS database, Your Lambda is going to go through your ENI. You know, it's invisible, we don't see it, but this is how it happens behind the scenes.

-- So it will go through the ENI into your Amazon RDS database.

-- so for this to work, we need to make sure that the RDS security group does allow network access from the Lambda security group, just like for EC2 instances and load balancer






------------------------------------------------------ Lambda in VPC – Internet Access


-- What if we deploy a Lambda function in a VPC, can we access the public internet?


• By default , A Lambda function in your VPC does not have internet access

• Deploying a Lambda function in a public subnet does not give it internet access or a public IP


------------- So what can we do then?


• Deploying a Lambda function in a private subnet gives it internet access if you have a NAT Gateway / Instance

    - The NAT gateway or instance will be talking to the internet gateway of our VPC and the internet gateway will give us access to the external API.


------------ Next, what if you want you to access DynamoDB?


-- we can access DynamoDB either through the public route and through your internet gateway.

-- So this would work once a NAT is put in place.

• You can use VPC endpoints to privately access AWS services without a NAT


IMP : So if you deploy a Lambda function in a private subnet, note that your CloudWatch Logs work even if you have no end points or NAT gateway. CloudWatch Logs is something that functions no matter what.





 ------------------------------------------------------ Lambda in VPC Hands On


-- create one lambda function with py 3.8

-- create one sg for lambda without any inbound rules 

--  go to configuration --> permissions --> add AWSLambdaENIManagementAccess to the IAM role 

-- So back into our Lambda function. Now let's go into the configuration and make sure that we can deploy this within our VPC

-- because right now the Lambda function is within the AWS cloud and it has internet access but it doesn't have VPC access.

-- if right now we publish our Lambda function in three subnets, And these are public subnets because they have access to the internet, as we know, The Lambda function can still not have access to the internet.

-- It needs instead to be published, deployed into a private subnets and have a NAT gateway or NAT instance in a public subnets to route traffic to.

-- our Lambda function is now deployed within our VPC. wait for 5 min to update

-- test ur code , now go to ENI we can see here three network interfaces have been created and the this respond to the network interfaces of my Lambda function within my VPC.
                
-- So these network interfaces each in one different AZ. So each in one different subnets is what allows my Lambda function to communicate with our VPC.








------------------------------------------------------ Lambda Function Improvement



Lambda Function Configuration


• RAM:

     • From 128MB to 10GB in 1MB increments

     • The more RAM you add, the more vCPU credits you get

     • At 1,792 MB of RAM, a function has the equivalent of one full vCPU

     • After 1,792 MB, you get more than one CPU, and need to use multi-threading in your code to benefit from it (up to 6 vCPU)


• If your application is CPU-bound (computation heavy), increase RAM

    - So if your application is CPU bound, that means that it has a lot of computations, and you want to improve the performance of your application, 
    
    - that means to decrease the amount of time your function will run for, then you need to increase your application,your lambda function RAM.


• Timeout: default 3 seconds, maximum is 900 seconds (15 minutes)

    - by default has a timeout of three seconds. That means that if your lambda function runs for more than three seconds, it will error out with a timeout,

    - Anything above 15 minutes is not a good use case for lambda and is something maybe that's going to be better for Fargate, ECS, or EC2.





------------------------------------------------------ Lambda Execution Context 


• The execution context is a temporary runtime environment that initializes any external dependencies of your lambda code

• Great for database connections, HTTP clients, SDK clients...

• The execution context is maintained for some time in anticipation of another Lambda function invocation

• The next function invocation can “re-use” the context to execution time and save time in initializing connections objects

• The execution context includes the /tmp directory

     - which is a space where you can write files and they will be available across executions.



------------------------------------------------------ Initialize outside the handler


BAD : The DB connection is established At every function invocation


import json
import time

def lambda_handler(event, context):
    connect_to_db()
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
def connect_to_db():
      time.sleep(3)




GOOD! : The DB connection is established Once And re-used across invocations

import json
import time

def connect_to_db():
      time.sleep(3)


connect_to_db()

def lambda_handler(event, context):
   
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }






------------------------------------------------------ Lambda Functions /tmp space


-- what if you need to write some temporary files and reuse them?

ANS : You can use the /tmp space.

• If your Lambda function needs to download a big file to work...

• If your Lambda function needs disk space to perform operations...

• You can use the /tmp directory

• Max size is 10GB

• The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations (helpful to checkpoint your work)

• For permanent persistence of object (non temporary), use S3

• To encrypt content on /tmp, you must generate KMS Data Keys




------------------------------------------------------ Lambda Function Improvement Hands On



step 1 :


-- lambda function --> configuraation --> general configuration --> keep timeout sec = 3 

-- now modify code for this demo 


import json
import time

def lambda_handler(event, context):
    time.sleep(2)
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }


-- here we make sleep = 2 sec , it will work 'coz we have given timeout sec = 3 , and see the executed time 

-- But what happens if we make the Lambda function sleep five seconds?

-- make sleep(5), do test 

-- the Lambda function will fail. Why? Because it will timeout. So we got an error message here saying, Hey the tasks timed out after three seconds

-- now change timeout = 6 secs in configuration 

-- now u will get response without any error



Step 2 :


------ Last thing to optimize your Lambda function performance is around where you set the initialization of your function.

-- So if you're connecting to a DB, modify code

import json
import time

def lambda_handler(event, context):
    connect_to_db()
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
def connect_to_db():
      time.sleep(3)


-- where we connect to the database within the Lambda handler,

-- that means that every time we invoke our function this function connects to DB is going to be run it's going to take three seconds 

-- because it takes a long time to connect your database and then is going to return the results you have.

-- test again , it will take 3 sec again because we are connecting to database every single time, 



Step 3 :


-- instead of doing the connection to the database within the Lambda handler, you do it outside of it.


import json
import time

def connect_to_db():
      time.sleep(3)


connect_to_db()

def lambda_handler(event, context):
   
  
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }



-- deploy and run the code , at first INIT time it will take 3 sec , then do test again 

-- it will take less time , now my function is much quicker because we've done the database initialization again outside of the function handler.

-- Imagine that instead of here, instead of sleeping, you actually connect to the database and you get a database object out of it that you can use within your Lambda handler






------------------------------------------------------ Lambda Layers


-- They're a newer feature of Lambda, they allow us to do two things.


• Custom Runtimes

      • Ex: C++ https://github.com/awslabs/aws-lambda-cpp

      • Ex: Rust https://github.com/awslabs/aws-lambda-rust-runtime



• Externalize Dependencies to re-use them:

     




------------------------------------------------------ Lambda Layers Hands On


-- create function with py 3.8

-- So we're going to use a layer provided by AWS because it is very complicated for us to create a layer

        https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/


--  go to layer in lambda function --> c.o layers --> add layer --> pythonSpicylayer ----> select latest version

-- copy code in website and paste in function 

-- So we can see the first two lines is import numpy as np, and imports a site by spatial and this function right here.

-- So this means that these things are being imported. 

-- And when you have imports on stuff that Lambda function do not know usually you have to pack it with dependencies just the way we did it for node.JS and NPM.

-- But it turns out that thanks to the layer the dependencies are actually available to our Lambda function, but we didn't have to pack them as dependencies from moving in our code, which is quite cool.

-- run code 

--  important part to remember is that the code itself of the function was using a library here.

-- And that library came from the Lambda layer we had added from before.





------------------------------------------------------ Lambda – File Systems Mounting


• Lambda functions can access EFS file systems if they are running in a VPC

• Configure Lambda to mount EFS file systems to local directory during initialization

• Must leverage EFS Access Points

• Limitations: watch out for the EFS connection limits (one function instance = one connection) and connection burst limits



-- see Lambda – Storage Options in documentation




------------------------------------------------------ Lambda Concurrency



-------------------- Lambda Concurrency and Throttling


-- In Lambda, concurrency is the number of in-flight requests that your function is currently handling. There are two types of concurrency controls available:


1 Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges.


2 Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account.






-- So the more we invoke our Lambda functions, the more we will have concurrent executions of our Lambda functions. We know this because Lambda can scale very, very easily and fast.

• Concurrency limit: up to 1000 concurrent executions

• Can set a “reserved concurrency” at the function level (=limit)

       EG : this Lambda function can only have "up to 50 concurrent executions.


• Each invocation over the concurrency limit will trigger a “Throttle”

• Throttle behavior:

       • If synchronous invocation => return ThrottleError - 429

       • If asynchronous invocation => retry automatically and then go to DLQ


• If you need a higher limit, open a support ticket




-------------------- Lambda Concurrency Issue


• If you don’t reserve (=limit) concurrency, the following can happen:

IMP : the concurrency limit applies to all the functions in your accounts, and so you have to be careful because if one function goes over the limit, it's possible that your other functions get throttled.




-------------------- Concurrency and Asynchronous Invocations


-- let's take the example of S3 event notifications. So we are uploading files into our S3 buckets,

-- and this creates a new file event that will invoke our Lambda functions,

-- say we are putting many, many files at the same time. So we get many, many different Lambda concurrent executions happening.

• If the function doesn't have enough concurrency available to process all events, additional requests are throttled.

• For throttling errors (429) and system errors (500-series), Lambda returns the event to the queue and attempts to run the function again for up to 6 hours. So there's a lot of retries that happens due to the throttling and so on.

       - 'coz it is asynchronous mode


• The retry interval increases exponentially from 1 second after the first attempt to a maximum of 5 minutes.





-------------------- Cold Starts & Provisioned Concurrency


• Cold Start:

      • New instance => code is loaded and code outside the handler run (init)

      • If the init is large (code, dependencies, SDK...) this process can take some time.

      • First request served by new instances has higher latency than the rest , user is unhappy

      - that may impact your users. So if your user is maybe waiting three seconds to get a request response, that may be very, very slow for them and 

      - they may experience a cold start and may be unhappy with your product.


• Provisioned Concurrency:

      • Concurrency is allocated before the function is invoked (in advance)

      • So the cold start never happens and all invocations have low latency

      • Application Auto Scaling can manage concurrency (schedule or target utilization),to make sure that you have enough reserved Lambda functions to be ready to be used and minimize this cold start problem.



• Note:
 
     • Note:cold starts inVPC have been dramatically reduced in Oct & Nov 2019

     - https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/





-------------------- Reserved and Provisioned Concurrency


https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html






------------------------------------------------------ Lambda external Dependencies


------------ Lambda Function Dependencies


-- we have been doing some pretty simple Lambda functions. Just some code, no external dependencies.

-- In the real world, you definitely need to add more dependencies with the packages and so on.

• If your Lambda function depends on external libraries: for example AWS X-Ray SDK, Database Clients, etc...

• You need to install the packages alongside your code and zip it together

      • For Node.js, use npm & “node_modules” directory

      • For Python, use pip --target options

      • For Java, include the relevant .jar files


• Upload the zip straight to Lambda if less than 50MB, else to S3 first

• Native libraries work: they need to be compiled on Amazon Linux

• AWS SDK comes by default with every Lambda function





------------------------------------------------------ Lambda external Dependencies Hands On


-- we're going to create a Lambda function with dependencies.


-- create 2 files 


1 index.js


// Require the X-Ray SDK (need to install it first)
const AWSXRay = require('aws-xray-sdk-core')

// Require the AWS SDK (comes with every Lambda function)
const AWS = AWSXRay.captureAWS(require('aws-sdk'))

// We'll use the S3 service, so we need a proper IAM role
const s3 = new AWS.S3()

exports.handler = async function(event) {
  return s3.listBuckets().promise()
}



2 steps.sh


#!/bin/bash

# You need to have nodejs / npm installed beforehand
npm install aws-xray-sdk

# Set proper permissions for project files
chmod a+r * 

# You need to have the zip command available
zip -r function.zip .

# create the Lambda function using the CLI
aws lambda create-function --zip-file fileb://function.zip --function-name lambda-xray-with-dependencies --runtime nodejs14.x --handler index.handler --role arn:aws:iam::001736599714:role/DemoLambdaWithDependencies





-- open cloushell

-- you need to create a Lambda folder.

       - mkdir lambda 
       - cd lambda
       - ll


-- the first thing is that you need to add this index.js file.

       - sudo yum install -y nano
       - nano index.js    (paste index.js content into this file )
       - ctrl + x  , then y , then enter
       - cat index.js



-- let's go into the steps to install some dependencies, and make sure the function will be packaged correctly to be uploaded into the Lambda service.

-- if we look at this index.js 

       - we can see that it requires the xray-sdk-core, and this is something that we need to bundle with our Lambda function.

       - Then we are opening the AWS SDK to talk to Amazon S3, and we return a ListBuckets operation into Amazon S3.


-- So what we need to do is to make sure that our Lambda function does have access to the xray-sdk for AWS.

       - npm install aws-xray-sdk        (which is going to install locally, all the necessary files and folders to have access to this SDK.)

       - ll  u will have the files 


-- Now in here, we need to edit the permissions of these files.

      - chmod a+r * 


-- Next, we need to zip all the, all the files in here into a zip file called functions.zip.

       - zip -r function.zip .

       - ll


-- now u have zip file , finally, we need to upload this .zip file into the Lambda console.

-- create lambda function through the CLI 

--  to do so , first u gave to create one role here IAM --> create new role for the lambda --> add AWSLambdaBasicExecutionRole permission ---> create role

-- now copy the role of ARN and paste in the cmnd 

  

      aws lambda create-function --zip-file fileb://function.zip --function-name lambda-xray-with-dependencies --runtime nodejs20.x --handler index.handler --role arn:aws:iam::298132369629:role/lambdawithdependencyrole


-- the function got created , go n check in console 

-- now do test , u will get errror , 'coz the lambda try to access s3 

-- add s3readonlypermissions in iam role

-- this version is not supporting , anyhow we can do this with external dependencies





------------------------------------------------------ Lambda and CloudFormation 


-- So we can use CloudFormation to upload Lambda function. And so the way we do it, we have two ways.


1 Lambda and CloudFormation – inline

     - so we would define our Lambda code inline of our CloudFormation templates.

     • Inline functions are very simple

     • Use the Code.ZipFile proper ty

     • You cannot include function dependencies with inline functions


2 Lambda and CloudFormation – through S3

     
     • You must store the Lambda zip in S3

     • You must refer the S3 zip location in the CloudFormation code

           • S3Bucket
           • S3Key: full path to zip
           • S3ObjectVersion: if versioned bucket


     • If you update the code in S3, but don’t update S3Bucket, S3Key or S3ObjectVersion, CloudFormation won’t update your function

          - this is why versioning is recommended because if you're enabled versioning and you overwrite the file and you specify a new S3 object version 
          
          - then CloudFormation will pick up the change and update your Lambda function.




------------------------------------------------------ Lambda and CloudFormation – through S3 Multiple accounts


-- say you will have an account which contains an S3 bucket with your Lambda code.

-- Now you want to deploy this Lambda code into Account 2 and Account 3.

-- How do this ?

-- first of all, we need to launch CloudFormation in Account 2. And the S3 bucket is going to be referencing is the S3 bucket in Account 1.

-- Now you need to ask yourself, how do we make sure that Account 2 has access to the Lambda code in Account 1?

ANS : - Well, we can use a bucket policy and a bucket policy on the S3 bucket in Account 1 should allow CloudFormation to access the code.

      - But also we can define an execution role on your CloudFormation service for the template itself which will allow to get and list to the S3 bucket in Account 1.

      - the two things combined is going to allow CloudFormation to retrieve the code from the S3 bucket and therefore create your Lambda function.

      - do same things for account 3 , 4.... also



------------------------------------------------------ Lambda and CloudFormation  Hands On


-- we're going to create a CloudFormation template upload it that will create a Lambda function for us.


lambda.yaml




Parameters:
  S3BucketParam:
    Type: String
  S3KeyParam:
    Type: String
  S3ObjectVersionParam:
    Type: String

Resources:
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: root
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:*
            Resource: arn:aws:logs:*:*:*
          - Effect: Allow
            Action:
            - xray:PutTraceSegments
            - xray:PutTelemetryRecords
            - xray:GetSamplingRules
            - xray:GetSamplingTargets
            - xray:GetSamplingStatisticSummaries
            Resource: "*"
          - Effect: Allow
            Action: 
            - s3:Get*
            - s3:List*
            Resource: "*"

  LambdaWithXRay: 
    Type: "AWS::Lambda::Function"
    Properties: 
      Handler: "index.handler"
      Role: 
        Fn::GetAtt: 
          - "LambdaExecutionRole"
          - "Arn"
      Code: 
        S3Bucket: 
          Ref: S3BucketParam
        S3Key: 
          Ref: S3KeyParam
        S3ObjectVersion:
          Ref: S3ObjectVersionParam
      Runtime: "nodejs20.x"
      Timeout: 10
      # Enable XRay
      TracingConfig:
        Mode: "Active"



-- here , we see that there are three parameters available to us. The S3 Bucket parameter, The S3 Key parameter and the S3 object version parameter.

-- these will be helpful to tell CloudFormation where to get the function zip from in Amazon S3.

-- Next we create resources.

-- the first resource we create is a Lambda execution role. So it's an IAM role. 

-- There is a policy document which allows Lambda functions to assume this IAM role  (AssumeRolePolicyDocument) 

-- as well as the policy itself. the policy contains multiple statements.

       - The first one is actions on CloudWatch logs

       - The second one is action on x-ray to be able to send traces to x-ray

       - the last one and finally some allow operations on S3, Get * , List * to be able to read it from Amazon S3.

-- Next to function itself, Lambda with x-ray.

-- So it's a function in which we have to specify the handlers so Index.handler

-- the role which is the Lambda execution role Arn that we get and retrieve thanks to this interesting function get attributes that will be get gotten from this Lambda role right here

-- So as we can see, the entire configuration of our Lambda function can be done through CloudFormation which is really awesome.




-- create one bucket in s3 , enable versioning (IMP), 

-- open cloud shell , cd lambda --> pwd --> copy path of it --> actions --> download file  and add /function.zip at last

        /home/cloudshell-user/lambda/function.zip


-- So now we can reuse this function.zip , upload this file in s3 bucket 

-- So now we are able to reference this function, zip directly from our CloudFormation templates.

-- now go to CF --> upload lambda.yaml file 

-- now add Parameters

       - S3BucketParam        = bucket Name 

       - S3KeyParam           = function.zip

       - S3ObjectVersionParam = BVhDrj7iqDmMhHmGC.TbSfmgQZZSNAzq (c.o on object --> versions)



-- create stack 

-- As soon as we opened this function we get an information message. This function belongs to an application because there was an integration between CloudFormation and Lambda that was detected by the Lambda console.

-- So it knows that it is managed by a CloudFormation template.

-- module is not supporting here  is not supporitng here but overall idea is that to understnad the concept 

-- u can also see x-ray tracing is enabled 




------------------------------------------------------ Lambda Container Images


-- Lambda was supporting container images and this is a new feature. And this allows to deploy Lambda Function as a container of images up to 10 gigabytes from ECR


• Deploy Lambda function as container images of up to 10GB from ECR

• Pack complex dependencies, large dependencies in a container

           - So Docker is very famous to allow you to help to put your application code, the dependencies, and the data sets it needs all together on top of a base image.

           - And that base image must implement the Lambda Runtime API.

           - So the idea is that to make it seemingly simple, Lambda runs a virtual machine, the container. And so you use the base image of that container,

           - you add on your application code and your dependencies and you pack this as an image that Lambda can run

           - because this base image implements the Lambda Runtime API. And this allows you to run your containers onto Lambda,

           - but it's not any Docker container. The base image must implement the Lambda Runtime API. So these base images exist for multiple languages.



• Base images are available for Python, Node.js, Java, .NET, Go, Ruby

• Can create your own image as long as it implements the Lambda Runtime API

• Test the containers locally using the Lambda Runtime Interface Emulator

• Unified workflow to build apps




            |-----------------------------------------------------
            |       Application Code                             |
            |------------------------------------                |
            |      Dependencies, datasets                        |
            |------------------------------------                |
            |  Base Image must implement the Lambda Runtime API  |
            |----------------------------------------------------
                           |
                           |  Build Publish
                           |
                           |

                        Amazon ECR

                           |
                           |   Deploy
                           |
                           |
                         Lambda
                          
                           



• Example: build from the base images provided by AWS 


# Use an image that implements the Lambda Runtime API

    FROM amazon/aws-lambda-nodejs:12

# Copy your application code and files

    COPY app.js package*.json ./

# Install the dependencies in the container

    RUN npm install

# Function to run when the Lambda function is invoked

    CMD [ "app.lambdaHandler" ]




------------------------------------------------------ Lambda Container Images – Best Practices


• Strategies for optimizing container images:


1 • Use AWS-provided Base Images

       • Stable,Built on AmazonLinux2, 'coz these base images are already cached by Lambdaservice 

       - so if you use them, then the Lambda service has to pull less information out of your containers.


2 • Use Multi-Stage Builds

    
       • Build your code in larger preliminary images, copy only the artifacts you need in your final container image, discard the preliminary steps

       - Therefore, the final image is going to be much smaller and much more simple.


3 • Build from Stable to Frequently Changing

       • Make your most frequently occurring changes as late in your Dockerfile as possible

       - for example installing base packages on your image should be as early as possible.


4 • Use a Single Repository for Functions with Large Layers

       • ECR compares each layer of a container image when it is pushed to avoid uploading and storing duplicates




• Use them to upload large Lambda Functions (up to 10 GB) 

       - instead of having some code pushed to Lambda as is, you can create a very large container image and use that as the basis for your Lambda Function.






------------------------------------------------------ AWS Lambda Versions


• When you work on a Lambda function, we work on $LATEST

     $LATEST (mutable) -  we can able to change / edit 



• When we’re ready to publish a Lambda function, we create a version

• Versions are immutable

     $LATEST (mutable) ---------> V1 (Immutable) and V2 (immutable)


• Versions have increasing version numbers (V1,v2,v3.......)


• Versions get their own ARN (Amazon Resource Name)

• Version = code + configuration (nothing can be changed - immutable)

• Each version of the lambda function can be accessed




------------------------- But what if he wants to give your end user a standard endpoint?



So for this, we can use Lambda Aliases.



------------------------------------------------------ AWS Lambda Aliases


• Aliases are ”pointers” to Lambda function versions

• We can define a “dev”, ”test”, “prod” aliases and have them point at different lambda versions

• Aliases are mutable

• Aliases enable Canary deployment by assigning weights to lambda functions

     - Because we can assign weights to the Lambda function versions we point to.

     - So for PROD for example, say we want to switch from the V1 function all the way to the V2 function.

     - Instead of switching the pointer, we can say 95% of the traffic is going to go to V1. And only 5% of the traffic is going to go to V2.

     - the purpose of this is that now we're testing V2 in PROD, making sure it works before we switch the full power onto V2 and 100% of the traffic there.



• Aliases enable stable configuration of our event triggers / destinations

• Aliases have their own ARNs

IMP : • Aliases cannot reference aliases , They can only reference version 




                                                          USER

                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                                      |    |      |
                                   --------------------    |       ---------------------      
                                  |                        |                           |
                                  |                        |                           |
                                  |                        |                           |
                                  |                        |                           |
                                  |                        |                           |
                         DEV Alias (mutable)         PROD Alias (mutable)        TEST Alias (mutable)

                                  |                       |          |                  |
                                  |                       |          |                  |
                                  |                       |    95%   |   5%             |
                                  |                       |          |                  |
                                  |                       |          |-------------     |
                                  |                       |                        |    |
                                                                                   |    |
                          $LATEST (mutable)          V1 (Immutable)                 V2 (Immutable)  






 ------------------------------------------------------ AWS Lambda Versions Aliases Hands On


 -- create lambda function with py 3.8

 -- So versions are here to allow us to fix a amount of code and removables and settings in time,

 -- say you are a specific version. And the idea is that we want to author our versions over time to allow our lambda function to evolve.

 -- make code like this to test 


 import json

def lambda_handler(event, context):
    # TODO implement
    return "this is version 1"



-- So say we're very happy with this code and we want to author this as the version one of our function.

-- actions --> publish new version --> publish --> here u were not able to  change ur code which is immutable

-- if u want to edit , u can do in main function

-- now go to function code and make it as v2 

-- do publish new version 2

-- main function --> versions --> u will get all versions here



--- now coming to Aliases 

-- main function --> aliases --> create aliases --> dev ---> latest version --> create

-- main function --> aliases --> create aliases --> Test ---> 2 --> create ,  it is latest published version

-- main function --> aliases --> create aliases --> Prod ---> 1 --> create ,  it is most stable version of our function 


--  we have created versions and Aliases 

-- in aliases , we have see the weight for deployment

-- So let's go into prod and we want to edit this alias because we want to upgrade from V1 to V2

-- go to aliases --> prod --> edit --> select weighted alias as V2 version --> give some portion of traffic for testing purpose (eg : 50%)

-- So now our alias "prod" has a weight between two versions.

-- open prod and do test , here 50% of traffic for v1 and remaing for v2 

-- then when you're happy with your production and say okay now my new function version is working just as expected,

-- edit prod from v1 to v2 






------------------------------------------------------ Lambda & CodeDeploy



• CodeDeploy can help you automate traffic shift for Lambda aliases

• CodeDeploy Feature is integrated within the SAM framework(Serverless Application Model)



    

  


                                                          Make X vary over time until X = 100%
                                                      | -----------------------------------------------   
                                                      |                                                |
                                                      |                                                |  
                                                      |                                                |
                                                      |                                                |
                                                      |           PROD Alias (lambda) -----(100 – X%)------>    V1                 
                                                      |                 |
                                                      |                 |
                                                      |                 | --------------V2
                                                      |                                  

                                                 Code Deploy ---------------> X%



-- So we have a PROD Alias, and say we want to upgrade it from the Lambda function Version 1 to the lambda function Version 2.

-- So we want to shift the traffic from 100% of V1 to V2.

-- CodeDeploy will be making the X, in this example, vary over time until X equals 100.

-- So that means that X will be first equal to 10%. And so we'll have 90% on V1 and 10% on V2.

-- finally, all the way up to 100% on V2 and 0% on V1.

------------- what strategies that CodeDeploy have?


• Linear: grow traffic every N minutes until 100%

      • Linear10PercentEvery3Minutes 

      • Linear10PercentEvery10Minutes

• Canary: try X percent then 100%

      • Canary10Percent5Minutes

      • Canary10Percent30Minutes


• AllAtOnce: immediate 

      - This is the quickest and most dangerous

      - because if we haven't tested our V2 function, then things can fail.



• Can create Pre & Post Traffic hooks to check the health of the Lambda function

     - the idea is that if anything goes wrong, then the traffic hooks can be failing or a CloudWatch alarm can be failing,

     - your CodeDeploy can know that something is going wrong. And therefore, it will do a rollback and put the traffic back all onto V1.





------------------------------------------------------ Lambda & CodeDeploy – AppSpec.yml


version 0.0

Resources :
  - myLambdaFunction:
       Type: AWS::lambda::Function
       Properties:
          Name: myLambdaFunction
          Alias: myLambdaFunctionAlias
          CurrentVersion: 1
          targetVersion: 2



-- • Name (required) – the name of the Lambda function to deploy

• Alias (required) – the name of the alias to the Lambda function

• CurrentVersion (required) – the version of the Lambda function traffic currently points to

• TargetVersion (required) – the version of the Lambda function traffic is shifted to





------------------------------------------------------ Lambda – Function URL


-- What if you wanted to just expose your Lambda function as an HTTP endpoint without having to go through the hassle of using API Gateway or an application balancer?



• Dedicated HTTP(S) endpoint for your Lambda function

• A unique URL endpoint is generated for you (never changes)
 
           • https://<url-id>.lambda-url.<region>.on.aws (dual-stack IPv4 & IPv6)


• Invoke via a web browser, curl, Postman, or any HTTP client

• Access your function URL through the public Internet only
   
      • Doesn’t support PrivateLink (Lambda functions do support)


• Supports Resource-based Policies & CORS configurations

• Can be applied to any function alias or to $LATEST (can’t be applied to other function versions)

• Create and configure using AWS Console or AWS API

• Throttle your function by using Reserved Concurrency





------------------------------------------ Lambda – Function URL Security


• Resource-based Policy , this gets attached to your Lambda function and this is going to be able to say

       • Authorize other accounts / specific CIDR / IAM principals


• Cross-Origin Resource Sharing (CORS)

       • If you call your Lambda function URL from a different domain



                example.com
              <------------------------>      <--------------------> S3 Bucket (Static Website Hosting)
       users                            C.F
              <------------------------
                api.example.com      |
                                     |
                                     |
                               Lambda Function URL      


        - So in this example, our S3 bucket is fronted by CloudFront to which we have a custom URL as example.com. But the API is hosted as a Lambda function URL,

        - Because the domains are different, you need to set the CORS setting on your Lambda function URL to make things work.



-- if u have 

• AuthType NONE – allow public and unauthenticated access

      • Resource-based Policy is always in effect (must grant public access)


• AuthType AWS_IAM – IAM is used to authenticate and authorize requests

      • Both Principal’s Identity-based Policy & Resource-based Policy are evaluated

      • Principal must have lambda:InvokeFunctionUrl permissions

      • Same account – Identity-based Policy OR Resource-based Policy as ALLOW

      • Cross account – Identity-based Policy AND Resource Based Policy as ALLOW






------------------------------------------------------ Lambda – Function URL Hands On


-- create a function with py 3.9

-- publish this function 

-- create alias for this version

-- lambda function --> alias --> function URL --> select Identity policy (None) --> save 

-- url is generated , paste in browser u will o/P

-- so we were able to access our Lambda Function publicly, through a URL.






------------------------------------------------------ Lambda and CodeGuru Profiling 


• Gain insights into runtime performance of your Lambda functions using CodeGuru Profiler

• CodeGuru creates a Profiler Group for your Lambda function

• Supported for Java and Python runtimes

• Activate from AWS Lambda Console

• When activated, Lambda adds:

       • CodeGuru Profiler layer to your function

       • Environment variables to your function

       • AmazonCodeGuruProfilerAgentAccess policy to your function





------------------------------------------------------ AWS Lambda Limits to Know - per region



• Execution:
  
      • Memory allocation: 128 MB – 10GB (1 MB increments)

      • Maximum execution time: 900 seconds (15 minutes)

      • Environment variables (4 KB)

      • Disk capacity in the “function container” (in /tmp): 512 MB to 10GB

      • Concurrency executions: 1000 (can be increased)



• Deployment:

      • Lambda function deployment size (compressed .zip): 50 MB

      • Size of uncompressed deployment (code + dependencies): 250 MB

      • Can use the /tmp directory to load other files at startup

      • Size of environment variables: 4 KB





------------------------------------------------------ AWS Lambda Best Practices



• Perform heavy-duty work outside of your function handler

      • Connect to databases outside of your function handler

      • Initialize the AWS SDK outside of your function handler

      • Pull in dependencies or datasets outside of your function handler



• Use environment variables for:

      • Database Connection Strings, S3 bucket, etc... don’t put these values in your code

      • Passwords, sensitive values... they can be encrypted using KMS


• Minimize your deployment package size to its runtime necessities.

      • Break down the function if need be

      • Remember the AWS Lambda limits

      • Use Layers where necessary


• Avoid using recursive code, never have a Lambda function call itself









====================================================================== Amazon DynamoDB (NoSQL Serverless Database)=======================================================



---------------------------------------------------------- Traditional Architecture


• Traditional applications leverage RDBMS databases

• These databases have the SQL query language

• Strong requirements about how the data should be modeled

• Ability to do query joins, aggregations, complex computations

• Vertical scaling (getting a more powerful CPU / RAM / IO)

• Horizontal scaling (increasing reading capability by adding EC2 / RDS Read Replicas)





---------------------------------------------------------- NoSQL databases


• NoSQL databases are non-relational databases and are distributed

• NoSQL databases include MongoDB, DynamoDB, ...

• NoSQL databases do not support query joins (or just limited support)

• All the data that is needed for a query is present in one row

• NoSQL databases don’t perform aggregations such as “SUM”, “AVG”, ...

• NoSQL databases scale horizontally

        - That means that if you need more write or read capacity, you can behind the scenes have more instances and it will scale really well.



• There’s no “right or wrong” for NoSQL vs SQL, they just require to model the data differently and think about user queries differently




---------------------------------------------------------- Amazon DynamoDB


• Fully managed, highly available with replication across multiple AZs

• NoSQL database - not a relational database

• Scales to massive workloads, distributed database

• Millions of requests per seconds, trillions of row, 100s of TB of storage

• Fast and consistent in performance (low latency on retrieval)

• Integrated with IAM for security, authorization and administration

• Enables event driven programming with DynamoDB Streams

• Low cost and auto-scaling capabilities

• Standard & Infrequent Access (IA) Table Class

-- our "DynamoDB supports "Document and key-value(JSON) pair" 




---------------------------------------------------------- DynamoDB - Basics


• DynamoDB is made of Tables

• Each table has a Primary Key (must be decided at creation time)

• Each table can have an infinite number of items (= rows)

• Each item has attributes (can be added over time – can be null)

         - But these attributes can also be nested. So it's a bit more powerful than columns,

         - they can be added over time, you don't need to define them all at creation time of your table, and some of them can be null.

         - so it's completely fine for an attribute to be missing in some data.


• Maximum size of an item is 400KB

• Data types supported are:
 
         • Scalar Types – String, Number, Binary, Boolean, Null

         • Document Types – List, Map

         • Set Types – String Set, Number Set, Binary Set




---------------------------------------------------------- DynamoDB – Primary Keys


--------- So you have two options for primary keys.



--------- • Option 1: Par tition Key (HASH)

       • Partition key must be unique for each item 

       • Partition key must be “diverse” so that the data is distributed

       • Example:“User_ID” for a users table


  

        User_ID              First_Name        Last_Name      Age

        7791a3d6             subbu             kattamedi      20
        u63527db             jncjdnc                          33
        7357336gv            lsidmckdmc                       23



--  User_ID  = Partition Key (primary key),    

--  First_Name        Last_Name      Age    = Attributes

-- here , missing attributes is fine in dynamoDB



--------- • Option 2: Partition Key + Sort Key (HASH + RANGE)

                   • The combination must be unique for each item

                   • Data is grouped by partition key , this is why it's very important to choose a good partition key.

                   • Example: users-games table,“User_ID” for Partition Key and “Game_ID” for Sort Key

                         - That means that users can attend multiple games.



          

          User_ID          Game_ID            Score         Result

          7791a3d6          2233              20              win
          98377366          3421              40             lost
           98377366         2123              33              win


-- “User_ID” for Partition Key and “Game_ID” for Sort Key  and    Score and Result = attributes

-- here for 2 and 3 are same partiton key but different Sort key





---------------------------------------------------------- DynamoDB – Partition Keys (Exercise)


• We’re building a movie database

• What is the best Partition Key to maximize data distribution?

       • movie_id

       • producer_name

       • leader_actor_name

       • movie_language


• “movie_id” has the highest cardinality so it’s a good candidate

• “movie_language” doesn’t take many values and may be skewed towards English so it’s not a great choice for the Partition Key





---------------------------------------------------------- DynamoDB – Read/Write Capacity Modes


• Control how you manage your table’s capacity (read/write throughput)


• Provisioned Mode (default) 

      • You specify the number of reads/writes per second

      • You need to plan capacity beforehand

      • Pay for provisioned read & write capacity units


• On-Demand Mode

      • Read/writes automatically scale up/down with your workloads

      • No capacity planning needed

      • Pay for what you use, more expensive ($$$)


• You can switch between different modes once every 24 hours      

       






---------------------------------------------------------- R/W Capacity Modes – Provisioned


• Table must have provisioned read and write capacity units

• Read Capacity Units (RCU) – throughput for reads

• Write Capacity Units (WCU) – throughput for writes

• Option to setup auto-scaling of throughput to meet demand

• Throughput can be exceeded temporarily using “Burst Capacity”

• If Burst Capacity has been consumed, you’ll get a “ProvisionedThroughputExceededException”

• It’s then advised to do an exponential backoff retry





---------------------------------------------------------- DynamoDB – Write Capacity Units (WCU)


• One Write Capacity Unit (WCU) represents one write per second for an item up to 1 KB in size

• If the items are larger than 1 KB, more WCUs are consumed


• Example 1: we write 10 items per second, with item size 2 KB

ANS :     • We need  10 * (2 KB / 1 KB)  = 20 WCU's


• Example 2: we write 6 items per second, with item size 4.5 KB

ANS : • We need  6 * (5 KB / 1 KB)  = 30 WCU's , because the 4.5 kilobytes is always going to be rounded to the upper kilobyte by DynamoDB to get an idea of how many WCUs you've consumed.


• Example 3: we write 120 items per minute, with item size 2 KB

ANS : we need (120 / 60 ) * (2KB / 1KB)  = 4 WCU'S , We need to do a small computation which is 120 divided by 60 to get items per second,


--  1 WCU = 2.5 million Writes 



---------------------------------------------------------- Strongly Consistent Read vs. Eventually Consistent Read




-- So if you consider DynamoDB, it's a serverless database, of course, but behind the scenes, there are servers.

-- You just don't see them or manage them.

-- So we have servers, and let's just consider three servers right now to make it very, very simple, but obviously a lot more,

-- your data is going to be distributed and replicated across all the servers.

-- Now, if you consider your application, your application is going to do writes to one of these servers,

-- internally DynamoDB is going to replicate these writes across different servers, such as Server 2 and Server 3.

-- Now, when your application reads from DynamoDB, there is a chance that you're going to read not from Server 1 but from Server 2.

-- Now, two things can happen



1 • Eventually Consistent Read (default) 

        • If we read just after a write, it’s possible we’ll get some stale(old) data because of replication has not happened yet


2 • Strongly Consistent Read

       • If we read just after a write, we will get the correct data

       • For this, we need to Set “ConsistentRead” parameter to True in API calls (GetItem, BatchGetItem, Query, Scan)    

       • Consumes twice the RCU

       - so it's going to be a more expensive query and also may have a slightly higher latency.  





---------------------------------------------------------- DynamoDB – Read Capacity Units (RCU)



• One Read Capacity Unit (RCU) represents one Strongly Consistent Read per second, or two Eventually Consistent Reads per second, for an item up to 4 KB in size


      - For ECR , 1 RCU = 2 Reads per second of 4KB size

      - For SCR, 1RCU = 1 read per second of 4KB Size 


• If the items are larger than 4 KB, more RCUs are consumed

      - it's going to be rounded up to the nearest upper four kilobytes.


• Example 1: 10 Strongly Consistent Reads per second, with item size 4 KB

ANS : 


-- we kanow the basic of 

   -- For ECR , 1 RCU = 2 Reads per second of 4KB size

   -- For SCR, 1RCU = 1 read per second of 4KB Size 


-- here it have mentioned SCR , so 1RCU = 1 read per second of 4KB Size 


we need 10 * (4Kb / 4KB) = 10 RCU'S




• Example 2: 16 Eventually Consistent Reads per second, with item size 12 KB 

ANS : here it is ECR so,


we need (16 / 2) * (12KB / 4KB)  = 8 * 3 = 24 RCU'S


• Example 3: 10 Strongly Consistent Reads per second, with item size 6 KB

ANS : 10 * (8KB / 4KB) = 20 RCU's (we must roundup 6KB to 8KB)

        - here we do not have 6 we have only 4 and 8 KB so,go for near (only up) so 8KB 






---------------------------------------------------------- DynamoDB – Partitions Internal



-- how DynamoDB works in the backend with partitions.?


• Data is stored in partitions , partitions are just copies of your data that live on specific servers.

       - when your application does writes into DynamoDB, what's going to happen is that your application will send a partition key, a sort key maybe, and some attributes.


• Partition Keys go through a hashing algorithm to know to which partition they go to

      - So if we take the partition key of ID_13, it's going to go through this internal hash function of DynamoDB, t's going to say, "Hey, any time I see ID_13, this is going to go into Partition 1."

      - if you have ID_45 in the second row, then ID_45 is going to go through the hash function, and the hash function will say, "Hey, this ID_45 should go to Partition 2."



-- So, obviously, if you have what's called a hot partition, the data is always going to be the hot key, the data is always going to be into the same partition.


• To compute the number of partitions:

     • # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by capacity = (RCU's Total / 3000) + (WCU's Total / 1000)


     • # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by Size     = Total Size / 10 GB


      • # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠            = ceil(max(# 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by capacity, # 𝑜𝑓 𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛𝑠 by Size ))



• WCUs and RCUs are spread evenly across partitions






---------------------------------------------------------- DynamoDB – Throttling


• If we exceed provisioned RCUs or WCUs, we get “ProvisionedThroughputExceededException”

• Reasons:

     • Hot Keys – one partition key is being read too many times (e.g., popular item) from a specific partition,

     • Hot Partitions

     • Very large items, remember RCU and WCU depends on size of items


• Solutions:

     • Exponential backoff when exception is encountered (already in SDK)

     • Distribute partition keys as much as possible (how we can choose a really good partition key.)

     • If RCU issue, we can use DynamoDB Accelerator (DAX)





---------------------------------------------------------- R/W Capacity Modes – On-Demand


• Read/writes automatically scale up/down with your workloads

• No capacity planning needed (WCU / RCU)

• Unlimited WCU & RCU, no throttle, more expensive

• You’re charged for reads/writes that you use in terms of RRU and WRU

• Read Request Units (RRU) – throughput for reads (same as RCU)

• Write Request Units (WRU) – throughput for writes (same as WCU)

• 2.5x more expensive than provisioned capacity (use with care)

• Use cases: unknown workloads, unpredictable application traffic, ...






---------------------------------------------------------- DynamoDB - Basic Operations 



--------------------------- DynamoDB – Writing Data


• PutItem

     • Creates a new item or fully replace an old item (same Primary Key)

     • ConsumesWCUs


• UpdateItem

     • Edits an existing item’s attributes or adds a new item if it doesn’t exist , with UpdateItem, we only edit a few attributes, not every other attribute.

     • Can be used to implement Atomic Counters – a numeric attribute that’s unconditionally incremented



• Conditional Writes

     • Accept a write/update/delete only if conditions are met, otherwise returns an error

     • Helps with concurrent access to items

     • No performance impact




--------------------------- DynamoDB – Reading Data


• GetItem

      • Read based on Primary key

      • Primary Key can be HASH or HASH+RANGE

      • Eventually Consistent Read (default)

      • Option to use Strongly Consistent Reads (more RCU - might take longer)

      • ProjectionExpression can be specified to retrieve only certain attributes




--------------------------- DynamoDB – Reading Data (Query)


• Query returns items based on:

       • KeyConditionExpression
           
            • Partition Key value (must be = operator) – required

            • SortKeyvalue(=,<,<=,>,>=,Between,Beginswith)–optional

      
      • FilterExpression

            • Additional filtering after the Query operation (before data returned to you)

            • Use only with non-key attributes (does not allow HASH or RANGE attributes)




• Returns:

       • The number of items specified in Limit

       • Or upto 1MB of data

• Ability to do pagination(Pagination is splitting a database output into manageable chunks or pages)  on the results

• Can query table, a Local Secondary Index, or a Global Secondary Index






--------------------------- DynamoDB – Reading Data (Scan)


• Scan the entire table and then filter out data (inefficient)

• Returns up to 1 MB of data – use pagination to keep on reading (page 1, page2 ,....)

• Consumes a lot of RCU

• Limit impact using Limit or reduce the size of the result and pause

• For faster performance, use "Parallel Scan"

     • Multiple workers scan multiple data segments at the same time

     • Increases the throughput and RCU consumed

     • Limit the impact of parallel scans just like you would for Scans


• Can use ProjectionExpression & FilterExpression (no changes to RCU)




--------------------------- DynamoDB – Deleting Data


• DeleteItem

     • Delete an individual item

     • Ability to perform a conditional delete


• DeleteTable

     • Delete a whole table and all its items

     • Much quicker deletion than calling DeleteItem on all items




--------------------------- DynamoDB – Batch Operations


• Allows you to save in latency by reducing the number of API calls

• Operations are done in parallel for better efficiency

• Part of a batch can fail; in which case we need to try again for the failed items


• BatchWriteItem

     • Up to 25 PutItem and/or DeleteItem in one call

     • Up to 16 MB of data written,up to 400 KB of data per item

     • Can’t update items (use UpdateItem)

     • UnprocessedItems for failed write operations(exponential backoff or add WCU)

          - you can retry the items within the UnprocessedItems. So two options to process them correctly.

          - Either you use an exponential backup strategy to keep on trying with longer and longer time until it succeeds,

          - if you consistently get these UnprocessedItems and scaling issues, then of course, you need to add write capacity units to allow your batch operations to complete efficiently.


• BatchGetItem

      • Return items from one or more tables

      • Up to 100 items,up to 16 MB of data

      • Items are retrieved in parallel to minimize latency

      • UnprocessedKeys for failed read operations (exponential backoff or add RCU)





--------------------------- DynamoDB – PartiQL


-- we have specific API calls to do specific things, but sometimes all that you know, as a data engineer or whatever, as a developer, may be SQL.

-- so you can use SQL on DynamoDB by using PartiQL.


• SQL-compatible query language for DynamoDB

• Allows you to select, insert, update, and delete data in DynamoDB using SQL , But this time, instead of doing the DynamoDB specific APIs you can just use SQL.

• Run queries across multiple DynamoDB tables , but you cannot do joins

• Run PartiQL queries from:

     • AWS Management Console
     • NoSQL Workbench for DynamoDB
     • DynamoDB APIs
     • AWS CLI
     • AWS SDK


-- the goal of it is really not to add new capabilities to DynamoDB because you have the same capabilities, but it's just to use SQL to write these API calls against DynamoDB.





-------- see hands on in course for basic API'S







---------------------------------------------------------- DynamoDB – Conditional Writes



-- So this is for the write operations.


• For PutItem, UpdateItem, DeleteItem, and BatchWriteItem

• You can specify a Condition expression to determine which items should be modified:

    • attribute_exists
    • attribute_not_exists
    • attribute_type
    • contains (for string)
    • begins_with (for string)
    • ProductCategory IN (:cat1, :cat2) and Price between :low and :high • size (string length)


• Note: Filter Expression filters the results of read queries, while Condition Expressions are for write operations





---------------------------------------------------------- Conditional Writes – Example on Update Item



-- To perform a conditional update, you use an UpdateItem operation with a condition expression. The condition expression must evaluate to true in order for the operation to succeed; otherwise, the operation fails.



EG :


aws dynamodb update-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --update-expression "SET Price = Price - :discount" \
    --condition-expression "Price > :limit" \
    --expression-attribute-values file://values.json



-- we say that we want to set the price to be equal to the current price minus discounts only if the price is over to a specific limit.

-- That's our condition expression.

-- We pass in the file://values.json , which has the discount to be a number of 150 and the limit to be a number of 500.


{
    ":discount": { "N": "150"},
    ":limit": {"N": "500"}
}



-- So that means that if we have this item in our DynamoDB table with a key 456 if we apply this Update Item command is going to be transformed to be now having a price of 500.


{
    "Id": { "N": "456"},
    "Price": {"N": "650"},
    "ProductCategory": {"S": "Sporting Goods"}
}


             |
             |
             |
             |


{
    "Id": { "N": "456"},
    "Price": {"N": "500"},
    "ProductCategory": {"S": "Sporting Goods"}
}


-- That's because indeed before the price was strictly over the limit of 500 because 650 is greater than 500.

-- But if we apply yet again the same command now it's not going to succeed. The price will never go below 500 

-- because, well, the condition expression would evaluate to being false.







---------------------------------------------------------- Conditional Writes – Example on Delete Item



• attribute_not_exists

      • Only succeeds if the attribute doesn’t exist yet (no value)


aws dynamodb update-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "attribute_not_exists(price)"



-- For example, we're saying hey, delete an item or delete many items if you have a batch right item we can do delete one item for our product catalog if the attribute price doesn't exist.



• attribute_exists

     • Opposite of attribute_not_exists


aws dynamodb update-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "attribute_exists(ProductReviews.onestar)"






---------------------------------------------------------- Conditional Writes – Do Not Overwrite Elements


• attribute_not_exists(partition_key)

     • Make sure the item isn’t overwritten


• attribute_not_exists(partition_key) and attribute_not_exists(sort_key)

     • Make sure the partition / sort key combination is not overwritten






---------------------------------------------------------- Conditional Writes – Example Complex Condition



aws dynamodb delete-item \
    --table-name ProductCatalog \
    --key '{"Id":{"N":"456"}}' \
    --condition-expression "(ProductCategory IN (:cat1, :cat2)) and (Price between :lo and :hi)" \
    --expression-attribute-values file://values.json



-- The arguments for --expression-attribute-values are stored in the values.json file.


{
    ":cat1": {"S": "Sporting Goods"},
    ":cat2": {"S": "Gardening Supplies"},
    ":lo": {"N": "500"},
    ":hi": {"N": "600"}
}


-- we can pass in this values.json file we're specifying category 1 and category 2 as well as the low of 500 and the high of 600.

-- And so when we apply Delete Item in case the item does belong to this range then we're good to go.

-- But as we can see, for example, even though the products category Sporting Good is compliant with our condition, which is the first part of our condition,

{
    "Id": { "N": "456"},
    "Price": {"N": "650"},
    "ProductCategory": {"S": "Sporting Goods"}
}


-- the price is 650, which is not between 500 and 600. And so the whole condition is false and the item will not be deleted in this case.






---------------------------------------------------------- Conditional Writes – Example of String Comparisons


• begins_with – check if prefix matches

• contains – check if string is contained in another string



aws dynamodb delete-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "begins_with(Pictures.FrontView, :v_sub)" \
    --expression-attribute-values file://expression-attribute-values.json



-- The arguments for --expression-attribute-values are stored in the expression-attribute-values.json file.


{
    ":v_sub":{"S":"http://"}
}



-- So here for example, we're saying, hey I want to make sure to delete items if it begins with http.://




--You can check for an element in a set or look for a substring within a string by using the contains function. If the condition expression evaluates to true, the operation succeeds; otherwise, it fails.


-- The following example uses contains to delete a product only if the Color String Set has an element with a specific value.



aws dynamodb delete-item \
    --table-name ProductCatalog \
    --key '{"Id": {"N": "456"}}' \
    --condition-expression "contains(Color, :v_sub)" \
    --expression-attribute-values file://expression-attribute-values.json



-- The arguments for --expression-attribute-values are stored in the expression-attribute-values.json file.


{
    ":v_sub":{"S":"Red"}
}







---------------------------------------------------------- DynamoDB Indexs – Local Secondary Index (LSI)




