

IMP NOTE : ALL the Topics have covered in the Solution architect Associate , here I have prepared the Most advanced topics for Associate Developer which was not covered in the solution architect associate



=========================================================== CloudFront =========================================


-------------------- CloudFront Caching 

• The cache lives at each CloudFront Edge Location

• CloudFront identifies each object in the cache using the Cache Key 

• You want to maximize the Cache Hit ratio to minimize requests to the origin

• You can invalidate part of the cache using the CreateInvalidation API


-------------------- What is CloudFront Cache Key?

• A unique identifier for every object in the cache

• By default, consists of hostname + resource portion of the URL

• If you have an application that serves up content that varies based on user, device, language, location...

• You can add other elements (HTTP headers, cookies, query strings) to the Cache Key using "CloudFront Cache Policies"


-------------------- CloudFront Policies – Cache Policy

• Cache based on:

    • HTTP Headers: None – Whitelist

    • Cookies: None – Whitelist – Include All-Except – All

    • Query Strings: None – Whitelist – Include All-Except – All


• Control the TTL (0 seconds to 1 year), can be set by the origin using the Cache-Control header, Expires header...

• Create your own policy or use Predefined Managed Policies

IMP : • All HTTP headers, cookies, and query strings that you include in the Cache Key are automatically included and forward to ur  origin requests



------------------ 1 CloudFront Caching – Cache Policy HTTP Headers

- let's say we have an example request in french language


GET /blogs/myblog.html HTTP/1.1
Host: mywebsite.com
User-Agent: Mozilla/5.0 (Mac OS X 10_15_2....) 
Date: Tue, 28 Jan 2021 17:01:57 GMT 
Authorization: SAPISIDHASH fdd00ecee39fe.... Keep-Alive: 300
Language: fr-fr


-- so if we define none : 

• None:
   
    • Don’t include any headers in the Cache Key (except default)
    • Headers are not forwarded (except default)
    • Best caching performance

• Whitelist: 

    • only specified headers included in the Cache Key
      
       -     if u want to whitelist specific Headers and that may mean necessary because while you want to have the language as a Cache Key, then you specify which headers you want to include in the Cache Key,

    • Specified headers are also forwarded to Origin

       - so that the origin can actually respond to the request and give you the blog in the correct language.




---------------------------- 2 CloudFront Cache – Cache Policy Query Strings


- for example we have a request like 

 GET /image/cat.jpg?border=red&size=large HTTP/1.1


-- So query strings are what happens in the URL after a question mark. 

-- So for example, border equals red and size equals large. So here, we want a cat image. But apparently, it's going to be customized a little bit by the origin.

-- if you have 

• None : 

    • Don’t include any query strings in the Cache Key

    • Query strings are not forwarded

• Whitelist

    • Only specified query strings included in the Cache Key

    • Only specified query strings are forwarded


• Include All-Except : you specify which ones you don't want but the rest passes

    • Include all query strings in the Cache Key except the specified list

    • All query strings are forwarded except the specified list


• All

    • Include all query strings in the Cache Key

    • All query strings are forwarded

    • Worst caching performance




------------------------------ CloudFront Policies – Origin Request Policy 


• Specify values that you want to include in origin requests without including them in the Cache Key (no duplicated cached content)

• You can include extra :
      
      • HTTP headers: None – Whitelist – All viewer headers options
      • Cookies: None – Whitelist – All
      • Query Strings: None – Whitelist – All

    - but they will be forwarded to the origin but they're not going to be used in the Cache Key.


• Ability to add CloudFront HTTP headers and Custom Headers to an origin request that were not included in the viewer request (eg :  if you wanted to pass an API key or a secret header.)

• Create your own policy or use Predefined Managed Policies



----------------------------- Cache Policy vs. Origin Request Policy


GET /blogs/myblog.html HTTP/1.1
Host: mywebsite.com
User-Agent: Mozilla/5.0 (Mac OS X 10_15_2....) 
Date: Tue, 28 Jan 2021 17:01:57 GMT 
Authorization: SAPISIDHASH fdd00ecee39fe.... Keep-Alive: 300
Language: fr-fr


--  so the request will come with some query strings, some cookies, some headers, and then we will cache based on the cache policy.

-- For example, we want to cache here on the host name, the resource, and a header called authorization.



-- But then, your origin may need more than these three things to actually work and to actually serve properly the request.

-- So you may want to add the user agents,the session ID and the ref query string as part of your request to the origin.

-- So in this case, the request to the origin is going to be enhanced but then the caching will not happen based on what we forwarded to the origin request policy.

-- It's only going to be based on the cache policy.







Client ------------------------------(request)----------------------------> C.F --------------------------(forward)--------------------------------> Origin (EC2 instance)
                                                                             |

                   Cache Policy                                           EDGE Location                   Origin Request Policy (whitelist)

                Cache Key (cache based on)                                  |                                  Type                        Value
                                                                            |                               HTTP Headers         User-Agent, Authorization
                 - mywebsite.com 
                 - /content/stories/example-story.html                     CACHE                            Cookies               session_id
                 - Header: Authorization                                                                    Query Strings          ref


    


----------------------------------- CloudFront – Cache Invalidations


• In case you update the back-end origin, CloudFront doesn’t know about it and will only get the refreshed content after the TTL has expired

• However, you can force an entire or partial cache refresh (thus bypassing the TTL) by performing a CloudFront Invalidation

• You can invalidate all files (*) or a special path (/images/*)


------------------------------------ CloudFront – Cache Behaviors


• Configure different settings for a given URL path pattern

• Example: one specific cache behavior to images/*.jpg files on your origin web server

• Route to different kind of origins/origin groups based on the content type or path pattern
  
      • /images/*
      • /api/*  --  EG : Load balancer 
      • /* (default cache behavior) EG : S3 


• When adding additional Cache Behaviors, the Default Cache Behavior is always the last to be processed and is always /*



------ use case for cache behavior  EG : CloudFront – Cache Behaviors – Sign In Page


                                                                

                                                                        cache behaviours                                                 origins 
           Signed Cookies
       <------------------------->                                    
Users                                           <--------------------------->  /* (default)      -------------------->                      S3
                                          CF Distribution                         
                                                <--------------------------->  /login             ------------------->                EC2 instance (generated signed cookies)
       <------------------------->                                                                <--------------------              
            authenticate                                                                    Signed Cookies              



-- So the way we do it is that we define a cache behavior ,for /login and so the users who hit the /login page will be redirected to our EC2 instance.

-- And the role of our EC2 instance is to generate CloudFront signed cookies.

-- So these signed cookies are sent back to the user and the user will then use the signed cookies to be able to access our default cache behavior, which is any other URL, then /login and then access our S3 bucket files.

-- if the users are trying to access the default cache behavior without doing a login first, what we can do is that we can set up the cache behavior to only accept the request if signed cookies are present.

-- Therefore, we can redirect to the /login page and we're good to go.



---------------------------------------- CloudFront – Maximize cache hits by separating static and dynamic distributions



Static requests --------------> CDN Layer CloudFront (No headers / session caching rules Required for maximizing cache hits) ------(Static content)-----------> S3

Dynamic         --------------> CDN Layer CloudFront (Cache based on correct headers and cookie)------------------(Dynamic Content (REST, HTTP server))----------------> ALB + EC2 





--------------------------------------- CloudFront Signed URL / Signed Cookies


• You want to distribute paid shared content to premium users over the world

• We can use CloudFront Signed URL / Cookie. We attach a policy with:

      • Includes URL expiration
      • Includes IP ranges to access the data from
      • Trusted signers (which AWS accounts can create signed URLs)

• How long should the URL be valid for?

     • Shared content (movie, music): make it short (a few minutes)
     • Private content (private to the user): you can make it last for years


• Signed URL = access to individual files (one signed URL per file)

• Signed Cookies = access to multiple files (one signed cookie for many files)



------------------------------------------ CloudFront Signed URL vs S3 Pre-Signed URL


          • CloudFront Signed URL:                                                                                                • S3 Pre-Signed URL:


• Allow access to a path, no matter the origin  (not only s3 , but HTTP, backend ....)                           • Issue a request as the person who pre-signed the URL

• Account wide key-pair, only the root can manage it                                                             • Uses the IAM key of the signing IAM principal

• Can filter by IP, path, date, expiration                                                                       • Limited lifetime

• Can leverage caching features



------------------------------------------ CloudFront Signed URL Process

• Two types of signers:

    1 • Either a trusted key group (recommended)
        
        • Can leverage APIs to create and rotate keys (and IAM for API security)


    2 • An AWS Account that contains a CloudFront Key Pair

        • Need to manage keys using the root account and the AWS console
        • Not recommended because you shouldn’t use the root account for this



• In your CloudFront distribution, create one or more trusted key groups

• You generate your own public / private key
       
       • The private key is used by your applications (e.g. EC2) to sign URLs
       • The public key (uploaded) is used by CloudFront to verify URLs




---------------------------------------------------------------- LAB Demo 

------------ Type 1 


-- open S3 and create private Bucket 

-- as u create private bucket , no one can access ur url through the S3 

-- Only access through by Cloud-Front directly coz, it is privtae bucket and we did not enable Static hosting also 

-- go to CF in console 

-- create ditrubtion on CF 

-- Origina Domain = load balancer / S3 -- these are the places wher u can host ur applications 

-- OAC --> Create control settings --> do not change any n create 

-- it is created access from S3 

-- Compress objects automatically : CloudFront can automatically compress certain files that it receives from the origin before delivering them to the viewer. CloudFront compresses files only when the viewer supports it, as specified in the Accept-Encoding header in the viewer request.


-- Default root object - optional = index.html  -----> must and should u have to give this , otherwise u won’t get o/p 


----- once u create distrubtion , the S3 bucket policy wil gwt generated copy that policy and paste in bucket policy 

-- now ur appn is getting deployed all over the world 

-- through the CF url customers will able to connect ur webiste through the edge locations 

-- once u change the content of ur website and do uploud again n if u do refresh u won't get new content 

-- u have too do "invalidate the Cache" 

-- go to CF and create invalidation for /index.html or /* , do upload again the file to s3 ,it will get latest file from the S3 and give latest content to customers 

-- By default, CloudFront caches files in edge locations for 24 hours. 


------------ Sign URL System 

--  now i want to make this private , and want to give access to only prime customers 

--  so do create Signed URL'S

-- for this first u have to create public key and do store this key in the Key group 

-- go to google and search for "RSA Key Generator" create 2048 bit key , copy public key and do paste in public key , create public key

-- now create key group and add this public key to this group , u can store upto 5 pulic keys in one group 

-- now go to distribution --> behaviour --> edit --> Restrict viewer access = yes --> save changes 

-- it will get create one URL




TYPE 2 

-- go to security credentials --> create cloudfront key pair --> download both public and private files 

-- now open terminals and enter the command like this 

      aws cloudfront sign --url https://dgw7w0gfg0nkb.cloudfront.net --key-pair-id APKAUK2QS6DOWVAB3OG5 --private-key file://pk-APKAUK2QS6DOWVAB3OG5.pem --date-less-than 2024-03-31


      -- it will generate one url 

      EG :    https://dgw7w0gfg0nkb.cloudfront.net?Expires=1711843200&Signature=Pa7-jzCpDyXKgwS6bqPh75zqiNnCHryUCWDgcQheZLwX7g0wyzLrBSiUmD2KNFtdnx-OKnYLO2zJSiLsORIQO1yDs5RBTCqW6y5BTGqE0-CUdQ5clls4LY4KKdwZWmRs2VyJtMMDNqiwjsID2nTHO8nRUkgWBB0Nx9FShrhsmMoqVYo2JnDmIWnLb8KE4r8vSxbKPmMKByRkqmUmHPSbR6ODct0njHdDbcDJuANZLh3NKXVPvfYNMGre1ipjwfPhz7neEbcZoMq3AYuXce83DzSQd2BN~P8lPKDyDOWy8C3kAHoUKUg~2tneTa9~Ksh2hFHHyOnIthSysoFKmsW5ug__&Key-Pair-Id=APKAUK2QS6DOWVAB3OG5%                                    





--------------------------------------- CloudFront – Multiple Origin

• To route to different kind of origins based on the content type

• Based on path pattern:

      • /images/*
      • /api/* 
      • /*



-------------------------------------- CloudFront – Origin Groups

• To increase high-availability and do failover

• Origin Group: one primary and one secondary origin

• If the primary origin fails, the second one is used


-------------------------------------- CloudFront – Field Level Encryption


• Protect user sensitive information through application stack

• Adds an additional layer of security along with HTTPS

• Sensitive information encrypted at the edge close to user

• Uses asymmetric encryption

• Usage: 

     • Specify set of fields in POST requests that you want to be encrypted (up to 10 fields)
     • Specify the public key to encrypt them


                                      
                                                Encrypt using Public Key

Client -----------(HTTPS)----------------------> EDGE Location ------------(HTTPS)--------------------> C.F -------------(HTTPS)-----------> ALB --------(HTTPS)------------------> WEB Servers

         POST /submit HTTP/1.1 
         Host: www.example.com                                             POST /submit HTTP/1.1                                                                               Decrypt using Private Key
                                                                           Host: www.example.com 



EG : 


LAB : https://325b057e.isolation.zscaler.com/profile/1233fc6e-6618-4022-a03b-96afce7da312/zia-session/?controls_id=5363df57-072a-41ec-8342-91ef13f84e51&region=bom&tenant=462f064b6b51&user=a88c06b13d4badbc3b1628a94e955f23c843810a6a0110d09b7d0213d4fe17aa&original_url=https%3A%2F%2Faws.amazon.com%2Fblogs%2Fsecurity%2Fhow-to-enhance-the-security-of-sensitive-customer-data-by-using-amazon-cloudfront-field-level-encryption%2F&key=sh-1&hmac=dfa6e28bf1bc65f977e7e1b8fb8cd99b505583c03072c2c6e52c6e286b86f799




------------------------------------------ CloudFront – Real Time Logs


• Get real-time requests received by CloudFront sent to Kinesis Data Streams

• Monitor, analyze, and take actions based on content delivery performance

• Allows you to choose:
  
      • Sampling Rate – percentage of requests for which you want to receive

      • Specific fields and specific Cache Behaviors (path patterns)



Real-time Processing  =          Users -----(Requests)--------> C.F -----(LOGS)-------> Kinesis Data Streams ------(records)--------------> Lambda


Near Real-time Processing  =       Users -----(Requests)--------> C.F -----(LOGS)-------> Kinesis Data Streams ------(records)--------------> Kinesis Data Firehose






===================================================== Containers on AWS ========================================

• Docker is a software development platform to deploy apps

• Apps are packaged in containers that can be run on any OS

• Apps run the same, regardless of where they’re run

  • Any machine
  • No compatibility issues
  • Predictable behavior
  • Less work
  • Easier to maintain and deploy
  • Works with any language, any OS, any technology


• Use cases: microservices architecture, lift-and-shift apps from on- premises to the AWS cloud, ...



---------------------------- Where are Docker images stored?

• Docker images are stored in Docker Repositories

• Docker Hub (https://hub.docker.com)

    • Public repository
    • Find base images for many technologies or OS (e.g., Ubuntu, MySQL, ...)


• Amazon ECR (Amazon Elastic Container Registry)

    • Private repository
    • Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)


-- Jfrog also we can store images





-------------------------- Docker vs.Virtual Machines

• Docker is ”sort of ” a virtualization technology, but not exactly

• Resources are shared with the host => many containers on one server

-- see pics in google for better understanding





-------------------------------- Getting Started with Docker



Dockerfile -------(Build)----------------> Docker Image ------------(Run)------------> Docker Container (Eg :python)
                                            |      |
                                            |      |
                                Push        |      |    Pull
                                            |      |
                                            |      |
              
                                        Docker Repositories

                                        Eg : DockerHub , ECR



-- Dockerfile  : which is defining how your Docker container will look. So we have a base Docker image , and we add some files and then we're going to build it.

-- DockerImage : And this will become a Docker image. And that Docker image, you can store it on a Docker repository , it's called a Push and you push it to either Docker hub which is a public repository, or Amazon is ECR
   
                 - Then you can pull back these images from these repositories and then you would run them.


-- Docker Container : And when you run a Docker image , it becomes a Docker container, which runs your code hat you had built from your Docker build.


--- That is the whole process with Docker.





---------------------------------------------- Docker Containers Management on AWS

• Amazon Elastic Container Service (Amazon ECS)
     
     • Amazon’s own container platform


• Amazon Elastic Kubernetes Service (Amazon EKS)

     • Amazon’s managed Kubernetes (open source)


• AWS Fargate
  
     • Amazon’s own Serverless container platform
     • Works with ECS and with EKS


• Amazon ECR:
  
     • Store container images





--------------------------------------------------- 1 Amazon ECS - EC2 Launch Type


• ECS = Elastic Container Service

• Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters 
    
        - an ECS Cluster is made of things.And with the EC2 Launch Type, well these things are EC2 instances.

        - group of servers is called "clusters"


• EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances)

• Each EC2 Instance must run the ECS Agent to register in the ECS Cluster

• AWS takes care of starting / stopping containers

     - AWS is going to be starting or stopping the containers.
     - That means that whenever we have a new Docker container it's going to be placed accordingly on each EC2 Instance.
     





----------------------------------------------------  2 Amazon ECS – Fargate LaunchType


• Launch Docker containers on AWS

• You do not provision the infrastructure (no EC2 instances to manage)

• It’s all Serverless!, because we don't manage servers (there are servers behind., but we are not managing the servers)

• if we have an ECS Cluster , we just create task definition to define our ECS tasks.

• AWS just runs ECSTasks for you based on the CPU / RAM you need

   - So when we want to run a new Docker container, simple as that, it's going to be run, without us knowing where it's run and without an EC2 Instance to be created in the backend in our accounts for it to work.
   - So it's a little bit magic.


• To scale, just increase the number of tasks. Simple - no more EC2 instances





------------------------------------------------------- Amazon ECS – IAM Roles for ECS


 -- So let's take an example of the EC2 Launch Type in which we have an EC2 Instance running the ECS Agent on Docker.

 -- So in this case, we can create an EC2 Instance Profile which is only valued of course if you use EC2 Launch Type.

 1  EC2 Instance Profile (EC2 Launch Type only):

       • Used by the ECS agent
       • Makes API calls to ECS service
       • Send container logs to CloudWatch Logs
       • Pull Docker image from ECR
       • Reference sensitive data in Secrets Manager or SSM Parameter Store



 2 ECSTask Role:

-- our ECS tasks are going to get ECS Task Roles. And so this is valued for both EC2 Launch Type and Fargate.

-- And so here I have two tasks.And we can create a specific role per task.

           TASK A -----------(EC2 Task A Role)-------> s3 

           TASK B -----------(EC2 Task B Role)-------> DynamoDB

-- Well, why do we have different roles? 

ANS : Because each role allows you to be linked to different ECS services.

1  so, for example, the ECS Task A Role allows you to have your Task A, runs some API calls against Amazon S3

2  so, for example, the ECS Task B Role allows you to have your Task B, runs some API calls against Dynamodb


NOTE : you define the Task Role in the task definition of your ECS service.



• Allows each task to have a specific role

• Use different roles for the different ECS Services you run

• Task Role is defined in the task definition 

     -- task definition : The Task definitions view lists each task definition family you've created.

     -- You can perform the following actions:
        
         - Deploy the task definision as a service or a task.
         - Create a new revision






---------------------------------------------- Amazon ECS – Load Balancer Integrations


1  Application Load Balancer 

    -- supported and works for most use cases

    -- multiple ECS Tasks running. It's all in the ECS Cluster.

    -- And we want to expose these tasks as a HTP or HTTPS endpoint.

    -- Therefore we can run an Application Load Balancer in front of it and then our users will be going to the ALB and in the back end to the ECS tasks directly.

    -- the ALB is supported and will support most use cases, and that's a good choice.


2   Network Load Balancer

    -- recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link


3   Classic Load Balancer supported but not recommended (no advanced features – no Fargate)

    - you cannot link your Elastic Load Balancer to Fargate.






----------------------------------------------------- Amazon ECS – Data Volumes (EFS)


• Mount EFS file systems onto ECS tasks

   - So say you have an ECS cluster and in this case are represented both the EC2 Instance , as well as the Fargate Launch Type for my ECS Cluster.

   - And we want to mount a file system onto the ECS task to share some data. In that case, we use an Amazon EFS file system,


• Works for both EC2 and Fargate launch types

   - because it's a network file system is going to be compatible with both EC2 and the Fargate launch types. And it allows us to mount the file system directly onto our ECS tasks.


• Tasks running in any AZ will share the same data in the EFS file system

• Fargate + EFS = Serverless


• Use cases: persistent multi-AZ shared storage for your containers


IMP NOTE : 

    • Amazon S3 cannot be mounted as a file system ( S3 isn't a file system, it is object storage. )



--------------------------------------------------------- Capacity providers ECS 

-- The capacity provider view provides details about your Fargate and EC2 capacity providers.

-- For Amazon ECS on AWS Fargate, the Capacity Provider is FARGATE and a FARGATE_SPOT and the Type is FargateProvider. When you select AWS Fargate, these providers are added automatically. You cannot update or delete them.

-- For Amazon ECS on Amazon EC2, the Capacity Provider is the Auto Scaling group name and the Type is ASGProvider.

-- For Amazon ECS on Amazon EC2, you can create, update, and delete the capacity provider.

--  When you delete the capacity provide the capacity provider association is removed. You must go the Amazon EC2 console to delete the Auto Scaling group.






---------------------------------------------- LAB for ECS 

-- open ECS , give name for ur cluster

--  do check Fargate and ec2 instances 

-- now it create new ASG automatically for u , go with amazon linux 2 (os) , t2.micro (instance type) 

-- if u want to deploy these in ur vpc then select ur vpc , otherwise go with default vpc 

-- remaining all are default 

-- once the cluster is get created , automatically one ASG get created for u 

-- FARGATE / FARGATE_SPOT / Our ASG  = these are capacity providers 

-- go to democluster --> infrastructure 

        - we can launch the ec2 instances directly in the cluster through an ASG 

-- if u change desired capacity is 1 , it will create 1 ec2 for u 

-- democluster --> infrastructure --> the created instance is show in Container instances , these instance can be create by FARGATE / FARGATE_SPOT / Our ASG 



---- create an ECS service 

-- u need to create task definition, create new task definition

-- create TD with the name "nginxdemos-hello"

     - nginxdemos-hello : this is from the official dockerhub page from internet ,we are using this in this demo

-- Infrastructure requirements = AWS Fargate 

     - u can also do select ec2 , but here i want to be serverless

-- OS, Architecture, Network mode = Linux/x86_64

-- Task size = CPU = .5 vcpu and memory = 1 GB

-- IN Container – 1 

     - Name = nginxdemos-hello   and Image URI = nginxdemos/hello : this will directly pull image from the dockerhub repo

-- keep remaining are default

-- create TD with this configuration , 

-- let's launch this task definition as a service.

-- democluster --> create service 

-- Compute options = Launch type 

-- Launch type = Fargate

-- Application type = Service : Launch a group of tasks handling a long-running computing work that can be stopped and restarted. For example, a web application

             - Task : Launch a standalone task that runs and terminates. For example, a batch job.

-- for this demo choose service

-- choose family and revision

-- Service name = same as family name

-- create one new sg in Network field , allow HTTP from anywhere and public ip turned On

-- and also create NEW ALB for this demo in load balcer section

-- create service , wait for few minutes to create 

-- once the service is get created do observe the service 

-- the service is linked to the target group and this target group is present infront of LB , 

-- in the target , u can see ip address , this is ip of ur container 

-- now go to load balancer and copy the DNS , paste in browser , u will get nginx welcome page  , /hello with dns in browser , ---> get o/p as /hello 

-- now here we have one service , u can also launch some more 

-- cluster --> service --> democluster --> update service

-- put desired = 3 1 per AZ , now we have two more tasks being provisioned and they are provisioned on the Fargate engine.

-- So that means that behind the scenes, AWS is going to provision automatically the resource that it needs to launch these tasks.

-- now do refresh in the browser , it will refresh for 3 services , ALB is distributing the load equally

-- to avoid charges , make sure our desired capacity = 0 in services and ASG both 






-------------------------------------------------- ECS Service Auto Scaling


• Automatically increase/decrease the desired number of ECS tasks

• Amazon ECS Auto Scaling uses "AWS Application Auto Scaling" , we have three metrics we can scale on using the service.

    • ECS Service Average CPU Utilization
    • ECS Service Average Memory Utilization-Scale on RAM
    • ALB Request Count Per Target–metric coming from the ALB


• Target Tracking – scale based on target value for a specific CloudWatch metric

• Step Scaling – scale based on a specified CloudWatch Alarm

• Scheduled Scaling – scale based on a specified date/time (predictable changes)

• ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level)

    - IMP NOTE : Remember, that scaling your service, your ECS Service, at the task level is not equal to scaling your cluster of EC2 instances if you are in the EC2 launch type.

• Fargate Auto Scaling is much easier to setup (because Serverless)



------------------------------------------------ EC2 Launch Type – Auto Scaling EC2 Instances

• Accommodate ECS Service Scaling by adding underlying EC2 Instances

-- We have 2 types


1 • Auto Scaling Group Scaling

    • Scale your ASG based on CPU Utilization 
    • Add EC2 instances over time


2 • ECS Cluster Capacity Provider
 
     • Used to automatically provision and scale the infrastructure for your ECSTasks
     • Capacity Provider paired with an Auto Scaling Group
     • Add EC2 Instances when you’re missing capacity (CPU, RAM...)



IMP NOTE : So if you have to choose between Auto Scaling Group Scaling and ECS Cluster Capacity Provider, please use ECS Cluster Capacity Provider for your EC2 launch type.


USer -------------> CloudWatch Metric (ECS Service CPU Usage) ---------------(Trigger)-------------->CloudWatch Alarm -----------(scale)-------> ASG / Capacity providers





------------------------------------------------ ECS Rolling Updates


• When updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order

-- you will have two settings, the minimum healthy percent and the maximum percent.

-- So by default they're 100 and 200

-- EG : So your ECS service, for example, this one is running nine tasks represents an actual running capacity of 100%. 
    
         - And then if you set a minimum healthy percent of less than 100, this is going to say, "Hey you're allowed to terminate all the tasks on the right hand side, as long as we have enough tasks to have a percentage over the minimum healthy percent."

         - And in the maximum percent shows you how many new tasks you can create of the version two, to basically roll updates your service.

         - So this is how these two settings would impact your updates. 

         - so you will go ahead, create new tasks, then terminate all tasks and so on. All to make sure that all your tasks are going to be terminated and then updated to a newer version.


-- lets discuss 2 scenarios


------------------------------------------- 1 ECS Rolling Update – Min 50%, Max 100% 


 • Starting number of tasks: 4

   - In this case, we're going to lose four tasks to be terminated , so that we're running at 50% capacity.

   - Then two new tasks are going to be created, Now we're back at 100% capacity.

   - Then two old tasks are going to be terminated, we're back at 50% capacity.And two new tasks are going to be created, we're back at 100 capacity.

   - we have done a rolling updates.

   - In this case, we have been terminating tasks because we set the minimum to 50% and the maximum to 100%.



------------------------------------------- 2 ECS Rolling Update – Min 100%, Max 150%


• Starting number of tasks: 4

   - We cannot terminate a task because the minimum is 100%.

   - Therefore we can go into create two new tasks and this will bring our capacity to 150%. (total 6)

   - Then because we are above the minimum 100% we can terminate two old task and we're back at 100%.

   - Then we will create two new tasks and finally, terminates two old tasks. And this will have performed our rolling updates for our ECS service.




----------------------------------------- Amazon ECS – Task Definitions

• Task definitions are metadata in JSON form to tell ECS how to run a Docker container

• It contains crucial information, such as:
    
    • Image Name
    • Port Binding for Container and Host • Memory and CPU required
    • Environment variables
    • Networking information
    • IAM Role
    • Logging configuration (ex CloudWatch)

• Can define up to 10 containers in a Task Definition


-- Amazon ECS allows you to run and maintain a specified number of instances of a task definition simultaneously in an Amazon ECS cluster. This is called a service.



-------------------------------------- Amazon ECS – Load Balancing (EC2 Launch Type)


• We get a Dynamic Host Port Mapping if you define only the container port in the task definition

    - So if you have load balancing and you're using the EC2 launch type, then you're going to get what's called a Dynamic Host Port Mapping. If you define only the container port and the task definition.

Explanation : 

    - So we are running for example, an ECS task, and all of them have the container port set to 80 but the host port set to zero,meaning not set.

    - the host port is going to be random, is going to be dynamic.

    - so, each ECS task from within the EC2 instance, is going to be accessible from a different port on the host,the EC2 instance.

    - therefore, if you define an application load balancer , then you may say, well, it is difficult for the ALB to connect to the ECS test because the port is changing.

    - But the ALB when linked to an ECS service knows how to find the right port, thanks to the Dynamic Host Port Mapping feature.

NOTE : but it does not work with a classic load balancer because it is older generation.


• You must allow on the EC2 instance’s Security Group any port from the ALB’s Security Group







-------------------------------------- Amazon ECS – Load Balancing (Fargate)


• Each task has a unique private IP

• Only define the container port (host port is not applicable)

   -  because this is Fargate, there is no host


-- for example, with four tasks each task is going to get its own private IP through an Elastic Network Interface or ENI. And then each ENI is going to get the same container ports.

-- And therefore, when you have an ALB, then to connect to the Fargate task, it's just going to connect to all all of them on the same port on port 80.



• Example

   • ECS ENI Security Group

         • Allow port 80 from the ALB


   • ALB Security Group

         • Allow port 80/443 from web





------------------------------------------------------ Amazon ECS One IAM Role per Task Definition


-- you should know that IAM roles are assigned per task definition.

-- you have a task definition and then you assign an ECS task role. And this will allow you, for example, for your ECS tasks out of your task definition, to access the Amazon S3 service.

-- And therefore when you create an ECS service from this task definition then each ECS task automatically is going to assume and inherit this ECS task role.

NOTE :  you should know that the role is defined at the task definition level, not at this service level.so, therefore all the tasks within your service, are going to get access to Amazon S3.





----------------------------------------------------- Amazon ECS – Environment Variables


• Environment Variable

    • Hardcoded – e.g., URLs
    • SSM Parameter Store – sensitive variables (e.g., API keys, shared configs)
    • Secrets Manager – sensitive variables (e.g., DB passwords)


• Environment Files (bulk) – Amazon S3



---------------------------------------------------- Amazon ECS – Data Volumes (Bind Mounts)


Bind Mount :

-- With bind mounts, a file or directory on a host, such as an Amazon EC2 instance, is mounted into a container. Bind mounts are supported for tasks that are hosted on both Fargate and Amazon EC2 instances.

-- Bind mounts are tied to the lifecycle of the container that uses them. After all of the containers that use a bind mount are stopped, such as when a task is stopped, the data is removed.

-- For tasks that are hosted on Amazon EC2 instances, the data can be tied to the lifecycle of the host Amazon EC2 instance by specifying a host and optional sourcePath value in your task definition. 

The following are common use cases for bind mounts.

  - To provide an empty data volume to mount in one or more containers.
  
  - To mount a host data volume in one or more containers.
  
  - To share a data volume from a source container with other containers in the same task.

  - To expose a path and its contents from a Dockerfile to one or more containers.




• Share data between multiple containers in the same Task Definition

• Works for both EC2 and Fargate tasks

• EC2 Tasks – using EC2 instance storage

     • Data are tied to the lifecycle of the EC2 instance

• Fargate Tasks – using ephemeral storage
  
     • Data are tied to the container(s) using them
     • 20 GiB – 200 GiB (default 20 GiB)


• Use cases:

   • Share ephemeral data between multiple containers

   • “Sidecar”container pattern, where the “sidecar” container used to send metrics/logs to other destinations (separation of conerns)







----------------------------------------------------  Amazon ECS – Task Placement


• When an ECS task is started with EC2 Launch Type, ECS must determine where to place it, with the constraints of CPU and memory (RAM) and available port

• Similarly, when a service scales in, ECS needs to determine which task to terminate

• You can define:
  
    • Task Placement Strategy
    • Task Placement Constraints


• Note: only for ECS Tasks with EC2 LaunchType (Fargate not supported)



----------------------------------------------------  Amazon ECS – Task Placement Process


• Task Placement Strategies are a best effort


• When Amazon ECS places a task, it uses the following process to select the appropriate EC2 Container instance:

   1. Identify which instances that satisfy the CPU, memory, and port requirements

   2. Identify which instances that satisfy the Task Placement Constraints

   3. Identify which instances that satisfy the Task Placement Strategies

   4. Select the instances for task placement




---------------------------------------------------- Amazon ECS –Task Placement Strategies


• Binpack

    • Tasks are placed on the least available amount of CPU and Memory

    • Minimizes the number of EC2 instances in use (cost savings)


JSON :


"placementStrategy": [
    {
        "field": "memory",
        "type": "binpack"
    }
]



• Random

    • Tasks are placed randomly


JSON : 



"placementStrategy": [
    {
       
        "type": "random"
    }
]



• Spread


  • Tasks are placed evenly based on the specified value

  • Example: instanceId, attribute:ecs.availability-zone, ...


  JSON :

  "placementStrategy": [
    {
       
        "type": "spread",
        "field": "attribute:ecs.availability-zone"
    }
]



-------------------- You can mix them together


1 we can have a spread on availability zone and then a spread on instance ID

   
EG :

 "placementStrategy": [
    {

         "field": "attribute:ecs.availability-zone"
         "type": "spread",
       
    },

    {
          "field": "instanceid"
         "type": "spread",
    }
]




2  we can have a spread on availability zone and then a binpack on memory.

    
EG :

"placementStrategy": [
    {

         "field": "attribute:ecs.availability-zone"
         "type": "spread",
       
    },

    {
          "field": "memory"
         "type": "binpack",
    }
]






---------------------------------------------------- Amazon ECS –Task Placement Constraints


1  distinctInstance

     • Tasks are placed on a different EC2 instance

     - So you will never have two tasks on the same instance.

    
 "placementStrategy": [
    {
       
        "type": " distinctInstance"
        
    }
]


2   memberOf

    • Tasks are placed on EC2 instances that satisfy a specified expression

    • Uses the Cluster Query Language (advanced)

EG: 1 

    "placementStrategy": [
    {
       
        "type": "memberOf"
        "expression": "attribute:ecs.availability-zone in [ap-south-1,ap-south-2]"
        
    }
]

EG : 2 


"placementStrategy": [
    {
       
        "type": "memberOf"
        "expression": "attribute:ecs.instance-type =~ t2.*"
        
    }
]






---------------------------------------------------- Amazon ECR

• ECR = Elastic Container Registry

• Store and manage Docker images on AWS

• Private and Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)

• Fully integrated with ECS, backed by Amazon S3

• Access is controlled through IAM (permission errors => policy)

• Supports image vulnerability scanning, versioning, image tags, image lifecycle, ...




----------------------------------------------------  Amazon ECR – Using AWS CLI

• Login Command

   • AWS CLI v2

        aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com



• Docker Commands
 
   • Push

       docker push aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest


   • Pull
     
       docker pull aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest



• In case an EC2 instance (or you) can’t pull a Docker image, check IAM permissions





---------------------------------------------------- AWS Copilot


• CLI tool to build, release, and operate production-ready containerized apps
      
      - So Copilot is not a service,


• The idea is that we want to remove the difficulty of running apps on AppRunner, ECS and Fargate, by just using a CLI tool to deploy to these environments.

• Helps you focus on building apps rather than setting up infrastructure

• Provisions all required infrastructure for containerized apps (ECS,VPC, ELB, ECR...) is done for you by Copilot.

• Automated deployments with one command using CodePipeline

• Deploy to multiple environments

• Troubleshooting, logs, health status...



Microservices Architecture
Use CLI or YAML to describe the architecture of your applications   --------------------------------> AWS Copilot CLI for containerized applications (Well-architected infrastructure setup, Deployment Pipeline, Effective Operations and Troubleshooting)----------------> Amazon ECS / AWS Fargate / AWS App Runner



------------------------------------- LAB for Copilot

-- open cloud9 , create environment

-- all are default , create environment

sudo curl -Lo /usr/local/bin/copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux \
   && sudo chmod +x /usr/local/bin/copilot \
   && copilot --help


-- by using the above command we can install the copilot in the Identify

-- make sure that docker should be installed in the IDE , check by typing docker

-- clone the url from the github
  
      git clone https://github.com/aws-samples/aws-copilot-sample-service example

-- do cd example

-- this example folder contains docker file , and index.html files 

-- do copilot init

-- So this is the Copilot CLI and it's going to give us some questions. And with it we're going to be able to set up a containerized application on AWS.

-- Application name: copilot-guide

 Which workload type best represents your architecture?  [Use arrows to move, type to filter, ? for more help]
  > Request-Driven Web Service  (App Runner)
    Load Balanced Web Service   (Public. ALB by default. Internet to ECS on Fargate)
    Backend Service             (Private. ALB optional. ECS on Fargate)
    Worker Service              (Events to SQS to ECS on Fargate)
    Static Site                 (Internet to CDN to S3 bucket)
    Scheduled Job               (Scheduled event to State Machine to Fargate)


-- u will get this once u give ur application load balancer 

-- here we have LB service web app

-- Service name: web-app
 
   Which Dockerfile would you like to use for web-app?  [Use arrows to move, type to filter, ? for more help]
  > ./Dockerfile
    Enter custom path for your Dockerfile
    Use an existing image instead

-- choose 1st option

-- select N 

-- in web-app folder one manifest file will get created here 

-- now create an environment to run our application in.

    copilot env init --name prod --profile default --app copilot-guide\

-- choose default 

-- now now it's going to update all the resources 

-- now go to cloudformation and check StackSet-copilot-guide-infrastructure --> resource , it will create ecr repo , s3 bucket and bucket policy and KMS created for us

-- next what we have to do is to actually go ahead and provision our application.

-- once u run copilot env deploy --name prod  

-- it will gives u InvalidClientTokenId error , so to avoid this , u can do  click on cloud9 logo --> preferences --> aws settings --> disable temporary credentials 

-- now go to IAM --> create user --> with administration access policy --> generate Access key 

-- now do aws configure and give details and o/p format is JSON

-- now it is working , see what it is created for u 

-- check in ecs cluster got vreated but no services are there 

-- so now run copilot deploy

-- so it will search for our appn "web-app" and choose environment to run our application

-- So now it goes ahead and uses Docker to actually build our final Docker image.

-- Then it pushes that Docker image into ECR, and then from ECR is going to create an ECS service that will be referencing that image, and will be started on our ECS cluster.

-- now wait for 5-7 minutes , it will get created the whole process for u  do not close the window unitl it get created 

- Creating the infrastructure for stack copilot-guide-prod-web-app                [create complete]  [351.8s]
  - Service discovery for your services to communicate within the VPC             [create complete]  [0.0s]
  - Update your environment's shared resources                                    [update complete]  [172.8s]
    - A security group for your load balancer allowing HTTP traffic               [create complete]  [3.6s]
    - An Application Load Balancer to distribute public traffic to your services  [create complete]  [151.8s]
    - A load balancer listener to route HTTP traffic                              [create complete]  [1.1s]
  - An IAM role to update your environment stack                                  [create complete]  [16.3s]
  - An IAM Role for the Fargate agent to make AWS API calls on your behalf        [create complete]  [16.3s]
  - An HTTP listener rule for path `/` that forwards HTTP traffic to your tasks   [create complete]  [0.0s]
  - A custom resource assigning priority for HTTP listener rules                  [create complete]  [3.0s]
  - A CloudWatch log group to hold your service logs                              [create complete]  [7.3s]
  - An IAM Role to describe load balancer rules for assigning a priority          [create complete]  [16.3s]
  - An ECS service to run and maintain your tasks in the environment cluster      [create complete]  [122.5s]
    Deployments                                                                                      
               Revision  Rollout      Desired  Running  Failed  Pending                                       
      PRIMARY  1         [completed]  1        1        0       0                                             
  - A target group to connect the load balancer to your service on port 80        [create complete]  [15.3s]
  - An ECS task definition to group your containers and run them on ECS           [create complete]  [0.0s]
  - An IAM role to control permissions for the containers in your tasks


-- these are things that it will create all the complexity of thinking about what you need to actually create when you run an application on ECS is taken away by Copilot.

-- now do check in ecs , the service is get created 

-- now copy the link and paste in the broswer , u will get o/p

-- all the complexity will tAKEN BY Copilot for u
    
-- now delete resource , do run copilot app delete

-- wait for some time it will get deleted 

-- delete user in iam also 


---------------------------------------------------- Amazon EKS(Elastic Kubernetes service) Overview

• Amazon EKS = Amazon Elastic Kubernetes Service

• It is a way to launch managed Kubernetes clusters on AWS

• Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) application

• It’s an alternative to ECS, similar goal but different API

• EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers

• Use case: if your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes

• Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP...)

• For multiple regions, deploy one EKS cluster per region

• Collect logs and metrics using CloudWatch Container Insights



----------------------- Amazon EKS – Node Types

• Managed Node Groups

    • Creates and manages Nodes (EC2 instances) for you
    • Nodes are part of an ASG managed by EKS
    • Supports On-Demand or Spot Instances

• Self-Managed Nodes

    • Nodes created by you and registered to the EKS cluster and managed by an ASG
    • You can use prebuilt AMI - Amazon EKS Optimized AMI
    • Supports On-Demand or Spot Instances


• AWS Fargate

    • No maintenance required; no nodes managed


------------------ Amazon EKS – Data Volumes

• Need to specify StorageClass manifest on your EKS cluster

• Leverages a Container Storage Interface (CSI) compliant driver

• Support for...
   
   • Amazon EBS
   • Amazon EFS (works with Fargate) 
   • Amazon FSx for Lustre
   • Amazon FSx for NetApp ONTAP





================================================= AWS Elastic Beanstalk ===============================================


Developer problems on AWS

  • Managing infrastructure
  • Deploying Code
  • Configuring all the databases, load balancers, etc
  • Scaling concerns


• Most web apps have the same architecture (ALB + ASG)

• All the developers want is for their code to run!

• Possibly, consistently across different applications and environments



------------------------------------- Elastic Beanstalk – Overview

• Elastic Beanstalk is a developer centric view of deploying an application on AWS

• It uses all the component’s we’ve seen before: EC2, ASG, ELB, RDS, ...

• Managed service

     • Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration, ...

     • Just the application code is the responsibility of the developer

• We still have full control over the configuration

• Beanstalk is free but you pay for the underlying instances



------------------------------------- Elastic Beanstalk – Components

• Application: collection of Elastic Beanstalk components (environments, versions, configurations, ...)

• Application Version: an iteration of your application code (v1, v2 , v3)

• Environment 

      • Collection of AWS resources running an application version (only one application version at a time in an environment),  where we can see we can actually update an application version within an environment from version one to version two.
   
      • Tiers: Web Server Environment Tier & Worker Environment Tier

      • You can create multiple environments (dev, test, prod, ...)



------------------------------------- Elastic Beanstalk – Supported Platforms

• Go
• Java SE
• Java withTomcat
• .NET Core on Linux
• .NET on Windows Server 
• Node.js
• PHP
• Python
• Ruby
• Packer Builder
• Single Container Docker 
• Multi-container Docker 
• Preconfigured Docker


------------------------------------- Web ServerTier vs. WorkerTier (environments)


 Web ServerTier :

 -- This is the traditional architecture that we know, where we have a load balancer and then it's sending traffic to an auto scaling group that has multiple EC2 instances that are going to be your web server.


 WorkerTier :

 -- So this time there is no clients accessing directly your EC2 instances. We're going to use an SQS queue, which is a message queue and the message will be sent into the SQS queue and the EC2 instances are going to be workers,

 -- because they're going to pull messages from the SQS queue to process them.

 • Scale based on the number of SQS messages

 • Can push messages to SQS queue from anotherWeb ServerTier




------------------------------------ Elastic Beanstalk Deployment Modes

1 Single Instance Great for dev

  -- In this case, you'll have one EC2 instance which will have an Elastic IP, potentially it can also launch an RDS database and so on,

  -- but it's all based on one instance with an Elastic IP. It's great for development purposes,


NOTE : but then if you wanted to scale a real Elastic Beanstalk mode, then you would go for high available with a load balancer,


2  High Availability with Load Balancer Great for prod

   -- which is great for production environments, in which case, you can have a load balancer distributing the loads across multiple EC2 instances that are managed for an auto scaling group and multiple available zones.

   -- And finally, you may have an RDS database that's also multi AZ with a master and a standby.





----------------------- 1st environment LAB 

Single Instance Great for dev

-- open Elasticbeanstalk --> Web server environment

-- choose nodejs

-- sample application --> single instance

-- choose Create and use new service role

-- sometimes the EC2 instance profile is not getting by default , so create ROLE for this 

    - IAM --> ROLES --> create role --> aws service --> ec2 --> attach permissions --> create role 

          AWSElasticBeanstalkMulticontainerDocker
          AWSElasticBeanstalkWebTier
          AWSElasticBeanstalkWorkerTier


-- now go directly to skip review

-- submit --> this will create our first environment




----------------------- 2nd environment LAB 



-- name = prod

-- sample application 

-- Presets  = high availability

-- select role , skip for review then submit , it will create for u 

-- it will take almsot 10 min

-- for this environment , the loadbalancer will created , SG with port 80 , and ASG 


NOTE : use the above method to create environment , in the new console there might be chance of getting error , so follow this only 





---------------------------------------- Beanstalk Deployment Options for Updates

1 • All at once (deploy all in one go) – fastest, but instances aren’t available to serve traffic for a bit (downtime)

2 • Rolling: update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy

3 • Rolling with additional batches: like rolling, but spins up new instances to move the batch (so that the old application is still available)

4 • Immutable: spins up new instances in a new ASG, deploys version to these instances, and then swaps all the instances when everything is healthy

5 • Blue Green: create a new environment and switch over when ready

6 • Traffic Splitting: canary testing – send a small % of traffic to new deployment



Explanation for Deployment Options



---------- 1 All at once (deploy all in one go)

-- Here is our four EC2 instances, and they all run the version one, which is blue, of our application.

-- Then we are going to do an all at once deployment. So we want deploy v2.

-- And what happens, that first Elastic Beanstalk will just stop the applications on all our EC2 instances.

-- So then I put it as gray, as in they don't run anything. for sometime

-- And then we will be running the new V2, because Elastic Beanstalk will deploy V2 to these instances.

-- So what do we notice? 

     • Fastest deployment

     • Application has downtime

     • Great for quick iterations in development environment

     • No additional cost



------------ 2 Elastic Beanstalk Deployment Rolling


• Application is running below capacity

• Can set the bucket size

• No additional cost

-- We have four instances running v1,and the bucket size will be two for the example.

-- So what happens is that the application on the instances will be stopped, and so they're gray.

-- But we still have the other two instances running v1, we have maybe half capacity here.

-- Then these first two instances will be updated, so they'll be running v2, and then we will roll on to the next bucket, or to the next batch.And so that's why it's called rolling.

-- first 2 will updated to 2nd version and then nxt 2 instancs updated to 2nd version , the application, at some point during the deployment, is running both versions simultaneously.

-- No additional cost

-- so if you set a very small bucket size and you have hundreds and hundreds of instances, it may be a very long deployment,

-- Right now, in this example, we have a bucket size of two and four instances, but we can have a bucket size of 2 and 100 instances. It will just take a very long time to upgrade everything.

-- Zero downtime



---------------- 3 Elastic Beanstalk Deployment Rolling with additional batches

-- so in this case, the application is not running under capacity, just like before.

-- Before, at some point, we were only running two instances out of four. So that was below capacity.

-- In this mode, we run at capacity,and we can also set the bucket size.

-- basically our application will still be running both versions simultaneously,

-- but at a small additional cost.

-- That additional batch, that we'll see in a second, will be removed at the end of the deployment.

-- the deployment is going to be long.

-- It's honestly a good way to deal with prod.

-- Zero downtime

EG : 

-- We have our four v1 EC2 instances, and the first thing we're going to do is deploy new EC2 instances, and they will have the v2 version on it. (now total they are 6 instances)

-- you can see that the additional two are running, already, the newer version.

-- Now we take the first batch to the first bucket of two and they get stopped, the application gets stopped,and the application gets updated to v2,

-- Then the process repeats again, just like in rolling.So the application running v1 gets stopped, and then the application is updated to v2.

-- so at the end, you can see, we have six EC2 instances running v2.

-- so at the end of it, the additional batch gets terminated and taken away.(now we have 4 insatnces)




---------------------------------- 4 Elastic Beanstalk Deployment Immutable


--  Zero downtime

--  New Code is deployed to new instances on a temporary ASG

• High cost, double capacity

• Longest deployment

• Quick rollback in case of failures (just terminate new ASG)

• Great for prod


EG : 

-- We have a current ASG with three applications v1 running on three instances.

-- And then we're going to have a new temporary ASG being created. At first, Beanstalk will launch one instance on it, just to make sure that one works.And if it works and it passes the health checks, it's going to launch all the remaining ones.

-- it's going to sort of merge the ASG with a temporary ASG. So it's going to move all the temporary ASG instances to the current ASG.

-- So now, in the current ASG, we have six instances, And when all of this is done and the temporary ASG is empty, then we have the current ASG that will terminate all the v1 applications, while the v2 applications are still there. And then, finally, the temporary ASG will just be removed.




--------------------------------- 5 Elastic Beanstalk Deployment Blue / Green

• Not a “direct feature” of Elastic Beanstalk

• Zero downtime and release facility

• Create a new “stage” environment and deploy v2 there

• The new environment (green) can be validated independently and roll back if issues

• Route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage environment

• Using Beanstalk, “swap URLs” when done with the environment test



-------------------------------- 6 Elastic Beanstalk - Traffic Splitting (Canary)

• Canary Testing

• New application version is deployed to a temporary ASG with the same capacity

• A small % of traffic is sent to the temporary ASG for a configurable amount of time

• Deployment health is monitored

• If there’s a deployment failure, this triggers an automated rollback (very quick)

• No application downtime

• New instances are migrated from the temporary to the original ASG

• Old application version is then terminated

-- this could be a big improvement on top of the blue/green technique



REF : https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html





--------------------------------- Deployment models LAB 


-- prod environment --> configuraations --> Updates, monitoring, and logging --> edit  --> select immutable --> apply 

-- it will take some time to update 

-- in the meanwhile go to google --> sample application node js beanstalk --> downlaod zip file for nodejs app

-- in this code --> index.html --> text coloumn --> change to blue colour

-- now upload and deploy zip file of this code

-- let's observe how it is creating in the events 

-- once it is create , check in brower , it will turn to blue 

-- here emv = green and prod = blue

-- now do environment swapping , that means prod become env and env become prod

-- And the reason we would do so is, for example, you take this environment, for example, prod. First, you're going to clone it.

-- So you're going to create a copy of prod,then you would deploy your new application to the new environment where you can perform some extensive testing. Call it prod number two.

-- in prod environment --> do swap environment domain --> choose env 

-- it will take sometime to update 

-- now check in browser , the pro become green and env become blue 





----------------------------------------------------------- Elastic Beanstalk CLI 


• We can install an additional CLI called the “EB cli” which makes working with Beanstalk from the CLI easier

• Basic commands are:
 
   • eb create
   • eb status 
   • eb health 
   • eb events 
   • eb logs
   • eb open
   • eb deploy
   • eb config
   • eb terminate

• It’s helpful for your automated deployment pipelines!


----------------------------------------------- Elastic Beanstalk Deployment Process


• Describe dependencies
  
      (requirements.txt for Python, package.json for Node.js)


• Package code as zip, and describe dependencies

      • Python: requirements.txt
      • Node.js: package.json

• Console: upload zip file (creates new app version), and then deploy

• CLI: create new app version using CLI (uploads zip), and then deploy


• Elastic Beanstalk will deploy the zip on each EC2 instance, resolve dependencies and start the application




------------------------------------------------------- Beanstalk Lifecycle Policy 


• Elastic Beanstalk can store at most 1000 application versions

• If you don’t remove old versions, you won’t be able to deploy anymore

• To phase out old application versions, use a lifecycle policy

   • Based on time (old versions are removed)
   • Based on space (when you have too many versions)

• Versions that are currently used won’t be deleted

• Option not to delete the source bundle in S3 to prevent data loss


-- All the applications are being registed in the amazon s3 bucket 

--  go to Elasticbean stalk --> appn versions --> activate lifecycle policy --> choose any one b/w the 2 options 




------------------------------------------------------- Elastic Beanstalk Extensions


• A zip file containing our code must be deployed to Elastic Beanstalk

• All the parameters set in the UI can be configured with code using files

• Requirements:

    • in the .ebextensions/ directory in the root of source code

    • YAML / JSON format

    • .config extensions (example: logging.config)

    • Able to modify some default settings using: option_settings

    • Ability to add resources such as RDS, ElastiCache, DynamoDB, etc...


• Resources managed by .ebextensions get deleted if the environment goes away


---- now add environmentvariables.config file  in the zip to the env environment (took from stphene codes)

-- u will see the environment variables in configuration section





------------------------------------------------------- Elastic Beanstalk Under the Hood


 Under the Hood : (The underlying implementation of a product (hardware, software, or idea)).doing work on something that isn't visible or apparent to someone else, or of something that is complex and not easily understood.


• Under the hood, Elastic Beanstalk relies on CloudFormation

• CloudFormation is used to provision other AWS services (we’ll see later)

• Use case: you can define CloudFormation resources in your .ebextensions to provision ElastiCache, an S3 bucket, anything you want!


-- go to CF and check in resource , that it has created for u 




------------------------------------------------------- Elastic Beanstalk Cloning

• Clone an environment with the exact same configuration

• Useful for deploying a “test” version of your application

• All resources and configuration are preserved:

    • Load Balancer type and configuration

    • RDS database type (but the data is not preserved)

    • Environment variables

• After cloning an environment, you can change settings 

-- we did during the lab 



------------------------------------------------------- Elastic Beanstalk Migration: Load Balancer


----------------------- 1 Elastic Beanstalk Migration: Load Balancer

• After creating an Elastic Beanstalk environment, you cannot change the Elastic Load Balancer type (only the configuration)

  -- if you want it to somehow upgraded from a Classic Load Balancer to an Application Load Balancer or from an Application Load Balancer to a Network Load Balancer. You will need to perform a migration and the steps are as such.

• To migrate:

1. create a new environment with the same configuration except LB (can’t clone) -- u can't use clone 'coz it uses same configuration.

2. deploy your application onto the new environment

3. perform a CNAME swap or Route 53 update -- to shift traffic



----------------------- 2 RDS with Elastic Beanstalk

• RDS can be provisioned with Beanstalk, which is great for dev / test

• This is not great for prod as the database lifecycle is tied to the Beanstalk environment lifecycle

• The best for prod is to separately create an RDS database and provide our EB application with the connection string (for example using an environment variable.)


-------- Elastic Beanstalk Migration: Decouple RDS

1. Create a snapshot of RDS DB (as a safeguard)

2. Go to the RDS console and protect the RDS database from deletion

3. Create a new Elastic Beanstalk environment, without RDS, point your application to existing RDS (for example, using an environment variable.)

4. perform a CNAME swap (blue/green) or Route 53 update, confirm working

5. Terminate the old environment (RDS won’t be deleted)

6. Delete CloudFormation stack (in DELETE_FAILED state) (so we need to just go in CloudFormation and delete that CloudFormation stack manually.)








============================================================================= AWS CloudFormation ========================================================================

Managing your infrastructure as code


• CloudFormation is a declarative way of outlining your AWS Infrastructure, for any resources (most of them are supported)

• For example, within a CloudFormation template, you say:

    • I want a security group
    • I want two EC2 instances using this security group
    • I want two Elastic IPs for these EC2 instances
    • I want an S3 bucket
    • I want a load balancer (ELB) in front of these EC2 instances


• Then CloudFormation creates those for you, in the right order, with the exact configuration that you specify




---------------------------------------------- Benefits of AWS CloudFormation 

• Infrastructure as code

    • No resources are manually created, which is excellent for control

    • The code can be version controlled for example using Git

    • Changes to the infrastructure are reviewed through code



• Cost

    • Each resources within the stack is tagged with an identifier so you can easily see how much a stack costs you

    • You can estimate the costs of your resources using the CloudFormation template

    • Savings strategy: In Dev, you could automation deletion of templates at 5 PM and recreated at 8 AM, safely




• Productivity


   • Ability to destroy and re-create an infrastructure on the cloud on the fly

   • Automated generation of Diagram for your templates!

   • Declarative programming (no need to figure out ordering and orchestration)



• Separation of concern: create many stacks for many apps, and many layers. Ex:

   • VPC stacks
   • Network stacks 
   • App stacks



• Don’t re-invent the wheel


    • Leverage existing templates on the web!

    • Leverage the documentation





---------------------------------------------------------- How CloudFormation Works


• Templates must be uploaded in S3 and then referenced in CloudFormation



   Template ------(upload)-------------> S3 bucket ------------- (reference)-----------> AWS CloudFormation ---------------(create)-----------> Stack ----------(create)----> AWS Resources



• To update a template, we can’t edit previous ones. We have to re- upload a new version of the template to AWS


• Stacks are identified by a name within a region 

• Deleting a stack deletes every single artifact that was created by CloudFormation.





---------------------------------------------------------- Deploying CloudFormation Templates


1 • Manual way

     • Editing templates in CloudFormation Designer or code editor

     • Using the console to input parameters, etc...

     • We’ll mostly do this way in the course for learning purposes


2 • Automated way

     • Editing templates in a YAML file

     • Using the AWS CLI (Command Line Interface) to deploy the templates, or using a Continuous Delivery (CD) tool

     • Recommended way when you fully want to automate your flow



NOTE : there is no order in which CloudFormation should create your resources. When you write a CloudFormation template





---------------------------------------------------------- CloudFormation – Building Blocks


• Template’s Components

    • AWSTemplateFormatVersion – identifies the capabilities of the template “2010-09-09”

    • Description – comments about the template

    • Resources (MANDATORY) – your AWS resources declared in the template

    • Parameters – the dynamic inputs for your template

    • Mappings – the static variables for your template

    • Outputs – references to what has been created

    • Conditionals – list of conditions to perform resource creation


• Template’s Helpers

    • References
    • Functions




---------------------------------------------------------- LAB for Stacks


-------------- 1 creating source


-- make sure to use us-east-1 region only for creating stacks , 'coz all the templates have been designed for that region, especially when talking about AMI IDs, which are region specific.

-- open cloudformation --> choose yaml files for easy purpose --> upload a yaml file which has template for ec2 creation --> do submit 
 
EG for ec2 
     Resources:
     MyInstance:
       Type: AWS::EC2::Instance
       Properties:
          AvailabilityZone: us-east-1a
          ImageId: ami-0a3c3a20c09d6f377
          InstanceType: t2.micro



-- our first stack is being created , do refresh it will created for u 

-- do explore in CF Properties





------------ 2 Update and Delete Stack 


-- here only way to update is to upload a new version of template to update 

-- upload updated template yaml file 

EG for updated template 



---
Parameters:
  SecurityGroupDescription:
    Description: Security Group Description
    Type: String

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
      SecurityGroups:
        - !Ref SSHSecurityGroup
        - !Ref ServerSecurityGroup

  # an elastic IP for our instance
  MyEIP:
    Type: AWS::EC2::EIP
    Properties:
      InstanceId: !Ref MyInstance

  # our EC2 security group
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  # our second EC2 security group
  ServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Ref SecurityGroupDescription
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 192.168.1.1/32



-- in above template we are attaching 2 security groups and EIP to the instance

-- choose nxt --> SecurityGroupDescription , u will get this option 'coz in this code u have written so give Description --> nxt 

-- in the summary page in step 2 u have stack deatils with parameters and in Changeset preview , u can able to see the changes and modifiers 

-- a "change set" represents a list of things that is going to change as part of your CloudFormation updates.

-- do submit , u can see that in the same stack the update is happening , go n check all the resources are created through the template 

-- to delete stack ,jst do delete stack 






------------------------------------------------------------------  YAML Crash Course

EG for YAML code 

Parameters:
  KeyName:
    Description: The EC2 Key Pair to allow SSH access to the instance
    Type: 'AWS::EC2::KeyPair::KeyName'
Resources:
  Ec2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
      SecurityGroups:
        - !Ref InstanceSecurityGroup
        - MyExistingSecurityGroup
      KeyName: !Ref KeyName
      ImageId: ami-7a11e213
  InstanceSecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0




 • YAML and JSON are the languages you can use for CloudFormation

 • JSON is horrible for CF , 'coz of of many string interpolations and so on,

 • YAML is great in so many ways

 • Key value Pairs

 • Nested objects  (SG is nested object, resource , preperties etc )

 • Support Arrays ( - symbol means which represents an array)

 • Multi line strings 





------------------------------------------------------------------ 1 CloudFormation – Resources


• Resources are the core of your CloudFormation template (MANDATORY)

• They represent the different AWS Components that will be created and configured

• Resources are declared and can reference each other

• AWS figures out creation, updates and deletes of resources for us

• There are over 700 types of resources (!)

• Resource types identifiers are of the form:

          service-provider::service-name::data-type-name




--------------------------- How do I find Resources documentation?

• All the resources can be found here:

      https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/a ws-template-resource-type-ref.html


• Then, we just read the docs 

• Example here (for an EC2 instance):

     https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/a ws-resource-ec2-instance.html




--------------------------------CloudFormation – Resources FAQ

• Can I create a dynamic number of resources?

  - Yes, you can by using CloudFormation Macros and Transform
  


• Is every AWS Service supported?

   - Almost. Only a select few niches are not there yet

   - You can work around that using CloudFormation Custom Resources



------------------------------------------------------------------ 2 CloudFormation – Parameters


• Parameters are a way to provide inputs to your AWS CloudFormation template

• They’re important to know about if:

     • You want to reuse your templates across the company

     • Some inputs can not be determined ahead of time


• Parameters are extremely powerful, controlled, and can prevent errors from happening in your templates, thanks to types    -- IMP to know




------------------------------- When should you use a Parameter?


EG : 

Parameters:
  KeyName:
    Description: The EC2 Key Pair to allow SSH access to the instance
    Type: 'AWS::EC2::KeyPair::KeyName'


-- EG for the parameter 

• Ask yourself this:

       • Is this CloudFormation resource configuration likely to change in the future?

       • If so, make it a parameter


• You won’t have to re-upload a template to change its content





------------------------------- CloudFormation – Parameters Settings


• Parameters can be controlled by all these settings:

   • Type:
       
       • String
       • Number
       • CommaDelimitedList
       • List<Number>
       • AWS-Specific Parameter (to help catch invalid values – match against existing values in the AWS account)
       • List<AWS-Specific Parameter>
       • SSM Parameter (get parameter value from SSM Parameter store)
       • Description
       • ConstraintDescription (String)
       • Min/MaxLength
       • Min/MaxValue
       • Default
       • AllowedValues (array)
       • AllowedPattern (regex)
       • NoEcho (Boolean)


-- So you don't have to remember all of those, but what you need remember is that the parameters are not just strings. You can have constraints and validation, allowing you to make sure they are safe to use.
 





------------------------------- CloudFormation – Parameters Example

     1 • AllowedValues (array)

Parameters:
 InstanceType:
   Description: Choose an ec2 type
   Type: String
   AllowedValues:
      - t2.micro
      - t3.micro
      - t2.small


Resources:
  Ec2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
       InstanceType: !Ref InstanceType
       ImageId: xxxxxxxxxxxxxx


-- Here , we have a parameter called InstanceType. To choose an EC2 InstanceType of Type: String.

-- But we have defined AllowedValues being t2.micro, t2.small, or t2.medium with a Default being t2.micro. And this parameter is reused in the EC2Instance.

-- So thanks to it, we'll have a dropdown and the user can only select one of these three values, hence giving them choice while giving you control.




     2 NoEcho (Boolean)


Parameters:
 DBPassword:
   Description: db password
   Type: String
   NoEcho: true

Resources:
  MyDBInstance:
   Type: 'AWS::RDS::DBInstance'
    Properties:
      DBInstanceClass: db.t2.micro
      AllocatedStorage: 20
      Engine: mysql 
      MasterUsername: administration
      MasterUserpassword: !Ref DBPassword
      DBInstanceIdentifier: MydbInstance


-- So for example, say we want as a parameter to put in the database password, but of course it is a password so we have to keep it secret.

-- So we want to remove it from the logs and so on. So we'll have NoEcho: true , so that the password is not displayed anywhere.






------------------------------- How to Reference a Parameter?

• The Fn::Ref function can be leveraged to reference parameters

• Parameters can be used anywhere in a template

• The shorthand for this inYAML is !Ref

• The function can also reference other elements within the template


IMP : you need to make sure that your resources don't have the same name as your parameters.



------------------------------- CloudFormation – Pseudo Parameters

• AWS offers us Pseudo Parameters in any CloudFormation template

• These can be used at any time and are enabled by default

• Important pseudo parameters:

    EG ;

    AWS::AccountId

    AWS::Region

    AWS::StackId

    AWS::StackName

    AWS::NotificationARNs

    AWS::NoValue





------------------------------------------------------------------ 3 CloudFormation – Mappings



• Mappings are fixed variables within your CloudFormation template

• They’re very handy to differentiate between different environments (dev vs prod), regions (AWS regions), AMI types...

• All the values are hardcoded within the template


EG : 

Mappings: 
  Mapping01: 
    Key01: 
      Name: Value01
    Key02: 
      Name: Value02
    Key03: 
      Name: Value03



RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309


-- So here, based on the region you have, so us-east-1, us-west-1, or eu-west-1, and based on the architecture you're using, for example, HVM64 or HVMG2, this is going to give you a different AMI ID every time.

-- Well, we know that the AMIs are specific based on the region, so of course it makes sense to have a different AMI per region.



--------------------------------- Accessing Mapping Values (Fn::FindInMap)



• We use Fn::FindInMap to return a named value from a specific key

• !FindInMap [ MapName,TopLevelKey, SecondLevelKey ]


EG :

AWSTemplateFormatVersion: "2010-09-09"
Mappings: 
  RegionMap: 
    us-east-1:
      HVM64: ami-0ff8a91507f77f867
      HVMG2: ami-0a584ac55a7631c0c
    us-west-1:
      HVM64: ami-0bdb828fd58c52235
      HVMG2: ami-066ee5fd4a9ef77f1
    eu-west-1:
      HVM64: ami-047bb4163c506cd98
      HVMG2: ami-0a7c483d527806435
    ap-northeast-1:
      HVM64: ami-06cd52961ce9f0d85
      HVMG2: ami-053cdd503598e4a9d
    ap-southeast-1:
      HVM64: ami-08569b978cc4dfa10
      HVMG2: ami-0be9df32ae9f92309
Resources: 
  myEC2Instance: 
    Type: "AWS::EC2::Instance"
    Properties: 
      ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", HVM64]
      InstanceType: m1.small


-- we have an EC2 instance that is using an ImageId. And this ImageId is using the FindInMap function.

-- to use this FindInMap function, we first need to use a map name. So here we have the RegionMap. Then we have the top level key.

-- So what we want to use in here, we have a reference to the pseudo parameter AWS::Region. So if you launch this template in us-east-1, it's going to be us-east-1, and if you launch it to us-west-1, automatically this pseudo parameter is going to resolve to us-west-1.
 
-- And then finally, the type of architecture you want, for example, HVM64. And this works great because, well, AMIs are region specific, and so you want to make sure you have the right AMI for the right region and the right architecture.



--------------------------------- When would you use Mappings vs. Parameters?



• Mappings are great when you know in advance all the values that can be taken and that they can be deduced from variables such as 

   • Region
   • Availability Zone
   • AWS Account
   • Environment (dev vs prod) 
   • etc...

• They allow safer control over the template

• Use parameters when the values are really user specific





------------------------------------------------------------------ 4  CloudFormation –  Outputs



• The Outputs section declares optional outputs values that we can import into other stacks (if you export them first)!

• You can also view the outputs in the AWS Console or in using the AWS CLI

• They’re very useful for example if you define a network CloudFormation, and output the variables such as VPC ID and your Subnet IDs

• It’s the best way to perform some collaboration cross stack, as you let expert handle their own part of the stack



-------------------------- CloudFormation – Outputs Example

• Creating a SSH Security Group as part of one template

• We create an output that references that security group


Outputs:
  StackSSHSecurityGroup:
    Description: The SSH SG
    Value: !Ref MycompanywideSShSG
    Export:
      Name: SSH SecurityGroup




-------------------------- CloudFormation – Outputs Cross-Stack Reference

• We then create a second template that leverages that security group

• For this, we use the Fn::ImportValue function

• You can’t delete the underlying stack until all the references are deleted





------------------------------------------------------------------ 5 CloudFormation –  Conditions


• Conditions are used to control the creation of resources or outputs based on a condition

• Conditions can be whatever you want them to be, but common ones are:
   • Environment (dev / test / prod) 
   • AWS Region
   • Any parameter value

• Each condition can reference another condition, parameter value or mapping


-------------------------- How to define a Condition

  Conditions:
     CreateprodResource: !Equals [!Ref EnvType, prod]
  

• The logical ID is for you to choose. It’s how you name condition

• The intrinsic function (logical) can be any of the following:

   • Fn::And
   • Fn::Equals
   • Fn::If
   • Fn::Not
   • Fn::Or


----------------- How to use a Condition

• Conditions can be applied to resources / outputs / etc...

   




-------------------------------------------------------------------- CloudFormation – Intrinsic Functions


mk = must know 



• Ref              mk
• Fn::GetAtt       mk
• Fn::FindInMap    mk
• Fn::ImportValue   mk
• Fn::Join
• Fn::Sub
• Fn::ForEach
• Fn::ToJsonString
• Condition Functions (Fn::If, Fn::Not, Fn::Equals, etc...)     mk
• Fn::Base64        mk
• Fn::Cidr
• Fn::GetAZs 
• Fn::Select
• Fn::Split
• Fn::Transform 
• Fn::Length



--------------------------- 1  Intrinsic Functions – Fn::Ref

• The Fn::Ref function can be leveraged to reference

     • Parameters – returns the value of the parameter

     • Resources – returns the physical ID of the underlying resource (e.g., EC2 ID)

     - it won’t work for conditions 
     

• The shorthand for this inYAML is !Ref

MyEIP:
  Type: "AWS::EC2::EIP"
  Properties:
    InstanceId: !Ref MyEC2Instance



--------------------------- 2 Intrinsic Functions – Fn::GetAtt


• Attributes are attached to any resources you create

• To know the attributes of your resources, the best place to look at is the documentation



-- The following example template returns the SourceSecurityGroup.OwnerAlias and SourceSecurityGroup.GroupName of the load balancer with the logical name myELB.


EG 

AWSTemplateFormatVersion: 2010-09-09
Resources:
  myELB:
    Type: AWS::ElasticLoadBalancing::LoadBalancer
    Properties:
      AvailabilityZones:
        - eu-west-1a
      Listeners:
        - LoadBalancerPort: '80'
          InstancePort: '80'
          Protocol: HTTP
  myELBIngressGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: ELB ingress group
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          SourceSecurityGroupOwnerId: !GetAtt myELB.SourceSecurityGroup.OwnerAlias
          SourceSecurityGroupName: !GetAtt myELB.SourceSecurityGroup.GroupName



-- in general So anytime we use Ref, we're going to get the reference ID out of it.

-- But if we use GetAtt to get an attribute, we have the option to get more information out of this EC2 instance. So here we have the AvailabilityZone. So for example, we can know in which AZ an instance was launched, for example, us-east-1b. You get the Id again,

-- you could get the PrivateDNSName, the PrivateIp, the PublicDNSName, and the PublicIp. So while the Ref gives you usually a reference to the ID of the resource you have created, the GetAtt allows you to get more out of the resource and you can only get what CloudFormation supports in terms of attributes that are defined in documentation







--------------------------- 3 Intrinsic Functions – Fn::FindInMap

• We use Fn::FindInMap to return a named value from a specific key

• !FindInMap [ MapName,TopLevelKey, SecondLevelKey ]


TopLevelKey: The top-level key name. Its value is a list of key-value pairs.

SecondLevelKey: The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.



Return value:

  The value that's assigned to SecondLevelKey.


EG 

Mappings: 
  RegionMap: 
    us-east-1: 
      HVM64: "ami-0ff8a91507f77f867"
      HVMG2: "ami-0a584ac55a7631c0c"
    us-west-1: 
      HVM64: "ami-0bdb828fd58c52235"
      HVMG2: "ami-066ee5fd4a9ef77f1"
    eu-west-1: 
      HVM64: "ami-047bb4163c506cd98"
      HVMG2: "ami-31c2f645"
    ap-southeast-1: 
      HVM64: "ami-08569b978cc4dfa10"
      HVMG2: "ami-0be9df32ae9f92309"
    ap-northeast-1: 
      HVM64: "ami-06cd52961ce9f0d85"
      HVMG2: "ami-053cdd503598e4a9d"
Resources: 
  myEC2Instance: 
    Type: "AWS::EC2::Instance"
    Properties: 
      ImageId: !FindInMap
        - RegionMap
        - !Ref 'AWS::Region'
        - HVM64
      InstanceType: m1.small






--------------------------- 4 Intrinsic Functions – Fn::ImportValue


• Import values that are exported in other stacks

• For this, we use the Fn::ImportValue function

-- The intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references. 

-- In the following example template snippets, Stack A exports VPC security group values and Stack B imports them.

EG :

Stack A Export


Outputs:
  PublicSubnet:
    Description: The subnet ID to use for public web servers
    Value:
      Ref: PublicSubnet
    Export:
      Name:
        'Fn::Sub': '${AWS::StackName}-SubnetID'
  WebServerSecurityGroup:
    Description: The security group ID to use for public web servers
    Value:
      'Fn::GetAtt':
        - WebServerSecurityGroup
        - GroupId
    Export:
      Name:
        'Fn::Sub': '${AWS::StackName}-SecurityGroupID'



Stack B Import


Resources:
  WebServerInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.micro
      ImageId: ami-a1b23456
      NetworkInterfaces:
        - GroupSet:
            - Fn::ImportValue: 
              'Fn::Sub': '${NetworkStackNameParameter}-SecurityGroupID'
          AssociatePublicIpAddress: 'true'
          DeviceIndex: '0'
          DeleteOnTermination: 'true'
          SubnetId: Fn::ImportValue: 
            'Fn::Sub': '${NetworkStackNameParameter}-SubnetID'



-- Declaration

   Fn::ImportValue: sharedValueToImport


-- Alternatively, you can use the short form:

   !ImportValue sharedValueToImport




------- Important to know : 

 - You can't use the short form of !ImportValue when it contains the short form of !Sub.

 # do not use
!ImportValue
  !Sub '${NetworkStack}-SubnetID' 


- Instead, you must use the full function name, for example:

Fn::ImportValue:
  !Sub "${NetworkStack}-SubnetID"







--------------------------- 5 Intrinsic Functions – Fn::Base64


• Convert String to it’s Base64 representation

     !Base64 "valueToEncode"


• Example: pass encoded data to EC2 Instance’s UserData property


-- The intrinsic function Fn::Base64 returns the Base64 representation of the input string. This function is typically used to pass encoded data to Amazon EC2 instances by way of the UserData property.


EG:

Resources:
  WebServerInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.micro
      ImageId: ami-a1b23456
        UserData:
          Fn::Base64: |
            #!/bin/bash
            # Use this for your user data (script from top to bottom)
            # install httpd (Linux 2 version)
            yum update -y
            yum install -y httpd
            systemctl start httpd





--------------------------- 6 Intrinsic Functions – Condition Functions


Conditions:
     CreateprodResource: !Equals [!Ref EnvType, prod]
  

• The logical ID is for you to choose. It’s how you name condition

• The intrinsic function (logical) can be any of the following:

   • Fn::And
   • Fn::Equals
   • Fn::If
   • Fn::Not
   • Fn::Or




Ref for other functions  : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-base64.html







-------------------------------------------------------------------- CloudFormation – Rollbacks


-- they're very important to understand at the exam.


• If Stack Creation Fails: you have two options.

     • Default: everything rolls back (gets deleted). We can look at the log

     • Option to disable rollback and troubleshoot what happened



• If it is a problem with Stack Update Fails:

     • The stack automatically rolls back to the previous known working state, again, deleting anything that was newly created.

     • Ability to see in the log what happened and error messages



• Rollback Failure? Fix resources manually then issue ContinueUpdateRollback API from Console

    • Or from the CLI using continue-update-rollback API call




----------------- LAB for failuers 


-- create one yaml file , make some wrong configurations in it , for eg : ImageId


ami-0a3c3a20c09d6f377


-- create stack with the yaml file which has wrong ami-id, it has 2 Security Groups


subbu1.yaml


---
Parameters:
  SecurityGroupDescription:
    Description: Security Group Description
    Type: String

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-123345
      InstanceType: t2.micro
      SecurityGroups:
        - !Ref SSHSecurityGroup
        - !Ref ServerSecurityGroup

  # an elastic IP for our instance
  MyEIP:
    Type: AWS::EC2::EIP
    Properties:
      InstanceId: !Ref MyInstance

  # our EC2 security group
  SSHSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  # our second EC2 security group
  ServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Ref SecurityGroupDescription
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 192.168.1.1/32



-- choose Stack failure options = Preserve successfully provisioned resources , create stack

-- here it is failed to create resource 'coz , i have not given description for security group

-- but i did not give description for "ServerSecurityGroup" but the SSHSecurityGroup is created 

-- now u cannot update the stack , so delete the stack , when there is create failure , u should delete the stack and troubleshoot and upload again



now create another yaml file with correct configuration 


subbu2.yaml 


---
Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro


-- now upload this file , 


-- choose  Stack failure options = Roll back all stack resources

-- create stack , now the resource got created 

-- now update the stack by uploading the 1st example yaml file (subbu1.yaml) file 

-- here u will get update option , 'coz u have created resources successfully 

-- upload the 1st yaml file 

-- now here provide group description

-- choose Stack failure options = Roll back all stack resources

-- create stack 

-- here the securitygropus were created but due to wrong ami-id , it is failed to create , so as per the condition that we ahve given (rollback), so, that means that my server security group and SSH security groups should disappear.




-- now update the stack with the same yaml file subbu1.yaml , bit choose Stack failure options = Preserve successfully provisioned resources

-- This would create SSH and server security groups, but it would not roll them back in case there is a rollback happening, a stack failure.

-- So, this is up to you to choose what you want, but both the behaviors can be desirable based on what you're trying to do.









-------------------------------------------------------------------- CloudFormation – Service Role


-- So CloudFormation can use service roles. What are they?


• IAM role that allows CloudFormation to create/update/delete stack resources on your behalf

   - Well, they are iam roles that you create and they're dedicated to CloudFormation and they allow CloudFormation to actually create update and delete stack resources on your behalf.


• Give ability to users to create/update/delete the stack resources even if they don’t have permissions to work with the resources in the stack


• Use cases:

    • You want to achieve the least privilege principle

    • But you don’t want to give the user all the required permissions to create the stack resources


• User must have iam:PassRole permissions which is a necessary permission to give a role to a specific service in AWS.




----------- LAB IAm roles with CF


-- open IAM --> create new role --> aws service --> CF --> give s3 full access --> create role 


-- now create stack , upload subbu2.yaml file for demo and in the permission sections = select IAM role that we have created now ,

-- So if I don't specify, then iam role is going to use my own personal permissions. 

-- But if I want to specify an iam role, I can look at this DemoRole for CFN with S3 capabilities. AWS CloudFormation will use this role for all stack operations. Other users that have permissions to operate on this stack will be able to use this role, even if they don't have permission to pass it. Ensure that this role grants the least privilege.

-- but this one, and because this one is just powered with Amazon S3 permissions,then actually my stack will fail because my stack is actually creating an EC2 instance.






-------------------------------------------------------------------- CloudFormation – Capabilities


• We have  CAPABILITY_NAMED_IAM and CAPABILITY_IAM 
       
       - So they are capabilities you need to give to CloudFormation whenever your CloudFormation template is going to create or update IAM resources, such as when you create a IAM user, a role, a group, a policy, and so on, through your CloudFormation templates.

      • Necessary to enable when you CloudFormation template is creating or updating IAM resources (IAM User, Role, Group, Policy, Access Keys, Instance Profile...)

      • Specify CAPABILITY_NAMED_IAM if the resources are named, otherwise, just CAPABILITY_IAM.

      - And the reason we do so is that we want to explicitly acknowledge the fact that CloudFormation is going to create IAM resources.


• CAPABILITY_AUTO_EXPAND

     - which is when your CloudFormation template is including macro and nested stacks, so stacks within stacks, to perform dynamic transformations.

     • Necessary when your CloudFormation template includes Macros or Nested Stacks (stacks within stacks) to perform dynamic transformations

     • You’re acknowledging that your template may change before deploying



• InsufficientCapabilitiesException

     • Exception that will be thrown by CloudFormation if the capabilities haven’t been acknowledged when deploying a template (security measure)

     - while launching a template, that means that the CloudFormation templates was requiring capabilities, but you haven't acknowledged them.

     - So, as a security measure, you need to redo the templates, upload, and launch using, this time, these capabilities. It's just an extra argument in your API call, or a box to tick on your AWS console.



-------------- lab

-- create one capabilities.yaml file 



AWSTemplateFormatVersion: '2010-09-09'
Description: An example CloudFormation that requires CAPABILITY_IAM and CAPABILITY_NAMED_IAM

Resources:
  MyCustomNamedRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: MyCustomRoleName
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: [ec2.amazonaws.com]
            Action: ['sts:AssumeRole']
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2FullAccess
      Policies:
        - PolicyName: MyPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 's3:*'
                Resource: '*'

Outputs:
  RoleArn:
    Description: The ARN of the created IAM Role
    Value: !GetAtt MyCustomNamedRole.Arn



-- here CloudFormation template, which actually has an IAM role being created. And it's named, so there's a role name, MyCustomRoleName. And it's using a managed policy, which is the AmazonEC2FullAccess. So we're dealing with IAM, and so as such, we are creating some IAM roles.


-- now try to create this stack in console with this file 

-- u will have a checkbox at last while u try to create this stack , do check and submit , then only u will able to create this stack 

-- the role has been created . check in console 







--------------------------------------------------------------------  1 CloudFormation – DeletionPolicy Delete


• DeletionPolicy:
  
     •  DeletionPolicy is a setting you can apply to resources on your CloudFormation templates, which allows you to Control what happens when the CloudFormation template is deleted or when a resource is removed from a CloudFormation template

     • Extra safety measure to preserve and backup resources

• Default DeletionPolicy=Delete 

-- So by default, we've seen that when we delete a CloudFormation template, all the resources within are also deleted. That means that the default DeletionPolicy is delete, so you don't have to specify it because it is the default.

EG:

Resources:
  MyInstance:
    Type: AWS::EC2::Instance
    Properties:
      AvailabilityZone: us-east-1a
      ImageId: ami-0a3c3a20c09d6f377
      InstanceType: t2.micro
    DeletionPolicy: Delete  




EG 2 :

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  myS3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete


-- in the above example , We have an S3 buckets and we have DeletionPolicy=Delete.

-- It will work only if the S3 bucket is empty. If it's not empty, then the delete will fail. So the way to fix this if you wanted to would be to either manually delete everything within the S3 bucket and then continue with the deletion of your CloudFormation templates.

-- Or it would be for you to implement a custom resource to actually delete everything within the S3 bucket before automatically having the S3 bucket go away.






----------------------------- 2 CloudFormation – DeletionPolicy Retain

• DeletionPolicy=Retain: 

     • Specify on resources to preserve in case of CloudFormation deletes

     • Works with any resources




EG :

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  myS3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain



-- So example here, we have a S3, and we know that by default, it would be deleted when I delete my CloudFormation template, 

-- but maybe we actually wanna keep it, keep the data within because we care about the data of this bucket. And so we would specify at the bottom DeletionPolicy retain.

-- And so even if I delete my CloudFormation templates, this S3 Bucket would stay, and this works with any resources.





----------------------------- 3 CloudFormation – DeletionPolicy Snapshot


• DeletionPolicy=Snapshot

• Create one final snapshot before deleting the resource

• Examples of supported resources:

    • EBS Volume, ElastiCache Cluster, ElastiCache ReplicationGroup

    • RDS DBInstance, RDS DBCluster, Redshift Cluster, Neptune DBCluster, DocumentDB DBCluster



----------- LAB 

-- create one yaml file that has DeletionPolicy


Resources:
  MySG:
    Type: AWS::EC2::SecurityGroup
    DeletionPolicy: Retain
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        FromPort: 22
        IpProtocol: tcp
        ToPort: 22

  MyEBS:
    Type: AWS::EC2::Volume
    DeletionPolicy: Snapshot
    Properties:
      AvailabilityZone: us-east-1a
      Size: 1
      VolumeType: gp2



-- in above example , there is a security group and the DeletionPolicy is retained. So that means that if I delete my transformation stack, this security group should stay.

-- And there is an EBS volume, and the DeletionPolicy is a snapshot. So that means that upon deleting the stack, the volume should go away, but a snapshot should be created first.

-- let's see how it is working 

-- upload yaml file and create one stack 

-- once the resource is created , do delete the stack

-- u can observe that MySG = DELETE_SKIPPED , 'coz we have given retain 

-- ebs volume is deleted but the snapshot is already created for u go n check in console 

-- u can delete manually through console , if u want to delete 







--------------------------------------------------------------------  CloudFormation – Stack Policies


• During a CloudFormation Stack update, all update actions are allowed on all resources (default)


-- but sometimes, you may want to protect your stack against updates, or part of your stack against updates. This is where Stack policies come in.


• A Stack Policy is a JSON document that defines the update actions that are allowed on specific resources during Stack updates


EG:

{
  "Statement" : [
  {
    "Effect" : "Allow",
    "Principal" : "*",
    "Action" : "update:*",
    "Resource" : "*",
   },

  {
    "Effect" : "Deny",
    "Principal" : "*",
    "Action" : "Update:*",
    "Resource" : "LogicalResourceId/ProductionDatabase"
  }
  ]
}


-- So here, we have an example where the first statement is saying "Allow update*" on everything, meaning that everything in your CloudFormation Stack can be updated,

-- second part is saying "Deny update*" on Resource Production Database. That means that whatever is named "Production Database" in your CloudFormation Stack is going to be protected against any kind of updates,



• Protect resources from unintentional updates

• When you set a Stack Policy, all resources in the Stack are protected by default

• Specify an explicit ALLOW for the resources you want to be allowed to be updated






--------------------------------------------------------------------  CloudFormation – Termination Protection


• To prevent accidental deletes of CloudFormation Stacks, use TerminationProtection


-- once u create stack --> stack actions --> set TerminationProtection





--------------------------------------------------------------------  CloudFormation – Custom Resources


• Used to

     • define resources not yet supported by CloudFormation

     • define custom provisioning logic for resources can that be outside of CloudFormation (on-premises resources, 3rd party resources...)

     • have custom scripts run during create / update / delete through Lambda functions (Eg : running a Lambda function to empty an S3 bucket before being deleted)


• Defined in the template using

      - AWS::CloudFormation::CustomResource or 
      
      - Custom::MyCustomResourceTypeName (recommended)


• Backed by a Lambda function (most common) or an SNS topic




------------------------------- How to define a Custom Resource?


• Ser viceToken specifies where CloudFormation sends requests to, such as Lambda ARN or SNS ARN (required & must be in the same region)

• Input data parameters (optional)


EG :

Resources:
  MyCustomResourceUsingLambda:
   Type: Custom::MylambdaResource
   properties:
      ServiceToken: arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME
      #input values (optional)
      ExamplePreperty: "exampleValue"




------------------------------- Use Case – Delete content from an S3 bucket


• You can’t delete a non-empty S3 bucket

• To delete a non-empty S3 bucket, you must first delete all the objects inside it

• We can use a custom resource to empty an S3 bucket before it gets deleted by CloudFormation


EG : 

-- Whenever we run delete stack on CloudFormation, then your custom resource backed by Lambda function is going to run API calls to empty your S3 bucket.

-- when your S3 bucket is emptied, then CloudFormation is going to actually attempt to delete your S3 bucket, and everything will work.





---------------- LAB 


-- in this demo , we are going to create lambda function , s3 bucket , lambdaexecutionrole, and custom resource 

-- as per the documentation we can not delete the s3 bucket directly which has objects in it , but through the custom resources with lambda function , we can delete the s3 bucket directly

-- create one yaml files which will create an s3 bucket , CustomResourceLambdaFunction , CustomResourceLambdaExecutionRole , CustomResource



EG :  delete-resource.yaml


----
Resources:

    CustomResourceLambdaExecutionRole:
        Type: 'AWS::IAM::Role'
        Properties:
            AssumeRolePolicyDocument:
                Version: 2012-10-17
                Statement:
                    - Effect: Allow
                      Principal:
                          Service: lambda.amazonaws.com
                      Action:
                          - 'sts:AssumeRole'
            Policies:
                - PolicyName: LoggingPolicy
                  PolicyDocument:
                      Version: 2012-10-17
                      Statement:
                          - Effect: Allow
                            Action:
                                - logs:CreateLogGroup
                                - logs:CreateLogStream
                                - logs:PutLogEvents
                            Resource: '*'
                - PolicyName: S3Policy
                  PolicyDocument:
                      Version: 2012-10-17
                      Statement:
                          - Effect: Allow
                            Action:
                                - s3:List*
                                - s3:DeleteObject
                            Resource: '*'

    CustomResourceLambdaFunction:
        Type: 'AWS::Lambda::Function'
        Properties:
            Code:
                ZipFile: |
                    import cfnresponse
                    import boto3

                    def handler(event, context):
                        print(event)
                        print('boto version ' + boto3.__version__)

                        # Globals
                        responseData = {}
                        ResponseStatus = cfnresponse.SUCCESS
                        s3bucketName = event['ResourceProperties']['s3bucketName']

                        if event['RequestType'] == 'Create':
                            responseData['Message'] = "Resource creation successful!"

                        elif event['RequestType'] == 'Update':
                            responseData['Message'] = "Resource update successful!"

                        elif event['RequestType'] == 'Delete':
                            # Need to empty the S3 bucket before it is deleted
                            s3 = boto3.resource('s3')
                            bucket = s3.Bucket(s3bucketName)
                            bucket.objects.all().delete()

                            responseData['Message'] = "Resource deletion successful!"

                        cfnresponse.send(event, context, ResponseStatus, responseData)

            Handler: index.handler
            Runtime: python3.12
            Role: !GetAtt CustomResourceLambdaExecutionRole.Arn

    CustomResource:
        Type: Custom::CustomResource
        Properties:
            ServiceToken: !GetAtt CustomResourceLambdaFunction.Arn
            s3bucketName: !Ref S3Bucket
        DependsOn: S3Bucket

    S3Bucket:
      Type: AWS::S3::Bucket
      Properties:
        AccessControl: Private
        BucketName: custom-resource-s3-bucket1


-- once u upload this file this will create s3 bucket , CustomResourceLambdaFunction , CustomResourceLambdaExecutionRole , CustomResource 

-- now go to bucket and upoad some files in it , now do dleete the stack u can able to delete the s3 bucket (in this process , the custom resource will delete first and then remaing will delete after this )



-- suppose u ahve s3 bucket already and have files in it so , u have to change 2 items in the 'CustomResource' resource:

     1 - Change the `DependsOn` property to the logical name of the S3 bucket in your template

     2  - change the parameter passed to the custom resource to reference your S3 bucket.  So you should change this line:

                `s3bucketName: !Ref S3Bucket`

        - to this:

                `s3bucketName: !Ref <your s3 bucket logical ID>`


--  DependsOn: < logical name of ur bucket >

--  s3bucketName: !Ref <your s3 bucket logical ID>

-- this is how u can automate the process


-- it will take almost 30 min to give status 

-- once u delete the stack , u will 2 cases 

1 Delete a custom resource that's stuck in DELETE_FAILED status

2 Delete a custom resource that's stuck in DELETE_IN_PROGRESS status




1 Delete a custom resource that's stuck in DELETE_FAILED status


-- To delete your stack, complete the following steps:

1.    Open the CloudFormation console.

2.    Choose the stack that contains your custom resource that's stuck in DELETE_FAILED status.

3.    Choose Actions, and then choose Delete Stack.

4.    In the pop-up window that provides a list of resources to retain, choose the custom resource that's stuck in DELETE_FAILED status. Then, choose Delete.

5.    Choose Actions, and then choose Delete Stack.

The status of your stack changes to DELETE_COMPLETE.

Note: Your custom resource isn't a physical resource, so you don't have to clean up your custom resource after stack deletion.




2 Delete a custom resource that's stuck in DELETE_IN_PROGRESS status


-- To force the stack to delete, you must manually send a SUCCESS signal. The signal requires the ResponseURL and RequestId values, which are both included in the event that's sent from CloudFormation to Lambda.

-- open lambda --> monitoring --> choose log it has values

Received event: {
  "RequestType": "Delete",
  "ServiceToken": "arn:aws:lambda:us-east-1:111122223333:function:awsexamplelambdafunction",
  "ResponseURL": "https://cloudformation-custom-resource-response-useast1.s3.us-east-1.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A111122223333%3Astack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2%7CMyCustomResource%7Ce2fc8f5c-0391-4a65-a645-7c695646739?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170313T0212304Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=QWERTYUIOLASDFGBHNZCV%2F20190415%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=dgvg36bh23mk44nj454bjb54689bg43r8v011uerehiubrjrug5689ghg94hb",
  "StackId": "arn:aws:cloudformation:us-east-1:111122223333:stack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2",
  "RequestId": "e2fc8f5c-0391-4a65-a645-7c695646739",
  "LogicalResourceId": "MyCustomResource",
  "PhysicalResourceId": "test-MyCustomResource-1URTEVUHSKSKDFF",
  "ResourceType": "Custom::PingTester"

}


--  To send a SUCCESS response signal in the response object to the delete request, run the following command in your local command-line interface. Be sure to include the values that you copied from above values 

curl -H 'Content-Type: ''' -X PUT -d '{
    "Status": "SUCCESS",
    "PhysicalResourceId": "test-CloudWatchtrigger-1URTEVUHSKSKDFF",
    "StackId": "arn:aws:cloudformation:us-east-1:111122223333:stack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2
  ",
    "RequestId": "e2fc8f5c-0391-4a65-a645-7c695646739",
    "LogicalResourceId": "CloudWatchtrigger"
  }' 'https://cloudformation-custom-resource-response-useast1.s3.us-east-1.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A111122223333%3Astack/awsexamplecloudformation/33ad60e0-5f25-11e9-a734-0aa6b80efab2%7CMyCustomResource%7Ce2fc8f5c-0391-4a65-a645-7c695646739?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170313T0212304Z&X-Amz-SignedHeaders=host&X-Amz-Expires=7200&X-Amz-Credential=QWERTYUIOLASDFGBHNZCV%2F20190415%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=dgvg36bh23mk44nj454bjb54689bg43r8v011uerehiubrjrug5689ghg94hb
  '



-- u will able to delete successfully 





--------------------------------------------------------------------  CloudFormation – StackSets


• Create, update, or delete stacks across multiple accounts and regions with a single operation/template

• Target accounts to create, update, delete stack instances from StackSets

• When you update a stack set, all associated stack instances are updated throughout all accounts and regions

• Can be applied into all accounts of an AWS Organization

• Only Administrator account (or Delegated Administrator) can create StackSets


        https://www.youtube.com/watch?v=SC6o6FnVt-M







======================================================================= AWS Integration & Messaging =======================================================================

SQS, SNS & Kinesis




---------------------------- Section Introduction


• When we start deploying multiple applications, they will inevitably need to communicate with one another

• There are two patterns of application communication

     1) Synchronous communications  (application to application) 

     2) Asynchronous / Event based  (applicahon to queue to application)


• Synchronous between applications can be problematic if there are sudden spikes of traffic

• What if you need to suddenly encode 1000 videos but usually it’s 10?


• In that case, it’s better to decouple your applications,

   • using SQS: queue model
   • using SNS: pub/sub model
   • using Kinesis: real-time streaming model


• These services can scale independently from our application!





---------------------------------------------------------------------- 1 Amazon SQS What’s a queue?


-- SQS is a simple queuing service.

-- So we have an SQS queue, and it is going to contain messages. And to contain messages, well, something needs to send messages into our SQS queue and whatever sends a message into our SQS queue is called a producer.

-- So it's possible for us to have one producer, but also have more. You can have multiple producers sending many messages into an SQS queue. And the message could be whatever he wants. For example, it could be process this order, or process this video. Whatever message you create goes into the queue.

-- Then something needs to process the messages from the queue and receive them, and it's called consumers.

-- So consumers will poll the messages from the queue, so that means that they will ask the queue, do you have any message for me? And the queue says, yes, here it is. And the consumer will poll these messages and get some information.

-- And then with that message, it will process it and then delete it back from the queue. And you may have multiple consumers consuming messages from an SQS queue.

-- So a queuing service is here to be a buffer to decouple between your producers and your consumers.



               Producer -----------------(send messagaes)-----------------> SQS Queue -----------------(Poll messages)---------------> Consumer




--------------------------------------- Amazon SQS – Standard Queue



• Oldest offering (over 10 years old)

• Fully managed service, used to decouple applications


• Attributes:

     • Unlimited throughput, unlimited number of messages in queue

     • Default retention of messages: 4 days, maximum of 14 days

     • Low latency (<10 ms on publish and receive)

     • Limitation of 256KB per message sent


• Can have duplicate messages (at least once delivery, occasionally)

• Can have out of order messages (best effort ordering)





--------------------------------------- SQS – Producing Messages


-- So, messages that are up to 256 kilobytes are sent into SQS by producers. how does this happen?

-- Well, the producers will send the messages to SQS using an SDK, software development kits. And the API to send a message to SQS is called SendMessage.

• Produced to SQS using the SDK (SendMessage API)

• The message, it will be written, it will be is persisted in SQS until a consumer deletes it , which signifies that the message has been processed.

• Message retention: default 4 days, up to 14 days


• Example: send an order to be processed

    • Order id
    • Customer id
    • Any attributes you want


-- so you will send a message into the SQS queue with maybe some information, such as the order ID, the customer ID, and any attributes you may want. For example, the address, and so on.

-- And then your consumer, that is in application rights, will have to deal with that message itself.

• SQS standard: unlimited throughput




--------------------------------------- SQS – Consuming Messages


• Consumers (running on EC2 instances, servers, or AWS Lambda)...

• Poll SQS for messages (receive up to 10 messages at a time)

• Process the messages (example: insert the message into an RDS database)

• Delete the messages using the DeleteMessage API



 


--------------------------------------- SQS – Multiple EC2 Instances Consumers


• Consumers receive and process messages in parallel

--  so each consumer will receive a different set of messages by calling the poll function.

-- if somehow a message is not processed fast enough by a consumer, it will be received by other consumers, and so this is why we have at least once delivery.

• At least once delivery

• Best-effort message ordering

• Consumers delete messages after processing them

     -- when the consumers are done with the messages, they will have to delete them, otherwise, other consumers will see these messages.

-- if we need to increase the throughputs because we have more messages, then we can add consumers and do horizontal scaling to improve the throughput of processing.

• We can scale consumers horizontally to improve throughput of processing




--------------------------------------- SQS with Auto Scaling Group (ASG)


-- Well, that means that your consumers will be running on EC2 instances inside of an Auto Scaling group and they will be polling for messages from the SQS queue.

-- But now your Auto Scaling group has to be scaling on some kind of metric, and a metric that is available to us is the Queue Length. It's called "ApproximateNumberOfMessages."

-- It is a CloudWatch Metric that's available in any SQS queue. And we could set up an alarm, such as whenever the queue length go over a certain level, then please set up a CloudWatch Alarm, and this alarm should increase the capacity of my Auto Scaling group by X amount.

-- this will guarantee that the more messages you have in your SQS queue, maybe because there's a surge of orders on your websites, the more EC2 instances will be provided by your Auto Scaling group, and you will accordingly process these messages at a higher throughputs.





--------------------------------------- SQS to decouple between application tiers


-- the use case is to decouple between applications, so application tiers.

-- So, for example, let's take an example of an application that processes videos. 

-- We could have just one big application that's called a front-end that will take the request and whenever a video needs to be processed, it will do the processing and then insert that into an S3 bucket.

-- But the problem is that processing may be very, very long to do and it may just slow down your websites if you do this in the front-end here.

-- So instead, you can decouple your application here the request of processing a file and the actual processing of a file can happen in two different applications.

-- therefore, whenever you take a request to process a file, you will send a message into an SQS queue.

-- Now, when you do the request to process, that file will be in the SQS queue and you can create a second processing tier called the back-end processing application that will be in its own Auto-Scaling group to receive these messages, process these videos, and insert them into an S3 bucket.

-- So as we can see here with this architecture, we can scale the front-end accordingly, and we can scale the back-end accordingly as well, but independently.

-- because the SQS queue has unlimited throughputs and it has unlimited number of messages in terms of the queue, then you are really safe, and this is a robust and scalable type of architecture.




   requests ------------ fronted ---------- (SendMessage)----------> SQS Queue -----------(ReceiveMessages) ----------> Back-end processing Application(video processing) -------------(insert) -----> S3





--------------------------------------- Amazon SQS - Security


• Encryption:

    • So we have encryption in-flight encryption by sending and producing messages  HTTPS API

    • At-rest encryption using KMS keys

    • Client-side encryption if the client wants to perform encryption/decryption itself (It's not something that's supported by SQS)


• Access Controls: IAM policies to regulate access to the SQS API

• SQS Access Policies (similar to S3 bucket policies)

     • Useful for cross-account access to SQS queues

     • Useful for allowing other services (SNS, S3...) to write to an SQS queue




--------------------------------------- LAB for SQS standard queue


-- refer to Solution Architect course




-----------------------------------------------------  SQS Queue Access Policy


-- SQS Queue Access Policies. And there are two good use cases for SQS Queue Access Policies.

-- They're similar to S3 Bucket policies in terms that they are resource policies, so JSON IAM policies that you're going to add directly onto your SQS Queue.


----------------- 1st use case : Cross Account Access




Account 444455556666 (SQS Queue (queue1))---------------(poll for messages)-------> Account 111122223333(ec2 insances)




-- Say you have a queue in an account and another account needs to access that queue. Maybe it has an EC2 Instance.

-- So for that EC2 Instance to be able to pull message across accounts, what you need to do is to create a Queue Access Policy look like this , and you attach it to the SQS Queue in the first account.


{
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_Send_Receive",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "111122223333"
         ]
      },
      "Action": [
         "sqs:SendMessage",
         "sqs:ReceiveMessage"
      ],
      "Resource": "arn:aws:sqs:*:444455556666:queue1"
   }]
}


-- What this Queue access policy will do 

-- it will allow the principle of AWS to be 111122223333, which represents the account on the right hand side on the sqs ReceiveMessage on this resource right here.

-- so this Queue Access Policy is really what will allow your EC2 Instance to pull from the SQS Queue in another account.





----------------------- 2nd use case : Publish S3 Event Notifications To SQS Queue


-- for example, when you have an S3 Bucket, and it will publish event notifications to an SQS Queue.

-- for example, you upload an object into an S3 Bucket, and what you want is to get automatically a message sent to the SQS Queue.



      Upload object ------------------>  S3 Bucket (bucket1) ---------(Send message)-------------> SQS Queue (queue1)


-- As you can see the SQS Queue, wIll need to give permission to the S3 Bucket to write a message to it.



{
"Version": "2012-10-17", "Statement" : [{
"Effect": "Allow",
"Principal": { "AWS": "*"},
"Action": [ "sqs:SendMessage" ],
"Resource": "arn:aws:sqs:us-east-1:444455556666:queue1", "Condilon": {
"ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:bucket1" },
"StringEquals": { "aws:SourceAccount": "<bucket1_owner_account_id>" }, }
}] }


-- look at the details for example, the action is sqs:SendMessage,

-- the condition is that the sourceArn of the bucket represents the S3 Bucket named bucket1, 

-- and that the source accounts needs to be the account owner of the S3 buckets.

-- So once you have this, then the S3 bucket is allowed to write to an SQS Queue.




---------------------------------------------- LAB for s3 event notifications 


-- create standard queue , IMP : encryption = disable

-- now create one s3 bucket --> properties --> create event notificatin for all prefix and suffix  --> choose destination is SQS queue , try to create event notificatin

              link : https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html#step1-create-sqs-queue-for-notification

-- here , u will get an Error "Unable to validate the following destination configurations"

-- to resolve this error , we need to modify it to allow our S3 Bucket to write into our SQS Queue.

-- go to queue --> Access policy --> c.o info (Access policy (Permissions) ) --> in the last right corner down side u will get documentation link --> open that Walkthrough: Configuring SNS or SQS 

-- the SQS queue will look like this 



{
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
        {
            "Sid": "example-statement-ID",
            "Effect": "Allow",
            "Principal": {
                "Service": "s3.amazonaws.com"
            },
            "Action": [
                "SQS:SendMessage"
            ],
            "Resource": "SQS-queue-ARN",
            "Condition": {
                "ArnLike": {
                    "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1"
                },
                "StringEquals": {
                    "aws:SourceAccount": "bucket-owner-account-id"
                }
            }
        }
    ]
}



-- paste this sqs queue policy in the sqs standard queue that u have created

-- u have modify some items in this policy like   "Resource": "SQS-queue-ARN",  "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1" ,  "aws:SourceAccount": "bucket-owner-account-id"

-- once u change these , save policy  , now go to s3 and try to create event notification , now u will able to create successfully

-- now do upload any file in the s3 and check in SQS , u will get details of that object 










----------------------------------------------  SQS – Message Visibility Timeout


-- we have a consumer doing a ReceiveMessage request, and therefore, a message will be returned from the queue. Now the visibility timeout begins.

• After a message is polled by a consumer, it becomes invisible to other consumers

• By default, the “message visibility timeout” is 30 seconds

-- That means that during these 30 seconds, the message has to be processed, if you do so, that means that if the same or other consumers do a message request API call, then the message will not be returned.

• After the message visibility timeout is over, the message is “visible” in SQS

-- So effectively, during the visibility timeout, that message is invisible to other consumers.

-- But after the visibility timeout is elapsed and if the message has not been deleted, then the message will be put back into the queue and therefore, another consumer or the same consumer doing a receive message API call will receive the message again, the same message as before.



IMP to know 

• If a message is not processed within the visibility timeout, it will be processed twice Because it will be received by two different consumers, or twice by the same consumer.

-- And so, if a consumer is actively processing a message but knows that it needs a bit more time to process the message because otherwise, it will go out of the visibility timeout window, there is an API called "ChangeMessageVisibility."

• A consumer could call the ChangeMessageVisibility API to get more time

-- so if a consumer knows that a message needs a bit more time to be processed and you don't want to process that message twice, then the consumer should call the ChangeMessageVisibility API to tell SQS, hey, do not make that message visible for now, I just need a bit more time to process this message.

-- how do you set this message visibility timeout ?
   
   • If visibility timeout is high (hours), and consumer crashes, re-processing will take time

   - if you set it to a really, really high value by default, say, hours, and the consumer crashes, then it will take hours until this message reappears, re-becomes visible in your SQS queue, and that will take a lot of time.

   • If visibility timeout is too low (seconds), we may get duplicates

   - If you set it to something really, really low, like a few seconds, what happens that if the consumer doesn't end up having enough time to process that message for whatever reason, then it will be read many times by different consumers and you may get duplicate processing.


-- So the idea is that the visibility timeout should be set to something reasonable for your application and your consumer should be programmed that if they know they need a bit more time, then they should call the ChangeMessageVisibility API to get more time and increase the timeout of that visibility window.





-------------- LAB to observe Visibility Timeout 

-- create one std queue

-- open 2 windows , in 1st window send some message and go to 2nd window try to refresh ,

-- in 2nd window u can't get any messages 'coz it is in visibility timeout period , after 30 sec it will visible to U

-- the more u poll , the receive count will increase for same message 

--  u can also change visibility time , but default is better 






---------------------------------------------- Amazon SQS – Dead Letter Queue (DLQ)


• If a consumer fails to process a message within the Visibility Timeout...the message goes back to the queue!

• We can set a threshold of how many times a message can go back to the queue

• After the "MaximumReceives" threshold is exceeded, the message goes into a Dead Letter Queue (DLQ)

• Useful for debugging!

• DLQ of a FIFO queue must also be a FIFO queue

• DLQ of a Standard queue must also be a Standard queue

• Make sure to process the messages in the DLQ before they expire:

      • Good to set a retention of 14 days in the DLQ





---------------------------------------------- SQS DLQ – Redrive to Source 


• Feature to help consume messages in the DLQ to understand what is wrong with them

-- So you have your messages now you know they haven't been processed in the source queue and therefore they are in the Dead Letter Queue and you're going to do a manual inspection and debugging of these messages.

-- then you're going to fix your consumer code understand why the message wasn't processed in case the message was correct.

-- And then what you can do is re drive that message from the dead letter queue into the source SQS queue

-- what's going to happen with this is that the consumer can now reprocess that message without even knowing that the message went into the Dead Letter Queue and then the message processing has happened and as a cool feature. 

• When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches without writing custom code






---------------------------------------------- LAB DLQ 


-- create one std queue and one dead letter queue with all default values 

-- now go to std queue --> change visibility time = 5 , and add DLQ , Maximum receives = 3 (keep below 5 ) , do save this queue

-- go to std queue --> send message --> poll message 

-- So here we're receiving the messages once and after five seconds, we will receive it a second time. And after, again, five seconds, we're going to receive a third time,

-- stop polling now , try to poll for messages again , u can not see any messages 

-- go to DLQ --> poll for messages --> u will see that message 

-- let me show you how to redrive that message from the DLQ into this first queue.

-- go to DLQ queue --> start DLQ redrive 

-- go to main Std queue --> start polling for messagaes , u will get message from the DLQ 






---------------------------------------------- Amazon SQS – Delay Queue


• Delay a message (consumers don’t see it immediately) up to 15 minutes

• Default is 0 seconds (message is available right away), That means that as soon as you send the message into an SQS queue, the message will be available right away to be read 

• Can set a default at queue level , to say all the messages should be delayed by X number of seconds

• Can override the default on send using the DelaySeconds parameter 

    - every time you send a message you can set a per-message delay if you wanted to using the DelaySeconds parameter.



Producer--------------------(Send messages)-----> SQS queue ----------(Poll messages) -------------> Consumer



----------------------------- LAB for Delay Queue


-- create new Delayqueue 

-- set Delivery delay = 10 sec , so that messages will wait 10 seconds before being read by a consumer.

-- create Queue , now type some message and poll first , now c.o send message , u will get message after 10 sec 

-- so, as you can see, there was a delay between the send and the actual delivery of that message.





-------------------------------------------------------------------------------  Certified Developer concepts


---------------------------------------------- 1 Amazon SQS - Long Polling


• When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue

• This is called Long Polling

-- So why do we do long polling ?

• LongPolling decreases the number of API calls made to SQS while increasing the efficiency and decreasing the latency of your application.

-- we do long polling because we are doing less API calls into the SQS queue. And on top of it, we know that as soon as the message will arrive in the SQS queue, then the SQS queue will send it back to the consumer. 

-- So we are increasing the efficiency, because we do less API calls, so less CPU cycles are used. And we are also decreasing the latency because as soon as a message will be received by your SQS queue, it will be received by your consumer.

• The wait time can be between 1 sec to 20 sec (20 sec preferable)

• Long Polling is preferable to Short Polling

• Long polling can be enabled at the queue level or at the API level using "ReceiveMessageWaitTimeSeconds"


--------important thing that you need to understand the difference is in short polling Amazon SQS sends the response right away even if the query found is no message 

------- long pooling Amazon SQS asked you sends and empty response only if the polling wait time expires and that is why we use long polling if we want to save API call costs.





---------------------------------------------- 2 SQS Extended Client


• Message size limit is 256KB, how to send large messages, e.g. 1GB?


• Using the SQS Extended Client (Java Library) , which does something very simple that you could implement in any other language, but the idea is that it will use an Amazon S3 buckets as a repository for the large data.


IMP :


-- for example , your producer wants to send a large message into SQS, but first what's going to happen is that the actual large message will end up in Amazon S3,

-- what will be sent into your SQS queue is it will be a small metadata message that has a pointer to the larger message in your Amazon S3 buckets.

-- So the SQS queue will contain small messages, and your Amazon S3 bucket will contain large objects.

-- your consumer when it reads from the SQS queue using this library, the SQS extended clients, then it will consume this small metadata message,

-- which will say to the consumer, "Hey, go read "that bigger message out of Amazon S3," and the consumer will be able to read and retrieve large messages from S3.

-- So a typical use case for this is if you're processing video files, you don't send the entire video file into your SQS queue node, you upload that video file into your Amazon S3 bucket, 

-- and you send a small message with a pointer to that video file into your SQS queue. And that allows you to accommodate any message size really through this pattern.



----------- WORKFLOW Structure


        Producer -------> Small metadata message ------------> SQS Queue ------------> Small metadata message -----------> Consumer
           |                                                                                                                 |
           |                                                                                                                 |
           |                                                                                                                 |
           |                                                                                                                 |
            -------------(Send large message to S3)-----------> S3 Bucket ------------(Retrieve large message from S3)--------
   
                         



---------------------------------------------- 3 SQS – Must know API


-- we'll, you will maybe see some API calls given through by the exam. And so it's just normal API calls


• CreateQueue (MessageRetentionPeriod), DeleteQueue

    - So CreateQueue is used to create a queue, and you can use the argument "MessageRetentionPeriod" to set how long a message should be kept in a queue before being discarded.

    - DeleteQueue is used to delete a queue and delete all the messages in the queue at the same time.


• PurgeQueue: delete all the messages in queue 


• SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage

    - when we are sending messages, we use to SendMessage API, and if we want to send messages with a delay, we can use the DelaySeconds parameter.

    - ReceiveMessage is to do polling, and DeleteMessage is to delete a message once it has been processed by a consumer.

• MaxNumberOfMessages: default 1, max 10 (for ReceiveMessage API)

    - So when you receive a message, by default, the parameter MaxNumberOfMessages is set to 1. That means that you receive one message at a time, but you can receive up to 10 messages in a time in SQS. So you can set the max number of messages parameter for the ReceiveMessage API to 10 to receive a batch of messages from SQS.


• ReceiveMessageWaitTimeSeconds: Long Polling

    - telling your consumer how long to wait before getting a response for from the queue. And this is equivalent of enabling long polling. 
    

• ChangeMessageVisibility: change the message timeout

    - And the ChangeMessageVisibility, it is used to change the message timeout in case you need more time to process a message.


• if you want to use Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility this helps decrease the number of API calls you're doing into API and therefore helps decrease your costs



--------------------------- LAB for Long Polling in AWS 


-- go to std queue --> edit --> Receive message wait time = 20 (0 - 20 sec ) --> save 

-- this is saying that you should wait up to 20 seconds to receive a message if the queue is empty.

-- send and receive messagaes , here I'm gonna go ahead and start a consumer. Now this consumer is doing long polling, and so that means that only one API call is happening, and it's waiting for a message coming from the SQS queue because right now there is none.

-- But if I say say hello world and just press Send, as soon as I press Send, the message was received by my consumer.

-- This was extremely low latency, because my consumer was in long polling mode, and it was waiting for a message from SQS thanks to the WaitMessageTime setting that we set from before.





------------------------------------------------------------------------ Amazon SQS – FIFO Queue


• FIFO = First In First Out (ordering of messages in the queue)



    Producer --------(Send messages 4,3,2,1) ---------> SQS Queue -----------(Poll messages 4,3,2,1)------------> Consumer


-- Now, because we have such a constraint and guarantee about ordering, this SQS queue has limited throughput.

• Limited throughput: 300 msg/s without batching, 3000 msg/s with Batching

• Exactly-once send capability (by removing duplicates) 

• Messages are processed in order by the consumer

-- So FIFO queue is something you should see whenever you have decoupling, but also the need to maintain the ordering of messages, and also make sure that you are on this throughput's constraints that you are not sending too many messages into SQS,

-- the FIFO queue is ends with .fifo only while u create any FIFO queue 




---------------- LAB 


-- create our first FIFO Queue , with all default settings 

-- send message and observe the things 




---------------------------------------------- SQS FIFO – Deduplication


• De-duplication interval is 5 minutes

• Two de-duplication methods:

      • Content-based deduplication: will do a SHA-256 hash of the message body , And if the same message body is encountered twice, the same hash will be the same twice. And so therefore, the second message will be refused.

      • Explicitly provide a Message Deduplication ID directly while sending a message. And if the same deduplication ID is encountered twice then the message will be gone.



---------------------------------------------- SQS FIFO – Message Grouping


-- The second advanced concept we need to look at is message grouping.

• If you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order

• To get ordering at the level of a subset of messages, specify different values for MessageGroupID

      • The idea is that the Messages that share a common Message Group ID will be in order within the group

      • Each Group ID can have a different consumer ( So you can enable parallel processing!) on your SQS FIFO queue.


• Ordering across groups is not guaranteed


for example , And we're grouping messages into three groups: A, B, and C. 

-- Say we have message A1, A2, A3. Then we can have a consumer for the group A.

-- Then we have another group of messages: B1, B2, B3, B4. We can have another consumer for that group B.

--  for consumer group C , we have C1 and C2.

-- The idea is that, for example, maybe sometimes you don't need the total ordering of all the messages. But you want ordering of all the messages for a specific customer ID.

-- for that one specific customer ID, you can use this as your message group ID.
     
      

                   ----------------(A3, A2 , A1) --------------------> Consumer for Group “A”

      SQS FIFO     ----------------(b4, b3, b2 , b1) --------------------> Consumer for Group “B” 
                  
                   ----------------(c2 , c1) --------------------> Consumer for Group “C”





--------------------------- LAB 


-- go to FIFO queue --> edit --> enable contenct based deduplication --Save 

-- now try to send message with group ID , u willl receive message , now try to send message again u can't send 'coz the deduplication is occuring here , now change the contect with same group id , u will able to send the message 'coz we have changed our content 


IMP to know 

-- here during the polling period only u can able to delete the messages once it gets polled after u cannot delete in the FIFO queue(The receipt handle has expired.)

-- where u can delete in std queue

-- now try to delete during the polling time , u can able to delete the message 





----------------------------------------------------------------- Amazon Simple Queue Service (Amazon SQS) temporary queues


-- Temporary queues help you save development time and deployment costs when using common message patterns such as request-response.

-- You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.

-- The following are the benefits of temporary queues:

   - They serve as lightweight communication channels for specific threads or processes.
   - They can be created and deleted without incurring additional costs.
   - They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.

-- To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client.

-- This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. 

-- The key concept behind the client is the virtual queue. 

-- Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. 

-- Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.





----------------------------------------------------------------- Amazon Simple Notification Service (SNS)


• What if you want to send one message to many receivers?


Method 1 : Direct integration 

   - So you could have a direct integration where for example a buying service application could send an email notification, then send a message to a fraud service send a message to a shipping service and maybe even send a message into an SQS Queue.

   - This is cumbersome because every time you have to add a new receiving service. You need to create and write that integration.


Method 2 :  Pub / Sub (Publish-subscribe.)
    
   - The idea is that the buying service will send a message into an SNS topic which is publishing a message into a topic. And that topic will have many subscribers.

   - And each of the subscriber will be able to receive that message from the SNS topic and have it for their own.



---------------------------------------- Amazon SNS


• The “event producer” only sends message to one SNS topic

• As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications 

• Each subscriber to the topic will get all the messages (note: new feature to filter messages), except if you're using a feature to filter messages and it is possible as well.

• Up to 12,500,000 subscriptions per topic

• 100,000 topics limit , and you can increase that limit as well.


        SNS -----------------(publish)------------> SQS / Lambda / Kinesis Data Firehose / HTTP(S) Endpoints / Emails / SMS and Mobile Nonficanons 




------------------------ SNS integrates with a lot of AWS services


• Many AWS services can send data directly to SNS for notifications

CloudWatch Alarms / AWS Budgets / Lambda / DynamoDB / S3 Bucket (Events) / Auto Scaling Group (Notifications) / RDS Events / AWS DMS (New Replic) / CloudFormation (State Changes) ------------(publish)--------> SNS



----------------------- Amazon SNS – How to publish


• Topic Publish (using the SDK)
  • Create a topic
  • Create a subscription (or many)
  • Publish to the topic

• Direct Publish (for mobile apps SDK)
  • Create a platform application
  • Create a platform endpoint
  • Publish to the platform endpoint
  • Works with Google GCM, Apple APNS, Amazon ADM...



----------------------  Amazon SNS – Security

• Encryption:
  • In-flight encryption using HTTPS API
  • At-rest encryption using KMS keys
  • Client-side encryption if the client wants to perform encryption/decryption itself

• Access Controls: IAM policies to regulate access to the SNS API

• SNS Access Policies (similar to S3 bucket policies)
  • Useful for cross-account access to SNS topics
  • Useful for allowing other services ( S3...) to write to an SNS topic 




---------------------- SNS + SQS: Fan Out











