
NOTE: NO Hands-On for this course , all are coverd in previous courses 





======================================================== Identity & Federation ========================================================





-------------------------------------------------------- IAM – What should you know by now




• Users: long term credentials

• Groups

• Roles: short-term credentials, uses STS

        • EC2 Instance Roles: uses the EC2 metadata service. One role at a time per instance

        • ServiceRoles:APIGateway,CodeDeploy,etc...

        • Cross Account roles


• Policies

        • AWS Managed

        • Customer Managed

        • Inline Policies


• Resource Based Policies (S3 bucket, SQS queue, etc...)        







-------------------------------------------------------- IAM Policies Deep Dive



• Anatomy of a policy: JSON doc with Effect, Action, Resource, Conditions, Policy Variables

• Explicit DENY has precedence over ALLOW

• Best practice: use least privilege for maximum security

        • Access Advisor: See permissions granted and when last accessed

        • AccessAnalyzer: Analyzeresourcesthatare shared with external entity


• Navigate Examples at:

        https://docs.aws.amazon.com/IAM/latest/User Guide/access_policies_examples.html




EG : Allows enabling and disabling AWS Regions


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "EnableDisableHongKong",
            "Effect": "Allow",
            "Action": [
                "account:EnableRegion",
                "account:DisableRegion"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {"account:TargetRegion": "ap-east-1"}
            }
        },
        {
            "Sid": "ViewConsole",
            "Effect": "Allow",
            "Action": [
                "account:ListRegions"
            ],
            "Resource": "*"
        }
    ]
}






-------------------------------------------------------- IAM AWS Managed Policies



AdministratorAccess


{
    "Version": "2012-10-17", "Statement": [
    {
        "Effect": "Allow", 
        "Action": "*", 
        "Resource": "*"
    } 
 ]
}



- A few common IAM policies we'll get across is going to be the AdministratorAccess which is meaning that everything should be allowed on any resource.

- And so this is a policy you have to specify

- so by default, if you set nothing in an IAM policy then everything will be denied.

- But if you add that statement, which is, allow action star resource star, then everything will be allowed and that will give you AdministratorAccess.






-------------------------------------------------------- IAM AWS Managed Policies



PowerUserAccess  : a bit less privileged, is called AWS PowerUserAccess.






{
  "Version" : "2012-10-17",
  "Statement" : [
    {
      "Effect" : "Allow",
      "NotAction" : [
        "iam:*",
        "organizations:*",
        "account:*"
      ],
      "Resource" : "*"
    },
    {
      "Effect" : "Allow",
      "Action" : [
        "account:GetAccountInformation",
        "account:GetPrimaryEmail",
        "account:ListRegions",
        "iam:CreateServiceLinkedRole",
        "iam:DeleteServiceLinkedRole",
        "iam:ListRoles",
        "organizations:DescribeOrganization"
      ],
      "Resource" : "*"
    }
  ]
}



IMP : Note how ”NotAction” is used instead of Deny



-- so in the first one, the "Effect" is "Allow", "Not action" on IAM star organizations and account star for resource stars, so that means that it's not going to allow anything to be done on IAM organizations and accounts.

-- And also it's going to allow still a few IAM actions such as create

         "account:GetAccountInformation",
        "account:GetPrimaryEmail",
        "account:ListRegions",
        "iam:CreateServiceLinkedRole",
        "iam:DeleteServiceLinkedRole",
        "iam:ListRoles",
        "organizations:DescribeOrganization"




IMP :

-- why on the left hand side is "NotAction" used and not just "Effect":"Deny."

        - So if you use "Effect":"Deny," and then you specify these three actions and then you specify "Effect":"Allow," and these seven actions.

        - These five actions will automatically be denied because there will be an explicit deny on the left of hand side.

        - So here we have a very interesting use case, in case we don't want to deny everything in there because we want to explicitly allow a few things in there, then we can use the "NotAction" instead of "Deny" to allow for these two things to coexist together.







-------------------------------------------------------- IAM Policies Conditions




"Condition" : { "{condition-operator}" : { "{condition-key}" : "{condition-value}" }}


Operators:

        • String (StringEquals, StringNotEquals, StringLike...)

                • "Condition": {"StringEquals": {"aws:PrincipalTag/job-category": "iamuser-admin"}}

                • "Condition": {"StringLike": {"s3:prefix": [ "", "home/", "home/${aws:username}/" ]}}



• Numeric (NumericEquals, NumericNotEquals, NumericLessThan...)   

• Date (DateEquals, DateNotEquals, DateLessThan...)

• Boolean (Bool):

        • “Condition": {"Bool": {"aws:SecureTransport": "true"}}

        • "Condition": {"Bool": {"aws:MultiFactorAuthPresent": "true"}}


• (Not)IpAddress:

        • "Condition": {"IpAddress": {"aws:SourceIp": "203.0.113.0/24"}}


• ArnEquals, ArnLike

• Null: "Condition":{"Null":{"aws:TokenIssueTime":"true"}}





-------------------------------------------------------- IAM Policies Variables and Tags



Example: ${aws:username}

        • "Resource":["arn:aws:s3:::mybucket/${aws:username}/*"]


AWS Specific:

        • aws:CurrentTime, aws:TokenIssueTime, aws:principaltype, aws:SecureTransport, aws:SourceIp, aws:userid, ec2:SourceInstanceARN


Service Specific:

        • s3:prefix, s3:max-keys, s3:x-amz-acl, sns:Endpoint, sns:Protocol...


Tag Based:

        • iam:ResourceTag/key-name, aws:PrincipalTag/key-name...






-------------------------------------------------------- IAM Roles vs Resource Based Policies




• Attach a policy to a resource (example: S3 bucket policy) versus attaching of a using a role as a proxy


-- So here is an example. We have a user in account A and is trying to access an S3 bucket in account B.

    - Now there's two ways of doing this,

        1 The first one is to use a role in account B and we'll be assuming that role from account A and once you assume the role in account B the role will have the necessary permissions to access the S3 buckets.

        2 Another option is to use an S3 bucket policy and in the S3 bucket policy, we're saying, okay user in account A can access my Amazon S3 bucket in account B.



-- both these solutions allow user account A to access the Amazon S3 bucket in account B.    

-- But there is a very, very big difference.

        • When you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role

        • When using a resource-based policy, the principal doesn’t have to give up any permissions

        • Example: User in account A needs to scan a DynamoDB table in Account A and dump it in an S3 bucket in Account B.

        • Suppor ted by: Amazon S3 buckets, SNS topics, SQS queues, Lambda functions, ECR, Backup, EFS, Glacier, Cloud9, AWS Ar tifact, Secrets Manager, ACM, KMS, CloudWatch Logs, API Gateway, EventBridge etc...






-------------------------------------------------------- IAM Permission Boundaries


• IAM Permission Boundaries are supported for users and roles (not groups)

• Advanced feature to use a managed policy to set the maximum permissions an IAM entity can get.

• Can be used in combinations of AWS Organizations SCP

Use cases:

        • Delegate responsibilities to non administrators within their permission boundaries, for example create new IAM users

        • Allow developers to self-assign policies and manage their own permissions, while making sure they can’t “escalate” their privileges (= make themselves admin)

        • Useful to restrict one specific user (instead of a whole account using Organizations & SCP)







---------------------------------------------------------- IAM Access Analyzer



• Find out which resources are shared externally


        • S3 Buckets
        • IAM Roles
        • KMS Keys
        • Lambda Functions and Layers 
        • SQS queues
        • Secrets Manager Secrets


• Define Zone of Trust = AWS Account or AWS Organization

• Access outside zone of trusts => findings



-- There are other sides of IAM Access Analyzer you need to know about.


• IAM Access Analyzer Policy Validation

        • Validates your policy against IAM policy grammar and best practices

        • General warnings, security warnings, errors, suggestions

        • Provides actionable recommendations



• IAM Access Analyzer Policy Generation

        • Generates IAM policy based on access activity

        • CloudTrail logs is reviewed to generate the policy with the fine-grained permissions and the appropriate Actions and Services

        • Reviews CloudTrail logs for up to 90 days        








----------------------------------------------------- Using STS to Assume a Role



• Define an IAM Role within your account or cross-account

• Define which principals can access this IAM Role and we authorize everything with IAM policies.

• Use AWS STS (Security Token Service) to retrieve credentials and impersonate the IAM Role you have access to (AssumeRole API)

• Temporary credentials can be valid between 15 minutes to 12 hour




----------------------------------------------------- Assuming a Role with STS


• Provide access for an IAM user in one AWS account that you own to access resources in another account that you own

• Provide access to IAM users in AWS accounts owned by third parties

• Provide access for services offered by AWS to AWS resources

• Provide access for externally authenticated users (identity federation)

• Ability to revoke active sessions and credentials for a role (by adding a policy using a time statement – AWSRevokeOlderSessions)

- When you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role






----------------------------------------------------- Providing Access to an IAM User in Your or Another AWS Account That You Own



• You can grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own.


• Benefits:

        • You must explicitly grant your users permission to assume the role.

        • Your users must actively switch to the role using the AWS Management Console or assume the role using the AWS CLI or AWS API

        • You can add multi-factor authentication (MFA) protection to the role so that only users who sign in with an MFA device can assume the role

        • Least privilege + auditing using CloudTrail




----------------------------------------------------- Cross account access with STS



- so we have a production accounts(ACC 1) with an S3 buckets and a development accounts(ACC 2) with two groups of users, for example, testers and developers, and we want to provide developers access to the S3 bucket production app.

- so the first thing we do is that in the production accounts,

         1. Admin creates role that grants Development account read/write access to productionapp bucket so this role will allow us to access the S3 buckets.

         2. Admin grants members of the group Developers permission to assume the UpdateApp Role

         - so what happens then?

         3 The users of that group, the developers group only, can access or request access to the role using the STS API, 
         
         4 STS will return the role temporary credentials, and using these credentials, the users can access the S3 bucket using the role credentials, so very, very, very simple.

         5 5. User can access the S3 bucket by using the role credentials




----------------------------------------------------- Providing Access to AWS Accounts Owned by Third Parties


• Zone of trust = accounts, organizations that you own

• Outside Zone of Trust = 3rd parties

        - For example, you work with a partner, and they provide you a service, and you want to give, like I say, consulting company, and you want to give that consulting company access to your accounts.

        - They're outside of your zone of trust. They're not directly in your organization,


• Use IAM Access Analyzer to find out which resources are exposed

• For granting access to a 3rd party to some of your resources:

        • The 3rd party AWS account ID

        • An External ID (secret between you and the 3rd party)

                - you define an external ID, and this external ID is a secret between you and the third party, and you have to define it,

                - and this is because if you give access to the third party to a role in your accounts, you want to make sure that only that third party has access to this role, so this is to uniquely associate the role between you and a third party,

                - Must be provided when defining the trust and when assuming the role

                - Must be chosen by the 3rd party

        • Define permissions in the IAM policy




----------------------------------------------------- The confused deputy



https://docs.aws.amazon.com/IAM/latest/UserGuide/confused-deputy.html



- The “confused deputy” problem in AWS is a security issue where an entity without permission to perform an action can trick a more-privileged entity into performing that action on its behalf. This can happen in scenarios involving cross-account or cross-service access.

- For example, if you delegate access to a third-party service to monitor your AWS resources, another customer of that service might trick it into accessing your resources

- To prevent this, AWS uses mechanisms like the ExternalId condition in IAM roles. This ensures that only the intended third party can assume the role and access your resources

- AWS also recommends using global condition context keys like aws:SourceArn and aws:SourceAccount in resource-based policies to limit permissions to specific resources






----------------------------------------------------- SessionTags in STS


• Tags that you pass when you assume an IAM Role or federate user in STS

        - for example, you have user federation that leverages STS, it is possible for you to pass a session tag,


• aws:PrincipalTag Condition

        • Compares the tags attached to the principal making the request with the tag you specified in the policy

        • Example: allow a principal to pass session tags only if the principal making the request has the specified tags




                sts:AssumeRole
                with Session Tag Department=HR
     ------------------------------------------------> 
USER                                                    STS
  |   <------------------------------------------------
  |              temporary security credentials
  |
  |
  |
  | ----------------------------------------------> S3 Bucket(hr-docs)



EXP: 

- your user is going to, for example, do the STS AssumeRole API call and pass a session tag as part of the API call, for example, for Department=HR, 

- STS will then return temporary security credentials for that user with the session tag,

- so why do we do this?

- in our IAM policies, we can use a condition called the aws:PrincipalTag, and the idea is that using this, the tag of the condition is going to be compared to the tag passed to the STS AssumeRole, and 

- so this allows us to have conditions to make sure that the principal making the request has the specified tags.





----------------------------------------------------- STS Important APIs


• AssumeRole: access a role within your account or cross-account

• AssumeRoleWithSAML: return credentials for users logged with SAML

• AssumeRoleWithWebIdentity: return creds for users logged with an IdP

        • Example providers include Amazon Cognito, Login with Amazon, Facebook, Google, or any OpenID Connect-compatible identity provider

        • AWS recommends using Cognito instead

• GetSessionToken: for MFA, from a user or AWS account root user

• GetFederationToken: obtain temporary creds for a federated user, usually a proxy app that will give the creds to a distributed app inside a corporate network






----------------------------------------------------- Identity Federation in AWS



• Give users outside of AWS permissions to access AWS resources in your account

• You don’t need to create IAM Users (user management is outside AWS)

        - the idea is that because the users already exist within your corporate directory, then you don't want to create specific IAM users for them, because you'd like to do user management outside of AWS, so this is why we need Identity Federation.


• Use cases:

        • A corporate has its own identity system (e.g., Active Directory)

        • Web/Mobile application that needs access to AWS resources


• Identity Federation can have many flavors:

        • SAML 2.0

        • Custom Identity Broker

        • Web Identity Federation With(out)Amazon Cognito

        • Single Sign-On (SSO)





----------------------------------------------------- 1 SAML 2.0 Federation



• Security Assertion Markup Language 2.0 (SAML 2.0)

• Open standard used by many identity providers (e.g.,ADFS)

        • Supports integration with Microsoft Active Directory Federations Services(ADFS)

        • Or any SAML2.0–compatible IdPs with AWS


• Access to AWS Console,AWS CLI,or AWS API using temporary credentials

        • No need to create IAM Users for each of your employees

        • Need to setup a trust between AWS IAM and SAML 2.0 Identity Provider (both ways)


• Under-the-hood: Uses the STS API AssumeRoleWithSAML , comes from the STS service and they will give us temporary credentials using SAML Assertion.

• SAML 2.0 Federation is the “old way”, Amazon Single Sign-On (AWS SSO) Federation is the new managed and simpler way

        • https://aws.amazon.com/blogs/security/enabling-federation-to-aws-using-windows-active-directory- adfs-and-saml-2-0/





-------------------- SAML 2.0 Federation – AWS API Access and SAML 2.0 Federation – AWS Console Access and SAML 2.0 Federation – Active Directory FS (ADFS)


check in course 




----------------------------------------------------- Custom Identity Broker Application



• Use only if Identity Provider is NOT compatible with SAML 2.0

• The Identity Broker Authenticates users & requests temporary credentials from AWS

• The Identity Broker must determine the appropriate IAM Role

• Uses the STS API AssumeRole or GetFederationToken




----------------------------------------------------- Web Identity Federation – Without Cognito


• Not recommended by AWS – use Cognito instead

- check in course for details more

----------------------------------------------------- Web Identity Federation – With Cognito


• Preferred over for Web Identity Federation

        • Create IAM Roles using Cognito with the least privilege needed

        • Build trust between the OIDC IdP and AWS


• Cognito benefits:

        • Supports anonymous users

        • Supports MFA

        • Data Synchronization


• Cognito replaces a Token Vending Machine (TVM)        






-------------------------------- Web Identity Federation – IAM Policy


• After being authenticated with Web Identity Federation, you can identify the user with an IAM policy variable

• Examples:

        • cognito-identity.amazonaws.com:sub

        • www.amazon.com:user_id

        • graph.facebook.com:id

        • accounts.google.com:sub





Eg :


{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"FullAccessToUserItems",
         "Effect":"Allow",
         "Action":[
            "dynamodb:GetItem",
            "dynamodb:BatchGetItem",
            "dynamodb:Query",
            "dynamodb:PutItem",
            "dynamodb:UpdateItem",
            "dynamodb:DeleteItem",
            "dynamodb:BatchWriteItem"
         ],
         "Resource":[
            "arn:aws:dynamodb:us-west-2:123456789012:table/GameScores"
         ],
         "Condition":{
            "ForAllValues:StringEquals":{
               "dynamodb:LeadingKeys":[
                  "${www.amazon.com:user_id}"
               ]
            }
         }
      }
   ]
}



-- So in this policy, the user can do get item, batch get item, query, put item, update item, delete item and batch right item on a specific table.

-- But there's a condition here. And the condition is saying,, only amazon userd_id will able to perform

• LeadingKeys – we only limit row-level access for users on the Primary Key , so therefore we make sure that the users can only modify and access their own data.

-- you can also specify conditions on attributes , this would be to limit the specific attributes a user can see in your DynamoDB table,

• Attributes – limit specific attributes the user can see





----- So to summarize, you have fine-grain access control by using a federated login 

-- and by specifying a condition on LeadingKeys,if you want it to limit at the role level

-- attributes, if you want your limits at the column level,









----------------------------------------------------- What is Microsoft Active Directory (AD)?


• Found on any Windows Server with AD Domain Services

• Database of objects: User Accounts, Computers, Printers, File Shares, Security Groups

• Centralized security management, create account, assign permissions

• Objects are organized in trees

• A group of trees is a forest



----------------------------------------------------- What is ADFS (AD Federation Services)?


• ADFS provides Single Sign-On across applications

• SAML across 3rd party:AWS Console,Dropbox,Office365,etc...

        - check in course for details 





----------------------------------------------------- AWS Directory Services




1  AWS Managed Microsoft AD


 • Create your own AD in AWS, manage users locally, supports MFA 

 • Establish “trust” connections with your on- premises AD

 On-prem AD <----(trust)-----> AWS Managed AD 

 -- AWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services.

 -- AWS Directory Service for Microsoft Active Directory (aka AWS Managed Microsoft AD) is powered by an actual Microsoft Windows Server Active Directory (AD), managed by AWS.

 -- With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud such as SQL Server-based applications. 

 -- You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).






2  AD Connector

  • Directory Gateway (proxy) to redirect to on-premises AD, supports MFA

  • Users are managed on the on-premises AD

   On-prem AD <-----(proxy)------- AD Connector <---(auth)----

  -- Use AD Connector if you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. 

  -- AD Connector simply connects your existing on-premises Active Directory to AWS. You cannot use it to run directory-aware workloads on AWS, 






3  Simple AD

  • AD-compatible managed directory on AWS

  • Cannot be joined with on-premises AD

  -- Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. 

  -- Simple AD is a standalone managed directory that is powered by a Samba 4 Active Directory Compatible Server. 

  -- Simple AD does not support features such as trust relationships with other domains. 





----------------------------------------------------- 1  AWS Managed Microsoft AD 


        • Managed Service: Microsoft AD in your AWS VPC , We have two AZs and we'll have two AD Domain Controller or AD DC deployed in two different for high availability.

        • EC2 Windows Instances:

                • EC2 Windows instances can join the domain and run traditional AD applications (sharepoint, etc)

                • Seamlessly Domain Join Amazon EC2Instances from Multiple Accounts & VPCs

        • Integrations:

                • RDS for SQL Server,AWS Workspaces,Quicksight...

                • AWS SSO to provide access to 3rd party applications

        • Standalone repository in AWS or joined to on- premises AD       

        • Multi AZ deployment of AD in 2 AZ, # of DC (Domain Controllers) can be increased for scaling

        • Automated backups

        • Automated Multi-Region replication of your directory 



---------------------- How to Connect to on-premises AD to your on-premise active directory


• Ability to connect your on- premises Active Directory to AWS Managed Microsoft AD

• Must establish a Direct Connect (DX) or VPN connection

• Can setup three kinds of forest trust:

        • One-way trust: AWS => on-premises

        • One-way trust: on-premises => AWS

        • Two-way forest trust: AWS <==> on-premises


• Forest trust is different than synchronization(replication is not supported)

        - So, replication is not something that is supported by AWS management Microsoft AD.

        - So that means that their users are living independently on the two different Microsoft active directory.

        - But thanks to this forest trust, they're able to talk to each other in case one user is missing from one and ask the other DC(Domain Controllers)




---------------------- Solution Architecture: Active Directory Replication



• You may want to create a replica of your AD on EC2 in the cloud to minimize latency of in case DX or VPN goes down.

• Establish trust between the AWS Managed Microsoft AD and EC2

So what does it look like?

        - Well, we have our on-premise Microsoft AD with one domain(onpremAD.example.com) and we have the VPC AWS Managed Microsoft AD DC with another domain(awsAD.example.com).

        - The only way to set up some kind of replication is for you to deploy active directory on an easy two windows instance, and you will have to set up replication.

        - by setting up those replication we'll have a replica on-premise Microsoft AD onto our VPC which may help minimize latency and also have a disaster recovery strategy.

        - check course for detail explanation





----------------------------------------------------- 2 AD Connector



• AD Connector is a directory gateway to redirect directory requests to your on-premises Microsoft Active Directory

• No caching capability

• Manage users solely on-premises, no possibility of setting up a trust

• VPN or Direct Connect

• Doesn’t work with SQL Server, doesn’t do seamless joining, can’t share directory







----------------------------------------------------- 3 Simple AD


• Simple AD is an inexpensive Active Directory–compatible service with the common directory features.

• Supports joining EC2 instances, manage users and groups

• Does not support MFA, RDS SQL ser ver, AWS SSO

• Small: 500 users, large: 5000 users

• Powered by Samba 4, compatible with Microsoft AD

• lower cost, low scale, basic AD compatible, or LDAP compatibility

• No trust relationship




----------------------------------------------------- AWS Organizations 




---- Organizational Units (OU) , 

-- In organzations u can create seperate units for different lify cycles for eg : Bussiness unit , environmental lifecycle , project based 

AWS Organizations :

 • Global service

 • Allows to manage multiple AWS accounts

 • The main account is the management account

 • Other accounts are member accounts

 • Member accounts can only be part of one organization -- IMP for Exam

 • Consolidated Billing across all accounts - single payment method

 • Pricing benefits from aggregated usage (volume discount for EC2, S3...) 

 • Shared reserved instances and Savings Plans discounts across accounts

        - So if a reserved instance is unused in one account, another account can benefit from it and therefor the discounts apply across the entire organization, which is really good for cost savings.

 • API is available to automate AWS account creation






 --------------------- Advantages

 • Multi Account vs One Account Multi VPC

 • Use tagging standards for billing purposes

 • Enable CloudTrail on all accounts, send logs to central S3 account

 • Send CloudWatch Logs to central logging account

 • Establish Cross Account Roles for Admin purposes




----------------------------------------------------- AWS Organizations - OrganizationAccountAccessRole



- So once we have all these accounts, how does Organizations perform its administration?


        • IAM role which grants full administrator permissions in the Member account to the Management account

                - So when we have the organization and the management accounts, if we happen to create a member account using an API from the organization service then automatically an IAM role within the member account is going to be created.

                - that IAM role is called the Organization Account Access Role.

                - Now, the management accounts, when it needs to perform administrative duties onto the member account, we'll assume that administrative role using the API.

        • Used to perform admin tasks in the Member accounts (e.g., creating IAM users)

        • Could be assumed by IAM users in the Management account

        • Automatically added to all new Member accounts created with AWS Organizations

        • Must be created manually if you invite an existing Member account (you must manually create this role)




----------------------------------------------------- Multi Account Strategies



• Create accounts per department, per cost center, per dev / test / prod, based on regulatory restrictions (using SCP), for better resource isolation (ex:VPC), to have separate per-account service limits, isolated account for logging,

• Multi Account vs. One Account Multi VPC

• Use tagging standards for billing purposes

• Enable CloudTrail on all accounts, send logs to central S3 account

• Send CloudWatch Logs to central logging account

• Strategy to create an account for security



----------------------------------------------------- AWS Organization - Feature Modes



• Consolidated billing features:

        - Consolidated Billing across all accounts - single payment method

        - Pricing benefits from aggregated usage (volume discount for EC2, S3...)

• All Features (Default):

        - Includes consolidated billing features, SCP

        - Invited accounts must approve enabling all features

        - Ability to apply an SCP to prevent member accounts from leaving the org 

        - once you have enabled all features, Can’t switch back to Consolidated Billing Features only




----------------------------------------------------- AWS Organizations – Reserved Instances



• For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account.

• This means that all accounts in the organization can receive the hourly cost benefit of Reserved Instances that are purchased by any other account.

• The payer account (Management account) of an organization can turn off Reserved Instance (RI) discount and Savings Plans discount sharing for any accounts in that organization, including the payer account

• This means that RIs and Savings Plans discounts aren't shared between any accounts that have sharing turned off.

• To share an RI or Savings Plans discount with an account, both accounts must have sharing turned on



----------------------------------------------------- AWS Organizations – Moving Accounts


- So if we have one AWS Organization and another one and we want to migrate a member account from one to the other.



1. Remove the member account from the AWS Organization

2. Send an invite to the member account from the AWS Organization

3. Accept the invite to the new Organization from the member account





----------------------------------------------------- Service Control Policies (SCP)



• Define allowlist or blocklist IAM actions on your organization for your accounts.

• Applied at the OU or Account level

• Does not apply to the Management Account

• SCP is applied to all the Users and Roles in the account, including Root user

• The SCP does not affect Service-linked roles

        • Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs.

• SCP must have an explicit Allow from the root a each OU in the direct path to the target account (does not allow anything by default)

• Use cases:

        • Restrict access to certain services (for example: can’t use EMR)

        • Enforce PCI compliance by explicitly disabling services




----------------------------------------------------- Restricting Tags with IAM Policies


• You can restrict specific Tags on AWS resources

• Using the aws:TagKeys Condition Key

        • Validate the Tag Keys attached to a resource against the Tag Keys in the IAM Policy

• Example: allow IAM users to create EBS Volumes only if it has the “Env” and “CostCenter” Tags


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "ec2:CreateVolume",
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestTag/Env": "*",
                    "aws:RequestTag/CostCenter": "*"
                },
                "ForAllValues:StringEquals": {
                    "aws:TagKeys": ["Env", "CostCenter"]
                }
            }
        }
    ]
}



• Use either ForAllValues (must have all keys) or ForAnyValue (must have any of these keys at a minimum)


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "ec2:CreateVolume",
            "Resource": "*",
            "Condition": {
                "ForAnyValue:StringEquals": {
                    "aws:RequestTag/Env": "*",
                    "aws:RequestTag/CostCenter": "*"
                },
                "ForAnyValue:StringEquals": {
                    "aws:TagKeys": ["Env", "CostCenter"]
                }
            }
        }
    ]
}







----------------------------------------------------- Using SCP to Deny a Region


- Another example of SCP you can have is an SCP to deny an entire region using the condition named aws:RequestRegion.aws:RequestedRegion

- So the idea is that you would specify a Deny effect, and then you would say that the region where everything is denied could be eu-central-1 and eu-west-1.

- EG 



{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Action": "*",
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestedRegion": "us-west-2"
                },
                "ArnNotLike": {
                    "aws:PrincipalArn": [
                        "arn:aws:iam::123456789012:role/Role1",
                        "arn:aws:iam::123456789012:role/Role2"
                    ]
                }
            }
        }
    ]
}






----------------------------------------------------- Using SCP to Restrict Creating Resources without appropriate Tags


• Prevent IAM Users/Roles in the affected Member accounts from creating resources if they don’t have a specific Tags

• Example: restrict launching an EC2 instance if it doesn’t have the “Project” and “CostCenter” Tags


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Action": "ec2:RunInstances",
            "Resource": "arn:aws:ec2:*:*:instance/*",
            "Condition": {
                "ForAllValues:StringEquals": {
                    "aws:TagKeys": ["Project", "CostCenter"]
                }
            }
        }
    ]
}




----------------------------------------------------- AWS Organizations – Tag Policies



• Helps you standardize tags across resources in an AWS Organization

• Ensure consistent tags, audit tagged resources, maintain proper resources categorization, ...

• You defineTag keys and their allowed values

• Helps with AWS Cost Allocation Tags and Attribute-based Access Control

• Prevent any non-compliant tagging operations on specified services and resources

• Generate a report that lists all tagged/non- compliant resources

• Use Amazon EventBridge to monitor non- compliant tags




----------------------------------------------------- AWS Organizations – AI Services Opt-out Policies



• Certain AWS AI services may use your content for continuous improvement of Amazon AI/ML services

• Example: Amazon Lex, Amazon Comprehend, Amazon Polly, ...

• You can opt-out of having your content stored or used by AWS AI services

• Create an Opt-out Policy that enforces this setting across all Member accounts and AWS Regions

• You can opt-out all AI services or selected services

• Can be attached to Organization Root, specific OU, or individual Member account





----------------------------------------------------- AWS Organizations – Backup Policies




• AWS Backup enables you to create Backup Plans that define how to backup your AWS resources

• JSON documents that define Backup Plans across an AWS Organization

• Gives you granular control over backing up your resources (e.g., backup frequency, time window, backup region, ...)

• Can be attached to Organization Root, specific OU, or individual Member account

• Immutable Backup Plans appear in Member accounts (view ONLY)




{
    "BackupPlanName": "OrganizationBackupPlan",
    "Rules": [
        {
            "RuleName": "DailyBackup",
            "TargetBackupVaultName": "Default",
            "ScheduleExpression": "cron(0 12 * * ? *)",  // Daily at 12:00 UTC
            "StartWindowMinutes": 60,
            "CompletionWindowMinutes": 180,
            "Lifecycle": {
                "MoveToColdStorageAfterDays": 30,
                "DeleteAfterDays": 365
            },
            "RecoveryPointTags": {
                "Project": "MyProject",
                "CostCenter": "12345"
            },
            "CopyActions": [
                {
                    "DestinationBackupVaultArn": "arn:aws:backup:us-west-2:123456789012:vault/SecondaryVault",
                    "Lifecycle": {
                        "MoveToColdStorageAfterDays": 30,
                        "DeleteAfterDays": 365
                    }
                }
            ]
        }
    ]
}



- The backup plan is named “OrganizationBackupPlan”.

- A rule named “DailyBackup” is defined to create backups daily at 12:00 UTC.

- The backup has a start window of 60 minutes and a completion window of 180 minutes.

- Backups are moved to cold storage after 30 days and deleted after 365 days.

- The backups are tagged with “Project” and “CostCenter”.

- The backups are copied to a secondary vault in the us-west-2 region.






----------------------------------------------------- AWS IAM Identity Center (successor to AWS Single Sign-On)




• One login (single sign-on) for all your

        • AWS accounts in AWS Organizations

        • Business cloud applications (e.g., Salesforce, Box, Microsoft 365, ...)

        • SAML2.0-enabled applications

        • EC2 Windows Instances



• Identity providers

        • Built-in identity store in IAM Identity Center

        • 3rd par ty: Active Directory (AD), OneLogin, Okta...




----------------------------------------------------- AWS IAM Identity Center Fine-grained Permissions and Assignments



• Multi-Account Permissions

        • Manage access across AWS accounts in your AWS Organization

        • Permission Sets – a collection of one or more IAM Policies assigned to users and groups to define AWS access



• Application Assignments

        • SSO access to many SAML 2.0 business applications (Salesforce, Box, Microsoft 365, ...)

        • Provide required URLs, certificates, and metadata


• Attribute-Based Access Control (ABAC)

        • Fine-grained permissions based on users’ attributes stored in IAM Identity Center Identity Store

        • Example:costcenter,title,locale,...

        • Usecase:Define permissions once,then modify AWSaccess by changing the attributes





----------------------------------------------------- AWS Control Tower



• Easy way to set up and govern a secure and compliant multi-account AWS environment based on best practices

• Benefits:

        • Automate the set up of your environment in a few clicks

        • Automate ongoing policy management using guardrails

        • Detect policy violations and remediate them

        • Monitor compliance through an interactive dashboard


• AWS Control Tower runs on top of AWS Organizations:

        • It automatically sets up AWS Organizations to organize accounts and implement SCPs (Service Control Policies)




----------------------------------------------------- AWS Control Tower – Account Factory



• Automates account provisioning and deployments

• Enables you to create pre-approved baselines and configuration options for AWS accounts in your organization (e.g.,VPC default configuration, subnets, region, ...)

• Uses AWS Service Catalog to provision new AWS accounts



----------------------------------------------------- AWS Control Tower – Detect and Remediate Policy Violations


• Guardrail

        • Provides ongoing governance for your Control Tower environment (AWS Accounts)

        • Preventive – using SCPs (e.g., Disallow Creation of Access Keys for the Root User)

        • Detective – using AWS Config (e.g., Detect Whether MFA for the Root User is Enabled)

        • Example: identify non-compliant resources (e.g., untagged resources)





----------------------------------------------------- AWS Control Tower – Guardrails Levels



• Mandatory

        • Automatically enabled and enforced by AWS Control Tower

        • Example: Disallow public Read access to the Log Archive account


• Strongly Recommended

        • Based on AWS best practices (optional)

        • Example: Enable encryption for EBS volumes attached to EC2 instances


• Elective

        • Commonly used by enterprises (optional)

        • Example: Disallow delete actions without MFA in S3 buckets






----------------------------------------------------- AWS Resource Access Manager (RAM)



• Share AWS resources that you own with other AWS accounts

• Share with any account or within your Organization

• Avoid resource duplication!

• VPC Subnets

        • Allow to have all the resources launched in the same subnets

        • Must be from the same AWS Organizations.

        • Cannot share security groups and default VPC

        • Participants can manage their own resources in there

        • Participants can't view, modify, delete resources that belong to other participants or the owner


• AWS Transit Gateway

• Route 53 (Resolver Rules, DNS Firewall Rule Groups)

• License Manager Configurations

• Aurora DB Clusters

• ACM Private Certificate Authority

• CodeBuild Project

• EC2 (Dedicated Hosts, Capacity Reservation)

• AWS Glue (Catalog, Database,Table)

• AWS Network Firewall Policies

• AWS Resource Groups

• Systems Manager Incident Manager (Contacts, Response Plans) • AWS Outposts (Outpost, Site)





----------------------------- Resource Access Manager – VPC example 1


• Each account...

        • is responsible for its own resources

        • cannot view, modify or delete other resources in other accounts


• Network is shared so...

        • Anything deployed in the VPC can talk to other resources in the VPC

        • Applications are accessed easily across accounts, using private IP!

        • Security groups from other accounts can be referenced for maximum security


• Use cases

        • Applications within the same trust boundaries

        • Applications with a high degree of interconnectivity





----------------------------- Resource Access Manager Managed Prefix List example 2



• A set of one or more CIDR blocks

• Makes it easier to configure and maintain Security Groups and Route Tables

• Customer-Managed Prefix List

        • Set of CIDRs that you define and manage by you

        • Can be shared with other AWS accounts or AWS Organization

        • Modify to update many security groups at once


• AWS-Managed Prefix List

        • Set of CIDRs for AWS services

        • You can’t create, modify, share, or delete them





----------------------------- Resource Access Manager Route 53 Outbound Resolver example 3


• Helps you scale forwarding rules to your DNS in case you have multiple accounts and VPC







----------------------------------------------------- Summary of Identity & Federation



• Users and Accounts all in AWS
• AWS Organizations
• AWS Control Tower to setup secure & complaint multi-account AWS environment (best practices)
• Federation with SAML
• Federation without SAML with a custom IdP (GetFederationToken)
• AWS Single Sign-On to connect to multiple AWS Accounts (Organization) and SAML apps
• Web Identity Federation (not recommended)
• Cognito for most web and mobile applications (has anonymous mode, MFA)
• AWS Directory Service:
• Managed Microsoft AD – standalone or setup trust AD with on-premises, has MFA, seamless join, RDS integration
• AD Connector – proxy requests to on-premises
• Simple AD – standalone & cheap AD-compatible with no MFA, no advanced capabilities
• AWS RAM to share resources (example VPC subnets)





-----------------------------------------------------------------------   Security   --------------------------------------------------------------------------





----------------------------------------------------- AWS CloudTrail




• Provides governance, compliance and audit for your AWS Account

• CloudTrail is enabled by default!

• Get an history of events / API calls made within your AWS Account by:

        • Console
        • SDK
        • CLI
        • AWS Services


• Can put logs from CloudTrail into CloudWatch Logs or S3

• A trail can be applied to All Regions (default) or a single Region.

• If a resource is deleted in AWS, investigate CloudTrail first!






---------------------------------------------------------------- CloudTrail Events





1 • Management Events:

     • Operations that are performed on resources in your AWS account

     • Examples:

          • Configuring security (IAM AttachRolePolicy)

          • Configuring rules for routing data (Amazon EC2 CreateSubnet)

          • Setting up logging (AWS CloudTrail CreateTrail)

          • By default, trails are configured to log management events.

          • Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)



2 • Data Events:

      • By default, data events are not logged (because high volume operations)

      - So what are Data Events?

      • Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject):as you can see, these can be happening a lot on an S3 bucket. so this is why they're not logged by default and u have the option can separate Read and Write Events

      - So a Read Event will be a GetObject whereas a wright Event would be a DeleteObject or a PutObject.

      - Another kind of event you can have in a CloudTrail are AWS Lambda function execution activities.

      • AWS Lambda function execution activity (the Invoke API)

          - So whenever someone uses the Invoke API so you can get insights about how many times your Lambda functions are being evoked.

          - this could be really high volumes, if your Lambda functions are executed a lot.



IMP : By default, CloudTrail tracks only bucket-level actions. To track object-level actions, you need to enable Amazon S3 data events


        - AWS CloudTrail supports Amazon S3 Data Events, apart from bucket Events.

        - You can record all API actions on S3 Objects and receive detailed information such as the AWS account of the caller, IAM user role of the caller, time of the API call, IP address of the API, and other details. 

        - All events are delivered to an S3 bucket and CloudWatch Events, allowing you to take programmatic actions on the events.






3  CloudTrail Insights (u have to pay)


-- So when we have so many Management Events across all types of services and so many APIs happening very quickly in your accounts, 

-- it can be quite difficult to understand what looks odd, what looks unusual and what doesn't.

-- so this is where CloudTrail Insights comes in.


-- So with CloudTrail Insights and you have to enable it and you have to pay for it, it will analyze your events and try to detect unusual activity in your accounts.

• Enable CloudTrail Insights to detect unusual activity in your account:

    EG : 
    
    • inaccurate resource provisioning

    • hitting service limits

    • Bursts of AWS IAM actions

    • Gaps in periodic maintenance activity


• CloudTrail Insights analyzes normal management events to create a baseline

• And then continuously analyzes write events to detect unusual patterns

    • Anomalies appear in the CloudTrail console
    • Event is sent to Amazon S3
    • An EventBridge event is generated (for automation needs)








IMP : As a SysOps Administrator, you have been asked to create a custom rule that evaluates whether CloudTrail trails in your account are turned on and logging for all regions. AWS Config should run the evaluations for the rule every time a trail is created. Also, AWS Config should run the rule every 8 hours.

Which is the most optimal option to meet the given requirements?


ANS : Create the rule with configuration change and periodic triggers


EXP : 

        - When you add a rule to your account, you can specify when you want AWS Config to run the rule; this is called a trigger.

        - AWS Config evaluates your resource configurations against the rule when the trigger occurs. There are two types of triggers:

                1 Configuration changes: AWS Config runs evaluations for the rule when certain types of resources are created, changed, or deleted.

                2 Periodic: AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).

        - If you choose configuration changes and periodic, AWS Config invokes your Lambda function when it detects a configuration change and also at the frequency that you specify.         




---------------------------------------------------------------- CloudTrail Events Retention


• Events are stored for 90 days in CloudTrail and then afterwards they're deleted,

• To keep events beyond this period, log them to S3 and use Athena





---------------------------------------------------------------- LAB 

-- open cloud trail --> event history u can able to see all the events which u have done in the aws account through the root user or normal user 

-- for lab , lets create a new cloud trail 

-- create new s3 bucket , all the API events that will send to s3 and stored in this bucket 

-- Log file SSE-KMS encryption  and Log file validation = uncheck no need for this demo 

-- management events --> create trail that's it 

-- wait for 5 min atleast to see the current time and date iin the trail 

-- after 5 min go n check in the s3 bucket --> it will create log files 

-- in the mean while create one simple ec2 instance for example purpose\

-- check in the history it will shows u the instance running in the trail

-- this is how u will get all the API calls that U have made in ur AWS Account 







------------------------------------------------------- Amazon EventBridge – Intercept API Calls


-- A very important cloud trail integration you need to know about is the one with Amazon EventBridge to intercept any API calls.

-- So let's say you wanted to receive an SNS notification, anytime a user would delete a table in DynamoDB by using the DeleteTable API Call.

-- So what happens that whenever we do an API call in AWS, as you know, the API call itself is going to be logged in CloudTrail. That's for any API call.

-- But also  these all these API calls will end up as events as well in Amazon EventBridge.

-- so we can look for that very specific delete table API call, and create a rule out of it. And this rule will have a destination the destination being Amazon SNS and therefore, we can create alerts.



 User -----------(DeleteTable API Call)------> DynamoDB -----------(Log API call)-----> CloudTrail (any API call) -----------(event)------> Amazon EventBridge -------(alert) ------------> SNS





-------------- few more examples on how you can integrate Amazon Eventbridge and CloudTrail.


1 

   -- For example, say, you wanted to be notified whenever a user was assuming a role in your accounts.

   -- So the AssumeRole is an API in the IAM service and therefore, is going to be logged by CloudTrail.

   -- And then using EventBridge integration, we can trigger a message into an SNS topic.


   User ------ (AssumeRole) ---------> IAM Role --------(API Call logs) ----------> CloudTrail --------(event)---------> EventBridge -------> SNS





2 

    -- Similarly, we can also intercept API calls that, for example, change the Security Group inbound rules. So the Security Group call is called AuthorizeSecurityGroupIngress, and it's an EC2 API call.

    -- So these are going to be logged again by CloudTrail and then they will appear in EventBridge and then we can trigger a notification in SNS.



    User -------------(AuthorizeSecurityGroupIngress)--------> EC2(Security Group) ------------- (API Call logs) -------------> CloudTrail ------------(event)------> EventBridge ---------> SNS









------------------------------------------------------- CloudTrail – Log File Integrity Validation (CloudTrail for SysOps)



-- So when you do API calls within AWS, they're going to be logged by CloudTrail and you can have these logs being sent into Amazon S3, okay, every one hour. 

-- But, you can also create what's called a digest file.

• Digest Files:

        • References the log files for the last hour and contains a hash of each

        • Stored in the same s3 bucket as log files (Different folder)

• Helps you determine whether a log file was modified/deleted after CloudTrail delivered it

• Hashing using SHA-256, Digital Signing using SHA- 256 with RSA

• Protect the S3 bucket using bucket policy, versioning, MFA Delete protection, encryption, object lock

• Protect CloudTrail using IAM






------------------------------------------------------- CloudTrail – Integration with EventBridge




• Used to react to any API call being made in your account

• CloudTrail is not “real-time”:

        • Delivers an event within 15 minutes of an API call

        • Delivers log files to an S3 bucket every 5 minutes


-- So this is not a real-time automation on top of API calls, but this is for you a way to get some kind of integration on top of any API calls made within CloudTrail when it is delivered into EventBridge.





------------------------------------------------------- CloudTrail – OrganizationsTrails



• A trail that will log all events for all AWS accounts in an AWS Organization

• Log events for management and member accounts

• Trail with the same name will be created in every AWS account (IAM permissions)

• Member accounts can’t remove or modify the organization trail (view only)






IMP : Member accounts will be able to see the organization trail, but cannot modify or delete it

        - Organization trails must be created in the master account, and when specified as applying to an organization, are automatically applied to all member accounts in the organization.

        - Member accounts will be able to see the organization trail, but cannot modify or delete it. 

        - By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket.



https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html        






------------------------------------------------------- CloudTrail - Solution Architecture: PRO Delivery to S3


check in course for detail explanation


S3 Enhancements:

        • EnableVersioning

        • MFA Delete Protection

        • S3 Lifecycle Policy (S3 IA, Glacier...)

        • S3 Object Lock

        • SSE-S3 or SSE-KMS encryption

        • Feature to perform CloudTrail Log File Integrity validation (SHA-256 for hashing and signing)







------------------------------------------------------- CloudTrail - Solution Architecture: PRO 


Multi Account, Multi Region Logging


- So say we have two accounts, Account A and Account B. And we have a security account total 3 accounts that we want to send the logs to.

- So we'll have CloudTrail in the first account and CloudTrail in the second account.

- And we'll set up an S3 bucket that will be holding the logs of all these CloudTrails in security account.

- The thing is CloudTrail needs to be delivering these log files into the S3 buckets of the security account.

- And so the only way for us to do it is to define an S3 bucket policy And the S3 bucket policy should allow CloudTrail to deliver files into S3.

- And the cool thing we have here is that the S3 bucket policy, first of all, is necessary and very simple to maintain for cross-account delivery.

- The second is that if Account A needs to access you know, the logs for the CloudTrail bucket we can create, for example, a cross-account role and assume the role in the security account.

- Or we can edit the bucket policy to allow reads from the Account A.






------------------------------------------------------- CloudTrail - Solution Architecture: Alert for API calls


- you want to create alerts when certain API calls are done.


• Log filter metrics can be used to detect a high level of API happening

• Ex: Count occurrences of EC2 TerminateInstances API

• Ex: Count of API calls per user

• Ex: Detect high level of Denied API calls





------------------------------------------------------- CloudTrail – Solution Architecture: OrganizationalTrail


check in course 



------------------------------------------------------- CloudTrail: How to react to events the fastest?


Overall, CloudTrail may take up to 15 minutes to deliver events


• EventBridge:

        • Can be triggered for any API call in CloudTrail

        • The fastest, most reactive way


• CloudTrail Delivery in CloudWatch Logs:

        • Events are streamed

        • Can perform a metric filter to analyze occurrences and detect anomalies


• CloudTrail Delivery in S3:

        • Events are delivered every 5 minutes

        • Possibility of analyzing logs integrity, deliver cross account, long-term storage








------------------------------------------------------- AWS KMS (Key Management Service)






• Anytime you hear “encryption” for an AWS service, it’s most likely KMS

• AWS manages encryption keys for us

• Fully integrated with IAM for authorization

• Easy way to control access to your data

• Able to audit KMS Key usage using CloudTrail

• Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM...)

        • Amazon EBS: encrypt volumes

        • Amazon S3: Server-side encryption of objects

        • Amazon Redshift: encryption of data

        • Amazon RDS: encryption of data

        • Amazon SSM: Parameter store


• But you can also use the CLI / SDK


• Never ever store your secrets in plaintext, especially in your code!

  • KMS Key Encryption also available through API calls (SDK, CLI)
  • Encrypted secrets can be stored in the code / environment variables





--------------------------------------- KMS KeysTypes :




• KMS Keys is the new name of KMS Customer Master Key

 • Symmetric (AES-256 keys)
        
        • Single encryption key that is used to Encrypt and Decrypt
        • AWS services that are integrated with KMS use Symmetric CMKs
        • You never get access to the KMS Key unencrypted (must call KMS API to use)

• Asymmetric (RSA & ECC key pairs)

        • Public (Encrypt) and Private Key (Decrypt) pair
        • Used for Encrypt/Decrypt, or Sign/Verify operations
        • The public key is downloadable, but you can’t access the Private Key unencrypted
        • Use case: encryption outside of AWS by users who can’t call the KMS API





------------------------------------------------------- Types of KMS Keys



• Customer Managed Keys

        • Create, manage and use, can enable or disable

        • Possibility of rotation policy (new key generated every year, old key preserved)

        • Can add a Key Policy (resource policy) & audit in CloudTrail

        • Leverage for envelope encryption


• AWS Managed Keys

        • Used by AWSservice (aws/s3,aws/ebs,aws/redshift)

        • Managed by AWS (automatically rotated every 1 year)

        • View Key Policy & audit in CloudTrail


• AWS Owned Keys

        • Created and managed by AWS, use by some AWS services to protect your resources

        • Used in multiple AWS accounts, but they are not in your AWS account

        • You can’t view, use, track, or audit






 KMS Key                         Customer Managed Key            AWS Managed Key                 AWS Owned Key


Can view metadata?                       Yes                          Yes                           No  

Can manage?                              Yes                          No                            No  

Used only for my AWS account?            Yes                         Yes                           No    

Automatic Rotation              90 – 2560 days (365 default)       Required (every 1 year)         Varies

On-demand Rotation                       Yes                            NO                         No









------------------------------------------------------- how do you create a KMS key? (KMS Key Material Origin)




• Identifies the source of the key material in the KMS key

• Can’t be changed after creation



• KMS (AWS_KMS) – default

        • AWS KMS creates and manages the key material in its own key store


• External (EXTERNAL)

        • You import the key material into the KMS key

        • You’re responsible for securing and managing this key material outside of AWS


• Custom Key Store (AWS_CLOUDHSM)

        • AWS KMS creates the key material in a custom key store (CloudHSM Cluster)






------------------------------------------------------- KMS Key Source – Custom Key Store (CloudHSM)



• Integrate KMS with CloudHSM cluster as a Custom Key Store

• Key materials are stored in a CloudHSM cluster that you own and manage

• The cryptographic operations are performed in the HSMs

• Use cases:

        • You need direct control over the HSMs

        • KMS keys needs to be stored in a dedicated HSMs






------------------------------------------------------- KMS Key Source - External



• Import your own key material into KMS key,Bring Your Own Key(BYOK)

• You’re responsible for key material’s security, availability, and durability outside of AWS

• Supports both Symmetric and Asymmetric KMS keys

• Can’t be used with Custom Key Store (CloudHSM)

• Manually rotate your KMS key (Automatic & On-demand Key Rotation are NOT supported)




------------------------------------------------------- KMS Multi-Region Keys



• A set of identical KMS keys in different AWS Regions that can be used interchangeably (~ same KMS key in multiple Regions)

• Encrypt in one Region and decrypt in other Regions (No need to re-encrypt or making cross-Region API calls)

• Multi-Region keys have the same key ID, key material, automatic rotation,

• KMS Multi-Region are NOT global (Primary + Replicas)

• Each Multi-Region key is managed independently

• Only one primary key at a time, can promote replicas into their own primary

• Use cases: Disaster Recovery, Global Data Management (e.g., DynamoDB Global Tables), Active-Active Applications that span multiple Regions, Distributed Signing applications, ...





------------------------------------------------------- SSM Parameter Store




• Secure storage for configuration and secrets

• Optional Seamless Encryption using KMS

• Serverless, scalable, durable, easy SDK

• Version tracking of configurations / secrets

• Security through IAM

• Notifications with Amazon EventBridge

• Integration with CloudFormation






---------------------------------------------------------- SSM Parameter Store Hierarchy


• /my-department/
    • my-app/
        • dev/
            • db-url
            • db-password

        • prod/
            • db-url
            • db-password

• other-app/



- You also have the opportunity to access Secrets of Secrets Manager through the Parameter Store by using this reference right here.

        • /aws/reference/secretsmanager/secret_ID_in_Secrets_Manager


- there are something called Public Parameters that are issued by AWS that you can use.

        • /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 (public)
           





---------------------------------------------------------- SSM Parameter Standard and advanced parameter tiers




Standard :

- Total number of parameters allowed (per AWS account and Region) :  10,000

- Maximum size of a parameter value                               :  4 KB

- Parameter policies available                                    :  No

- Cost                                                            :  No additional charge

- Storage Pricing                                                 :  Free






Advanced :

- Total number of parameters allowed (per AWS account and Region) :  100,000

- Maximum size of a parameter value                               :  8 KB

- Parameter policies available                                    : yes

- Cost                                                            :  charges Apply

- Storage Pricing                                                 :  $0.05 per advanced parameter per month






---------------------------------------------------------- Parameters Policies (for advanced parameters)



• Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as passwords

• Can assign multiple policies at a time




1 Expiration (to delete a parameter) : 

{
    "Type": "Expiration",
    "Version": "1.0",
    "Attributes": {
        "Timestamp": "2018-12-02T21:34:33.000Z"
    }
}





2 ExpirationNotification (EventBridge)



{
    "Type": "ExpirationNotification",
    "Version": "1.0",
    "Attributes": {
        "Before": "15",
        "Unit": "Days"
    }
}


-- So in this example, 15 days before the parameter expires we'll receive notification in EventBridge which gives us enough time to actually update it

-- and make sure the parameter is not getting deleted because of the TTL.





3 NoChangeNotification (EventBridge)


{
    "Type": "NoChangeNotification",
    "Version": "1.0",
    "Attributes": {
        "After": "20",
        "Unit": "Days"
    }
}


-- maybe sometimes you wanna make sure the parameters change once in a while.

-- So you can have a no change notification in EventBridge so that if a parameter has not been updated for 20 days, then you will be notified as well.








---------------------------------------------------------- SSM Parameter Store Hands ON CLI



-- open ssm in console --> choose parameter store on left side panel ---> give path to store value (/my-app/dev/db-url) ---> string --> value = dev.database.subbu.com:3306 --> create parameter

-- dev.database.subbu.com:3306 = u can give any value 

-- now do create dev password
  
-- give path to store value (/my-app/dev/db-password) ---> securestring --> value = give password here --> KMS Key ID = i am using my own key (eg: tutorial) -----> create parameter       

-- now do create for prod environment also same like as Dev

EG : /my-app/prod/db-url , /my-app/prod/db-password


-- So we are going to use this CLI to get the parameters.

-- open cloudshell

       aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password


-- for the password it's a SecureString, and here is the value of it, which is an encrypted value.

-- So for this, you basically need to decrypt it.

-- for this you have a special parameter and it's called with-decryption,

-- so this will check whether or not I have the KMS permission to decrypt this secret that was encrypted with the KMS tutorial key.

         aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password --with-decryption


-- now observe changes 


  aws ssm get-parameters-by-path --path /my-app/dev     =  u will get all parameters from specific path if u want 

   aws ssm get-parameters-by-path --path /my-app/ --recursive  --with-decryption     = u will get all parameters under /my-app/ 









---------------------------------------------------------- SSM Parameter Store Hands ON with LAMBDA 


-- create one function with py 3.8 runtime 




import json

import boto3

ssm = boto3.client('ssm', region_name="ap-south-1")

def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"])
    print(db_password)
    return "Worked!"



-- now go to configuration ---> permissons --->  create inline policy --> system manager ---> give all access --> all resources --> nxt 

-- now do refresht he lambda page 

-- if u get errror , after adding permissions , then wait for 5 min 

-- now u will get this

-- u can see 'SecureString' is encrypted here 

-- So what we'd like to do is now decrypt it,

-- so in code for password decrypt , add 

        db_password = ssm.get_parameters(Names=["/my-app/dev/db-password"], WithDecryption = True)


-- now do test , u will get (AccessDeniedException) error 

-- because we're not allowed to use the customer master key and decrypt our secrets.

-- So it turns out that because having given KMS access to my IAM role we're not allowed to decrypt the secrets,

-- so this is a good proof that even though I have access to this database password, because it's encrypted and I don't have access to KMS I'm not able to decrypt it,

-- and so that DB password is really safe and secure.

-- to fix this add permissions 

     permissons --> create inline policy --> kms --> add all permissions --> all resources--> create 


-- now do test , u will get decrypted values 


-- now to access through the Environment Variables 

-- create  Environment Variable --> DEV_OR_PROD	 = dev

-- add this in code 



import json

import boto3
import os 

ssm = boto3.client('ssm', region_name="ap-south-1")
dev_or_prod = os.environ['DEV_OR_PROD']


def lambda_handler(event, context):
    # TODO implement
    db_url = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-url"])
    print(db_url)
    db_password = ssm.get_parameters(Names=["/my-app/" + dev_or_prod + "/dev/db-password"], WithDecryption = True)
    print(db_password)
    return "Worked!"




-- if u test this u will get dev values 

--  go to env variable , change to prod (DEV_OR_PROD	 = prod )

--  do test again , u will prod values 

        








---------------------------------------------------------- AWS Secrets Manager




• Meant for storing secrets (e.g., passwords, API keys, ...)

• Capability to force rotation of secrets every X days

        • Automate generation of secrets on rotation (uses Lambda)

        • Natively supports Amazon RDS (all supported DB engines), Redshift, DocumentDB

        • Support other databases and services (custom Lambda function)


• Control access to secrets using Resource-based Policy

• Integration with other AWS services to natively pull secrets from Secrets Manager: CloudFormation, CodeBuild, ECS, EMR, Fargate, EKS, Parameter Store...






---------------------------------------------------------- Secrets Manager – with CloudFormation





Resources:
  MyRDSInstanceSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Description: 'Secret for RDS instance'
      GenerateSecretString:
        SecretStringTemplate: '{"username": "admin"}'
        GenerateStringKey: 'password'
        PasswordLength: 16
        ExcludeCharacters: '"@/\\'

MyDBInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      AllocatedStorage: 20
      DBInstanceClass: db.t2.micro
      Engine: mysql
      MasterUsername: !Join ['', ['{{resolve:secretsmanager:', !Ref MyRDSInstanceSecret, ':SecretString:username}}']]
      MasterUserPassword: !Join ['', ['{{resolve:secretsmanager:', !Ref MyRDSInstanceSecret, ':SecretString:password}}']]
      BackupRetentionPeriod: 0
      DBInstanceIdentifier: 'my-db-instance'

MySecretAttachment:
    Type: AWS::SecretsManager::SecretTargetAttachment
    Properties:
      SecretId: !Ref MyRDSInstanceSecret
      TargetId: !Ref MyDBInstance
      TargetType: AWS::RDS::DBInstance




1 we generate the secrets.

2 we're going to reference the secrets in your RDS DB instance.

3 we create what's called a "secret attachment", which is going to link the secrets to the RDS DB instance.






---------------------------------------------------------- SSM Parameter Store vs Secrets Manager






• Secrets Manager ($$$): 

       • Automatic rotation of secrets with AWS Lambda

       • Lambda function is provided for RDS, Redshift, DocumentDB

       • KMS encryption is mandatory

       • Can integration with CloudFormation


• SSM Parameter Store ($):

       • Simple API

       • No secret rotation (can enable rotation using Lambda triggered by EventBridge)

       • KMS encryption is optional

       • Can integration with CloudFormation

       • Can pull a Secrets Manager secret using the SSM Parameter Store API






---------------------------------------------------------- RDS - Security


• KMS encryption at rest for underlying EBS volumes / snapshots

• Transparent Data Encryption (TDE) for Oracle and SQL Server

• SSL encryption to RDS is possible for all DB (in-flight)

• IAM authentication for MySQL, PostgreSQL and MariaDB

• Authorization still happens within RDS (not in IAM)

• Can copy an un-encrypted RDS snapshot into an encrypted one

• CloudTrail cannot be used to track queries made within RDS








 ---------------------------------------------------- SSL/TLS - Basics




• An SSL Certificate allows traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption)

• SSL refers to Secure Sockets Layer, used to encrypt connections

• TLS refers to Transport Layer Security, which is a newer version

• Nowadays, TLS cer tificates are mainly used, but people still refer as SSL

• Public SSL certificates are issued by Certificate Authorities (CA)

• Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc...

• SSL certificates have an expiration date (you set) and must be renewed





---------------------------------------------------- SSL Encryption – How it works



• Asymmetric Encryption is expensive (SSL)

• Symmetric encryption is cheaper

• Asymmetric handshake is used to exchange a per- client random symmetric key

• Possibility of client sending an SSL certificate as well (two-way certificate)



Client side :

        1 Client sends hello, cipher suits & random

        3. Client verifies SSL certificate

        4. Master key (symmetric) generated and sent encrypted using the Public Key


Server Side:

        2. Server Response with server random & SSL certificate (Public Key)

        5. Server verifies Client SSL cert (optional)

        6. Master key is decrypted using Private Key

        7. Secure Symmetric Communication in Place (b/w server and client)









---------------------------------------------------- SSL – Server Name Indication (SNI)



• SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)

• It’s a “newer” protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake

• The server will then find the correct certificate, or return the default one


Note:

        • Only works for ALB & NLB (newer generation), CloudFront

        • Does not work for CLB (older gen)





---------------------------------------------------- SSL – Man in the Middle Attacks



          HTTP                                          HTTP

User <----------------->    Pirate Server        <----------------------> Good Server
                        (can intercept packets)




                                                                        HTTPS                              HTTPS

User(If infected, the user may trust the “pirate SSL certificate”) <-----------------> Pirate Server  <-----------------> Good Server

                                                           (Send fake SSL cert to User Decrypts and re-encrypts packets)






---------------------------------------------------- SSL – Man in the Middle Attack How to prevent



1.  Don’t use public-facing HTTP, use HTTPS (meaning, use SSL/TLS cer tificates)

2. Use a DNS that has DNSSEC

        • To send a client to a pirate server, a DNS response needs to be “forged” by a server which intercepts them

        • It is possible to protect your domain name by configuring DNSSEC

        • Amazon Route 53 supports DNSSEC for domain registration.

        • Route 53 supports DNSSEC for DNS service as of December 2020 (using KMS)

        • You could also run a custom DNS server on Amazon EC2 for example (Bind is the most popular, dnsmasq, KnotDNS, PowerDNS).





---------------------------------------------------- AWS Certificate Manager (ACM)





• Easily provision, manage, and deploy TLS Certificates

• Provide in-flight encryption for websites (HTTPS)

• Supports both public and private TLS certificates

• Free of charge for public TLS certificates

• Automatic TLS certificate renewal

• Integrations with (load TLS certificates on)

  • Elastic Load Balancers(CLB,ALB,NLB)
  • CloudFront Distributions 
  • APIs on API Gateway

• Cannot use ACM with EC2 (can’t be extracted)




• To host public SSL certificates in AWS, you can: 

        • Buy your own and upload them using the CLI

        • Have ACM provision and renew public SSL certificates for you (free of cost)


• ACM loads SSL certificates on the following integrations:

        • Load Balancers (including the ones created by EB(Elastic Beanstalk))

        • CloudFront distributions

        • APIs on API Gateways



• SSL certificates is overall a pain to manually manage, so ACM is great to leverage in your AWS infrastructure!



• Possibility of creating public certificates

        • Must verify public DNS

        • Must be issued by a trusted public certificate authority (CA)



• Possibility of creating private certificates

        • For your internal applications

        • You create your own private CA

        • Your applications must trust your private CA


• Certificate renewal:

        • Automatically done if generated provisioned by ACM

        • Any manually uploaded certificates must be renewed manually and re-uploaded


• ACM is a regional service

        • To use with a global application (multiple ALB for example), you need to issue an SSL certificate in each region where you application is deployed.

        • You cannot copy certs across regions







---------------------------------------------------------- CloudHSM


• KMS => AWS manages the software for encryption , and will have control over the encryption keys.

• CloudHSM => AWS provisions encryption hardware , AWS will provision some encryption hardware.

              - It's called an HSM device, so a dedicated hardware which is a hardware security module.

• Dedicated Hardware (HSM = Hardware Security Module)

• You manage your own encryption keys entirely (not AWS)


- The HSM device is going to be set up within the cloud of AWS,

• HSM device is tamper resistant, FIPS 140-2 Level 3 compliance

• Supports both symmetric and asymmetric encryption (SSL/TLS keys)

• No free tier available

• Must use the CloudHSM Client Software

• Redshift supports CloudHSM for database encryption and key management

• Good option to use with SSE-C encryption


- IAM permissions:

       • CRUD an HSM Cluster

- CloudHSM Software:

       • Manage the Keys
       • Manage the Users






---------------------------------------------------------- CloudHSM High Availability


• CloudHSM clusters are spread across Multi AZ (HA)

• Great for availability and durability
 



---------------------------------------------------------- CloudHSM – Integration with AWS Services


• Through integration with AWS KMS

• Configure KMS Custom Key Store with CloudHSM

• Example: EBS, S3, RDS ...





---------------------------------------------------------- S3 Encryption for Objects


• SSE-S3: encrypts S3 objects using keys handled & managed by AWS

• SSE-KMS: leverage KMS to manage encryption keys

        • Key usage appears in CloudTrail

        • objects made public can never be read

        • On s3:PutObject, make the permission kms:GenerateDataKey is allowed


• SSE-C: when you want to manage your own encryption keys

• Client-Side Encryption


• Glacier: all data is AES-256 encrypted, key under AWS control




---------------------------------------------------------- Encryption in transit (SSL / TLS)






-- Encryption in flight is also called SSL/TLS

-- Amazon S3 exposes two endpoints:
  • HTTP Endpoint – non encrypted
  • HTTPS Endpoint – encryption in flight

-- HTTPS is recommended

-- HTTPS is mandatory for SSE-C

-- Most clients would use the HTTPS endpoint by default

Encrypting Data-at-Rest and Data-in-Transit

To protect data in transit, AWS encourages customers to leverage a multi-level approach. All network traffic between AWS data centers is transparently encrypted at the physical layer. All traffic within a VPC and between peered VPCs across regions is transparently encrypted at the network layer when using supported Amazon EC2 instance types. At the application layer, customers have a choice about whether and how to use encryption using a protocol like Transport Layer Security (TLS). All AWS service endpoints support TLS to create a secure HTTPS connection to make API requests.



• To enforce HTTPS, use a Bucket Policy with aws:SecureTransport



------------------------------------------------------ Amazon S3 – Force Encryption in Transit aws:SecureTransport




{
  "Version": "2012-10-17",
  "Id": "ExamplePolicy01",
  "Statement": [
    {
      "Sid": "VisualEditior0",
      "Effect": "Allow",
      "Principal": {
        "AWS": "*"
      },
      "Action": [
"elasticfilesystem:ClientRootAccess",
"elasticfilesystem:ClientMount",
"elasticfilesystem:ClientWrite"
      ],
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "true"
        }
      }
    }
  ]
}



-- So SecureTransport is going to be true whenever using HTTPS and false whenever you're not using an encryption,

--  so, therefore, any user trying to use HTTP on your bucket is going to be blocked, but users using HTTPS may be allowed.





------------------------------------------------------ Events in S3 Buckets



• S3 Access Logs:

        • Detailed records for the requests that are made to a bucket

        • Might take hours to deliver

        • Might be incomplete (best effort)


• S3 Events Notifications:

        • Receive notifications when certain events happen in your bucket

        • E.g.:new objects created,object removal,restore objects,replication events

        • Destinations : SNS,SQSqueue,Lambda

        • Typically delivered in seconds but can take minutes,notification for every object if versioning is enabled, else risk of one notification for two same object write done simultaneously


• Trusted Advisor:

        • Check the bucket permission (is the bucket public?)


• Amazon EventBridge:

        • Need to enable CloudTrail object level logging on S3 first

        • Target can be Lambda,SQS,SNS,etc...





------------------------------------------------------ S3 Security


• User based

        • IAM policies - which API calls should be allowed for a specific user from IAM console


• Resource Based

        • Bucket Policies - bucket wide rules from the S3 console - allows cross account

        • Object Access Control List (ACL) – finer grain

        • Bucket Access Control List (ACL) – less common





------------------------------------------------------ S3 Bucket Policies



• Use S3 bucket for policy to:

        • Grant public access to the bucket

        • Force objects to be encrypted at upload

        • Grant access to another account (Cross Account)


• Optional Conditions on:

        • SourceIp: Public IP or Elastic IP | VpcSourceIp: Private IP (through VPC Endpoint)

        • Source VPC or Source VPC Endpoint – only works with VPC Endpoints

        • CloudFront Origin Identity

        • MFA


• Examples here: https://docs.aws.amazon.com/AmazonS3/latest/dev/example- bucket-policies.html





------------------------------------------------------ S3 pre-signed URLs



• Can generate pre-signed URLs using SDK or CLI

        • For downloads (easy, can use the CLI)

        • For uploads (harder, must use the SDK)


• Valid for a default of 3600 seconds, can change timeout with --expires-in [TIME_BY_SECONDS] argument

• Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT




• Examples :

        • Allow only logged-in users to download a premium video on your S3 bucket

        • Allow an ever changing list of users to download files by generating URLs dynamically

        • Allow temporarily a user to upload a file to a precise location in our bucket





------------------------------------------------------ S3 Object Lock & Glacier Vault Lock


• S3 Object Lock

        • Adopt a WORM (Write Once Read Many) model

        • Block an object version deletion for a specified amount of time


• Glacier Vault Lock

        • Adopt a WORM (Write Once Read Many) model

        • Lock the policy for future edits (can no longer be changed)

        • Helpful for compliance and data retention







------------------------------------------------------ S3 – Access Points



• Access Points simplify security management for S3 Buckets

• Each Access Point has:

        • its own DNS name (Internet Origin or VPC Origin)

        • an access point policy (similar to bucket policy) – manage security at scale







------------------------------------------------------ S3 – Access Points - VPC Origin



• We can define the access point to be accessible only from within the VPC

• You must create a VPC Endpoint to access the Access Point (Gateway or Interface Endpoint)

• The VPC Endpoint Policy must allow access to the target bucket and Access Point




- The following example policy statement configures a VPC endpoint to allow calls to GetObject for a bucket named awsexamplebucket1 and an access point named example-vpc-ap.



{
    "Version": "2012-10-17",
    "Statement": [
    {
        "Principal": "*",
        "Action": [
            "s3:GetObject"
        ],
        "Effect": "Allow",
        "Resource": [
            "arn:aws:s3:::awsexamplebucket1/*",
            "arn:aws:s3:us-west-2:123456789012:accesspoint/example-vpc-ap/object/*"
        ]
    }]
}



 

 ------------------------------------------------------ S3 – Access Points Hands ON check in course 






------------------------------------------------------ S3 – Multi-Region Access Points


• Provide a global endpoint that span S3 buckets in multiple AWS regions

• Dynamically route requests to the nearest S3 bucket (lowest latency)

• Bi-directional S3 bucket replication rules are created to keep data in sync across regions

• Failover Controls – allows you to shift requests across S3 buckets in different AWS regions within minutes (Active-Active or Active- Passive)



------------------------------------------------------ S3 – Multi-Region Access PointsHands ON check in course 





------------------------------------------------------ S3 Object Lambda



• Use AWS Lambda Functions to change the object before it is retrieved by the caller application

        - instead of, for example duplicating our buckets to have different versions of each object, we can use S3 Object Lambda instead.

• Only one S3 bucket is needed, on top of which we create S3 Access Point and S3 Object Lambda Access Points.

• Use Cases:

        • Redacting personally identifiable information for analytics or non- production environments.

        • Convertingacrossdataformats,such as converting XML to JSON.

        • Resizing and watermarking images on the fly using caller-specific details, such as the user who requested the object.







------------------------------------------------------ What’s a DDOS* Attack?


*Distributed Denial-of-Service



------------------------------------ Type of Attacks on your infrastructure



• Distributed Denial of Service (DDoS):

        • When your service is unavailable because it’s receiving too many requests

        • SYN Flood (Layer 4): send too many TCP connection requests

        • UDP Reflection (Layer 4): get other servers to send many big UDP requests

        • DNS flood attack: overwhelm the DNS so legitimate users can’t find the site

        • Slow Loris attack: a lot of HTTP connections are opened and maintained


• Application level attacks:

        • more complex, more specific (HTTP level)

        • Cache bursting strategies: overload the backend database by invalidating cache






------------------------------------------------------- DDoS Protection on AWS



• AWS Shield Standard: protects against DDoS attack for your website and applications, for all customers at no additional costs

• AWS Shield Advanced: 24/7 premium DDoS protection

• AWS WAF: Filter specific requests based on rules

• CloudFront and Route 53:

        • Availability protection using global edge network

        • Combined with AWS Shield, provides attack mitigation at the edge


• Be ready to scale – leverage AWS Auto Scaling

• Separate static resources (S3 / CloudFront) from dynamic ones (EC2 / ALB)


• Read the whitepaper for details: https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf


Sample Reference Architecture  : https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/




------------------------------------------------------- AWS Shield




• DDoS: Distributed Denial of Service – many requests at the same time

1  AWS Shield Standard:

  • Free service that is activated for every AWS customer
  • Provides protection from attacks such as SYN("synchronize")/UDP Floods, Reflection attacks and other layer 3/layer 4 attacks


2  AWS Shield Advanced:

  • Optional DDoS mitigation service ($3,000 per month per organization)

  • Protect against more sophisticated attack on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53

  • 24/7 access to AWS DDoS response team (DRP)
 
  • Protect against higher fees during usage spikes due to DDoS 

  • Shield Advanced automatic application layer DDoS mitigation automatically creates, evaluates and deploys AWS WAF rules to mitigate layer 7 attacks







------------------------------------------------------- AWS WAF – Web Application Firewall




-- AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources.

-- AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.


EPV : If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more "IP match conditions." An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.



• Protects your web applications from common web exploits (Layer 7)

• Layer 7 is HTTP (vs Layer 4 is TCP/UDP)

• Deploy on

 • Application Load Balancer
 • API Gateway
 • CloudFront
 • AppSync GraphQL API
 • Cognito User Pool 

• WAF is not for DDoS protection 

• Define Web ACL (Web Access Control List) Rules:

        • IP Set: up to 10,000 IP addresses – use multiple Rules for more IPs
        • HTTP headers, HTTP body, or URI strings Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
        • Size constraints, geo-match (block countries)
        • Rate-based rules (to count occurrences of events) – for DDoS protection

• Web ACL are Regional except for CloudFront

• A rule group is a reusable set of rules that you can add to a web ACL

• Rule Actions: Count | Allow | Block | CAPTCHA






---------------- WAF – Fixed IP while using WAF with a Load Balancer



• WAF does not support the Network Load Balancer (Layer 4)

• We can use Global Accelerator for fixed IP and WAF on the ALB


Users <---------> Global Accelerator (fixed IP) <-----------> ALB <----------------> EC2 instances 
                                                               |
                                                               |   (attached)
                                                               |
                                                              AWS WAF  , it has WebACL

                                       IMP NOTE : WebACL must be in the same AWS Region as ALB


EPV : 

AWS WAF - How it Works?:

-- To block specific countries, you can create a AWS WAF geo match statement listing the countries that you want to block, 

-- to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. 



EPV : Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.

ANS : Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule






------------------------------------------------------- AWS WAF – Managed Rules



• Library of over 190 managed rules

• Ready-to-use rules that are managed by AWS and AWS Marketplace Sellers

• Baseline Rule Groups – general protection from common threats

        • AWSManagedRulesCommonRuleSet,AWSManagedRulesAdminProtectionRuleSet,...

• Use-case Specific Rule Groups – protection for many AWS WAF use cases

        • AWSManagedRulesSQLiRuleSet,AWSManagedRulesWindowsRuleSet,

        AWSManagedRulesPHPRuleSet, AWSManagedRulesWordPressRuleSet, ...


• IP Reputation Rule Groups – block requests based on source (e.g., malicious IPs)

        • AWSManagedRulesAmazonIpReputationList,AWSManagedRulesAnonymousIpList


• Bot Control Managed Rule Group – block and manage requests from bots

        • AWSManagedRulesBotControlRuleSet





------------------------------------------------------- WAF - Web ACL – Logging



• You can send your logs to an:

        • Amazon CloudWatch Logs log group – 5 MB per second

        • Amazon Simple Storage Service (Amazon S3) bucket – 5 minutes interval

        • Amazon Kinesis Data Firehose – limited by Firehose quotas






------------------------------------------------------- AWS Firewall Manager





• Manage rules in all accounts of an AWS Organization

• Security policy: common set of security rules
 
• WAF rules (Application Load Balancer, API Gateways, CloudFront)
• AWS Shield Advanced (ALB, CLB, NLB, Elastic IP, CloudFront)
• Security Groups for EC2, Application Load Balancer and ENI resources in VPC 
• AWS Network Firewall (VPC Level)
• Amazon Route 53 Resolver DNS Firewall
• Policies are created at the region level

• Rules are applied to new resources as they are created (good for compliance) across all and future accounts in your Organization





------------------------------------------------------- WAF vs. Firewall Manager vs. Shield



• WAF, Shield and Firewall Manager are used together for comprehensive protection

• Define your Web ACL rules in WAF

• For granular protection of your resources, WAF alone is the correct choice

• If you want to use AWS WAF across accounts, accelerate WAF configuration, automate the protection of new resources, use Firewall Manager with AWS WAF

• Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the Shield Response Team (SRT) and advanced reporting.

• If you’re prone to frequent DDoS attacks, consider purchasing Shield Advanced





------------------------------------------------------- Blocking an IP address


- So sometimes we may want to block an IP address from a client because it's going to be a bad actor. Maybe it's trying to access our application. And so, we want to know the line of defenses.

- we have an EC2 instance in a security group, in a VPC. And that instance has a public IP, so it's publicly accessible. this how our clients get into our EC2 instance.

- So say we wanted to block that client.

        - The first line of defense would be the network ACL in our VPC, which is at the VPC level.

        - in this network ACL, we can create a denial rule for this client IP address. Very simple, very quick, very cheap,and the client will just be ejected.

        - Then for the security group of the EC2 instance, we cannot have deny rules, we can only have allow rules.

        - So if we know that only a subset of authorized client can access our EC2 instance, then it is good in our security group to just define a subset of IP to allow into our EC2 instance.

        - But if our application is global, we obviously don't know all the IP addresses that will access our application and so the security group here will not be very helpful.

        - Finally, you could run an optional firewall software on your EC2 to block from within your software the request from the client.

        - Now obviously, if the requests already reached your EC2 instance, then it will have to be processed and there will be a CPU cost to processing that request.





------------------------------------------------------- Blocking an IP address – with an ALB



- We introduce an application load balancer. So again, this ALB is defined within our VPC and we still have our EC2 instance, but now we have two security groups.

- We have the ALB security group and we have the EC2 security group.

- so in this case, our load balancer in this architecture is going to be in between our clients and our EC2 and it will do something called connection termination.

- So the client actually connects to the ALB which will terminate the connection and initiate a new connection from the ALB into our EC2 instance.

- In this case, our EC2 security group must be configured to allow the security group of the ALB because the EC2 instance can be deployed in a private subnet with a private IP, and the source of the traffic it sees comes from the ALB, not the client.

- So from the security group perspective here we only allow the ALB security group and we're safe on this side.

- Now, for the security group of the ALB, we need to allow the clients.

- And again, if we have the range of IP we know, then we can configure the secret to group. If it's a global application we have to allow everything.

- And then, our line of defense is going to be at the network ACL level.





------------------------------------------------------- Blocking an IP address – with an NLB



same like ALB 




------------------------------------------------------- Blocking an IP address – ALB + WAF



- Something we can do to deny an IP is to install WAF or web application firewall.

- Now this WAF is going to be a little bit more expensive because this is an additional service and a firewall service.

- But in here we are able to do some complex filtering on IP addresses and we can establish rules that will count for the request to prevent a lot of requests going at the same time from the client and so we have more power over our security of our ALB.

- so, WAF is not a service in between your client and your ALB, it's a service we have installed on the ALB and we can define a bunch of rules so this is one more line of defense.





------------------------------------------------------- Blocking an IP address – ALB, CloudFront WAF



- Similarly, if we use CloudFront in front of the ALB, CloudFront front sits outside our VPC, okay?

- So as such our ALB needs to allow all the CloudFront public IPs coming from the edge locations and there's a list of it online, but that's it.

- So coming from the ALB, it does not see the client IP. What it sees is the CloudFront public IP.

- so as such, the network is ACL here,which sits at the boundary of our VPC, is not helpful at all because it cannot help us block the client's IP address.

- so in this case, if we are trying to block a client from CloudFront we have two possibilities.

        1 Say we're attacked by a country, then we can use the CloudFront Geo Restriction feature to restrict all the country from a client to be denied on CloudFront.

        2 Or if there's one specific IP that annoys us we can again use WAF or web application firewall to do some IP address filtering just like we did before.






------------------------------------------------------- 





• Automated Security Assessments

it onlt evaluates 

1 For EC2 instances

  • Leveraging the AWS System Manager (SSM) agent
  • Analyze against unintended network accessibility
  • Analyze the running OS against known vulnerabilities


2 For Container Images push to Amazon ECR

  • Assessment of Container Images as they are pushed

3  For Lambda Functions

  • Identifies software vulnerabilities in function code and package dependencies

  • Assessment of functions as they are deployed



• Reporting & integration with AWS Security Hub 

• Send findings to Amazon Event Bridge






------------------------------ What does Amazon Inspector evaluate?


• Remember : only for EC2 instances, Container Images & Lambda functions

• Continuous scanning of the infrastructure, only when needed

• Package vulnerabilities (EC2, ECR & Lambda) – database of CVE

• Network reachability (EC2)

• A risk score is associated with all vulnerabilities for prioritization




 
 ------------------------------------------------------- Amazon Inspector Hands ON 




 -- open in console --> activate Inspector 

 -- now create ec2 instance linux 2023 

 -- as we launch this instance it is going to be scanned by the Inspector service.

 -- check in Environment coverage in Inspector console , u can see 0/1 instance 

 -- go to Account management --> instances --> u can see that not scanning instance list coz Unmanaged EC2 instance --> 

  -- click on Unmanaged EC2 instance , follow these instructions --> give instanceid --> give role blank --> execute   ( if you get error then go for nxt step )

 -- go to ssm --> quicksetup --> Host Management --> Targets = current region --> all instances --> create 



 -- now scanning happening , you can see all vulnerabilities if any happens for lambda and ecr and ec2 

 -- go to general setting in console --> deactivates the inspector 







------------------------------------------------------- AWS Config



• Helps with auditing and recording compliance of your AWS resources

• Helps record configurations and changes over time

• Questions that can be solved by AWS Config:

        • Is there unrestricted SSH access to my security groups?

        • Do my buckets have any public access?

        • How has my ALB configuration changed over time?


• You can receive alerts (SNS notifications) for any changes

• AWS Config is a per-region service

• Can be aggregated across regions and accounts

• Possibility of storing the configuration data into S3 (analyzed by Athena)







------------------------------------------------------- Config Rules



• Can use AWS managed config rules (over 75)

• Can make custom config rules (must be defined in AWS Lambda)

        • Ex: evaluate if each EBS disk is of type gp2

        • Ex: evaluate if each EC2 instance is t2.micro

• Rules can be evaluated / triggered:

        • For each config change

        • And / or: at regular time intervals


• AWS Config Rules does not prevent actions from happening (no deny)

• Trigger Amazon EventBridge if the rule is non-compliant (chain with Lambda)

• Rules can have auto remediations through SSM Automations

        • If a resource is not compliant, you can trigger an auto remediation

        • Ex: remediate security group rules, stop instances with non-approved tags


• Pricing: no free tier, $0.003 per configuration item recorded per region, $0.001 per config rule evaluation per region






------------------------------------------------------- Automatically re-enable AWS CloudTrail by using a custom remediation rule in AWS Config




https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html










------------------------------------------------------- Config Rules – Remediations




-- Now, although you cannot deny any action from happening from within the config, you can do remediations of your non-compliant resources using an SSM Automation Documents.

-- So the idea is, for example, you are monitoring whether or not your IAM access keys have expired.

        - For example, they are older than 90 days. In which case you want to mark them as non-compliant.

        - So this will not prevent them from not being compliant, but you can trigger whenever a resource is not compliant, a remediation action.


• Use AWS-Managed Automation Documents or create custom Automation Documents

        • Tip: you can create custom Automation Documents that invokes Lambda function


• You can set Remediation Retries if the resource is still non-compliant after auto-remediation






------------------------------------------------------- Config Rules – Notifications



• Use EventBridge to trigger notifications when AWS resources are non- compliant

• Ability to send configuration changes and compliance state notifications to SNS (all events – use SNS Filtering or filter at client-side)







------------------------------------------------------- Config  hands On

-- Customize AWS Config to record configuration changes for all supported resource types, or for only the supported resource types that are relevant to you. Globally recorded resources (RDS global clusters and IAM users, groups, roles, and customer managed policies) may be recorded in more than this Region. You are charged based on the number of configuration items recorded. 

-- open AWS config in AWS 

-- this is not free 

-- Recording method = All resource types with customizable overrides

-- choose role created by aws ( u no need to create aws role , aws will create for you)

-- and also bucket will create for you 

-- in rules search for the SSH and select this ssh this will check Checks whether security groups that are in use disallow unrestricted incoming SSH traffic.

-- create config , now config going to monitor the entire AWS environment 

--  changes that made in AWS will send to s3 

-- wait for 1-3 min to get details 

-- u will see the noncomplianct resources on dashboard

-- for me there are 4 noncompliance records are there so , open resources(manage resource) and delete ssh connection in Security groups 

-- now wait 1-2 min and check , now i do not have any noncompliance resources 

-- u can also check resources in left side , u can check time line of the resource when n how the changes happen 

-- it is best for auditing 



---------------------------------- IMP : once u delete rules in console , it won’t delete and u will get charge , so to delete this 

-- create one instance and connect instance install aws cli and login with ur keys 

-- follow these steps 

1 Turn off Recording for that region using the console

2 Delete the Rule by going to actions, delete rule

3 Use the AWS CLI and delete the default recording by

aws configservice delete-configuration-recorder --configuration-recorder-name default --region <region-name>

4 Delete the service linked role created for AWS Config  ( search for AWSConfigService role in roles)


-- Refresh the Config home page to make it appear fresh.

-- If necessary delete the config bucket and its objects.








------------------------------------------------------- AWS Config – Aggregators



-- Well, say you are managing multiple accounts and multiple regions within these accounts. So we have Account A, Account B, and they're called "source accounts", why?

        - Because they all have a deployment of AWS Config.

        - But you want to aggregate all this information into a central AWS account. And this account is called an aggregator account, and this is only in the aggregator account that you're going to create an aggregator


• The aggregator is created in one central aggregator account , they're not created in each individual source accounts.

• Aggregates rules, resources, etc... across multiple accounts & regions

• If using AWS Organizations, no need for individual Authorization , You just create your aggregator in your management account in Organization, and the authorizations will happen automatically.

        -- But if you're not using AWS Organizations then you would need to create an authorization in an account A to say "Hey, I authorize the AWS accounts aggregator "to collect data,"

        -- and then you would create a second authorization, obviously in Account B, to do the exact same thing.

        -- Once these authorizations are in place, and this is only, again, if you're not using AWS Organizations, then the aggregator will be able to collect data, to pull data from these targeted accounts and aggregate them.



• Rules are created in each individual source AWS account

• Can deploy rules to multiple target accounts using CloudFormation StackSets






------------------------------------------------------- CloudWatch vs CloudTrail vs Config





• CloudWatch 

  • Performance monitoring (metrics, CPU, network, etc...) & dashboards
  • Events & Alerting
  • Log Aggregation & Analysis

• CloudTrail
  • Record API calls made within your Account by everyone 
  • Can define trails for specific resources
  • Global Service

• Config

 • Record configuration changes
 • Evaluate resources against compliance rules
 • Get timeline of changes and compliance





------------------------------------ For an Elastic Load Balancer


• CloudWatch:

        • Monitoring Incoming connections metric

        • Visualize error codes as a % over time

        • Make a dashboard to get an idea of your load balancer performance


• Config:

        • Track security group rules for the Load Balancer

        • Track configuration changes for the Load Balancer

        • Ensure an SSL certificate is always assigned to the Load Balancer (compliance)


• CloudTrail:

        • Track who made any changes to the Load Balancer with API calls








------------------------------------------------------- AWS Managed Logs



• Load Balancer Access Logs (ALB, NLB, CLB) => to S3

        • Access logs for your Load Balancers

• CloudTrail Logs => to S3 and CloudWatch Logs

        • Logs for API calls made within your account

• VPC Flow Logs => to S3, CloudWatch Logs, Kinesis Data Firehose

        • Information about IP traffic going to and from network interfaces in yourVPC

• Route 53 Access Logs => to CloudWatch Logs

        • Log information about the queries that Route 53 receives

• S3 Access Logs => to S3

        • Server access logging provides detailed records for the requests that are made to a bucket


• CloudFront Access Logs => to S3

        • Detailed information about every user request that CloudFront receives

• AWS Config => to S3        








------------------------------------------------------- Amazon GuardDuty




• Intelligent Threat discovery to protect your AWS Account , (malicious activities)

• Uses Machine Learning algorithms, anomaly detection, 3rd party data

• One click to enable (30 days trial), no need to install software

• Input data includes:

  • CloudTrail Events Logs – unusual API calls, unauthorized deployments

     • CloudTrail Management Events–createVPCsubnet,createtrail,... 
     • CloudTrail S3 Data Events–getobject,listobjects,deleteobject,...

  • VPC Flow Logs – unusual internal traffic, unusual IP address

  • DNS Logs – compromised EC2 instances sending encoded data within DNS queries 

  • Optional Features – EKS Audit Logs, RDS & Aurora, EBS, Lambda, S3 Data Events...

• Can setup EventBridge rules to be notified in case of findings

• EventBridge rules can target AWS Lambda or SNS

• Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)



VPC Flow Logs / CloudTrail Logs / DNS Logs (AWS DNS) 

optional features = / S3 Logs / EBS Volumes / RDS & Aurora Login Activity / Lambda Network Activity / EKS Audit Logs & Runtime Monitoring ------ > GuardDuty ---> EventBridge --> SNS / Lambda



IMP :  GuardDuty cannot block any malicious attempts to access the APIs illegally. Rather, it can only monitor/detect such attempts.




------------------------------------------------------- GuardDuty – Delegated Administrator


• AWS Organization member accounts can be designated to be a GuardDuty Delegated Administrator

• Have full permissions to enable and manage GuardDuty for all accounts in the Organization

• Can be done only using the Organization Management Account






------------------------------------------------------- IAM Advanced Policies 



1 aws:SourceIp

        - restrict the client IP from which the API calls are being made



{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Action": "execute-api:Invoke",
            "Resource": "*",
            "Condition": {
                "NotIpAddress": {
                    "aws:SourceIp": ["203.0.113.0/24","204.0.113.0/24"]
                }
            }
        }
    ]
}



- So the first one is aws:SourceIP. And this is used to restrict the client IP from which the API calls are being made.

- So if we look at this one right here, it has a Deny star on everything if it's not an IP address.

- then we have a list of two CIDRs, of two IP address ranges. So that means that unless the client makes an API call from within these IP addresses, then the API call is being denied.

- this can be used, for example, to restrict usage on AWS only to, for example,your company network and, therefore, guaranteeing that only your company can access your own AWS environment.




2 aws:RequestedRegion

        - restrict the region the API calls are made to





{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Action": "execute-api:Invoke",
            "Resource": "arn:aws:execute-api:region:account-id:api-id/stage/METHOD/resource-path",
            "Condition": {
                "StringEquals": {
                    "aws:RequestedRegion": "us-west-2"
                }
            }
        }
    ]
}



- So in this one we deny anything if we are in the region  "us-west-2"

- this can be applied more globally on their organization SCP to deny or to allow only access to a specific region.





3 ec2:ResourceTag


        - restrict based on tags



{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "ec2:StartInstances",
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "ec2:ResourceTag/Environment": "Production"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": "ec2:StopInstances",
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "ec2:ResourceTag/Environment": "Production"
                }
            }
        }
    ]
}







------------------------------------------------------- Resource Policies & aws:PrincipalOrgID


• aws:PrincipalOrgID can be used in any resource policies to restrict access to accounts that are member of an AWS Organization


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::example-bucket/*",
            "Condition": {
                "StringEquals": {
                    "aws:PrincipalOrgID": "o-exampleorgid"
                }
            }
        }
    ]
}




------------------------------------------------------- EC2 Instance Connect (SendSSHPublicKey API)

check in course 




------------------------------------------------------- AWS Security Hub



• Central security tool to manage security across several AWS accounts and automate security checks

• Integrated dashboards showing current security and compliance status to quickly take actions

• Automatically aggregates alerts in predefined or personal findings formats from various AWS services & AWS partner tools:

        • Config
        • GuardDuty
        • Inspector
        • Macie
        • IAMAccessAnalyzer
        • AWS Systems Manager
        • AWS Firewall Manager
        • AWS Health
        • AWS Partner Network Solutions

• Must first enable the AWS Config Service





------------------------------------------------------- Amazon Detective



• GuardDuty, Macie, and Security Hub are used to identify potential security issues, or findings

• Sometimes security findings require deeper analysis to isolate the root cause and take action – it’s a complex process

• Amazon Detective analyzes, investigates, and quickly identifies the root cause of security issues or suspicious activities (using ML and graphs)

• Automatically collects and processes events from VPC Flow Logs, CloudTrail, GuardDuty and create a unified view

• Produces visualizations with details and context to get to the root cause





======================================================== Compute & Load Balancing =================================================================





------------------------------------------------------- EC2 InstanceTypes – Main ones


• R: applications that needs a lot of RAM – in-memory caches

• C: applications that needs good CPU – compute / databases

• M: applications that are balanced (think “medium”) – general / web app

• I: applications that need good local I/O (instance storage) – databases

• G: applications that need a GPU – video rendering / machine learning


• T2 / T3: burstable instances (up to a capacity)

• T2 / T3 - unlimited: unlimited burst

• Real-world tip: use https://www.ec2instances.info







----------------------------------------------- Placement Groups



• Sometimes you want control over the EC2 Instance placement strategy

• That strategy can be defined using placement groups

     - So we don't get direct interaction with the hardware of AWS, but we let AWS know how we would like our EC2 instance to be placed compared to one another.


• When you create a placement group, you specify one of the following strategies for the group:

    • Cluster — clusters instances into a low-latency group in a single Availability Zone (This is going to give you high performance but high risk.)

    • Spread — spreads instances across underlying hardware (max 7 instances per group per AZ) – critical applications

    • Partition—spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)



• You can move an instance into or out of a placement group

        • Your first need to stop it

        • You then need to use the CLI (modify-instance-placement)

        • You can then start your instance







----------------------------------------------- Placement Groups Cluster(Same AZ)



• Pros: Great network (10 Gbps bandwidth between instances with Enhanced Networking enabled - recommended)

• Cons: If the AZ fails, all instances fails at the same time

• Use case:

     • Big Data job that needs to complete fast

     • Application that needs extremely low latency and high network throughput





----------------------------------------------- Placement Groups Spread



-- so in this case, when we ask for spread placement group, all the EC2 instances are going to be located on different hardware.

• Pros:

        • Can span across Availability Zones (AZ)

        • Reduced risk is simultaneous failure

        • EC2 Instances are on different physical hardware


• Cons:

        • Limited to 7 instances per AZ per placement group


• Use case:
        • Application that needs to maximize high availability

        • Critical Applications where each instance must be isolated from failure from each other








----------------------------------------------- Placements Groups Partition




-- for the partition placement group, we can have instances spread across partitions in multiple availability-zones.

-- So why do we use a partition placement group?

        - Well, each partition represents a rack in AWS. And so by having many partitions, you're making sure that your instances are distributed across many hardware racks, 
        
        - and so therefore, they're safe from a rack failure from one another.

• Up to 7 partitions per AZ

• Can span across multiple AZs in the same region

• Up to 100s of EC2 instances

• The instances in a partition do not share racks with the instances in the other partitions

• A partition failure can affect many EC2 but won’t affect other partitions

• EC2 instances get access to the partition information as metadata

• Use cases: HDFS, HBase, Cassandra, Kafka





----------------------------------------------- EC2 Instance LaunchTypes




• On Demand Instances: short workload, predictable pricing, reliable

• Spot Instances: short workloads, for cheap, can lose instances (not reliable)

• Reserved: (MINIMUM 1 year)

        • Reserved Instances: long workloads

        • Convertible Reserved Instances: long workloads with flexible instances

        • Highest to lowest discount: All Upfront payment, Partial Upfront payment, no Upfront


• Dedicated Instances: no other customers will share your hardware

• Dedicated Hosts: book an entire physical server, control instance placement

        • Great for software licenses that operate at the core, or CPU socket level

        • Can define host affinity so that instance reboots are kept on the same host







----------------------------------------------- EC2 Graviton


- AWS Graviton Processors deliver the best price performance

- Suppor ts many Linux OS, Amazon Linux 2, RedHat, SUSE, Ubuntu

- Not available for Windows instances

- Graviton2 – 40% better price performance over comparable 5th generation x86-based instances

- Graviton3 – Up to 3x better performance compared to Graviton2

- Use cases: app servers, microservices, HPC, CPU-based ML, video encoding, gaming, in-memory caches, ...





----------------------------------------------- EC2 included metrics


• CPU: CPU Utilization + Credit Usage / Balance

• Network: Network In / Out

• Status Check:

        • Instance status = check the EC2 VM

        • System status = check the underlying hardware


• Disk: Read / Write for Ops / Bytes (only for instance store)


• RAM is NOT included in the AWS EC2 metrics




----------------------------------------------- EC2 Instance Recovery



• Status Check:

     • Instance status = check the EC2 VM

     • System status = check the underlying hardware


• Recovery: Same Private, Public, Elastic IP, metadata, placement group




----------------------------------------------- High Performance Computing (HPC)




• The cloud is the perfect place to perform HPC

• You can create a very high number of resources in no time

• You can speed up time to results by adding more resources

• You can pay only for the systems you have used

• Perform genomics, computational chemistry, financial risk modeling, weather prediction, machine learning, deep learning, autonomous driving


• Which services help perform HPC?


-------------------- Data Management & Transfer

• AWS Direct Connect:
  • Move GB/s of data to the cloud, over a private secure network

• Snowball & Snowmobile
  • Move PB of data to the cloud

• AWS DataSync
  • Move large amount of data between on-premises and S3, EFS, FSx for Windows




------------------ Compute and Networking

• EC2 Instances:
   • CPU optimized, GPU optimized
   • Spot Instances / Spot Fleets for cost savings + Auto Scaling

• EC2 Placement Groups: Cluster for good network performance(same rack and same AZ)


• EC2 Enhanced Networking (SR-IOV)
   • Higher bandwidth, higher PPS (packet per second), lower latency
   • Option 1: Elastic Network Adapter (ENA) up to 100 Gbps
   • Option 2: Intel 82599 VF up to 10 Gbps – LEGACY

• Elastic Fabric Adapter (EFA)
   • Improved ENA for HPC, only works for Linux
   • Great for inter-node communications, tightly coupled workloads
   • Leverages Message Passing Interface (MPI) standard
   • Bypasses the underlying Linux OS to provide low-latency, reliable transport



------------ Storage

• Instance-attached storage:
   • EBS: scale up to 256,000 IOPS with io2 Block Express
   • Instance Store: scale to millions of IOPS, linked to EC2 instance, low latency

• Network storage:
  • Amazon S3: large blob, not a file system
  • Amazon EFS: scale IOPS based on total size, or use provisioned IOPS
  • Amazon FSx for Lustre:
    • HPC optimized distributed file system, millions of IOPS
    • Backed by S3





----------------------------------------------- Automation and Orchestration



• AWS Batch

        • AWS Batch supports multi-node parallel jobs, which enables you to run single jobs that span multiple EC2 instances.

        • Easily schedule jobs and launch EC2 instances accordingly


• AWS ParallelCluster

        • Open source cluster management tool to deploy HPC on AWS

        • Configure with text files

        • Automate creation of VPC, Subnet, cluster type and instance types





----------------------------------------------- Auto Scaling Groups – Dynamic Scaling Policies

check in previous course





----------------------------------------------- Auto Scaling – Good to know


• Spot Fleet support (mix on Spot and On-Demand instances)

• Lifecycle Hooks:

        • Perform actions before an instance is in service, or before it is terminated

        • Examples: cleanup, log extraction, special health checks


• To upgrade an AMI, must update the launch configuration / template

        • Then terminate instances manually (CloudFormation can help)

        • Or use EC2 Instance Refresh for Auto Scaling




----------------------------------------------- Auto Scaling – Instance Refresh


• Goal: update launch template and then re-creating all EC2 instances

• For this we can use the native feature of Instance Refresh

• Setting of minimum healthy percentage

• Specify warm-up time (how long until the instance is ready to use)



----------------------------------------------- Auto Scaling – Scaling Processes


• Launch: Add a new EC2 to the group, increasing the capacity

• Terminate: Removes an EC2 instance from the group, decreasing its capacity.

• HealthCheck: Checks the health of the instances

• ReplaceUnhealthy: Terminate unhealthy instances and re-create them

• AZRebalance: Balancer the number of EC2 instances across AZ

• AlarmNotification: Accept notification from CloudWatch

• ScheduledActions: Performs scheduled actions that you create.

• AddToLoadBalancer: Adds instances to the load balancer or target group

• InstanceRefresh: Perform an instance refresh


• We can suspend these processes!





----------------------------------------------- Auto Scaling – Health Checks



- Your ASG depends on health checks to function correctly

• Health checks available:

        • EC2 Status Checks

        • ELB Health Checks (HTTP)

        • Custom Health Checks – send instance’s health to an ASG using AWS CLI or AWS SDK (set-instance- health)


• ASG will launch a new instance after terminating an unhealthy one

• Make sure the health check is simple and checks the correct thing






----------------------------------------------- What is Docker?



• Docker is a software development platform to deploy apps

• Apps are packaged in containers that can be run on any OS

• Apps run the same, regardless of where they’re run

  • Any machine
  • No compatibility issues
  • Predictable behavior
  • Less work
  • Easier to maintain and deploy
  • Works with any language, any OS, any technology


• Use cases: microservices architecture, lift-and-shift apps from on- premises to the AWS cloud, ...



---------------------------- Where are Docker images stored?



• Docker images are stored in Docker Repositories

• Docker Hub (https://hub.docker.com)

    • Public repository
    • Find base images for many technologies or OS (e.g., Ubuntu, MySQL, ...)


• Amazon ECR (Amazon Elastic Container Registry)

    • Private repository
    • Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)


-- Jfrog also we can store images





-------------------------- Docker vs.Virtual Machines

• Docker is ”sort of ” a virtualization technology, but not exactly

• Resources are shared with the host => many containers on one server

-- see pics in google for better understanding





-------------------------------- Getting Started with Docker



Dockerfile -------(Build)----------------> Docker Image ------------(Run)------------> Docker Container (Eg :python)
                                            |      |
                                            |      |
                                Push        |      |    Pull
                                            |      |
                                            |      |
              
                                        Docker Repositories

                                        Eg : DockerHub , ECR



-- Dockerfile  : which is defining how your Docker container will look. So we have a base Docker image , and we add some files and then we're going to build it.

-- DockerImage : And this will become a Docker image. And that Docker image, you can store it on a Docker repository , it's called a Push and you push it to either Docker hub which is a public repository, or Amazon is ECR
   
                 - Then you can pull back these images from these repositories and then you would run them.


-- Docker Container : And when you run a Docker image , it becomes a Docker container, which runs your code  you had built from your Docker build.


--- That is the whole process with Docker.





---------------------------------------------- Docker Containers Management on AWS



• Amazon Elastic Container Service (Amazon ECS)
     
     • Amazon’s own container platform


• Amazon Elastic Kubernetes Service (Amazon EKS)

     • Amazon’s managed Kubernetes (open source)


• AWS Fargate
  
     • Amazon’s own Serverless container platform
     • Works with ECS and with EKS


• Amazon ECR:
  
     • Store container images





---------------------------------------------- Amazon ECS – Use cases


• Run Microservices

        • Run multiple Docker containers on the same machine

        • Easy Service Discovery features to enhance communication

        • Direct integration with Application Load Balancer and Network Load Balancer

        • Auto Scaling capability


• Run Batch Processing / Scheduled Tasks

        • Schedule ECS tasks to run on On-demand / Reserved / Spot instances


• Migrate Applications to the Cloud

        • Dockerize legacy applications running on-premises

        • Move Docker containers to run on Amazon ECS




---------------------------------------------- Amazon ECS – Concepts




• ECS Cluster – logical grouping of EC2 instances

• ECS Service – defines how many tasks should run and how they should be run

• Task Definitions – metadata in JSON form to tell ECS how to run a Docker container (image name, CPU, RAM, ...)

• ECS Task – an instance of a Task Definition, a running Docker container(s)

• ECS IAM Roles

        • EC2 Instance Profile – used by the EC2 instance (e.g., make API calls to ECS, send logs, ...)

        • ECS Task IAM Role – allow each task to have a specific role (e.g., make API calls to S3, DynamoDB, ...)





---------------------------------------------- Amazon ECS – ALB Integration


• We get Dynamic Port Mapping

• Allows you to run multiple instances of the same application on the same EC2 instance

• The ALB finds the right port on your EC2 Instances

• Use cases:

        • Increased resiliency even if running on one EC2 instance

        • Maximize utilization of CPU / cores

        • Ability to perform rolling upgrades without impacting app uptime




---------------------------------------------- AWS Fargate


• Launch Docker containers on AWS

• You do not provision the infrastructure (no EC2 instances to manage)

• It’s all Serverless!, because we don't manage servers (there are servers behind., but we are not managing the servers)

• if we have an ECS Cluster , we just create task definition to define our ECS tasks.

• AWS just runs ECSTasks for you based on the CPU / RAM you need

   - So when we want to run a new Docker container, simple as that, it's going to be run, without us knowing where it's run and without an EC2 Instance to be created in the backend in our accounts for it to work.
   - So it's a little bit magic.


• To scale, just increase the number of tasks. Simple - no more EC2 instances






---------------------------------------------- Amazon ECS – Security & Networking



• You can inject secrets and configurations as Environment Variables into running Docker containers

        • Integration with SSM Parameter Store and Secrets Manager


• ECS Tasks Networking

        • none – no network connectivity, no port mappings

        • bridge – uses Docker’s virtual container-based network

        • host – bypass Docker’s network, uses the underlying host network interface

        • awsvpc

                • Every tasks launched on the instance gets its own ENI and a private IP address

                • Simplifiednetworking,enhancedsecurity,SecurityGroups,monitoring,VPCFlowLogs

                • Default mode for Fargate tasks






---------------------------------------------- Amazon ECS – Service Auto Scaling



• Automatically increase/decrease the desired number of tasks

• Amazon ECS leverages AWS Application Auto Scaling

• CPU and RAM is tracked in CloudWatch at the ECS Service level

• Target Tracking – scale based on target value for a specific CloudWatch metric

• Step Scaling – scale based on a specified CloudWatch Alarm

• Scheduled Scaling – scale based on a specified date/time (predictable changes)

• ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level)

• Fargate Auto Scaling is much easier to setup (because Serverless)




---------------------------------------------- Amazon ECS – Spot Instances


• ECS Classic (EC2 Launch Type)

        • Can have the underlying EC2 instances as Spot Instances (managed by an ASG)

        • Instances may go into draining mode to remove running tasks

        • Good for cost savings, but will impact reliability



• AWS Fargate

        • Specify minimum of tasks for on-demand baseline workload

        • Add tasks running on FARGATE_SPOT for cost-savings (can be reclaimed by AWS)

        • Regardless of On-demand or Spot, Fargate scales well based on load








---------------------------- Where are Docker images stored?



• Docker images are stored in Docker Repositories

• Docker Hub (https://hub.docker.com)

    • Public repository
    • Find base images for many technologies or OS (e.g., Ubuntu, MySQL, ...)


• Amazon ECR (Amazon Elastic Container Registry)

    • Private repository
    • Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)


-- Jfrog also we can store images
 





---------------------------------------------- Amazon ECR – Cross Region Replication


 • ECR private registry supports both cross-Region and cross-account replication




---------------------------------------------- Amazon ECR – Image Scanning



• Manual Scan or Scan on Push

• Basic Scanning – Common CVE

        - So we push an image into ECR. ECR is going to scan the image through its service itself and then the results in case, for example, you have vulnerabilities.

        - They can be triggered and sent as events into EventBridge. That's for the basic scanning.

• Enhanced Scanning – Leverages Amazon Inspector (OS & Programming Language vulnerabilities)

        - that actually leverages the Amazon Inspector Service, and it's going to do more than just looking for common CVE is going to look for operating system and programming language vulnerability.

        - we push an image into ECR but this time the inspector service is going to scan our image.

        - then in case we have any results of that scan, it's going to be in EventBridge.

        - But this time the trigger service is inspector, it's not ECR.


• Scan results can be retrieved from within the AWS console






---------------------------------------------- Amazon EKS Overview


• Amazon EKS = Amazon Elastic Kubernetes Service

• It is a way to launch managed Kubernetes clusters on AWS

• Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) application

• It’s an alternative to ECS, similar goal but different API

• EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers

• Use case: if your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes

• Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP...)

• For multiple regions, deploy one EKS cluster per region

• Collect logs and metrics using CloudWatch Container Insights






----------------------- Amazon EKS – Node Types

• Managed Node Groups

    • Creates and manages Nodes (EC2 instances) for you
    • Nodes are part of an ASG managed by EKS
    • Supports On-Demand or Spot Instances

• Self-Managed Nodes

    • Nodes created by you and registered to the EKS cluster and managed by an ASG
    • You can use prebuilt AMI - Amazon EKS Optimized AMI
    • Supports On-Demand or Spot Instances


• AWS Fargate

    • No maintenance required; no nodes managed





------------------ Amazon EKS – Data Volumes

• Need to specify StorageClass manifest on your EKS cluster

• Leverages a Container Storage Interface (CSI) compliant driver

• Support for...
   
   • Amazon EBS
   • Amazon EFS (works with Fargate) 
   • Amazon FSx for Lustre
   • Amazon FSx for NetApp ONTAP






---------------------------------------------- AWS App Runner



• Fully managed service that makes it easy to deploy web applications and APIs at scale

• No infrastructure experience required

• Start with your source code or container image

• Automatically builds and deploy the web app

• Automatic scaling, highly available, load balancer, encryption

• VPC access support

• Connect to database, cache, and message queue services


• Use cases: web apps, APIs, microservices, rapid production deployments







---------------------------------------------- Amazon ECS Anywhere




- the integration between the container services of AWS and your on-premises deployments.


• Easily run containers on Customer-managed infrastructure (on-premises,VMs, ...)

• Allows customers to deploy native Amazon ECS tasks in any environment

• Fully-managed Amazon ECS Control Plane

• ECS Container Agent and SSM Agent needs to be installed

• “EXTERNAL”LaunchType

• Must have a stable connection to the AWS Region

• Use cases:

        • Meet compliance,regulatory,and latency requirements

        • Run apps outside AWSRegions and closer to their other services

        • On-premisesML,videoprocessing,dataprocessing,...








---------------------------------------------- Amazon EKS Anywhere



• Create and operate Kubernetes clusters created outside AWS

• Leverage the Amazon EKS Distro (AWS’ bundled release of Kubernetes)

• Reduce support costs and avoid maintaing redundant 3rd party tools

• Install using the EKS Anywhere Installer

• Optionally use the EKS Connector to connect the EKS Anywhere clusters to AWS

        • Fully Connected & Partially Disconnected:youcan connect to Amazon EKS Anywhere clusters to AWS, and leverage the EKS console

        • Fully Disconnected : must install the EKS Distro and leverage open-source tools to manage your clusters






---------------------------------------------- check for lambda , Load balancers and Api in previous courses 





---------------------------------------------- AWS AppSync - Overview




• AppSync is a managed service that uses GraphQL

         - So if we wants to build a GraphQL API on AWS, look no further than AppSync.

• GraphQL makes it easy for applications to get exactly the data they need.

• This includes combining data from one or more sources

      • NoSQL data stores, Relational databases, HTTP APIs...

      • Integrates with DynamoDB, Aurora, OpenSearch & others

      • Custom sources with AWS Lambda


• Retrieve data in real-time with WebSocket or MQTT on WebSocket

• For mobile apps: local data access & data synchronization

• It all starts with uploading one GraphQL schema





----------------------------------------------------- AppSync – Security

• There are four ways you can authorize applications to interact with your AWS AppSync GraphQL API:

    • API_KEY

    • AWS_IAM: IAM users / roles / cross-account access

    • OPENID_CONNECT: OpenID Connect provider / JSON Web Token

    • AMAZON_COGNITO_USER_POOLS


• For custom domain & HTTPS, use CloudFront in front of AppSync





----------------------------------------------------- AppSync – Cognito Integration



• Perform authorization on Cognito users based on the groups they belong to

• In the GraphQL schema, you can specify the security for Cognito groups



type Post @model @auth(rules: [
  { allow: groups, groups: ["Admin"], operations: [create, update, delete] },
  { allow: groups, groups: ["Editor"], operations: [create, update] },
  { allow: groups, groups: ["Viewer"], operations: [read] }
]) {
  id: ID!
  title: String!
  content: String!
  createdAt: AWSDateTime!
  updatedAt: AWSDateTime!
}






----------------------------------------------------- AppSync Hands On


-- open appsync in console --> GraphQL APIs ---> Design from scratch ---> Create type backed by a DynamoDB table now --> add fiels as u want ---> give table name as u want and Primary key --> Create

-- if u look at schema in left side automatically it creates for u 

-- go to dynamodb and check whether the table is created or not 

-- now in appsync --> queries --> run --> liststudents , u won't get any 'coz u do not have any list of students 

--  so run --> createstudent --> give data in below and c.o run --> createstudent 

-- now run--> liststudent , u will get list of students from the API

-- go to dynamodb and check the users data created or not as items in table 

-- the cool thing about it is that now we have a full GraphQL-compatible API on top of our DynamoDB table.

-- So we can expose it to our mobile clients or any websites that is using GraphQL and API mechanism, which is very, very handy.






----------------------------------------------------- check R53 in previous course






----------------------------------------------------- AWS Global Accelerator




-- Leverage the AWS internal network to route to your application

-- 2 Anycast IP are created for your application

-- The Anycast IP send traffic directly to Edge Locations

-- The Edge locations send the traffic to your application

-- Works with Elastic IP, EC2 instances, ALB, NLB, public or private

-- Consistent Performance
• Intelligent routing to lowest latency and fast regional failover 
• No issue with client cache (because the IP doesn’t change) 
• Internal AWS network

-- Health Checks
• Global Accelerator performs a health check of your applications
• Helps make your application global (failover less than 1 minute for unhealthy) 
• Great for disaster recovery (thanks to the health checks)

-- Security
• only 2 external IP need to be whitelisted 
• DDoS protection thanks to AWS Shield


----------------------------------- AWS Global Accelerator vs CloudFront

-- They both use the AWS global network and its edge locations around the world

-- Both services integrate with AWS Shield for DDoS protection.

1 CloudFront
• Improves performance for both cacheable content (such as images and videos)
• Dynamic content(such as API acceleration and dynamic site delivery) 
• Content is served at the edge

2 Global Accelerator
• Improvesper formance for a wide range of applications over TCP or UDP
• Proxying packets at the edge to applications running in one or more AWS Regions.
• Good fit for non-HTTP use cases,such as gaming(UDP),IoT(MQTT),or Voice over IP 
• Good for HTTP use cases that require static IP addresses
• Good for HTTP use cases that required deterministic,fast regional failover


--------------------------- CloudFront Prac 

-- open S3 and create private Bucket 

-- as u create private bucket , no one can access ur url through the S3 

-- Only access through by Cloud-Front directly coz, it is privtae bucket and we did not enable Static hosting also 

-- go to CF in console 

-- create ditrubtion on CF 

-- Origina Domain = load balancer / S3 -- these are the places wher u can host ur applications 

-- OAC --> Create control settings --> do not change any n create 

-- it is created access from S3 

-- Compress objects automatically : CloudFront can automatically compress certain files that it receives from the origin before delivering them to the viewer. CloudFront compresses files only when the viewer supports it, as specified in the Accept-Encoding header in the viewer request.


-- Default root object - optional = index.html  -----> must and should u have to give this , otherwise u won’t get o/p 


----- once u create distrubtion , the S3 bucket policy wil gwt generated copy that policy and paste in bucket policy 

-- now ur appn is getting deployed all over the world 

-- through the CF url customers will able to connect ur webiste through the edge locations 

-- once u change the content of ur website and do uploud again n if u do refresh u won't get new content 

-- u have too do "invalidate the Cache" 

-- go to CF and create invalidation for /index.html , it will get latest file from the S3 and give latest content to customers 

-- By default, CloudFront caches files in edge locations for 24 hours. 







----------------------------------------------------- Solution Architecture Comparisons



• EC2 on its own with Elastic IP

• EC2 with Route53

• ALB + ASG

• ALB + ECS on EC2

• ALB + ECS on Fargate

• ALB + Lambda

• API Gateway + Lambda

• API Gateway + AWS Service

• API Gateway + HTTP backend (ex: ALB)





1 EC2 on its own with Elastic IP


        • Quick failover

        • The client should not see the change happen

        • Helpful if the client needs to resolve by static Public IP address

        • Does not scale

        • Cheap



2 EC2 with Route53

        - Stateless web app - scaling horizontally

        • “DNS-based load balancing”

        • Ability to use multiple instances

        • Route53 TTL implies client may get outdated information

        • Clients must have logic to deal with hostname resolution failures

        • Adding an instance may not receive full traffic right away due to DNS TTL



3 ALB + ASG

        • Scaleswell,classicarchitecture

        • New instances are in service right away.

        • Users are not sent to instances that are out-of-ser vice

        • Time to scale is slow (EC2 instance startup + bootstrap) – AMI can help

        • ALB is elastic but can’t handle sudden, huge peak of demand (pre-warm)

        • Could lose a few requests if instances are overloaded

        • CloudWatch used for scaling

        • Cross-Zone balancing for even traffic distribution

        • Target utilization should be between 40% and 70%
 


4 ALB + ECS on EC2(backed by ASG)

        • Same properties as ALB + ASG

        • Application is run on Docker

        • ASG + ECS allows to have dynamic port mappings

        • Tough to orchestrate ECS service auto-scaling + ASG auto-scaling




5 ALB + ECS on Fargate

        • Application is run on Docker

        • Service Auto Scaling is easy

        • Time to be in-service is quick (no need to launch an EC2 instance in advance)

        • Still limited by the ALB in case of sudden peaks

        • “serverless” application tier

        • “managed” load balancer


6 ALB + Lambda

        • Limited to Lambda’s runtimes

        • Seamless scaling thanks to Lambda

        • Simple way to expose Lambda functions as HTTP/S without all the features from API Gateway

        • Can combine with WAF (Web Application Firewall)

        • Good for hybrid microservices

        • Example: use ECS for some requests, use Lambda for others



7 API Gateway + Lambda

        • Pay per request, seamless scaling, fully serverless

        • Soft limits: 10000/s API Gateway, 1000 concurrent Lambda

        • API Gateway features: authentication, rate limiting, caching, etc...

        • Lambda Cold Start time may increase latency for some requests

        • Fully integrated with X-Ray


8 API Gateway + AWS Service (as a proxy)

        • Lower latency, cheaper

        • Not using Lambda concurrent capacity, no custom code

        • Expose AWS APIs securely through API Gateway

        • SQS, SNS, Step Functions...

        • Remember API Gateway has a payload limit of 10 MB (can be a problem for S3 proxy)



9 API Gateway + HTTP backend (ex: ALB)

        • Use API Gateway features on top of custom HTTP backend (authentication, rate control, API keys, caching...)

        • Can connect to...

                • on-premises service

                • Application Load Balancer

                • 3rd party HTTP service






 


--------------------------------------------------------- AWS Outposts


stephen notes :


• Hybrid Cloud: businesses that keep an on- premises infrastructure alongside a cloud infrastructure

• Therefore, two ways of dealing with IT systems:

        • One for the AWS cloud (using the AWS console, CLI, and AWS APIs)

        • One for their on-premises infrastructure


• AWS Outposts are “server racks” that offers the same AWS infrastructure, services, APIs & tools to build your own applications on-premises just as in the cloud

• AWS will setup and manage “Outposts Racks” within your on-premises infrastructure and you can start leveraging AWS services on-premises

• You are responsible for the Outposts Rack physical security




• Benefits:

        • Low-latency access to on-premises systems

        • Local data processing

        • Data residency

        • Easier migration from on-premises to the cloud

        • Fully managed service



• Some services that work on Outposts:

        - Amazon EC2 
        - Amazon EBS  
        - Amazon S3 
        - Amazon EKS 
        - Amazon ECS
        - Amazon RDS
        - Amazon EMR




S3 on AWS Outposts

        • Use S3 APIs to store and retrieve data locally on AWS Outposts

        • Keeping data close to on-premises applications

        • Reduce data transfers to AWS Regions

        • S3 Storage Class named S3 Outposts

        • Default encryption using SSE-S3



so how is the data accessed from Outposts onto AWS?


S3 on Outposts <------------------------------ S3 Access Point <------------------------------ Ec2 (in vpc)


        - make sure that an easy to instance in your VPC can access your S3 on Outposts through this S3 access point.



                        sync                                       sync

S3 on Outposts ------------------------------> DataSync ------------------------------> Amazon S3











Personal notes :




--  AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises. 

--  By providing local access to AWS managed infrastructure, AWS Outposts enables customers to build and run applications on premises using the same programming interfaces as in AWS Regions, while using local compute and storage resources for lower latency and local data processing needs.

--  An Outpost is a pool of AWS compute and storage capacity deployed at a customer site. AWS operates, monitors, and manages this capacity as part of an AWS Region. 

--  You can create subnets on your Outpost and specify them when you create AWS resources such as EC2 instances, EBS volumes, ECS clusters, and RDS instances.

--  Instances in Outpost subnets communicate with other instances in the AWS Region using private IP addresses, all within the same VPC.


IMP Note You cannot connect an Outpost to another Outpost or Local Zone that is within the same VPC.


-------- Key concepts

-- These are the key concepts for AWS Outposts.


1 Outpost site 

         – The customer-managed physical buildings where AWS will install your Outpost. A site must meet the facility, networking, and power requirements for your Outpost.

2 Outpost capacity

         – Compute and storage resources available on the Outpost. You can view and manage the capacity for your Outpost from the AWS Outposts console.

3 Outpost equipment 

         – Physical hardware that provides access to the AWS Outposts service. The hardware includes racks, servers, switches, and cabling owned and managed by AWS.

4 Outposts racks 

         – An Outpost form factor that is an industry-standard 42U rack. Outpost racks include rack-mountable servers, switches, a network patch panel, a power shelf and blank panels.

5 Outposts servers 

         – An Outpost form factor that is an industry-standard 1U or 2U server, which can be installed in a standard EIA-310D 19 compliant 4 post rack. Outpost servers provide local compute and networking services to sites that have limited space or smaller capacity requirements.

6 Service link 

         – Network route that enables communication between your Outpost and its associated AWS Region. Each Outpost is an extension of an Availability Zone and its associated Region.

7 Local gateway (LGW) 

         – A logical interconnect virtual router that enables communication between an Outpost rack and your on-premises network.

8 Local network interface 

         – A network interface that enables communication from an Outpost server and your on-premises network.





------- AWS resources on Outposts


-- You can create the following resources on your Outpost to support low-latency workloads that must run in close proximity to on-premises data and applications:

1 Compute  
 
    Resource type	                  Racks	       Servers

    Amazon EC2 instances             Yes	         Yes	

    Amazon ECS clusters              Yes           Yes

    Amazon EKS nodes                 Yes	         No


2 Database and analytics

     
         Resource type	                                                                Racks	       Servers
         
        Amazon ElastiCache nodes (Redis cluster, Memcached cluster)                      YES          NO

        Amazon EMR clusters                                                              YES         NO

        Amazon RDS DB instances                                                          YES         NO



3 Networking

                 Resource type	                              Racks	       Servers
                  
                App Mesh Envoy proxy                           YES          YES

                Application Load Balancers                     YES          NO

                Amazon VPC subnets	                           YES          YES

                Amazon Route 53	                               YES          NO



4 Storage

                 Resource type	                              Racks	       Servers
                           
                 Amazon EBS volumes                            YES          NO

                 Amazon S3 buckets                             YES          NO


5 Other AWS services

                 Service                               Racks	       Servers

                 AWS IoT Greengrass	                    YES            YES

                 Amazon SageMaker Edge Manager	        YES             YES





----- Pricing

-- You can choose from a variety of Outpost configurations, each providing a combination of EC2 instance types and storage options. The price for rack configurations includes installation, removal, and maintenance. For servers, you must install and maintain the equipment.

-- You purchase a configuration for a 3-year term and can choose from three payment options: All Upfront, Partial Upfront, and No Upfront. 

-- If you choose the Partial option or the No Upfront payment option, monthly charges will apply. 

-- Any upfront charges apply 24 hours after your Outpost is installed and the compute and storage capacity is available for use. 





----------------------- Local Zones vs Outposts


Outposts:


-- for example , u have a company , ur employess are going to cream stone to have a ice cream during a break time which is three miles away from ur office  , u have noticed that it will get impact on the work , so u have asked cream stone and set up their outlet in ur compnay building ,  to improve productivity and make employees also happy at the same time , here u are providing the facilities and cream stone will set up their services , this what exactly happen in the AWS outposts 

-- so many companies have applications that need single digit millisecond latency not only that in some cases there are regulatory constraints that your data has to be processed in hosts we know that AWS regions are not there in every country .

-- it is possible that the companies in one country may be using the region in the neighboring country but there can be regulatory restrictions that the data which they are processing using the AWS Services it cannot leave the boundary of country

-- so in such cases what the clients do is they will request AWS to come and set up their services AWS services in their own data center 

-- so basically just like you provide the facility to creamstone to set up the shop , your it department will have to provide the Rackspace in your own Data Center and this will be very useful when you want to use applications which need  single digit millisecond latency such as  arvr applications virtualized desktops , gaming development projects , media content creation 




Local Zones :

-- now assume that let us go back to that same  shop example assume that your company is part of an a big huge it Park where there are dozens of companies

-- and every company has a lot of employees who are ice cream lovers so every company is facing the same issue of productivity impact 

-- so these companies May request creamstone to come and set up a outlet in the neighborhood somewhere in the campus in the it Parks campus or just outside.

-- so here in this case creamstone will set up the facility just remember in the other case your company is providing the facility

-- but in this case creamstone will take up their own building and they will set up all the infrastructure and they will start providing the service

-- local zone is exactly same AWS when they find that there is a concentration of their Enterprise clients in a particular City they can actually come  and set up a local Zone with limited set of services uch as ec2 VPC EFS Etc 


-- all the companies in the nearby areas can actually make use of the services in  that local Zone that is called local zone 



IMP to know 

-- Q: How are Local Zones different from Availability Zones? 

ANS : 

-- Local Zones are designed to bring the core services needed for the latency-sensitive portions of your workload closer to end users,

-- while Availability Zones provide access to the full array of AWS services. 

-- Services like Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Block Store (Amazon EBS), Amazon Virtual Private Cloud (Amazon VPC), and others are locally available and can be used to serve end users in geographic proximity with extremely low latency.

-- Other AWS services like Amazon Simple Storage Service (Amazon S3) and Amazon Aurora are accessible privately through VPC over AWS private network. Both Local Zones and Availability Zones allow you to build applications for high availability.






------ conclusion :


-- AWS Outpost is an extension of aws's infrastructure to your data center 

-- whereas local zone is provided from a small data center which is owned and maintained and operated by AWS in your neighborhood , so that all the companies in that area can get low latency services

-- it provides only limited services but still the local zones have proved successful







--------------------------------------------------------- AWS WaveLength



• WaveLength Zones are infrastructure deployments embedded within the telecommunications providers’ datacenters at the edge of the 5G networks

• Brings AWS services to the edge of the 5G networks

• Example:EC2,EBS,VPC...

• Ultra-low latency applications through 5G networks

• Traffic doesn’t leave the Communication Service Provider’s (CSP) network

• High-bandwidth and secure connection to the parent AWS Region

• No additional charges or service agreements

• Use cases: Smart Cities, ML-assisted diagnostics, Connected Vehicles, Interactive Live Video Streams, AR/VR, Real-time Gaming, ...







--------------------------------------------------------- AWS Local Zones



• Places AWS compute, storage, database, and other selected AWS services closer to end users to run latency-sensitive applications

• Extend your VPC to more locations – “Extension of an AWS Region”

• Compatible with EC2, RDS, ECS, EBS, ElastiCache, Direct Connect ...

• Example:

        • AWS Region: N.Virginia (us-east-1)

        • AWS Local Zones: Boston, Chicago, Dallas, Houston, Miami, ...








=========================================================== Storage ===========================================================




-------------------------------------------------------- EBS



• Network drive you attach to ONE instance only

• Linked to a specific availability zone (transfer: snapshot => restore)

• Volumes can be resized

• Make sure you choose an instance type that is EBS optimized to enjoy maximum throughput





-------------------------------------------------------- EBS Volume Types





• EBS Volumes come in 6 types

        • gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads

        • io1 / io2 Block Express (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads

        • st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput- intensive workloads

        • sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads



• EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)

• When in doubt always consult the AWS documentation – it’s good!

• Only gp2/gp3 and io1/io2 Block Express can be used as boot volumes





-------------------------------------------------------- EBS Snapshots


• Incremental – only backup changed blocks

• EBS backups use IO, and you shouldn’t run them while your application is handling a lot of traffic

• Snapshots will be stored in S3 (but you won’t directly see them)

• Not necessary to detach volume to do snapshot, but recommended

• Can copy snapshots across region (for DR)

• Can make Image (AMI) from Snapshot

• EBS volumes restored by snapshots need to be pre-warmed (use the Fast Snapshot Restore FSR feature or fio/dd command to read the entire volume)







------------------------------------------------------------- Amazon Data Lifecycle Manager


• Automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs

• Schedule backups, cross-account snapshot copies, delete outdated backups, ...

• Uses resource tags to identify the resources (EC2 instances, EBS volumes)

• Can’t be used to manage snapshots/AMIs created outside DLM

• Can’t be used to manage instance-store backed AMIs



------------------------------------------------------------- Amazon Data Lifecycle Manager vs. AWS Backup


• Use Data Lifecycle Manager

        • when you want to automate the creation, retention, and deletion of EBS Snapshots


• Use AWS Backup

        • to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place








------------------------------------------------------------- EBS Encryption


• When you create an encrypted EBS volume, you get the following:

        • Data at rest is encrypted inside the volume

        • All the data in flight moving between the instance and the volume is encrypted

        • All snapshots are encrypted

        • All volumes created from the snapshot


• Encryption and decryption are handled transparently (you have nothing to do) It's all handled by EC2 and EBS behind the scenes.

• Encryption has a minimal impact on latency

• EBS Encryption leverages keys from KMS (AES-256)

• Copying an unencrypted snapshot allows encryption

• Snapshots of encrypted volumes are encrypted





------------------------------------------------------------- Encryption: encrypt an unencrypted EBS volume 



• Create an EBS snapshot of the volume

• Encrypt the EBS snapshot ( using copy )

• Create new ebs volume from the snapshot ( the volume will also be encrypted )

• Now you can attach the encrypted volume to the original instance





------------------------------------------------------------- Encryption: encrypt an unencrypted EBS volume Hands ON 




-- any snapshots encrypted from a non-encrypted EBS volume, will be not encrypted.

-- create volm withput encryption 

-- now take snapshot from the volm u have created 

-- this snapshot, as we observe is not encrypted.

-- so, to create an encrypted snapshot, now, what you have to do is to do action --> copy snapshot

-- when you copy the snapshots, you have the option right here to enable encryption into the same destination region.

-- from this encrypted snapshots that is now completed I can create a volume.

-- We saw how we can encrypt one EBS volume this way, by going through a snapshot copying and so on.


-- There's a shortcut.

        - go to unencrypted snapshot in console --> create volumes from snapshot

        - you can actually on the fly enable encryption for the EBS volume directly from here,

        - select an EBS key, and you would create an encrypted EBS volume this way through a unencrypted snapshots.






------------------------------------------------------------- EBS Encryption – Account level setting



• New Amazon EBS volumes aren’t encrypted by default

• There’s an account-level setting to encrypt automatically new EBS volumes and Snapshots

• This setting needs to be enabled on a per-region basis






------------------------------------------------------------- EBS Multi-Attach – io1/io2 family




• Attach the same EBS volume to multiple EC2 instances in the same AZ

• Each instance has full read & write permissions to the high-performance volume

• Use case:

        • Achieve higher application availability in clustered Linux applications (ex:Teradata)

        • Applications must manage concurrent write operations

• Up to 16 EC2 Instances at a time

- this Multi-Attach feature is only available from within a specified availability zone, of course. It doesn't allow you to attach an EBS volume from one AZ to another AZ.

• Must use a file system that’s cluster-aware (not XFS, EXT4, etc...)






------------------------------------------------------------- Local EC2 Instance Store



• Physical disk attached to the physical server where your EC2 is

• Very High IOPS (because physical)

• Disks up to 7.5 TiB (can change over time), stripped to reach 60 TiB (can change over time...)

• Block Storage (just like EBS)

• Cannot be increased in size

• Risk of data loss if hardware fails





------------------------------------------------------------- Instance Store vs EBS


• Instance store is physically attached to the machine (ephemeral storage)

• EBS is a network drive (persistent)

• Pros:

        • Better I/O performance (EBS gp2 has a max IOPS of 16000, io1 of 64000, io2 Block Express of 256000)

        • Good for buffer / cache / scratch data / temporary content

        • Data survives reboots


• Cons:

        • On stop or termination, the instance store is lost

        • You can’t resize the instance store

        • Backups must be operated by the user









------------------------------------------------------------- Amazon EFS – Elastic File System 



• Managed NFS (network file system) that can be mounted on many EC2

• EFS works with EC2 instances in multi-AZ

• Highly available, scalable, expensive (3x gp2), pay per use

• Use cases: content management, web serving, data sharing,Wordpress

• Uses NFSv4.1 protocol

• Uses security group to control access to EFS

• Compatible with Linux based AMI (not Windows)

• Encryption at rest using KMS




• POSIX file system (~Linux) that has a standard file API

• File system scales automatically, pay-per-use, no capacity planning!









IMP : A company stores all of its data on Amazon EFS that is accessed by different applications hosted on Amazon EC2 instances. The company's new security policy mandates encrypting all data-at-rest.

How will you enforce the creation of the Amazon EFS file system that is encrypted at rest? (Select two)


ANS :

        1 Use the elasticfilesystem:Encrypted IAM condition key in AWS IAM identity-based policies to mandate users for creating only encrypted-at-rest Amazon EFS file systems

        
                - You can create an AWS Identity and Access Management (IAM) identity-based policy to control whether users can create Amazon EFS file systems that are encrypted at rest. 

                - The Boolean condition key elasticfilesystem:Encrypted specifies the type of file system, encrypted or unencrypted, that the policy applies to.

                - You use the condition key with the elasticfilesystem:CreateFileSystem action and the policy effect, allow or deny, to create a policy for creating encrypted or unencrypted file systems.

        
        2 Define Service Control Policies (SCPs) inside AWS Organizations to enforce EFS encryption for all AWS accounts in your organization


                - Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. 

                - SCPs offer central control over the maximum available permissions for all accounts in your organization.

                - An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user.

                -  If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action.

                - You can also define service control policies (SCPs) inside AWS Organizations to enforce EFS encryption for all AWS accounts in your organization.         






------------------------------------------------------------- EFS – Performance & Storage Classes



• EFS Scale

        • 1000s of concurrent NFS clients, 10 GB+ /s throughput

        • Grow to Petabyte-scale network file system, automatically


• Performance Mode (set at EFS creation time)

        • General Purpose (default) – latency-sensitive use cases (web server, CMS, etc...)

        • Max I/O – higher latency, throughput, highly parallel (big data, media processing)


• Throughput Mode

        • Bursting – 1TB = 50MiB/s + burst of up to 100MiB/s

        • Provisioned – set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage

        • Elastic – automatically scales throughput up or down based on your workloads

                • Upto 3GiB/s for reads and 1GiB/s for writes

                • Used for unpredictable work loads







------------------------------------------------------------- EFS – Storage Classes



• Storage Tiers (lifecycle management feature – move file after N days)

        • Standard: for frequently accessed files price to store.

        • Archive: rarely accessed data (few times each year), 50% cheaper

        • Archive: rarely accessed data (few times each year), 50% cheaper

        • Implement lifecycle policies to move files between storage tiers


• Availability and durability

        • Standard: Multi-AZ, greatforprod

        • OneZone: OneAZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone-IA)







------------------------------  EFS mount helper 



IMP : A company uses Amazon Elastic File System (EFS) to share storage space across multiple instances of an application. As a SysOps Administrator, you work with an EFS mount helper to mount the file system.

Which of the following options are available with the EFS mount helper? (Select two)


ANS :

        1 Mounting with IAM authorization

                EXP : You can use the EFS mount helper to mount your Amazon EFS file system on Linux instances using AWS Identity and Access Management (IAM) authorization.


        2 Auto-mounting when an EC2 instance reboots :


                - To automatically remount your Amazon EFS file system directory when the Amazon EC2 instance reboots, use the file /etc/fstab.

                - The /etc/fstab file contains information about file systems.

                - Using the EFS mount helper, you have the following options for mounting your Amazon EFS file system:

                         1. Mounting on supported EC2 instances 

                         2. Mounting with IAM authorization 

                         3. Mounting with Amazon EFS access points 

                         4. Mounting with an on-premise Linux client 

                         5. Auto-mounting when an EC2 instance reboots 

                         6. Mounting a file system when a new EC2 instance launches


-- The EFS mount helper is part of the amazon-efs-utils package. The amazon-efs-utils package is an open-source collection of Amazon EFS tools.                         









------------------------------------------------------------- EFS Hands ON 





-- create EFS 

-- throughput : speed b/w ec2 and EFS 

-- EFS is regional 

-- Replicatio is posssible in EFS , cost calulated how much data is transferred 

-- launch 1 ec2 instances 

-- in S.G add NFS inbound rule 

-- connect instance 1 and follow some commands 

1  sudo -s 

2  yum install -y nfs-utils

3   mkdir efs  --- create one folder 

4   now do mount with folder 


mount -t nfs4 fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com:/ efs/


explanation :   fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com  -- u wil get from efs dns name    and efs is folder name 

-- cd efs 

-- now create some files here 


---- now create 2nd instance in diff A.Z 

-- follow the same steps , here u can give mkdir names anything u want 

-- once u do ls u will get same filesin diff A.Z zone instances also 

-- create one file new in ec2 1 and check in 2nd ec2 u will get that file 

-- same for deletion also 

-- this is how u do with Efs amd u can also do repliction EFS 







------------------------------------------------------------- EBS vs EFS – Elastic Block Storage


EBS vs EFS – Elastic Block Storage 


• EBS volumes...

    • one instance (except multi-attach io1/io2)
    • are locked at the Availability Zone (AZ) level 
    • gp2: IO increases if the disk size increases
    • gp3 & io1: can increase IO independently



• To migrate an EBS volume across AZ

   • Take a snapshot
   • Restore the snapshot to another AZ
   • EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic

• Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated. (you can disable that by using "DeleteOnTermination")




EBS vs EFS – Elastic File System

• Mounting 100s of instances across AZ 

• EFS share website files (WordPress)

• Only for Linux Instances (POSIX)

• EFS has a higher price point than EBS 

• Can leverage EFS-IA for cost savings







------------------------------------------------------------- EFS – Access Points





• Easily manage applications access to NFS environments

• Enforce a POSIX user and group to use when accessing the file system

• Restrict access to a directory within the file system and optionally specify a different root directory

• Can restrict access from NFS clients using IAM policies



-- So here's an example, we have an EFS file system that's going to be shared across your entire company but has different folders under the routes.

-- There is a data folder, the secret folder and the config folder, and we want different users to access different parts of your EFS file system.








------------------------------------------------------------- EFS – File System Policies



• Resource-based policy to control access to EFS File Systems (same as S3 bucket policy)

• By default, it grants full access to all clients



{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": [
                    "arn:aws:iam::account-id:role/RoleName",
                    "arn:aws:iam::account-id:user/UserName"
                ]
            },
            "Action": [
                "elasticfilesystem:ClientMount",
                "elasticfilesystem:ClientWrite",
                "elasticfilesystem:ClientRootAccess"
            ],
            "Resource": "arn:aws:elasticfilesystem:region:account-id:file-system/file-system-id"
        }
    ]
}





------------------------------------------------------------- EFS – Cross-Region Replication




• Replicate objects in an EFS file system to another AWS Region

• Setup for new or existing EFS file systems

• Provides RPO and RTO of minutes

• Doesn’t affect the provisioned throughput of the EFS file system

• Use cases: meet your compliance and business continuity goals






------------------------------------------------------------- EFS - Operations




• Operations that can be done in place:

        • Lifecycle Policy (enable IA or change IA settings)

        • Throughput Mode and Provisioned Throughput Numbers

        • EFS Access Points


-- But, some operations require a full migration of your EFS file system

• Operations that require a migration using DataSync (replicates all file attributes and metadata)

        • Migration to encrypted EFS

        • Performance Mode (e.g. Max IO)




-- So let's have an example.

        - we have one source file system and destination file system which is encrypted.

        - so to migrate between the two, using the DataSync service,

        - then you can migrate your E2 instances from the first file system to the second file system.






-------------------------------------------------------------  EFS – CloudWatch Metrics



• PercentIOLimit

        • How close the file system reaching the I/O limit(General Purpose)        

        • If at 100%, move to Max I/O (migration) to increase your I/O capacity of your EFS file system.

        - So, though if you wanted to enable Max I/O, if you remember, you would have to perform a migration because this operation cannot be done in place.

        - So that means creating a new file system with a Max I/O already enabled and then migrating from the previous file system to the new file system using a data sync service.        


• BurstCreditBalance

         • The number of burst credits the file system can use to achieve higher throughput levels


• StorageBytes

        • File system’s size in bytes (15 minutes interval)

        • Dimensions: Standard, IA,Total (Standard + IA)



- ClientConnections

        - To track the number of Amazon EC2 instances that are connected to a file system, you can monitor the Sum statistic of the ClientConnections metric. To calculate the average ClientConnections for periods greater than one minute, divide the sum by the number of minutes in the period.


 - TotalIOBytes

        - To determine the throughput, you can monitor the daily Sum statistic of the TotalIOBytes metric to see your throughput.       



-- go and check efs monitoiring tab 







------------------------------------------------------------- s3 check in course 





------------------------------------------------------------- S3 – Storage Lens



• Understand, analyze, and optimize storage across entire AWS Organization

• Discover anomalies, identify cost efficiencies, and apply data protection best practices across entire AWS Organization (30 days usage & activity metrics)

• Aggregate data for Organization, specific accounts, regions, buckets, or prefixes

• Default dashboard or create your own dashboards

• Can be configured to export metrics daily to an S3 bucket (CSV, Parquet)





------------------------------------------------------------- Storage Lens – Default Dashboard




• Visualize summarized insights and trends for both free and advanced metrics

• Default dashboard shows Multi-Region and Multi-Account data

• Preconfigured by Amazon S3

• Can’t be deleted, but can be disabled




------------------------------------------------------------- Storage Lens – Metrics



• Summary Metrics

        • General insights about your S3 storage

        • StorageBytes, ObjectCount...

        • Use cases: identify the fastest-growing (or not used) buckets and prefixes


• Cost-Optimization Metrics

        • Provide insights to manage and optimize your storage costs

        • NonCurrentVersionStorageBytes, IncompleteMultipartUploadStorageBytes...

        • Use cases: identify buckets with incomplete multipart uploaded older than 7 days, Identify which objects could be transitioned to lower-cost storage class




• Data-Protection Metrics

        • Provide insights for data protection features

        • VersioningEnabledBucketCount,MFADeleteEnabledBucketCount,SSEKMSEnabledBucketCount, CrossRegionReplicationRuleCount...

        • Use cases: identify buckets that aren’t following data-protection best practices



• Access-management Metrics

        • Provide insights for S3 Object Ownership

        • ObjectOwnershipBucketOwnerEnforcedBucketCount...

        • Use cases: identify which Object Ownership settings your buckets use



• Event Metrics

        • Provide insights for S3 Event Notifications

        • EventNotificationEnabledBucketCount (identify which buckets have S3 Event Notifications configured)




• Performance Metrics

        • Provide insights for S3 Transfer Acceleration

        • TransferAccelerationEnabledBucketCount (identify which buckets have S3 Transfer Acceleration enabled)



• Activity Metrics

        • Provide insights about how your storage is requested

        • AllRequests,GetRequests,PutRequests,ListRequests,BytesDownloaded...



• Detailed Status Code Metrics

        • Provide insights for HTTP status codes

        • 200OKStatusCount,403ForbiddenErrorCount,404NotFoundErrorCount...





------------------------------------------------------------- Storage Lens – Free vs. Paid


• Free Metrics

        • Automatically available for all customers

        • Contains around 28 usage metrics

        • Data is available for queries for 14 days


• Advanced Metrics and Recommendations

        • Additional paid metrics and features

        • Advanced Metrics – Activity, Advanced Cost Optimization, Advanced Data Protection, Status Code

        • CloudWatch Publishing – Access metrics in CloudWatch without additional charges

        • Prefix Aggregation – Collect metrics at the prefix level

        • Data is available for queries for 15 months





------------------------------------------------------------- solution architect on s3 


check in course 




------------------------------------------------------------- Amazon FSx – Overview






• Launch 3rd party high-performance file systems on AWS

• Fully managed service

        FSx for Lustre

        FSx for Windows File Server

        FSx for NetApp ONTAP

        FSx for OpenZFS







---------------- 1 Amazon FSx for Lustre



• Lustre is a type of parallel distributed file system, for large-scale computing

• The name Lustre is derived from “Linux” and “cluster

• Machine Learning, High Performance Computing (HPC)

• Video Processing, Financial Modeling, Electronic Design Automation

• Scales up to 100s GB/s, millions of IOPS, sub-ms latencies

• Storage Options:
     • SSD – low-latency, IOPS intensive workloads, small & random file operations
     • HDD – throughput-intensive workloads, large & sequential file operations


• Seamless integration with S3 
     • Can “read S3” as a file system (through FSx)
     • Can write the output of the computations back to S3 (through FSx)


• Can be used from on-premises servers (VPN or Direct Connect)



----- FSx Lustre - File System Deployment Options


1 • Scratch File System
     • Temporary storage
     • Data is not replicated (doesn’t persist if file server fails)
     • High burst (6x faster, 200MBps per TiB)
     • Usage: short-term processing, optimize costs


2 • Persistent File System

     • Long-term storage
     • Data is replicated within same AZ
     • Replace failed files within minutes
     • Usage: long-term processing, sensitive data





------------------- 2 Amazon FSx for Windows (File Server)

• FSx for Windows is a fully managed Windows file system share drive

• Suppor ts SMB protocol & Windows NTFS

• Microsoft ActiveDirectory integration, ACLs, userquotas

• Can be mounted on Linux EC2 instances

• Supports Microsoft's Distributed File System (DFS) Namespaces (group files across multiple FS)

• Scale up to 10s of GB/s, millions of IOPS, 100s PB of data

• Storage Options:
    • SSD – latency sensitive workloads (databases, media processing, data analytics, ...)
    • HDD – broad spectrum of workloads (home directory, CMS, ...)

• Can be accessed from your on-premises infrastructure (VPN or Direct Connect)

• Can be configured to be Multi-AZ (high availability)

• Data is backed-up daily to S3



--------------------- 3 Amazon FSx for NetApp ONTAP

• Managed NetApp ONTAP on AWS

• File System compatible with NFS, SMB, iSCSI protocol

• Move workloads running on ONTAP or NAS to AWS

• Works with:
      • Linux
      • Windows
      • MacOS
      • VMware Cloud on AWS
      • Amazon Workspaces & AppStream 2.0
      • Amazon EC2, ECS and EKS


• Storage shrinks or grows automatically

• Snapshots,replication,low-cost,compression and data de-duplication

• Point-in-time instantaneous cloning (helpful for testing new workloads)


----- Scheduled replication using NetApp SnapMirror

-- You can use NetApp SnapMirror to schedule periodic replication of your FSx for ONTAP file system to or from a second file system. This capability is available for both in-Region and cross-Region deployments.

-- NetApp SnapMirror replicates data at high speeds, so you get high data availability and fast data replication across ONTAP systems, whether you're replicating between two Amazon FSx file systems in AWS, or from on-premises to AWS.

-- Replication can be scheduled as frequently as every 5 minutes, although intervals should be carefully chosen based on RPOs (Recovery Point Objectives), RTOs (Recovery Time Objectives), and performance considerations.

-- There are two types of SnapMirror replication: Volume-level SnapMirror and SVM Disaster Recovery (SVMDR). Only volume-level SnapMirror replication is supported by FSx for ONTAP.




----------------------- 4 Amazon FSx for OpenZFS

• Managed OpenZFS file system on AWS

• File System compatible with NFS (v3, v4, v4.1, v4.2)

• Move workloads running on ZFS to AWS

• Works with:
      • Linux
      • Windows
      • MacOS
      • VMwareCloudonAWS
      • AmazonWorkspaces&AppStream2.0 
      • AmazonEC2, ECS and EKS


• Up to 1,000,000 IOPS with < 0.5ms latency

• Snapshots, compression and low-cost

• Point-in-time instantaneous cloning (helpful for testing new workloads)










------------------------------------------------------ FSx for SysOps



• FSx for Windows – Single-AZ

        • Automatically replicates data within an AZ

        • Two generations: Single-AZ 1 (SSD), Single-AZ 2 (SSD & HDD)



• FSx for Windows – Multi-AZ

        • Automatically replicates data across AZs (synchronous)

        • Standby file server in a different AZ (automatic failover)





------------------------------------------------------ FSx – Solution Architecture Migration from Single AZ to Multi AZ






FSx for Windows File Server (Single AZ) ----------------------> AWS DataSync ----------------------> FSx for Windows File Server (Multi-AZ)




                                                Backup                          restore 

FSx for Windows File Server (Single AZ) ----------------------> Backup ----------------------> FSx for Windows File Server (Multi-AZ)





------------------------------------------------------ FSx – Solution Architecture Decrease FSx Volume Size




• If you take a backup, you can only restore to a same size

• You can only increase the amount of storage capacity for a file system; you cannot decrease storage capacity.

• Instead, create a new FSx (smaller), use DataSync to sync data and then migrate your app over




FSx for Window File Server (2TB) ----------------------> AWS DataSync ----------------------> FSx for Window File Server (1TB)



       ---------------------------------    Application ---------------------------------


                - So say you have a two terabytes FSx for Windows file server app file system, and it's going to be used by an application and you know you only use it say 500 gigabytes and you want to migrate to one terabyte.

                - Therefore, you will create that one terabyte FSx for Windows file server and then use DataSync to migrate the data over continuously.

                - then when you have reached parity you can just migrate your application to use the new FSx for Windows file server and stop using the old one.

        





------------------------------------------------------ FSx for Lustre – Data Lazy Loading



• Any data processing job on Lustre with S3 as an input data source can be started without Lustre doing a full download of the dataset first

• Data is lazy loaded: only the data that is actually processed is loaded, meaning you can decrease your costs and latency

• Data is also loaded only once, therefore you reduce your requests on Amazon S3





------------------------------------------------------- AWS DataSync


• Move large amount of data to and from

        • On-premises / other cloud to AWS (NFS, SMB, HDFS, S3 API...) – needs agent

        • AWS to AWS (different storage services) – no agent needed


• Can synchronize to:

        • Amazon S3 (any storage classes – including Glacier)

        • Amazon EFS

        • Amazon FSx (Windows, Lustre, NetApp, OpenZFS...)


• Replication tasks can be scheduled hourly, daily, weekly

• File permissions and metadata are preserved (NFS POSIX, SMB...)

• One agent task can use 10 Gbps, can setup a bandwidth limit



-- AWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the AWS DataSync API and Console, and Amazon CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. 

-- AWS DataSync performs data integrity verification both during the transfer and at the end of the transfer.







------------------------------------------------------- diff b/w storage gateway and data sync 



1 Description :

DataSync : AWS DataSync is an online data transfer service that simplifies, automates, and accelerates the process of copying large amounts of data to and from AWS storage services over the Internet or over AWS Direct Connect.

Storage Gateway : AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage by linking it to S3. Storage Gateway provides 3 types of storage interfaces for your on-premises applications: file, volume, and tape


2  How it Work ?

DataSync : Uses an agent which is a virtual machine (VM) that is owned by the user and is used to read or write data from your storage systems. You can activate the agent from the Management Console. The agent will then read from a source location, and sync your data to Amazon S3, Amazon EFS, or Amazon Fsx for Windows File Server.

Storage Gateway : Uses a Storage Gateway Appliance – a VM from Amazon – which is installed and hosted on your data center. After the setup, you can use the AWS console to provision your storage options: File Gateway, Cached Volumes, or Stored Volumes, in which data will be saved to Amazon S3.
- You can also purchase the hardware appliance to facilitate the transfer instead of installing the VM

3  Protocols 

DataSync :  AWS DataSync can copy data between Network File Systems (NFS), SMB file servers or self-managed object storages. It can also move data between your on-premises storage and AWS Snowcone, Amazon S3, Amazon EFS, or Amazon FSx,

Storage Gateway : File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as NFS and SMB.
- Volume Gateway stores your data locally in the gateway and syncs them to Amazon S3. It also allows you to take point-in-time copies of your volumes with EBS snapshots which you can restore and mount to your appliance as iSCSI device. 
- Tape Gateway data is immediately stored in Amazon S3 and can be archived to Amazon S3 Glacier or Amazon S3 Glacier Deep Archive.

4  Pricing 

DataSync : You are charged standard request, storage, and data transfer rates to read from and write to AWS services, such as Amazon S3, Amazon EFS, AmazonFSx for Windows File Server, and AWS KMS.

Storage Gateway :  You are charged based on the type and amount of storage you use, the requests you make, and the amount of data transferred out of AWS.


combination : You can use a combination of DataSync and File Gateway to minimize your on-premises’ operational costs while seamlessly connecting on-premises applications to your cloud storage. AWS DataSync enables you to automate and accelerate online data transfers to AWS storage services. File Gateway then provides your on-premises applications with low latency access to the migrated data.






------------------------------------------------------- AWS Data Exchange



• Find, subscribe to, and use third-party data in the cloud

        • Reuters, who curate data from over 2.2 million unique news stories per year in multiple languages

        • Change Healthcare, who process and anonymize more than 14 billion healthcare transactions and $1 trillion in claims annually

        • Dun & Bradstreet, who maintain a database of more than 330 million global business records.

        • Foursquare, whose location data is derived from 220 million unique consumers and includes more than 60 million global commercial venues.


• Once subscribed to a data product, you can use the AWS Data Exchange API to load data directly into Amazon S3 and then analyze it with a wide variety of AWS analytics and machine learning services







------------------------------------------------------- AWS Data Exchange – Other Products



• AWS Data Exchange for Redshift

        • Find and subscribe to third-party data in AWS Data Exchange that you can query in an Amazon Redshift data warehouse in minutes

        • Easily license your data in Amazon Redshift through AWS Data Exchange


• AWS Data Exchange for APIs

        • Find and subscribe to third-party APIs with a consistent access using AWS SDKs

        • Consistent AWS-native authentication and governance






------------------------------------------------------- AWS Transfer Family





-- Fully managed Service for file transer securely 

-- A fully-managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol

-- Supported Protocols


        • AWS Transfer for FTP (File Transfer Protocol (FTP))
        • AWS Transfer for FTPS (File Transfer Protocol over SSL (FTPS)) 
        • AWS Transfer for SFTP (Secure File Transfer Protocol (SFTP))

-- Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)

-- Pay per provisioned endpoint per hour + data transfers in GB

-- Usage: sharing files, public datasets, CRM, ERP, ...

-- data can be transferred in and out of s3 buckets and EFS 

-- generally we are uploaing our files in S3 through the console  but we are doing through the http , 

-- now w r going to transfer files securely from ur laptop 

-- for this we have to install software called "FileZilla" which is used to transfer fis very securley 

-- b/w ur laptop and s3 or EFS we have to create One "Transfer Server" , this server allows us to do  SFTP,FTP,FTPS, we can choose any one , and we hav to create one user in Transfer server 

-- in FileZilla u can login as with user name 

-- once u create server it will give u endpoint 








-------------------------- Hands ON 

-- create public bucket 

-- create one bucket policy through policy generator 

eg :   

{
  "Id": "Policy1704630072890",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1704629976563",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket",
        "s3:PutObject"
      ],
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::transfer-dem",
                       "arn:aws:s3:::transfer-dem/*" ],
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "49.205.119.80/32"
        }
      },
      "Principal": "*"
    }
  ]
}

-- transfer-dem = name 

-- now create transfer server 

-- open aws transfer family in console 

-- create server , selete SFTP --> Srvice managed --> select as per ur requriemnets 

--now add user in server 

-- select server c.o add user --> create one role in IAM --> truste entity = ec2 --> use case = Transfer (imp) --> give s3 full access --> create role 

-- go to puttyGen n create SSH keys(mouseover)  and save private key with .ppk 

-- now download Filezilla 

-- open filezilla --File(top) --> site manager --> new site --> copy end point of server --> selet SFTP n paste endpoint of server and give port number as 22--> logon type : key file --> give ur private file u downoaded -- connect 


-- once u coonect u jst drag do upload ur files u want and go check in s3 in console --- getting files 

-- this is how u can transfer files throug SFTP from ur lap to s3 or EFS 








------------------------------------------------------- Price of Storage (GB Month)



1 s3


Class                   Price

Standard                $0.023

Standard IA             $0.0125

One-Zone IA             $0.01

Intelligent Tiering     $0.023

Glacier Instant Retrieval       $0.004

Glacier Flexible Retrieval      $0.0036

Glacier Deep Arcvhive           $0.00099



2 EBS (GB/Month)


Class                   Price


io1             $0.125 + $0.065/iops

io1             $0.125 + $0.065/iops
io2             $0.125 + $0.065/iops
gp2             $0.10
gp3             $0.08
st1             $0.045
sc1             $0.015



EFS (GB/Month)


Class                   Price

Standard                $0.30
One-Zone                $0.16
Standard IA             $0.025
One-Zone IA             $0.0133





expensive : EBS io2, io1   EBS gp2, gp3 , EFS Standard IA S3 Standard , EFS One-Zone IA

cheap : EFS Standard, EFS One-Zone , EBS st1 , EBS sc1, S3 Standard IA, etc...








================================================================ Caching ================================================================



------------------------------------------------------- check for cloudfront , CloudFront Functions & Lambda@Edge, Amazon ElastiCache in previous course 




------------------------------------------------------- Handling Extreme Rates


check in course for details 







================================================================ Databases ================================================================




check for Dynamodb and opensearch,RDS and  aurora in previous course 



------------------------------------------------------- Global Aurora



• Aurora Cross Region Read Replicas

        • Useful for disaster recovery

        • Simple to put in place


• Aurora Global Database (recommended)

        • 1 Primary Region (read / write)

        • Up to 5 secondary (read-only) regions, replication lag is less than 1 second

        • Up to 16 Read Replicas per secondary region

        • Helps for decreasing latency

        • Promoting another region (for disaster recovery) has an RTO of < 1 minute

        • Ability to manage the RPO in Aurora for PostgreSQL



Aurora Global – Write Forwarding

        • Enables Secondary DB Clusters to forward SQL statements that perform write operations to the Primary DB Cluster

        • Data is always changed first on the Primary DB Cluster, then replicated to the Secondary DB Clusters

        • Primary DB Cluster always has an up-to-date copy of all data

        • Reduces the number of endpoints to manage







------------------------------------------------------- Convert RDS to Aurora


                        snapshot                                restore

RDS DB Instance ----------------------------> DB Snapshot ----------------------------> Aurora DB Instance




                        New replica                                     promote
RDS DB Instance ----------------------------> Aurora Read Replica ----------------------------> Aurora DB Instance 








================================================================ Service Communication ================================================================





check step functions , SQS , Amazon MQ , SNS , Amazon SNS - SQS Fan Out Pattern in previous course 




------------------------------------------------------- SNS – Message Delivery Retries




• When a message is delivered to an SNS subscriber, in case of server-side errors, a delivery policy is applied

        - So, it means if the endpoint that SNS is calling is having issues.


check in course for more details 




------------------------------------------------------- SNS – Custom Delivery Policies


• Only HTTP/S supports custom policies

        - In case you have an HTP endpoint that you set up as a subscriber to your SNS topic. Then you have support for custom policies.




- So, you may want to say what is the number of retry, the min delay, the max delay the backoff function, the throttle policy, and so on to really customize how SNS will retry for your backend because your backend may not like heavy load or a heavy number of retries.


{
    "http": {
        "defaultHealthyRetryPolicy": {
            "numRetries": 3,
            "minDelayTarget": 20,
            "maxDelayTarget": 20,
            "numNoDelayRetries": 0,
            "numMinDelayRetries": 0,
            "numMaxDelayRetries": 3,
            "backoffFunction": "Exponential"
        },
        "disableSubscriptionOverrides": false
    }
}





------------------------------------------------------- SNS – Dead Letter Queues



• After exhausting the delivery policy, messages that haven’t been delivered are discarded unless you set a DLQ (Dead Letter Queue)

• DLQ are Amazon SQS queues or Amazon SQS FIFO queues (for SNS FIFO)

• Dead Letter Queues are attached to a subscription (rather than a topic)









================================================================== Data Engineering =============================================================================




check kinesis data streams , Kinesis data Firehose , Kinesis data analytics , Amazon Msk , AWS Glue , Redshift Overview, amazon Athena , Amazon QuickSight , in previous courses





------------------------------------------------------- Streaming Architectures 3000 messages of 1 KB per second



Kinesis

        • 3 shards: 3MB/s in

        • 3 * $0.015/hr = $32.4/mth

        • Must use KDF for output to S3


DynamoDB + Streams

        • 3000 WCU = 3 MB/s

        • = $1,450.90 / month

        • Storage in DynamoDB



--  check in course  for "Comparison Charts" 







------------------------------------------------------- AWS Batch


• Run batch jobs as Docker images

• Two options:

        1. Run on AWS Fargate (fully serverless offering)

        2. Dynamic provisioning of the instances (EC2 & Spot Instances) – in VPC


• Optimal quantity and type based on volume and requirements

• No need to manage clusters, fully serverless

• You just pay for the underlying resources used

• Example: batch process of images, running thousands of concurrent jobs

• Schedule Batch Jobs using Amazon EventBridge

• Orchestrate Batch Jobs using AWS Step Functions



• Fully managed batch processing at any scale

• Efficiently run 100,000s of computing batch jobs on AWS

• A “batch” job is a job with a start and an end (opposed to continuous)

• Batch will dynamically launch EC2 instances or Spot Instances

• AWS Batch provisions the right amount of compute / memory

• You submit or schedule batch jobs and AWS Batch does the rest!

• Batch jobs are defined as Docker images and run on ECS

• Helpful for cost optimizations and focusing less on the infrastructure


---------------- Batch vs Lambda

• Lambda:
  • Time limit
  • Limited runtimes
  • Limited temporary disk space • Serverless


• Batch:
 • No time limit
 • Any runtime as long as it’s packaged as a Docker image
 • Rely on EBS / instance store for disk space
 • Relies on EC2 (can be managed by AWS)





------------------------------------------------------- AWS Batch – Compute Environments


• Managed Compute Environment:

        • AWS Batch managed the capacity and instance types within the environment 
        
        • You can choose EC2 On-Demand or Spot Instances

        • You can choose Fargate On-Demand or Fargate Spot Instances

        • You can set a maximum price for Spot Instances

        • Launched within your own VPC

                • If you launch within your own private subnet, make sure it has access to the ECS service

                • Either using a NAT gateway / instance or using VPC Endpoints for ECS


• Unmanaged Compute Environment

        • You control and manage EC2 instance configuration, provisioning and scaling





------------------------------------------------------- AWS Batch – Multi Node Mode


• Multi Node: large scale, good for HPC (high performance computing)

        • Leverages multiple EC2 / ECS instances at the same time

        • Good for tightly coupled workloads

        • Represents a single job, and specified how many nodes to create for the job

        • 1 main node, and many child node.

        • Does not work with Spot Instances

        • Works better if your EC2 launch mode is a placement group ”cluster”




------------------------------------------------------- Amazon EMR



• EMR stands for “Elastic MapReduce”

• EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data

• The clusters can be made of hundreds of EC2 instances

• EMR comes bundled with Apache Spark, HBase, Presto, Flink...

• EMR takes care of all the provisioning and configuration

• Auto-scaling and integrated with Spot instances

• Use cases: data processing, machine learning, web indexing, big data...


-- it is not serverless service , u have to provision EMR and then create EMR jobs



--------------------------------------------- Amazon EMR – Node types & purchasing



• Master Node: Manage the cluster, coordinate, manage health – long running

• Core Node: Run tasks and store data – long running

• Task Node (optional): Just to run tasks – usually Spot

• Purchasing options:
   • On-demand: reliable, predictable, won’t be terminated
   • Reserved (min 1 year): cost savings (EMR will automatically use if available) 
   • Spot Instances: cheaper, can be terminated, less reliable

• Can have long-running cluster, or transient (temporary) cluster





--------------------------------------------- Amazon EMR – Instance Configuration



• Uniform instance groups:

        - select a single instance type and purchasing option for each node (has auto scaling)


• Instance fleet:

        - select target capacity, mix instance types and purchasing options (no Auto Scaling)







------------------------------------------------------- Running Jobs on AWS


Strategy 1: Provision EC2 instance (long running - CRON jobs)

Strategy 2: Amazon EventBridge + Lambda (cron)

Strategy 3: Reactive Workflow

Strategy 4: use AWS Batch

Strategy 5: use Fargate

Strategy 6: Use EMR (step execution or cluster)





-------------------------------------------------------  DocumentDB



• Aurora is an “AWS-implementation” of PostgreSQL / MySQL ...

• DocumentDB is the same for MongoDB (which is a NoSQL database)

• MongoDB is used to store, query, and index JSON data

• Similar “deployment concepts” as Aurora

• Fully Managed, highly available with replication across 3 AZ

• DocumentDB storage automatically grows in increments of 10GB

• Automatically scales to workloads with millions of requests per seconds



------------------------------------------------------- DocumentDB – Pricing


• Pay for what you use, no upfront costs

• On-demand Instances (per second with minimum of 10 minutes)

• Database I/O – amount of I/O used when read and write (per million I/Os)

• Database Storage (per GB/month)

• Backup Storage (per GB/month)





-------------------------------------------------------  Amazon Timestream



• Fully managed, fast, scalable, serverless time series database
• Automatically scales up/down to adjust capacity
• Store and analyze trillions of events per day
• 1000s times faster & 1/10th the cost of relational databases
• Scheduled queries, multi-measure records, SQL compatibility
• Data storage tiering: recent data kept in memory and historical data kept in a cost-optimized storage
• Built-in time series analytics functions (helps you identify patterns in your data in near real-time)
• Encryption in transit and at rest

• Use cases: IoT apps, operational applications, real-time analytics, 







------------------------------------------------------- Comparison of warehousing technologies



• EMR

        • Need to use Big Data tools such as Apache Hive, Spark

        • One long-running cluster, many jobs, with auto-scaling, or one cluster per job?

        • Purchasing options – Spot, On Demand, Reserved Instances

        • Can access data in DynamoDB and / or S3

        • Scratch data on EBS disks (HDFS) and long term storage in S3 (EMRFS)


• Athena

        • Simple queries and aggregations, data must live in S3

        • Serverless, simple SQL queries, out-of-the-box queries for many services (cost & billing..)

        • Audit queries through CloudTrail



• Redshift

        • Advanced SQL queries, must provision servers

        • Can leverage Redshift Spectrum for serverless queries on S3









======================================================================== Monitoring ========================================================================



check CloudWatch, CloudWatch Logs , Amazon EventBridge , X-ray, Aws personal health DashBoard in previous course 






======================================================================== Deployment & Instance Management ========================================================================




check Elasticbeanstalk , codedeploy,CF,Service catalog, SAm, AWS CDK,AWS SSM , in previous course 




------------------------------------------------------- AWS Cloud Map



• A fully managed resource discovery service

• Creates a map of the backend services/resources that your applications depend on

• You register your application components, their locations, attributes, and health status with AWS Cloud Map

• Integrated health checking (stop sending traffic to unhealthy endpoints)

• Your applications can query AWS Cloud Map using AWS SDK, API, or DNS








======================================================================== Cost Control ========================================================================




AWS Cost Allocation Tags , AWS Tag Editor, Trusted Advisor, AWS Service Quotas, EC2 Instance LaunchTypes,AWS Savings Plan, S3 Storage Classes, AWS Budgets, Cost Explorer, AWS Compute Optimizer in previous course 



------------- AWS Support Plans - check in course 



------------------------------------------------------- Trusted Advisor – Good to know



• Can check if an S3 bucket is made public

        • But cannot check for S3 objects that are public inside of your bucket!

        • Use Amazon EventBridge / S3 Events instead / AWS Config Rules


• Service Limits

        • Limits can only be monitored in Trusted Advisor (cannot be changed)

        • Cases must be created manually in AWS Support Centre to increase limits

        • OR use the AWS Service Quotas ser vice








======================================================================== Migrations ========================================================================
 




------------------------------------------------------- Cloud Migration Strategies:The 7Rs



• Retire

        • Turn off things you don’t need (maybe as a result of Re-architecting)

        • Helps with reducing the surface areas for attacks (more security)

        • Save cost, maybe up to 10% to 20%

        • Focus your attention on resources that must be maintained


• Retain

        • Do nothing for now (it’s still a decision to make in a Cloud Migration)

        • Security, data compliance, performance, unresolved dependencies

        • No business value to migrate, mainframe or mid-range and non-x86 Unix apps



• Relocate

        • Move apps from on-premises to its Cloud version

        • Move EC2 instances to a different VPC, AWS account or AWS Region

        • Example: transfer servers from VMware Software-defined Data Center (SSDC) to VMware Cloud on AWS


• Rehost “lift and shift”

        • Simple migrations by re-hosting on AWS (applications, databases, data...)

        • Migrate machines (physical, virtual, another Cloud) to AWS Cloud

        • No cloud optimizations being done, applications is migrated as is

        • Could save as much as 30% on cost

        • Example: Migrate using AWS Application Migration Service



• Replatform “lift and reshape”

        • Example: migrate your database to RDS

        • Example: migrate your application to Elastic Beanstalk

        • Not changing the core architecture, but leverage some Cloud optimizations

        • Save time and money by moving to a fully managed service or Serverless


• Repurchase “drop and shop”

        • Moving to a different product while moving to the Cloud

        • Often you move to a SaaS platform

        • Expensive in the short term, but quick to deploy

        • Example: CRM to Salesforce.com, HR to Workday, CMS to Drupal



• Refactor / Re-architect

        • Reimagining how the application is architected using Cloud Native features

        • Driven by the need of the business to add features and improve scalability, performance, security, and agility

        • Move from a monolithic application to micro-services

        • Example: move an application to Serverless architectures, use AWS S3






------------------------------------------------------- AWS Storage Gateway



check in previous course




------------------------------------------------------- AWS Storage Gateway advanced


check i course 





------------------------------------------------------- AWS Snow Family 


check in previous 




------------------------------------------------------- Snow Family – Improving Transfer Performance


• Most impactful to least:

        • Perform multiple write operations at one time - from multiple terminals

        • Transfer small files in batches – zip up small files until at least 1MB

        • Don't perform other operations on files during transfer

        • Reduce local network use

        • Eliminate unnecessary hops – directly connect to the computer


• The data transfer rate using the file interface is typically between 25 MB/s and 40 MB/s. If you need to transfer data faster than this, use the Amazon S3 Adapter for Snowball, which has a data transfer rate typically between 250 MB/s and 400 MB/s






------------------------------------------------------- DMS – Database Migration Service


check in previous 





------------------------------------------------------- DMS – Good things to know


• Works over VPC Peering,VPN (site to site, software), Direct Connect

• Supports Full Load, Full Load + CDC, or CDC only

• Oracle:

        • Source: Suppor ts TDE for the source using “BinaryReader”

        • Target: Supports BLOBs in tables that have a primary key, and TDE


• OpenSearch:

        • Source: does not exist

        • Target: possible to migrate from a relational database using DMS

        • Therefore, DMS cannot be used to replicate OpenSearch data





------------------------------------------------------- Snowball + Database Migration Service (DMS)


• Larger data migrations can include many terabytes of information.

• Can be limited due to network bandwidth or size of data

• AWS DMS can use Snowball Edge & Amazon S3 to speed up migration

• Following stages:

        1. You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.

        2. You ship the Edge device or devices back to AWS.

        3. After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.

        4. AWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store.






------------------------------------------------------- AWS Cloud Adoption Readiness Tool (CART)




• Helps organizations develop efficient and effective plans for cloud adoption and migrations

• Transforms your idea of moving to the cloud into a detailed plan that follows AWS best practices

• Answer a set of questions across six perspectives (business, people, process, platform, operations, security)

• Generates a custom report on your level of migration readiness





------------------------------------------------------- Disaster Recovery Overview


check in previous course 




------------------------------------------------------- AWS Fault Injection Simulator (FIS)



• A fully managed service for running fault injection experiments on AWS workloads

• Based on Chaos Engineering – stressing an application by creating disruptive events (e.g., sudden increase in CPU or memory), observing how the system responds, and implementing improvements

• Helps you uncover hidden bugs and performance bottlenecks

• Supports the following AWS services: EC2, ECS, EKS, RDS...

• Use pre-built templates that generate the desired disruptions

- So FIS is definitely an advanced kind of monitoring and advanced kind of debugging, but it's definitely helpful. And I'm so glad that AWS is now having this as a native feature from within AWS.






------------------------------------------------------- VM Migration Services 


- here is a quick overview of the different services you can use on AWS to help you with your on-premise migration.





1 AWS Application Discovery Service

        • Plan migration projects by gathering information about on-premises data centers

        • It will track some stuff, such as the Server utilization data and dependency mapping are important for migrations

        • Agentless Discovery (AWS Agentless Discovery Connector)

                • Open Virtual Appliance(OVA)package that can be deployed to a VM warehost

                • VM inventory,configuration,and performance history such as CPU,memory,and diskusage

                • OS agnostic , It is going to work for any kind of operating system.

        • Agent-based Discovery (AWS Application Discovery Agent):

                • System configuration, system performance, running processes, and details of the network connections between systems

                • Supports Microsoft Server,Amazon Linux,Ubuntu,RedHat,CentOS,SUSE...

        • Resulting data can be exported as CSV or viewed within AWS Migration Hub

        • Data can be explorer using pre-defined queries in Amazon Athena





------------------------------------------------------- AWS Application Discovery Service – Migration Hub Data Exploration



• Allows you to use Amazon Athena to analyze data collected from on-premises servers during discovery

• Data is automatically stored in S3 bucket at regular inter vals

• Use Pre-defined or custom queries in Amazon Athena to analyze data

• Example: type of processes running on each server

• Ability to upload additional data sources such as Configuration Management Database (CMDB) exports

• Integrate Athena with QuickSight to visualize data





------------------------------------------------------- AWS Application Migration Service (MGN)


• The “AWS evolution” of CloudEndure Migration, replacing AWS Server Migration Service (SMS)

• Lift-and-shift (rehost) solution which simplify migrating applications to AWS

• Converts your physical, virtual, and cloud-based servers to run natively on AWS

• Supports wide range of platforms, Operating Systems, and databases

• Minimal downtime, reduced costs




------------------------------------------------------- AWS Elastic Disaster Recovery (DRS)



• Used to be named “CloudEndure Disaster Recovery”

• Quickly and easily recover your physical, virtual, and cloud-based servers into AWS

• Example: protect your most critical databases (including Oracle, MySQL, and SQL Server), enterprise apps (SAP), protect your data from ransomware attacks, ...

• Continuous block-level replication for your servers




------------------------------------------------------- On-premises strategy with AWS


• Ability to download Amazon Linux2 AMI as a VM(.isoformat)

        • VMWare, KVM,VirtualBox (OracleVM), Microsoft Hyper-V


• AWS Application Discover y Ser vice

        • Gather information about your on-premises servers to plan a migration

        • Ser ver utilization and dependency mappings

        • Track with AWS Migration Hub


• AWS Application Migration Service(MGN)

        • Replacing AWS Server Migration Services & CloudEndure Migration

        • Incremental replication of on-premises live servers to AWS

        • Migrates the entire VM into AWS


• AWS Elastic Disaster Recovery (DRS)

        • Replacing CloudEndure Disaster Recovery

        • Recover on-premises workloads onto AWS


• AWS Database Migration Service (DMS)

        • replicate on-premises => AWS , AWS => AWS, AWS => on-premises

        • Works with various database technologies (Oracle, MySQL, DynamoDB, etc..)







------------------------------------------------------- AWS Migration Evaluator



• Helps you build a data-driven business case for migration to AWS

• Provides a clear baseline of what your organization is running today

• Install Agentless Collector to conduct broad-based discovery



------------------------------------------------------- AWS Backup

check in previous 






=============================================================== Amazon VPC ===============================================================



check in previous couse for detail notes for WHole VPC topic 






------------------------------------------------------- VPC Flow Logs – with NAT Gateway



• My Virtual Private Cloud (VPC) flow logs show Action = ACCEPT for inbound traffic coming from public IP addresses. However, my understanding of network address translation (NAT) gateways was that they don't accept traffic from the internet. Is my NAT gateway accepting inbound traffic from the internet?


• Inbound traffic is permitted by Security Group or NACLs

        • Traffis isn’t permitted by the NAT Gateway, it’s dropped

        • To confirm run the following query in CloudWatch Log Group


                filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP’)
                | stats sum(bytes) as bytesTransferred by srcAddr, dstAddr | limit 10

• Make ‘xxx.xxx’ the first two octets of your VPC CIDR

• Replace Public IP with the IP you see in logs

• You will see traffic on the Private IP of the NAT Gateway but nowhere else: traffic was unsolicited and then dropped




------------------------------------------------------- AWS Network Firewall


check in previous 







========================================================== Machine Learning================================================




----------------------------- 1 Amazon Rekognition

• Find objects, people, text, scenes in images and videos using ML

• Facial analysis and facial search to do user verification, people counting

• Create a database of “familiar faces” or compare against celebrities

• Use cases:
 • Labeling
 • Content Moderation
 • Text Detection
 • Face Detection and Analysis (gender, age range, emotions...)
 • Face Search and Verification
 • Celebrity Recognition
 • Pathing (ex: for sports game analysis)


 ---------- Amazon Rekognition – Content Moderation

 • Detect content that is inappropriate, unwanted, or offensive (image and videos)

 • Used in social media, broadcast media, advertising, and e-commerce situations to create a safer user experience

 • Set a Minimum Confidence Threshold for items that will be flagged

 • Flag sensitive content for manual review in Amazon Augmented AI (A2I)

 • Help comply with regulations


 Image --> Amazon Rekognition --> Confidence Level and Threshold --> Optional Manual review in A2I



 ------------------------------ 2 Amazon Transcribe

 • Automatically convert speech to text

 • Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately

 • Automatically remove Personally Identifiable Information (PII) using Redaction

 • Supports Automatic Language Identification for multi-lingual audio

 • Use cases:
   • transcribe customer service calls
   • automate closed captioning and subtitling
   • generate metadata for media assets to create a fully searchable archive

------------------ LAB 

-- create one s3 bucket and upload audio or video file in it 

-- open transcribe --> create job and copy URI of object and paste in the bucket path --> create job in transribe 

- once it get completed it will convert speech to text 

-- go n check 

----- working ......


---------------------------------------------- 3 Amazon Polly

• Turn text into life like speech using deep learning

• Allowing you to create applications that talk

--------------- Amazon Polly – Lexicon & SSML

• Customize the pronunciation of words with Pronunciation lexicons
  • Stylized words: Su3b4u=> “subbu”
  • Acronyms:AWS=>“AmazonWebServices”

• Upload the lexicons and use them in the SynthesizeSpeech operation

• Generate speech from plain text or from documents marked up with Speech Synthesis Markup Language (SSML) – enables more customization

 • emphasizing specific words or phrases
 • using phonetic pronunciation
 • including breathing sounds, whispering
 • using the Newscaster speaking style


------------LAB 

-- open polly , type text u want n click on listen 

-- turn on SSML , try to add break point in the middle of the text 

eg : <speak>Hi! My name is subbu <break time="2s" /> I will read any text you type here.</speak>

-- it will stop 2 sec and continue 

--- now if u have text AWS and it would like to convert Abbrevations like Amazon web service

-- create one .xml file and add content to it like 

<?xml version="1.0" encoding="UTF-8"?>
 <lexicon version="1.0" 
     xmlns="http://www.w3.org/2005/01/pronunciation-lexicon"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
     xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon 
       http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd"
     alphabet="x-sampa" xml:lang="en-US">

<lexeme>
<grapheme>AWS</grapheme>
<alias>Amazon Web Service</alias>
</lexeme>

 </lexicon>


 -- here the language is very imp , u have to give same as ur polly language like en-US

 -- so u have AWS in text but once u upload lex file in .xml format it will read amazon web service instead of AWS

 -- additional settings --> add lex.xml file 



 ----------------------------------- 4 Amazon Translate


 • Natural and accurate language translation

 • Amazon Translate allows you to localize content - such as websites and applications - for international users, and to easily translate large volumes of text efficiently.



 -------------------------------------- 5 Amazon Lex & Connect

• Amazon Lex: (same technology that powers Alexa)
  • Automatic Speech Recognition (ASR) to convert speech to text
  • Natural Language Understanding to recognize the intent of text, callers 
  • Helps build chatbots, call center bots

• Amazon Connect:
 • Receive calls, create contact flows, cloud-based vir tual contact center
 • Can integrate with other CRM systems or AWS
 • No upfront payments, 80% cheaper than traditional contact center solutions

Phone Call Schedule an Appointment ----(call)---> Connect ------(stream)----> lex Intent recognized -----(invoke)---> lambda ----(schedule)----> CRM


--------------------------- 6 Amazon Comprehend

• For Natural Language Processing – NLP

• Fully managed and serverless service

• Uses machine learning to find insights and relationships in text
 • Language of the text
 • Extractskeyphrases,places,people,brands,orevents
 • Understands how positive or negative the text is
 • Analyzes text using tokenization and parts of speech
 • Automatically organizes a collection of text files by topic 

• Sample use cases:
  • analyze customer interactions (emails) to find what leads to a positive or negative experience
  • Create and groups articles by topics that Comprehend will uncover


----------- Amazon Comprehend Medical

• Amazon Comprehend Medical detects and returns useful information in unstructured clinical text:

 • Physician’s notes
 • Discharge summaries • Test results
 • Case notes 

• Uses NLP to detect Protected Health Information (PHI) – DetectPHI API

• Store your documents in Amazon S3, analyze real-time data with Kinesis Data Firehose, or use Amazon Transcribe to transcribe patient narratives into text that can be analyzed by Amazon Comprehend Medical.


------------------------------------------------ 7 Amazon SageMaker

• Fully managed service for developers / data scientists to build ML models 

• Typically, difficult to do all the processes in one place + provision servers 

• Machine learning process (simplified): predicting your exam score

Historical Data:
# years of experience in IT                                -----------> score ------> ML model <----Train and Tune
# years of experience with AWS Time spent on the course
...



--------------------------------------- 8 Amazon Forecast

• Fully managed service that uses ML to deliver highly accurate forecasts • Example: predict the future sales of a raincoat

• 50% more accurate than looking at the data itself

• Reduce forecasting time from months to hours

• Use cases: Product Demand Planning, Financial Planning, Resource Planning, ...



Historical Time-series Data: ---> Amazon S3 ---> Amazon Forecast -----(produce)--> Forecasting Model --> Future sales of raincoat: $500,000


------------------------------------- 9 Amazon Kendra

• Fully managed document search service powered by Machine Learning

• Extract answers from within a document (text, pdf, HTML, PowerPoint, MS Word, FAQs...) 

• Natural language search capabilities

• Learn from user interactions/feedback to promote preferred results (Incremental Learning) 

• Ability to manually fine-tune search results (importance of data, freshness, custom, ...)



------------------------------------- 10 Amazon Personalize

• Fully managed ML-service to build apps with real-time personalized recommendations

• Example: personalized product recommendations/re-ranking, customized direct marketing

 • Example:User bought gardening tools,provide recommendations on the next one to buy



------------------------------------- 11 AmazonTextract

• Automatically extracts text, handwriting, and data from any scanned documents using AI and ML

• Extract data from forms and tables

• Read and process any type of document (PDFs, images, ...)

• Use cases:

 • Financial Services (e.g., invoices, financial reports)
 • Healthcare (e.g., medical records, insurance claims)
 • Public Sector (e.g., tax forms, ID documents, passports)


------------------------------------ AWS Machine Learning - Summary

• Rekognition: face detection, labeling, celebrity recognition • Transcribe: audio to text (ex: subtitles)

• Polly: text to audio

• Translate: translations

• Lex: build conversational bots – chatbots

• Connect: cloud contact center

• Comprehend: natural language processing

• SageMaker: machine learning for every developer and data scientist • Forecast: build highly accurate forecasts

• Kendra: ML-powered search engine

• Personalize: real-time personalized recommendations • Textract: detect text and data in documents








==================================================================== Other Services ====================================================================


check CI/CD in previous 



------------------------------------------------------- CodeCommit Trigger for AWS Lambda


• Every push to CodeCommit can trigger a Lambda function

• The Lambda function can scan for leaked AWS credentials on every code push, and disable them automatically to remedy the issue



------------------------------------------------------- Good to know – CICD


• You can use a manual approval stage in CodePipeline

• Running unit tests CodeCommit + CodeBuild + Code Pipeline

• Build and Store Docker Images: CodeBuild + ECR

• Automated CloudFormation deployment: CodePipeline







----------------------------------------------------------------- Amazon CodeGuru -----------------



• An ML-powered service used for "automated code reviews" and "application performance recommendations"

• Provides two functionalities

     • CodeGuru Reviewer: automated code reviews for static code analysis (development)

     • CodeGuru Profiler: visibility/recommendations about application performance during runtime (production)





 
----------------------------------------------------------------- Amazon CodeGuru Reviewer


• Identify critical issues, security vulnerabilities, and hard-to-find bugs

• Example: common coding best practices, resource leaks, security detection, input validation

• Uses Machine Learning and automated reasoning

• Hard-learned lessons across millions of code reviews on 1000s of open-source and Amazon repositories

• Supports Java and Python

• Integrates with GitHub, Bitbucket, and AWS CodeCommit





----------------------------------------------------------------- Amazon CodeGuru Profiler


• Helps understand the runtime behavior of your application

• Example: identify if your application is consuming excessive CPU capacity on a logging routine

• Features:

     • Identify and remove code inefficiencies

     • Improve application performance (e.g.,reduceCPU utilization)

     • Decrease compute costs

     • Provides heap summary (identify which objects using up memory)

     • Anomaly Detection


• Support applications running on AWS or on- premise

• Minimal overhead on application





----------------------------------------------------------------- Amazon CodeGuru – Agent Configuration


• MaxStackDepth – the maximum depth of the stacks in the code that is represented in the profile

      • Example: if CodeGuru Profiler finds a method A, which calls method B, which calls method C, which calls method D, then the depth is 4

      • If the MaxStackDepth is set to 2, then the profiler evaluates A and B


• MemoryUsageLimitPercent – the memory percentage used by the profiler

• MinimumTimeForReportingInMilliseconds – the minimum time between sending reports (milliseconds)

• ReportingIntervalInMilliseconds – the reporting interval used to report profiles (milliseconds)

• SamplingIntervalInMilliseconds – the sampling interval that is used to profile samples (milliseconds) 

      • Reduce to have a higher sampling rate






----------------------------------------------------------------- Alexa for Business, Lex & Connect



• Alexa for Business:

        • Use Alexa to help employees be more productive in meeting rooms and their desk

        • Measure and increase the utilization of meeting rooms in their workplace


• Amazon Lex: (same technology that powers Alexa)

        • Automatic Speech Recognition (ASR) to convert speech to text

        • Natural Language Understanding to recognize the intent of text, callers

        • Helps build chatbots, call center bots



• Amazon Connect:

        • Receive calls, create contact flows, cloud-based virtual contact center

        • Can integrate with other CRM systems or AWS



                                        call            Stream                                  invoke           schedule

Phone Call Schedule an Appointment ----------> Connect ----------> Lex (Intent recognized) ------------> Lambda -------------> CRM







- So say someone wants to ^schedule an appointment, and it places a ^phone call.

- The call is going to be received by Amazon ^connect.^

- Amazon Connect will stream that call into Amazon ^Lex^ to understand the ^intent^ of it.

- So Amazon Lex will look at the speech in will use the automatic speech recognition technology to understand what is the call about.

- Then, once it has understood what the call is about, you are going to invoke a ^lambda^ function, and that lambda function is going to insert, maybe into your ^CRM,^ a new appointment, based on the intent it received from Lex.








----------------------------------------------------------------- Kinesis Video Streams


• One video stream per streaming device (producers)

        • Security cameras, body worn camera, smartphone

        • Can use a KinesisVideo Streams Producer library


• Underlying data is stored in S3 (but we don’t have access to it)

• Cannot output the stream data to S3 (must build custom solution)

• Consumers:

        • Consumed by EC2 instances for real time analysis, or in batch

        • Can leverage the KinesisVideo Stream Parser Library

        • Integration with AWS Rekognition for facial detection


- remember that you can use Kinesis Video Streams in combination with Rekognition to extract some metadata out of your stream and compare that against a database of face collection that's going to be internal to rekognition.





----------------------------------------------------------------- Amazon WorkSpaces


• Managed, Secure Cloud Desktop

• Great to eliminate management of on-premisesVDI (Virtual Desktop Infrastructure)

• Pricing is either on-demand (pay per hour) or monthly subscription

• Secure, Encrypted, Network Isolation

• Integrated with Microsoft Active Directory




• WorkSpaces Application Manager (WAM)

                • Deploy and Manage applications as virtualized application containers

                • Provision at scale, and keep the applications updated using WAM


• Windows Updates

        • By default, Amazon Workspaces are configured to install software updates

        • Amazon WorkSpaces with Windows will have Windows Update turned on

        • You have full control over the Windows Update frequency


• Maintenance Windows

        • Updates are installed during maintenance windows (you define them)

        • Always On WorkSpaces: default is from 00h00 to 04h00 on Sunday morning

        • AutoStop WorkSpaces: automatically starts once a month to install updates

        • Manual maintenance: you define your windows and perform maintenance






----------------------------------------------------------------- Amazon WorkSpaces IP Access Control Groups



• Similar to security groups for Amazon WorkSpaces

• List of IP addresses / CIDR address ranges that users are authorized to connect from

• If users access WorkSpaces through VPN or NAT, the IP Access Control Group must authorize the public IP of these





----------------------------------------------------------------- Amazon AppStream 2.0


• Desktop Application Streaming Service

• Deliver to any computer, without acquiring, provisioning infrastructure

• The application is delivered from within a web browser





----------------------------------------------------------------- Amazon AppStream 2.0 vs WorkSpaces



• Workspaces

        • Fully managed VDI and desktop available

        • The users connect to the VDI and open native or WAM applications

        • Workspaces are on-demand or always on


• AppStream 2.0

        • Stream a desktop application to web browsers (no need to connect to aVDI)

        • Works with any device (that has a web browser)

        • Allow to configure an instance type per application type (CPU, RAM, GPU)






----------------------------------------------------------------- AWS Device Farm



• Application testing service for your mobile and web applications

• Test across real browsers and real mobiles devices

• Fully automated using framework

• Improve the quality of web and mobile apps

• Generates videos and logs to document the issues encountered

• Can remotely log-in to devices for debugging





----------------------------------------------------------------- AWS Macie


• Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.

• Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)




----------------------------------------------------------------- Amazon Simple Email Service (Amazon SES)



• Fully managed service to send emails securely, globally and at scale

• Allows inbound/outbound emails

• Reputation dashboard, performance insights, anti-spam feedback

• Provides statistics such as email deliveries, bounces, feedback loop results, email open

• Supports DomainKeys Identified Mail (DKIM) and Sender Policy Framework (SPF)

• Flexible IP deployment: shared, dedicated, and customer-owned IPs

• Send emails using your application using AWS Console, APIs, or SMTP

• Use cases: transactional, marketing and bulk email communications







----------------------------------------------------------------- Amazon SES – Configuration Sets



• Configuration sets help you customize and analyze your email send events

• Event destinations:

        • Kinesis Data Firehose: receives metrics (numbers of sends, deliveries, opens, clicks, bounces, and complaints) for each email

        • SNS: for immediate feedback on bounce and complaint information


• IP pool management: use IP pools to send particular types of emails





----------------------------------------------------------------- Amazon Pinpoint


• Scalable 2-way (outbound/inbound) marketing communications service

• Supports email, SMS, push, voice, and in-app messaging

• Ability to segment and personalize messages with the right content to customers

• Possibility to receive replies

• Scales to billions of messages per day

• Use cases: run campaigns by sending marketing, bulk, transactional SMS messages

• Versus Amazon SNS or Amazon SES 

  • In SNS & SES you managed each message's audience, content, and delivery schedule

  • InAmazonPinpoint,youcreatemessagetemplates, delivery schedules, highly-targeted segments, and full campaigns





----------------------------------------------------------------- EC2 Image Builder


• Used to automate the creation ofVirtual Machines or container images

• => Automate the creation, maintain, validate and test EC2 AMIs

• Can be run on a schedule (weekly, whenever packages are updated, etc...)

• Free service (only pay for the underlying resources)

• Can publish AMI to multiple regions and multiple accounts





----------------------------------------------------------------- AWS IoT Core


• IoT stands for “Internet of Things” – the network of internet-connected devices that are able to collect and transfer data

• AWS IoT Core allows you to easily connect IoT devices to the AWS Cloud

• Serverless, secure & scalable to billions of devices and trillions of messages

• Integrates with a lot of AWS services (Lambda, S3, SageMaker, etc.)

• Build IoT applications that gather, process, analyze, and act on data




----------------------------------------------------------------- IoT Core - Integrations


Kinesis DynamoDB SQS MSK SNS S3 AWS Lambda


----------------------------------------------------------------- Summary 




To help you remember them quickly before going into the exam

        CodeCommit: store code in version-controlled repositories. Code can live on multiple branches

        CodeBuild: build & test code on-demand in your CICD pipelines.

        CodeDeploy: deploy code on EC2, ASG, Lambda or ECS

        CodePipeline: orchestrate CICD pipelines. If using CodeCommit as a source, matches to only one branch

        CodeGuru: automated code reviews and application performance recommendations using ML

        CloudSearch: managed search solution to perform a full-text search, auto-completion in your applications

        Alexa for Business: use Alexa to help employees be more productive in meeting rooms and their desk

        Lex: Automatic Speech Recognition (ASR) to convert speech to text. Helpful to build chatbots

        Connect: receive calls, create contact flows, cloud-based virtual contact center

        Rekognition: find objects, people, text, scenes in images and videos using Machine Learning

        Kinesis Video Stream: one stream per video device, analyze using EC2 instances or Rekognition

        WorkSpaces: on-demand Windows workstations. WAM is used to manage applications

        AppStream 2.0: stream desktop applications into web browsers

        Device Farm: Application testing service for your mobile and web applications across real devices
