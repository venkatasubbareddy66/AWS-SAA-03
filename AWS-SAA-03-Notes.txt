
=================================================== AWS Cloud History ======================================

2002: Internally launched

2003: Amazon infrastructure is one of their core strength. Idea to market

2004: Launched publicly with SQS

2006: Re-launched publicly with SQS, S3 & Ec2

2007: Launched in Europe

users : netflix , dropbox , nasa etc......



--------- AWS Cloud Use Cases

• AWS enables you to build sophisticated, scalable applications

• Applicable to a diverse set of industries

• Use cases include

   • Enterprise IT, Backup & Storage, Big Data analytics
   • Website hosting, Mobile & Social Apps
   • Gaming


--------------------------- AWS Global Infrastructure

• AWS Regions
• AWS Availability Zones
• AWS Data Centers
• AWS Edge Locations / Points of Presence


EPV : ------------------------- Here are some of the best practices while creating an AWS account root user:

1) Use a strong password to help protect account-level access to the AWS Management Console. 

2) Never share your AWS account root user password or access keys with anyone.

3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3.

4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to.

5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account.



========================================== AWS Global, Regional, AZ resource Availability ========================

-- AWS provides a lot of services and these services are either Global, Regional, or Availability Zone specific and cannot be accessed outside

-- Most of the AWS-managed services are regional-based services with few exceptions being Global (e.g. IAM, Route53, CloudFront, etc) or AZ bound.

Eg : 




                         A  Virtual Private Cloud = Regional

VPCs are created within a region

- Subnet – Availability Zone , A subnet can span only a single Availability Zone

  
- Security groups – Regional , A security group is tied to a region and can be assigned only to instances in the same region.


- VPC Endpoints – Regional , VPC Gateway & Interface Endpoints cannot be created between a VPC and an AWS service in a different region.


- Elastic IP Address – Regional , Elastic IP addresses created within the region can be assigned to instances within the region only.

- Elastic Network Interface – Availability Zone


2  Route 53 – Global , Route53 services are offered at AWS edge locations and are global

3 CloudFront – Global , CloudFront is the global content delivery network (CDN) services are offered at AWS edge locations

4 ELB, ALB, NLB, GWLB – Regional , Elastic Load Balancer distributes traffic across instances in multiple Availability Zones in the same region

- Use Route 53 to route traffic to load balancers across regions.

5 Direct Connect Gateway – Global , is a globally available resource that can be created in any Region and accessed from all other Regions.

6 Transit Gateway – Regional , is a Regional resource and can connect VPCs within the same AWS Region. Transit Gateway Peering can be used to attach TGWs across regions.


7 AWS Global Accelerator – Global , is a global service that supports endpoints in multiple AWS Regions.



                          B AWS Compute Services : 

EC2

1 Resource Identifiers – Regional , Each resource identifier, such as an AMI ID, instance ID, EBS volume ID, or EBS snapshot ID, is tied to its region and can be used only in the region where you created the resource.

2 Instances – Availability Zone , An instance is tied to the Availability Zones in which you launched it. However, note that its instance ID is tied to the region.

3 EBS Volumes – Availability Zone , Amazon EBS volume is tied to its Availability Zone and can be attached only to instances in the same Availability Zone.

4 EBS Snapshot – Regional , An EBS snapshot is tied to its region and can only be used to create volumes in the same region and has to be copied from one region to another if needed.

5 AMIs – Regional , AMI provides templates to launch EC2 instances , AMI is tied to the Region where its files are located with Amazon S3. For using AMI in different regions, the AMI can be copied to other regions

6 Auto Scaling – Regional , Auto Scaling spans across multiple Availability Zones within the same region but cannot span across regions

7 Cluster Placement Groups – Availability Zone , Cluster Placement groups can span across Instances within the same Availability Zones

8 ECS – Regional

9 ECR – Regional, Images can be pushed/pulled within the same AWS Region. Images can also be pulled between Regions or out to the internet with additional latency and data transfer costs.


                                  C  AWS Storage Services

1 S3 – Global but Data is Regional 

  - S3 buckets are created within the selected region
  - Objects stored are replicated across Availability Zones to provide high durability but are not cross-region replicated unless done explicitly.
  - S3 cross-region replication can be used to replicate data across regions.

2 DynamoDB – Regional
 
 - All data objects are stored within the same region and replicated across multiple Availability Zones in the same region
 - Data objects can be explicitly replicated across regions using cross-region replication

3 DynamoDB Global Tables – Across Regions
 
  - is a new multi-master, cross-region replication capability of DynamoDB to support data access locality and regional fault tolerance for database workloads

4 Storage Gateway – Regional

 - AWS Storage Gateway stores volume, snapshot, and tape data in the AWS region in which the gateway is activated



                           D AWS Identity & Security Services

1 Users, Groups, Roles, Accounts – Global
 
 - Same AWS accounts, users, groups, and roles can be used in all regions

2 Key Pairs – Global or Regional

 - EC2 created key pairs are specific to the region

 - RSA key pair can be created and uploaded that can be used in all regions

3 Web Access Firewall – WAF – Global

 - protect web applications from common web exploits and is offered at AWS edge locations globally.

4 AWS GuardDuty – Regional
 
  - findings remain in the same Regions where the underlying data was generated.

5 Amazon Detective – Regional

6 Amazon Inspector – Regional

7 Amazon Macie – Regional
 
  - must be enabled on a region-by-region basis and helps view findings across all the accounts within each Region.

  - verifies that all data analyzed is regionally based and doesn’t cross AWS regional boundaries.

8 AWS Security Hub – Regional.

  - supports cross-region aggregation of findings via the designation of an aggregator region.

9 AWS Migration Hub – Regional.

 - runs in a single home region, however, can collect data from all regions


                      E AWS Management & Governance Tools

 1 AWS Config – Regional
 
 2 AWS Service Catalog – Regional



=================================================== EC2 (Elastic Compute Cloud = Infrastructure as a Service) ===================================================

-- elastic compute cloud

• EC2 = Elastic Compute Cloud = Infrastructure as a Service

-- ec2 is specific at region only 

-- the datacentre handle by us is called "on-premises"

-- in aws L.B never down coz it is not a server it is a service from a AWS 

-- ELB is regional level

-- u can use this ELB through URL / DNS

-- in generl we dont have any control on servers , but we have full control on servers which is created by ElasticBeanStalk (platform as a Service)

----------------- "LightSail" :if u want to setup and crete virtual server which is already has everything installed (Readymade )

-- it doesn't support auto-Scaling


===========================AWS - Storage S3,EBS=======================

-- in olden days 

we have Floppy -- 2MB

-- CD's = 700mb
-- DVD == 4.7GB
-- Pendrive == 12gb
-- Hard Disk == 2TB 


--- aws came with S3 (simple storage Service), it has unlimited storage 

--- u can store any type of files , u can upload and download the files

--- it is "Server less "

--- s3 --> Bucket--> Objects

-- bucket is collection of objects 

-- objects is a file

-- name of the file is called "key"

-- S3 is "Objected Based Storage"

-- it also provided "static host website"

-- s3 is global and buckets are regional

--------------------------------- EBS

-- default volume is created when instance is created

-- default for windows ec2 instance  in aws == it has storage capacity has 30GB 

-- for Linux ec2 instace based on os == 8 r 10 GB

-- ebs is centralized storage , u go n create volume and then attach to the instance 

-- volumes can be attached and dettach 

-- s3 is object storage and ebs is block type storage

-- default volm is called "root volume" , root volume contains o.s(like linux, windows)

-- u can attach multiple volumes to the ec2 instances

-- ec2 supports only server os, not client os 

-- max capacity of ebs volume and windows is 16TB

-- volumes can be pre provision

-- u cant attach a single volume to multiple ec2 at the same time 

-- volume size can be increased on FLY(no need to stop the ec2 instances)

-- volum size cant decrease once u created , u can only able to increase not decrease the volumse size , so u need to delete the volume n create new volume n migrate ur data to new volm n delete the old volume .

-- u can able to change from one volm type to another (gp2,gp3..)

-- Root volm has a device name for win/linux == /dev/sda1 or /dev/xvda

-- for ubuntu == /dev/xvda

-- u need to stop instance to deattach the root volm

-- it is poosible to deattach the additional volume but it is not recommended to stop the instance

-- u can't delete the volm while it is attach, so deattach first 

-- ec2 instace and EBS is should be in same A.Z 

====================================EFS=====================

• Managed NFS (network file system) that can be mounted on many EC2

• EFS works with EC2 instances in multi-AZ

• Highly available, scalable, expensive (3x gp2), pay per use

• Use cases: content management, web serving, data sharing, Wordpress

• Uses NFSv4.1 protocol

• Uses security group to control access to EFS

• Encryption at rest using KMS

• POSIX(Portable Operating System Interface) file system (~Linux) that has a standard file API

• File system scales automatically, pay-per-use, no capacity planning!


-- elastic file system , it is file based storage system, it has unlimited storage

-- efs is only for the linux instances only

-- duplicate is not possible (same name with 2 files)

-- for windows ec2 instances we used == Fsx 

-- EFS works with "NFSv4" protocal 

-- in EFS u no need to pre-provision , it will automaticaaly increase and decrese based on ur file 

-- it is used to mounted to multiple ec2 instances across A.Z

-- it is regional limit

-- expensive(3 times of GP2)



------------ EFS – Performance & Storage Classes

• EFS Scale

   • 1000s of concurrent NFS clients, 10 GB+ /s throughput
   • Grow to Petabyte-scale network file system, automatically

• Performance Mode (set at EFS creation time)
 
   • General Purpose (default) – latency-sensitive use cases (web server, CMS, etc...)
      
      - General Purpose performance mode is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving.

      - If you don't choose a performance mode when you create your file system, Amazon EFS selects the General Purpose mode for you by default.


   • Max I/O – higher latency, throughput, highly parallel (big data, media processing)

     - Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. 

     - This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. 

     - Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode.

     - it is not available for file systems using One Zone storage classes.



• Throughput Mode

   • Bursting – 1TB = 50MiB/s + burst of up to 100MiB/s
   • Provisioned – set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage

     - There are two throughput modes to choose from for your file system, Bursting Throughput and Provisioned Throughput.

     - With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. 

     - With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored.


   • Elastic – automatically scales throughput up or down based on your workloads
      • Upto 3GiB/s for reads and 1GiB/s for writes
      • Used for unpredictable workloads



------------- EFS – Storage Classes

• Storage Tiers (lifecycle management feature – move file after N days)

   • Standard: for frequently accessed files
   • Infrequent access (EFS-IA): cost to retrieve files, lower price to store. Enable EFS-IA with a Lifecycle Policy

• Availability and durability

   • Standard: Multi-AZ, great for prod
   • One Zone: One AZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone-IA)
   
• Over 90% in cost savings



--------------- EBS vs EFS – Elastic Block Storage

• EBS volumes...

    • one instance (except multi-attach io1/io2)
    • are locked at the Availability Zone (AZ) level 
    • gp2: IO increases if the disk size increases
    • gp3 & io1: can increase IO independently

• To migrate an EBS volume across AZ

   • Take a snapshot
   • Restore the snapshot to another AZ
   • EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic

• Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated. (you can disable that by using "DeleteOnTermination")


EBS vs EFS – Elastic File System

• Mounting 100s of instances across AZ 

• EFS share website files (WordPress)

• Only for Linux Instances (POSIX)

• EFS has a higher price point than EBS 

• Can leverage EFS-IA for cost savings


===================================================SNOW FAMILY=======================================

-- SnowCone --> 8TB

-- SnowEdge--> 100TB

-- SnowMobile --> PB's

-- it is used for physical data transfer 

-- all these snow family data  will be pushed to S3 

-- S.F is used to transfer the huge data from on-premises to AWS and vice-versa

-- it is encrypted and very safe to transfer 

-------------------------------------------- Glacier 

-- it is used for "Archiving Purpose"

-- it chaper then the s3

--  all ur infrequent data stored in the Glacier


------------------------------------------- storage Gateway

-- it works as hybrid cloud service 

-- the data that u have in S3, ebs,fsx,glacier  move these data into the on-premises through the "storage gateway", it it support only "S3, ebs,fsx,glacier" 

-- Storage Gateway provides a standard set of storage protocols such as iSCSI(Internet Small Computer Systems Interface ), SMB, and NFS, which allow you to use AWS storage without rewriting your existing applications.


------------------------------------------- Database services

-- RDs -- relational database service , 

     NOTE : it is not a database it is database service 

-- all issue will be take care by the AWS 

-- rds db instance, rds supports only rdbms db only  

-- rds is a service where u can setup, configure, manage and maintain databases in aws 


-- RDS supports 6 Engines ----- MOMPMAD

-- Mysql = Opensource
-- Oracel = oracle
-- Mssql = Microsoft
-- Postgresql = opensource
-- MariaDb = community based
-- IBM DB2 = IBM 
-- Aurora = AWS 


-- if u want to transfer ur database u can use the service called "Database Migratin Service" (from on-premises to aws)


---- AWS has introduce its own NOSQL database called "DynamoDB"

-- it is nosql database service in aws 

--  if u are storing huge data it is called " DatawareHouse"

-- the dataware house service in the aws is called "Redshift" 

-- if u have frequently data then db server has stored in the Cached Memory 

-- in aws cached memory service is called "Elastic cache" which is in-memory database service

       In-Memory Database : An in-memory database (IMDB) stores computer data in a computer’s main memory instead of a disk drive to produce quicker response times. Accessing data stored in memory eliminates the time needed to query data from a disk.

       - In-memory databases are used by applications that depend on rapid response times and real-time data management. Industries that benefit from in-memory databases include telecommunications, banking, travel and gaming. An in-memory database is also referred to as a main memory database (MMDB), real-time database (RTDB) or in-memory database system (IMDS).

-- low latency

-- high performance

-- "Elastic cache" supports 2 engines 

1 Redis 

2 Memcached 

-- these are databases only but these are cache databases


=======================================Route53=======================

-- it is DNS service from AWS 

-- DNS pot number is 53 , so it is called r53

-- it contians records 

--- it is "Global"

-------------VPC 

-- whatever resource that we have created all these are inside VPC only, 

-- it is regional limit

-- 5 vpc per region we are able to create

-- this is we can called as "virtual Data center on AWS/cloud"

-- by deault 2 vpc can't talk each other, but if it is required we can use "VPC Peering"

-- aws provided default vpc for us 

-- every region has a default vpc

-- VPN : virtual private network , in the shared internet we use vpn for secure purpose 

-- Direct Connect : if u want to connect directly to aws without shared internet , it is quite expensive around 16k/month


---- if u have customers all over the world , but ur appn in mumbai, the soln is to avoid high latency to the customers through out the world u can use "CloudFront" service


-- in C.F we have Edge Locations ,every regions have edge locations and all these are cache ur application

-- in C.F u create "Distribution" , while creating distribution u have to give "Origin"(elb,s3,Elasticbeanstalk..etc ) and TTL Value

-- then u have to specify , which continent aws will cache ur appn(us,europe,asia) and geo-restriction also apply if u dont want to cache in specific continents

-- how does cache ur appn ?

   - by using CDN = Content Delivery Network , by using this network appn is cached in the edge location 

-- how much time it will get Cache ?

ans: TTL(time to live) , if u give ttl is 5 hours then it will cache for 5 hours 

---- IMP: once u want to change the file , then it wont effect immediatly on edge-location , it will reflect only after 5 hrs only

-- so if u want to update the data immediatly in the edge locations we have concept called "invalidate Cache" 

-- which is used to force the update content in the edge loction

-- all the data cached based on TTL

IMP : C.F will cache Static and Dynamic content in the edge locations

     --  c.f has edge locatins and these are connected with CDN



==============================IAM========================

-- Identity and access management 

-- in the root account(main account) -- root user and IAM account 

-- inside the root accnt we will create users called "iam user"

-- permissions = policies both are same 

-- u can control the entire AWS using IAM by providing proper pemissions to the IAM user

-- two types of accounts 

1 ROOT USER  -- super user have all permissions == log with Email/pwd

2 IAM user --- limited permissions == login with username/pwd

--- IAM is Access control on AWS Resource



----------------------Organizations

--- it will help to manage multiple root accounts 

-- in organzations will have member accounts 

-- u can control member accounts using SCP (service Control Policies)




------------------- AWS Support 

-- we have different supports 

1 basic suppport  : FREEE 

2 Developer support : 100$

3 bussiness support : this is paid service , this is meant for enterprise level

-- u will get reply within an hour 

4 enterprise support   : a paid support service with the highest SLA available in 15 minutes 

-- in this support u wil get one TAM(technical Account Manager) who are subject matter experts in their own fields 

$15000 


--- these are all based on SLA (service level Aggrement) 


AWS Support offers five support plans:

1 Basic

2 Developer

3 Business

4 Enterprise On-Ramp

5 Enterprise


1  Basic Support offers support for account and billing questions and service quota increases. The other plans offer a number of technical support cases with pay-by-the-month pricing and no long-term contracts.
 
  -- All AWS customers automatically have 24x7 access to these features of Basic Support:

     - One-on-one responses to account and billing questions
     - Support forums
     - Service health checks
     - Documentation, technical papers, and best practice guides

2  Customers with a Developer Support plan have access to these additional features:

   - Best practice guidance
   - Client-side diagnostic tools
   - Building-block architecture support: guidance on how to use AWS products, features, and services together
   - Supports an unlimited number of support cases that can be opened by one primary contact, which is the AWS account root user.

3 , 4 , 5  : In addition, customers with a Business, Enterprise On-Ramp, or Enterprise Support plan have access to these features:

   - Use-case guidance – What AWS products, features, and services to use to best support your specific needs.
   - AWS Trusted Advisor – A feature of AWS Support, which inspects customer environments and identifies opportunities to save money, close security gaps, and improve system reliability and performance. You can access all Trusted Advisor checks.
   - The AWS Support API to interact with Support Center and Trusted Advisor. You can use the AWS Support API to automate support case management and Trusted Advisor operations.
   - Third-party software support – Help with Amazon Elastic Compute Cloud (Amazon EC2) instance operating systems and configuration. Also, help with the performance of the most popular third-party software components on AWS. Third-party software support isn't available for customers on Basic or Developer Support plans.
   - Supports an unlimited number of AWS Identity and Access Management (IAM) users who can open technical support cases.


In addition, customers with an Enterprise On-Ramp or Enterprise Support plan have access to these features:

- Application architecture guidance – Consultative guidance on how services fit together to meet your specific use case, workload, or application.
- Infrastructure event management – Short-term engagement with AWS Support to get a deep understanding of your use case. After analysis, provide architectural and scaling guidance for an event.
- Technical account manager – Work with a technical account manager (TAM) for your specific use cases and applications.
- White-glove case routing.
- Management business reviews.


==================================IAM -- depth 

-- with iam u can conrol aws centrally 

-- IAM is "Global"

-- with the iam user/root user details u can't login into the EC2 instances but u can access ec2 service

-- with the iam user/root user details used to just login into the aws console 

-- it is not recommended to use root account for daily work , instead of that use IAM user 

-- to provide more security for ur account enable MFA (multi factor authentication) like google authenticator 

-- MFA is ighly recommended for ROOT and IAM user account as well 

-- 2 ways to access aws 

1 AWS Console (GUI)

2 programmatical Access : CLI, SDK's ,developer tools , it dont have any MFA

-- every IAM user can have max 2 set of keys 

-- dont create keys for root account

===============================================IAM Groups===========================

-- collection of IAM users

-- group under groups are not possible

-- it is possible to attach multiple policies to the IAM user, max is 10 policies

-- u cant assign or create keys to the groups

-- an IAM user can be attached to the multiple groups at the same time

-- policy documents contain permissions

-- policies are written in "JSON" format

-- they are 2 types of policies

1 manage policy : pre-defined policies created and managed by AWS 

2 inline policy : created and mnaged by the customers

-- ARN : Amazon Resource Name : if u want to give permissions to certain group or user by the id (ARN)

   - Amazon Resource Names (ARNs) uniquely identify AWS resources. We require an ARN when you need to specify a resource unambiguously across all of AWS, such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls.



EPV : A DevOps engineer at an IT company was recently added to the admin group of the company's AWS account. The AdministratorAccess managed policy is attached to this group.

Can you identify the AWS tasks that the DevOps engineer CANNOT perform even though he has full Administrator privileges (Select two)?


ANS : 1 Configure an Amazon S3 bucket to enable AWS Multi-Factor Authentication (AWS MFA) delete

      2 Close the company's AWS account


EXP : 

-- An IAM user with full administrator access can perform almost all AWS tasks except a few tasks designated only for the root account user. 

-- Some of the AWS tasks that only a root account user can do are as follows: 
  
    - change account name or root password or root email address, change AWS support plan, close AWS account, enable AWS Multi-Factor Authentication (AWS MFA) on S3 bucket delete, create Cloudfront key pair, register for GovCloud. 

-- Even though the DevOps engineer is part of the admin group, he cannot configure an Amazon S3 bucket to enable AWS MFA delete or close the company's AWS account.



EPV : A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets.

How can you provide these users access in the least possible time, with minimal changes?

ANS : Create a group, attach the policy to the group and place the users in the group





----------------------------------------------    IAM ROLES

--  Temporary Access without credentials

-- u need to attach the roles to the ec2 instance in the backend 

-- if u use the roles, u no need to configure keys on the ec2 instance

-- based on the permissions that you attached to the ROLE, those pemissions are Available from the ec2 instance

-- 1 ec2 instance can have only 1 ROLE attached

-- 1 ROLE can be attached to multiple ec2 instance


NOTE : u can't attach roles to the IAM Groups , U can attach only policies to the IAM groups 



------------------------- Identity Povider / Federation / IAM Identity Center

-- SSO = Single Sign On 


-- ACTIVE DIRECTORY also know as "Domain Controller"

-- these are Directory Services


-- to setup sso in AWS 

-- Feeration to from LDAP to AWS 


EPV : A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B.

ANS : Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role

EXP : If the IAM role that you create for the Lambda function is in the same AWS account as the bucket, then you don't need to grant Amazon S3 permissions on both the IAM role and the bucket policy.

-- Instead, you can grant the permissions on the IAM role and then verify that the bucket policy doesn't explicitly deny access to the Lambda function role. 

-- If the IAM role and the bucket are in different accounts, then you need to grant Amazon S3 permissions on both the IAM role and the bucket policy. 


====================================IAM TAGS=======================

-- Tags are key vaue pair 

-- tags are used for identification purpose

-- for every resource u have tags

-- ARN are generated by AWS , where as tags are providede by Customer

-- Tags are used for automation purpose

-- Tags are used for cost optimization purpose also

-- u can give "50 tags" max per resource

-- tags are imp but these are optional

-- Access Advisior : used for auditing purpose , it shows the service permissions granted to the user and when those were last used 

-- it is usd to revise ur policies to the users


-- Access Analyzer : it is used to analyze the access for all IAM users and take actions on the findings




IMP : to access the other accounts , by using ROLES u can access others account with the required permissions , once u set the policies on ur account and create role it will create one URL and these url u can share it to the others to acces ur account with the given policies through the Roles 

-- Always use inline policy for the Roles



===========================IAM PRACTICALS=======================

-- create alias for ur account 

-- open IAM in console nd create Alias name for ur account, instead of remember the account number u jst create alias name 


-- explore allthings in IAM 


========================================================IAM Policies

-- You can manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources.

-- A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. 

-- Permissions in the policies determine whether the request is allowed or denied. 

-- Most policies are stored in AWS as JSON documents. 

-- AWS supports six types of policies : 

1 identity-based policies,

2 resource-based policies,

3 permissions boundaries, 

4 Organizations service control policy (SCPs),

5 access control lists (ACLs),

6 session policies.


-- Inline policy 

eg: how to cretae groups and users only 


-- Policies > create policy > visual editor > select Service > slect acccoding to rquiremts > in json automatically it will create json code > c.o nxt to create policy 




-----------------

EPV :  What does this IAM policy do?

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Mystery Policy",
      "Action": [
        "ec2:RunInstances"
      ],
      "Effect": "Allow",
      "Resource": "*",
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "34.50.31.0/24"
        }
      }
    }
  ]
}

ANS : It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block

EXP : Consider the following snippet from the given policy document:

      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "34.50.31.0/24"
        }
      }

-- The aws:SourceIP in this condition always represents the IP of the caller of the API. That is very helpful if you want to restrict access to certain AWS API for example from the public IP of your on-premises infrastructure.

-- Please see this overview of Elastic vs Public vs Private IP addresses:

elastic IP address (EIP) - An elastic IP address (EIP) is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.

Private IP address - A private IPv4 address is an IP address that's not reachable over the Internet. You can use private IPv4 addresses for communication between instances in the same VPC.

Public IP address - A public IP address is an IPv4 address that's reachable from the Internet. You can use public addresses for communication between your instances and the Internet.

-- Please note 34.50.31.0/24 is a public IP range, not a private IP range. Private IP ranges are: 192.168.0.0 - 192.168.255.255 (65,536 IP addresses) 172.16.0.0 - 172.31.255.255 (1,048,576 IP addresses) 10.0.0.0 - 10.255.255.255 (16,777,216 IP addresses)




EPV : how to give IAM policies provides read-only access to the Amazon S3 bucket mybucket and its content?

ANS : 

{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Effect":"Allow",
         "Action":[
            "s3:ListBucket"
         ],
         "Resource":"arn:aws:s3:::mybucket"
      },
      {
         "Effect":"Allow",
         "Action":[
            "s3:GetObject"
         ],
         "Resource":"arn:aws:s3:::mybucket/*"
      }
   ]
}

EXP : s3:ListBucket is applied to buckets, so the ARN is in the form "Resource":"arn:aws:s3:::mybucket", without a trailing / s3:GetObject is applied to objects within the bucket, so the ARN is in the form "Resource":"arn:aws:s3:::mybucket/*", with a trailing /* to indicate all objects within the bucket mybucket






EPV : eg 

{
    "Version":"2012-10-17",
    "Id":"EC2TerminationPolicy",
    "Statement":[
        {
            "Effect":"Deny",
            "Action":"ec2:*",
            "Resource":"*",
            "Condition":{
                "StringNotEquals":{
                    "ec2:Region":"us-west-1"
                }
            }
        },
        {
            "Effect":"Allow",
            "Action":"ec2:TerminateInstances",
            "Resource":"*",
            "Condition":{
                "IpAddress":{
                    "aws:SourceIp":"10.200.200.0/24"
                }
            }
        }
    ]
}


ANS : Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200

EXP : The given policy denies all EC2 specification actions on all resources when the region of the underlying resource is not us-west-1. 

-- The policy allows the terminate EC2 action on all resources when the source IP address is in the CIDR range 10.200.200.0/24, therefore it would allow the user with the source IP 10.200.200.200 to terminate the Amazon EC2 instance.



========================================== Roles

--  without credentials we can acess ervice for certain period

-- go to roles > aws service > ec2 > select permision 


=================================== identity center 

eg :if someone want to access 3 aws accounts u no need to create iam user in 3 aws accounts , u jst go to I.C and create user name and give URL to the person he will able to login with 3 aws accounts with SSO(single sign one ) as they wants to login into which account with same credentils 



============================================== Advanced Identity in AWS =========================

---- Organizational Units (OU) , 

-- In organzations u can create seperate units for different lify cycles for eg : Bussiness unit , environmental lifecycle , project based 

AWS Organizations :

 • Global service

 • Allows to manage multiple AWS accounts

 • The main account is the management account

 • Other accounts are member accounts

 • Member accounts can only be part of one organization -- IMP for Exam

 • Consolidated Billing across all accounts - single payment method

 • Pricing benefits from aggregated usage (volume discount for EC2, S3...) 

 • Shared reserved instances and Savings Plans discounts across accounts

 • API is available to automate AWS account creation


 EPV : You would like to migrate an AWS account from an AWS Organization A to an AWS Organization B. What are the steps do to it?

 ANS : Remove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account



 --------------------- Advantages

 • Multi Account vs One Account Multi VPC

 • Use tagging standards for billing purposes

 • Enable CloudTrail on all accounts, send logs to central S3 account

 • Send CloudWatch Logs to central logging account

 • Establish Cross Account Roles for Admin purposes



------------------ Security:  Service Control Policy (SCP)

• IAM policies applied to OU or Accounts to restrict Users and Roles

• They do not apply to the management account (full admin power)

• Must have an explicit allow (does not allow anything by default – like IAM)

--  Service control policy (SCP) is one type of policy that you can use to manage your organization.

--  SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.

-- SCPs are available only in an organization that has all features enabled. 

-- SCPs aren't available if your organization has enabled only the consolidated billing features.

-- Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform. 


EPV : An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines.

ANS : 

1 If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action

2 Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts

3 Service control policy (SCP) does not affect service-linked role

EXP : In service control policy (SCP), you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. 

--  You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization.





---------------- SCP Hierarchy

Root OU            = FullAWSAccess SCP

Management Account = DenyAccessAthena SCP

OU (Prod)          = DenyRedshift SCP

Account A          = AuthorizedRedshift SCP

OU (HR)            = DenyAWSLambda SCP 

OU (Finance)       = DenyAWSLambda SCP 

like thi su can grant permissions  tp specific accounts 



----------------------------- Resource Policies & aws:PrincipalOrgID

• aws:PrincipalOrgID can be used in any resource policies to restrict access to accounts that are member of an AWS Organization

-- it will deny the members from outside of an organization , it works for organization only to whom grant access or not to grant permissions with organization


--------------------- IAM Roles vs Resource Based Policies

• Cross account:

   • attaching a resource-based policy to a resource (example: S3 bucket policy)

   • OR using a role as a proxy

User Account A -----> Role Account B ----> Amazon S3


User Account A -----> S3 Bucket Policy ---> Amazon S3 

• When you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role

• When using a resource-based policy, the principal doesn’t have to give up his permissions

• Example: User in account A needs to scan a DynamoDB table in Account A and dump it in an S3 bucket in Account B.

• Supported by: Amazon S3 buckets, SNS topics, SQS queues, etc...

identity-based policy : Attach AWS managed , customer managed or inline policies to IAM dentities (users, groups to which user belongs , or roles ), these policies grant permission to an identity


Resource - based policy : 

  - Attch inline policies to the resource . The most common examples of resource -based policies are Amazon s3 bucket policies 

1 In resource based policies , we specify what resources affected and what actions are permitted 

2 u explicitly list who is allowed to the resource(principal)


-- The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies.



IMP : the main diff b/w identity based and resource based is no pricipal is attached in identity based policy but in resource based u have to inclues a principal element that specifies who is granted the permissions 



---------------------------------- LAB ( resource based policy)

-- in root account u should create 2 or 3 users and give s3fullaccess to all the users 

-- now u have 2 users in ur account but u do not want to give permissions to access the s3 bucket to user2 but access to user 1 

-- create one bucket and upload some files in it 

-- now create resource based policy ? 

https://awspolicygen.s3.amazonaws.com/policygen.html

-- open bucket --> permission --> add bucket policy (resource based policy) --> click on policy generator --> Select Type of Policy = s3 bucket policy --> Effect = Deny --> principal = copy ARN of user which do not access of s3 bucket --> Actions = all actions --> Amazon Resource Name (ARN) = ur bucket ARN 

-- generate policy , do copy and paste in s3 bucket --> pemissions --> C.O edit policy and paste 

eg :  


{
  "Id": "Policy1708586782753",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1708586773122",
      "Action": "s3:*",
      "Effect": "Deny",
      "Resource": "arn:aws:s3:::dhbchdbcdhbc",
      "Principal": {
        "AWS": [
          "arn:aws:iam::298132369629:user/dharma"
        ]
      }
    }
  ]
}


-- now login with 2 users one by one and check the differences 

-- here i have given permision to only user1 so, i am able to see buckets for user1 and not able to see for user2 

IMP : here we have just given principal for resource based policy 




EPV : A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies.

Which is the only resource-based policy that the IAM service supports?

ANS : Trust policy

EXP : Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. 

-- For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.




-------------------------------------------------- Access control list (ACL) 

-- Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource.

-- ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.



----------------------------------------------------------- permissions boundary 

-- AWS supports permissions boundaries for IAM entities (users or roles).

-- A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. 

-- An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. 

-- Here we have to use an IAM permission boundary. They can only be applied to roles or users, not IAM groups.

• Can be used in combinations of AWS Organizations SCP


Use cases :

   • Delegate responsibilities to non administrators within their permission boundaries, for example create new IAM users

   • Allow developers to self-assign policies and manage their own permissions, while making sure they can’t “escalate” their privileges (= make themselves admin)

   • Useful to restrict one specific user (instead of a whole account using Organizations & SCP)




======================================= AWS IAM Identity Center (successor to AWS Single Sign-On) ==========================================


• One login (single sign-on) for all your

 • AWS accounts in AWS Organizations
 • Business cloud applications (e.g., Salesforce, Box, Microsoft 365, ...)
 • SAML2.0-enabled applications
 • EC2 Windows Instances


 • Identity providers

  • Built-in identity store in IAM Identity Center
  • 3rd party: Active Directory (AD), OneLogin, Okta...


---------------------------- AWS IAM Identity Center Fine-grained Permissions and Assignments

1 Multi-Account Permissions

  • Manage access across AWS accounts in your AWS Organization
  • Permission Sets – a collection of one or more IAM Policies assigned to users and groups to define AWS access

2 Application Assignments

  • SSO access to many SAML 2.0 business applications (Salesforce, Box, Microsoft 365, ...)
  • Provide required URLs, certificates, and metadata

3 Attribute-Based Access Control (ABAC)

  • Fine-grained permissions based on users’ attributes stored in IAM Identity Center Identity Store
  • Example:costcenter,title,locale,...
  • Usecase:Define permissions once,then modify AWS access by changing the attributes


-------------------------- What is Microsoft Active Directory (AD)? (Domain Controller)

• Found on any Windows Server with AD Domain Services

• Database of objects: User Accounts, Computers, Printers, File Shares, Security Groups

• Centralized security management, create account, assign permissions

• Objects are organized in trees

• A group of trees is a forest


------------------------- AWS Directory Services

1  AWS Managed Microsoft AD

 • Create your own AD in AWS, manage users locally, supports MFA 

 • Establish “trust” connections with your on- premises AD

 On-prem AD <----(trust)-----> AWS Managed AD 

 -- AWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services.

 -- AWS Directory Service for Microsoft Active Directory (aka AWS Managed Microsoft AD) is powered by an actual Microsoft Windows Server Active Directory (AD), managed by AWS.

 -- With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud such as SQL Server-based applications. 

 -- You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).


2  AD Connector

  • Directory Gateway (proxy) to redirect to on-premises AD, supports MFA

  • Users are managed on the on-premises AD

   On-prem AD <-----(proxy------- AD Connector <---(auth)----

  -- Use AD Connector if you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. 

  -- AD Connector simply connects your existing on-premises Active Directory to AWS. You cannot use it to run directory-aware workloads on AWS, 



3  Simple AD

  • AD-compatible managed directory on AWS

  • Cannot be joined with on-premises AD

  -- Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. 

  -- Simple AD is a standalone managed directory that is powered by a Samba 4 Active Directory Compatible Server. 

  -- Simple AD does not support features such as trust relationships with other domains. 




---------------------------- IAM Identity Center – Active Directory Setup


• Connect to an AWS Managed Microsoft AD (Directory Service)

  • Integration is out of the box

  IAM Identity Center <------------------(connect)-------------->AWS Managed Microsoft AD


• Connect to a Self-Managed Directory

  • Create Two-way Trust Relationship using AWS Managed Microsoft AD

  • Create an AD Connector

  IAM Identity Center <------------------(connect)-------------->AWS Managed Microsoft AD <------------------(two-way trust relationship)-------> Active directory
    IAM Identity Center <------------------(connect)-------------->AD Connector<------------------(proxy)-------> Active directory



--------------------------- LAB (SSO)

-- open IAM Identity center

--  choose Organization

-- Confirm your identity source = Identity Center directory  , if u want to change u can like Active direcory , etc ..

-- now create user in IICenter , IAM users and IIC users are different , copy details of user 

-- create an admin permission set 

-- permission sets on the left --> create permision set --> Predefined permission set --> AdministratorAccess --> create 

-- noe set up account Access for IC admin user 

-- AWS accounts on left side --> choose ur management account --> assign user and groups --> users --> select user u have created --> submit 

-- sign into console with the url in dashboard 

-- once u login with username and password , u will redirected to the accounts that the permissions u have , here i have only one account so it is showing one account \

-- in real time there will so many accounts under the one organizations , u can grant permissions for applications with SSO(single-sign-on)

-- delete this IIC --> login with root account --> IIC --> settings --> management --> delete IIC 




------------------------------------ LAB 2 How do I integrate IAM Identity Center with an Amazon Cognito User Pool ?


Theory 

• Give users an identity to interact with our web or mobile application

• Cognito User Pools:
  
   • Sign in functionality for app users
   • Integrate with API Gateway & Application Load Balancer

• Cognito Identity Pools (Federated Identity):

  • Provide AWS credentials to users so they can access AWS resources directly 
  • Integrate with Cognito User Pools as an identity providers

• Cognito vs IAM: “hundreds of users”, ”mobile users”, “authenticate with SAML”


-- now demo 

---- open IAM Identity center

--  choose Organization

-- Confirm your identity source = Identity Center directory  , if u want to change u can like Active direcory , etc ..

-- now create user in IICenter , IAM users and IIC users are different , copy details of user 

-- now open cognito --> select add user directories to ur app in drop down list --> create userpool --> choose Federated identity providers --> email --> SAML --> no MFA , this is not applicable for the federation users --> nxt --> Send email with Cognito --> skip Step 4 for now(we will do after creating SAML application in IIC) --> give username for pool --> domain name --> client name --> give callback URL --> advanced app client settings --> OAuth 2.0 grant type = choose only Implicit grant --> create userpool

-- now copy the user pool id , we will use them in SAML creation in IIC 

-- now create SAML 2.O application in IIC 

-- go to IIC --> choose application on left side --> add application --> I have an application I want to set up --> SAMl 2.O --> copy link of "IAM Identity Center SAML metadata file" --> Application ACS URL = <cognito domain url>/saml2/idpresponse (eg : https://myuserpool123.auth.ap-south-1.amazoncognito.com/saml2/idpresponse)--> Application SAML audience = urn:amazon:cognito:sp:<cognito pool id> 

-- once u submit --> under actions --> edit attribute mapping --> 

${user:subject}    format = persistent 

-- add new attribute 

email     ${user:email}    format = basic 

-- save changes 

-- now choose assign users --> select user that we have created --> assign user

-- let's configure IAM identity center as a SAML identity provider n cognito user pool 

-- go to cognito in console --> inside user pool --> sign-in experience --> Federated identity provider sign-in =  add Federated identity provider = SAML --> name of ur SAML --> Metadata document source = Enter metadata document endpoint URL (paste url that u have copied while creating SAML , eg : https://portal.sso.ap-south-1.amazonaws.com/saml/metadata/Mjk4MTMyMzY5NjI5X2lucy0zNzMxZjY2ODg5OTY3NjBh) --> saml attribute provide email--> add identity provider 

-- now u must add this identity provider to userpool app client , to do this 

-- go to cognito in console --> inside user pool --> app integration --> open client --> in hosted UI choose edit --> choose identity provider = SAML name that u have created --> save changes 

-- now do test 

-- go to cognito in console --> inside user pool --> app integration --> open client --> view hosted UI --> here u are do lofin with SAML give credentials and login 

-- it is same like when u want to sign up with some other website it will ask u to sign in with fackbook , google , twitter etc..... same we have done here 

-- by this we have successfully configured IIC as identity provider of ur userpool 






------------------------------- cognito vs IIC (AWS IAM Identity Center)

----------- AWS Cognito

AWS Cognito is a service that helps you manage user identities for your web and mobile applications. It provides a variety of features, including:

- User authentication and authorization

- User sign-in and sign-up

- Social media integration

- Multi-factor authentication (MFA)

- Identity federation

- User profiling

- Analytics


AWS Cognito is a good choice for applications that need to manage user identities and authentication independently of other AWS services. It is also a good choice for applications that need to integrate with social media or other identity providers.


--------------- AWS IAM Identity Center

AWS IAM Identity Center is a service that helps you manage sign-in security for your workforce identities. It provides a single place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. You can use AWS IAM Identity Center to:

Create and manage workforce identities

Connect to external identity providers

Centrally manage access to AWS accounts and applications

Implement MFA and other security features

Monitor user activity and audit access


AWS IAM Identity Center is a good choice for organizations that need to manage a large number of workforce identities and access to multiple AWS accounts and applications. It is also a good choice for organizations that need to implement strict security controls.



============================================= AWS Control Tower ==========================

 • Easy way to set up and govern a secure and compliant multi-account AWS environment based on best practices

 • AWS Control Tower uses AWS Organizations to create accounts

 • Benefits:
   • Automate the set up of your environment in a few clicks 
   • Automate ongoing policy management using guardrails
   • Detect policy violations and remediate them
   • Monitor compliance through an interactive dashboard



============================================== AWS Security & Encryption =======================


-- do google for the pictures of encryption for better understanding 


1 Encryption in flight (TLS / SSL)

   • Data is encrypted before sending and decrypted after receiving

   • TLS certificates help with encryption (HTTPS)

   • Encryption in flight ensures no MITM (man in the middle attack) can happen


         client                                                                                            server 

   Client ---(username : admin )------> TLS encryption ----(hbchdubchdubvchutsvhsbcv)------->     TLS Decryption------(username : admin )----Https website           



2  Server-side encryption at rest

  • Data is encrypted after being received by the server

  • Data is decrypted before being sent

  • It is stored in an encrypted form thanks to a key (usually a data key)

  • The encryption / decryption keys must be managed somewhere, and the server must have access to it

  eg : AWS s3 


3  Client-side encryption 

  • Data is encrypted by the client and never decrypted by the server

  • Data will be decrypted by a receiving client

  • The server should not be able to decrypt the data

  • Could leverage Envelope Encryption

  eg : any storage service Like s3 , FTP 


========================================== AWS KMS (Key Management Service) ==============================

• Anytime you hear “encryption” for an AWS service, it’s most likely KMS

• AWS manages encryption keys for us

• Fully integrated with IAM for authorization

• Easy way to control access to your data

• Able to audit KMS Key usage using CloudTrail

• Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM...)

• Never ever store your secrets in plaintext, especially in your code!

  • KMS Key Encryption also available through API calls (SDK, CLI)
  • Encrypted secrets can be stored in the code / environment variables

KMS KeysTypes :

• KMS Keys is the new name of KMS Customer Master Key

• Symmetric (AES-256 keys)
 • Single encryption key that is used to Encrypt and Decrypt
 • AWS services that are integrated with KMS use Symmetric CMKs
 • You never get access to the KMS Key unencrypted (must call KMS API to use)

• Asymmetric (RSA & ECC key pairs)
  • Public (Encrypt) and Private Key (Decrypt) pair
  • Used for Encrypt/Decrypt, or Sign/Verify operations
  • The public key is downloadable, but you can’t access the Private Key unencrypted
  • Use case: encryption outside of AWS by users who can’t call the KMS API


• Types of KMS Keys:

  • AWS Owned Keys (free): SSE-S3, SSE-SQS, SSE-DDB (default key)
  • AWS Managed Key: free (aws/service-name, example: aws/rds or aws/ebs) 
  • Customer managed keys created in KMS: $1 / month
  • Customer managed keys imported (must be symmetric key): $1 / month
  • + pay for API call to KMS ($0.03 / 10000 calls)

• Automatic Key rotation:

  • AWS-managed KMS Key: automatic every 1 year
  • Customer-managed KMS Key: (must be enabled) automatic every 1 year
  • Imported KMS Key: only manual rotation possible using alias

----------------------- KMS Key Policies

• Control access to KMS keys, “similar” to S3 bucket policies

• Difference: you cannot control access without them

• Default KMS Key Policy:
  • Created if you don’t provide a specific KMS Key Policy
  • Complete access to the key to the root user = entire AWS account

• Custom KMS Key Policy:
  • Define users, roles that can access the KMS key
  • Define who can administer the key
  • Useful for cross-account access of your KMS key


------------------------- KMS Multi-Region Keys

• Identical KMS keys in different AWS Regions that can be used interchangeably

• Multi-Region keys have the same key ID, key material, automatic rotation...

• Encrypt in one Region and decrypt in other Regions

• No need to re-encrypt or making cross-Region API calls

• KMS Multi-Region are NOT global (Primary + Replicas)

• Each Multi-Region key is managed independently

• Use cases: global client-side encryption, encryption on Global DynamoDB, Global Aurora


EPV : 

-- Deleting an AWS KMS key in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous.

-- AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2.

-- Deleting an AWS KMS key in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a KMS key in AWS KMS you schedule key deletion.

-- You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the KMS key status and key state is Pending deletion.

-- To recover the KMS key, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the KMS key.




========================================== SSM Parameter Store ================================



• Secure storage for configuration and secrets

• Optional Seamless Encryption using KMS 

• Serverless, scalable, durable, easy SDK

• Version tracking of configurations / secrets

• Security through IAM

• Notifications with Amazon EventBridge • Integration with CloudFormation

-- AWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. 

-- You can store data such as passwords, database strings, Amazon EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values.

-- You can store values as plain text or encrypted data. 

-- You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.

-- AWS Systems Manager Parameter Store can serve as a secrets store, but you must rotate the secrets yourself, it doesn't have an automatic capability for this.


========================================== AWS Secrets Manager =========================

• Newer service, meant for storing secrets

• Capability to force rotation of secrets every X days

• Automate generation of secrets on rotation (uses Lambda)

• Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)

• Secrets are encrypted using KMS

• Mostly meant for RDS integration 



=========================================== AWS Certificate Manager (ACM) ==================================

• Easily provision, manage, and deploy TLS Certificates

• Provide in-flight encryption for websites (HTTPS)

• Supports both public and private TLS certificates

• Free of charge for public TLS certificates

• Automatic TLS certificate renewal

• Integrations with (load TLS certificates on)

  • Elastic Load Balancers(CLB,ALB,NLB)
  • CloudFront Distributions 
  • APIs on API Gateway

• Cannot use ACM with EC2 (can’t be extracted)




-------------------------- ACM – Requesting Public Certificates

1 List domain names to be included in the certificate

   • Fully Qualified Domain Name (FQDN): corp.example.com
   • WildcardDomain:*.example.com


2 Select Validation Method: DNS Validation or Email validation

   • DNS Validation is preferred for automation purposes
   • Email validation will send emails to contact addresses in the WHOIS database
   • DNS Validation will leverage a CNAME record to DNS config (ex: Route 53)

3 It will take a few hours to get verified

4 The Public Certificate will be enrolled for automatic renewal

   • ACM automatically renews ACM-generated certificates 60 days before expiry



------------------------- ACM – Importing Public Certificates

• Option to generate the certificate outside of ACM and then import it

• No automatic renewal, must import a new certificate before expiry

• ACM sends daily expiration events starting 45 days prior to expiration
    • The # of days can be configured
    • Events are appearing in EventBridge

• AWS Config has a managed rule named acm-certificate-expiration-check to check for expiring certificates (configurable number of days) ---- IMP for exam 





===================================================== AWS WAF – Web Application Firewall ======================


-- AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources.

-- AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.


EPV : If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more "IP match conditions." An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.



• Protects your web applications from common web exploits (Layer 7)

• Layer 7 is HTTP (vs Layer 4 is TCP/UDP)

• Deploy on

 • Application Load Balancer
 • API Gateway
 • CloudFront
 • AppSync GraphQL API
 • Cognito User Pool 

• Define Web ACL (Web Access Control List) Rules:

 • IP Set: up to 10,000 IP addresses – use multiple Rules for more IPs
 • HTTP headers, HTTP body, or URI strings Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
 • Size constraints, geo-match (block countries)
 • Rate-based rules (to count occurrences of events) – for DDoS protection

• Web ACL are Regional except for CloudFront

• A rule group is a reusable set of rules that you can add to a web ACL


---------------- WAF – Fixed IP while using WAF with a Load Balancer

• WAF does not support the Network Load Balancer (Layer 4)

• We can use Global Accelerator for fixed IP and WAF on the ALB


Users <---------> Global Accelerator (fixed IP) <-----------> ALB <----------------> EC2 instances 
                                                               |
                                                               |   (attached)
                                                               |
                                                              AWS WAF  , it has WebACL

                                       IMP NOTE : WebACL must be in the same AWS Region as ALB


EPV : 

AWS WAF - How it Works?:

-- To block specific countries, you can create a AWS WAF geo match statement listing the countries that you want to block, 

-- to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. 



EPV : Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.

ANS : Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule




================================================== AWS Shield: protect from DDoS attack =================

• DDoS: Distributed Denial of Service – many requests at the same time

1  AWS Shield Standard:

  • Free service that is activated for every AWS customer
  • Provides protection from attacks such as SYN("synchronize")/UDP Floods, Reflection attacks and other layer 3/layer 4 attacks


2  AWS Shield Advanced:

  • Optional DDoS mitigation service ($3,000 per month per organization)

  • Protect against more sophisticated attack on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53

  • 24/7 access to AWS DDoS response team (DRP)
 
  • Protect against higher fees during usage spikes due to DDoS 

  • Shield Advanced automatic application layer DDoS mitigation automatically creates, evaluates and deploys AWS WAF rules to mitigate layer 7 attacks



=================================================== AWS Firewall Manager ====================================

• Manage rules in all accounts of an AWS Organization

• Security policy: common set of security rules
 
• WAF rules (Application Load Balancer, API Gateways, CloudFront)
• AWS Shield Advanced (ALB, CLB, NLB, Elastic IP, CloudFront)
• Security Groups for EC2, Application Load Balancer and ENI resources in VPC 
• AWS Network Firewall (VPC Level)
• Amazon Route 53 Resolver DNS Firewall
• Policies are created at the region level

• Rules are applied to new resources as they are created (good for compliance) across all and future accounts in your Organization




============================================= WAF vs. Firewall Manager vs. Shield =====================

• WAF, Shield and Firewall Manager are used together for comprehensive protection

• Define your Web ACL rules in WAF

• For granular protection of your resources, WAF alone is the correct choice

• If you want to use AWS WAF across accounts, accelerate WAF configuration, automate the protection of new resources, use Firewall Manager with AWS WAF

• Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the Shield Response Team (SRT) and advanced reporting.

• If you’re prone to frequent DDoS attacks, consider purchasing Shield Advanced


============================================ Amazon GuardDuty =================================

• Intelligent Threat discovery to protect your AWS Account , (malicious activities)

• Uses Machine Learning algorithms, anomaly detection, 3rd party data

• One click to enable (30 days trial), no need to install software

• Input data includes:

  • CloudTrail Events Logs – unusual API calls, unauthorized deployments
     • CloudTrailManagementEvents–createVPCsubnet,createtrail,... 
     • CloudTrailS3DataEvents–getobject,listobjects,deleteobject,...

  • VPC Flow Logs – unusual internal traffic, unusual IP address

  • DNS Logs – compromised EC2 instances sending encoded data within DNS queries 

  • Optional Features – EKS Audit Logs, RDS & Aurora, EBS, Lambda, S3 Data Events...

• Can setup EventBridge rules to be notified in case of findings

• EventBridge rules can target AWS Lambda or SNS

• Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)



VPC Flow Logs / CloudTrail Logs / DNS Logs (AWS DNS) 

optional features = / S3 Logs / EBS Volumes / RDS & Aurora Login Activity / Lambda Network Activity / EKS Audit Logs & Runtime Monitoring ------ > GuardDuty ---> EventBridge --> SNS / Lambda



=========================================================== Amazon Inspector ===================================

• Automated Security Assessments

it onlt evaluates 

1 For EC2 instances

  • Leveraging the AWS System Manager (SSM) agent
  • Analyze against unintended network accessibility
  • Analyze the running OS against known vulnerabilities


2 For Container Images push to Amazon ECR

  • Assessment of Container Images as they are pushed

3  For Lambda Functions

  • Identifies software vulnerabilities in function code and package dependencies

  • Assessment of functions as they are deployed



• Reporting & integration with AWS Security Hub 

• Send findings to Amazon Event Bridge



------------------------------ What does Amazon Inspector evaluate?

• Remember : only for EC2 instances, Container Images & Lambda functions

• Continuous scanning of the infrastructure, only when needed

• Package vulnerabilities (EC2, ECR & Lambda) – database of CVE

• Network reachability (EC2)

• A risk score is associated with all vulnerabilities for prioritization


=========================================================AWS Macie ===========================

• Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.

• Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)

S3 Buckets-------(analyze)------> Macie Discover Sensitive Data (PII)-------(notify)------->Amazon EventBridge--------(integrations)---....




======================================== ec2 (elastic compute cloud )============================================

--it is  a web sevice from aws that provides resizable compute in the cloud

-- resizable: scale in n scale out == elasticity

-- scaleup n scale in == scalability 

-- ec2 is regional 


==================================== Tenancy =====================

-- Tenancy defines how EC2 instances are distributed across physical hardware and affects pricing. There are three tenancy options available: Shared ( default ) — Multiple AWS accounts may share the same physical hardware. Dedicated Instance ( dedicated ) — Your instance runs on single-tenant hardware. and dedicated hosts

-- When it comes to deploying EC2 instances within Amazon Web Services VPCs, you may find yourself confused when presented with those tenancy options.

-- First and foremost, what do we mean by tenancy? Well, tenancy determines who is the owner of a resource. It might be easiest to think of tenancy in terms of housing.

--  For instance if you have a house then you could consider it a dedicated tenant since only one family presumably lives there. However, if you have an apartment building, there is a good chance that several families have rooms in a single building which would be more like a shared tenancy model.

-- AWS provides a few options for tenancy including dedicated or the default type of shared. These models work in a very similar fashion to the housing example above.

-- Shared tenancy means that multiple EC2 instances from different customers may reside on the same piece of physical hardware.

-- The dedicated model means that your EC2 instances will only run on hardware with other instances that you’ve deployed, no other customers will use the same piece of hardware as you.


    1 Shared ( default ) 

    -- The default tenancy model is the one most commonly used with AWS. Multiple customers will share the same pieces of hardware even though they don’t interact with each other.
    
    -- Remember that underneath the covers in AWS, there is a physical host with a hypervisor running on it to handle the virtualization of CPU, Memory, Storage etc. 
    
    -- Customers will choose to deploy a new EC2 instance and AWS fits that instance onto the appropriate physical host and isolate it from other customers even if they’re sharing the same physical resources. This is generally the option that you will want to use unless you have regulatory compliance or licensing restrictions causing you to pick a dedicated model.
    
    -- The shared tenancy model is also the cheapest option for running your EC2 instances.

                                  (or)

    -- This is the default mode, unless we select otherwise, all instances are launched under a shared tenancy model.

    -- When an instance is launched, it is fired up inside a large server in an AWS warehouse. This server not only caters to your instance but also to other instances launched by other AWS users. 

    -- Your instance shares the physical resources of this large server with other AWS accounts.

    -- This is the cheapest option as you only pay for what you use

    EG : just like renting a room inside a house, you pay for the room only and share the kitchen, washroom, and living room with your roommates.


    2 Dedicated Tenancy

    -- As mentioned previously, dedicated tenancy ensures that your EC2 instances are run on hardware specific to your account but comes at a price. AWS usually focuses on driving down costs to operate their data centers and providing you your own isolated hosts to use makes that difficult. The result is that different charges need to be added to make it worthwhile to offer to their customers.
    
    -- Now, you might be asking why you’d want to use a dedicated tenancy model when there are pricing complications associated with them.
    
    -- In some cases due to licensing restrictions some software isn’t allowed to be run on a shared tenancy model. For instance if you’re trying to use Bring Your Own License (BYOL) to AWS, some licenses are based on the Socket model where the number of hosts sockets are used for licensing.
    
    -- In other circumstances, regulatory compliance may dictate that you can’t use the shared model. HIPAA up until earlier this year required dedicated tenancy to ensure data confidentiality. This restriction has since been removed.

        There are two different options for dedicated tenancy with AWS: Dedicated Hosts and Dedicated Instances.

                                        (or)
    
    -- When an instance is launched under a dedicated instance model, AWS starts your virtual machine inside “single-tenant hardware”. This means that all instances launched in this physical server will be from your account only.

    -- This is very valuable when it comes to compliance and regulatory requirements that your business has to adhere to — such as physical isolation of hardware for security, privacy and government regulatory reasons.

    -- This option is slightly more pricey because a flat fee of $2/hour is charged for every region you may fire up your instance.

    -- Also, on top of the regular EC2 fees, AWS charges 10% more for dedicated instances compared to instances under the shared tenancy model.



EPV : An IT company is looking to move its on-premises infrastructure to AWS Cloud. The company has a portfolio of applications with a few of them using server bound licenses that are valid for the next year. To utilize the licenses, the CTO wants to use dedicated hosts for a one year term and then migrate the given instances to default tenancy thereafter.

ANS : 

-- You can change the tenancy of an instance from dedicated to host

-- You can change the tenancy of an instance from host to dedicated

EXP : Each Amazon EC2 instance that you launch into a VPC has a tenancy attribute. This attribute has the following values.

Tenancy Value                      Description

default                         ur instance runs on a shared hardware

dedicated                       ur instance runs on single-tenant hardware

host                            ur instance runs on a dedicated hosts , which is an isolated server with configurations that u an control


-- By default, Amazon EC2 instances run on a shared-tenancy basis.

-- Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at the hardware level.

--  However, Dedicated Instances may share hardware with other instances from the same AWS account that is not Dedicated Instances.

-- A Dedicated Host is also a physical server that's dedicated to your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.




-----------------------------EC2 Instances Purchasing Options


• On-Demand Instances – short workload, predictable pricing, pay by second

• Reserved (1 & 3 years)
   • Reserved Instances – long workloads
   • Convertible Reserved Instances – long workloads with flexible instances

• Savings Plans (1 & 3 years) –commitment to an amount of usage, long workload

• Spot Instances – short workloads, cheap, can lose instances (less reliable)

• Dedicated Hosts – book an entire physical server, control instance placement

• Dedicated Instances – no other customers will share your hardware

• Capacity Reservations – reserve capacity in a specific AZ for any duration



              1  ON-Demand instances: pay as u go 

-fixed price , pay per hour 
- no upfront payments
- no predictable
- no long-term commitment
- short term committment

• Pay for what you use:
   • Linux or Windows - billing per second, after the first minute
   • All other operating systems - billing per hour

• Has the highest cost but no upfront payment

• Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave


             2  Reserved instances 

- Long term commitent 
- 1 or 3 years
- upfront payments(full , partial)
- 75% discount approx

------- we have 3 types of RI

a   Standard RI : where u get 75% discount

b   convertible RI : to change the capacity of the instance 66% discount 
    • Can change the EC2 instance type, instance family, OS, scope and tenancy

    
c   Schedule RI :  reserve it for short term like fraction of day , week, or month


• Up to 72% discount compared to On-demand

• You reserve a specific instance attributes (Instance Type, Region,Tenancy, OS)

• Reservation Period – 1 year (+discount) or 3 years (+++discount)

• Payment Options – No Upfront (+), Partial Upfront (++), All Upfront (+++)

• Reserved Instance’s Scope – Regional or Zonal (reserve capacity in an AZ)

• Recommended for steady-state usage applications (think database)

• You can buy and sell in the Reserved Instance Marketplace




            3 Spot Instances: 

• Can get a discount of up to 90% compared to On-demand

• Instances that you can “lose” at any point of time if your max price is less than the current spot price

• The MOST cost-efficient instances in AWS

• Useful for workloads that are resilient to failure
  • Batch jobs
  • Data analysis
  • Image processing
  • Any distributed workloads
  • Workloads with a flexible start and end time

• Not suitable for critical jobs or databases
  
- Biding 
- huge capacity for cheaper price
- 90% discount


-- Spot blocks allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted

   EXP : Spot blocks are designed not to be interrupted and will run continuously for the duration you select (1 to 6 hours), independent of the Spot market price.

   -- In rare situations, Spot blocks may be interrupted due to Amazon Web Services' capacity needs. In these cases, AWS will provide a two-minute warning before it terminates your instance and you will not be charged for the affected instance(s).


-- Spot Instances with a defined duration (also known as Spot blocks) are designed not to be interrupted and will run continuously for the duration you select. 

-- This makes them ideal for jobs that take a finite time to complete, such as batch processing, encoding and rendering, modeling and analysis, and continuous integration.

-- Running our load on a Spot Instance with Spot Block sounds like the perfect use case, as we can block the spot instance for 1 hour, run the script there, and then the instance will be terminated.





some point to know about sopt instances : 

1  If a spot request is persistent, then it is opened again after your Spot Instance is interrupted

EXP : -- A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. 

      --  If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance.

2  Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated

EXP : -- The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.

      -- You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request.

3  When you cancel an active spot request, it does not terminate the associated instance

EXP : -- If your Spot Instance request is active and has an associated running Spot Instance, or your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance; 

      -- you must terminate the running Spot Instance manually. Moreover, to cancel a persistent Spot request and terminate its Spot Instances, you must cancel the Spot request first and then terminate the Spot Instances.





                4 Dedicated Host

• A physical server with EC2 instance capacity fully dedicated to your use

• Allows you address compliance requirements and use your existing server- bound software licenses (per-socket, per-core, pe—VM software licenses)

• Purchasing Options:
   • On-demand – pay per second for active Dedicated Host
   • Reserved - 1 or 3 years (No Upfront, Partial Upfront,All Upfront)

• The most expensive option

• Useful for software that have complicated licensing model (BYOL – Bring Your
Own License)

• Or for companies that have strong regulatory or compliance needs

-- With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance


- if u need a physical machine with VM's for this model 

- privaacy

- high security

- Dedicated Hosts enable you to use your existing server-bound software licenses like Windows Server and address corporate compliance and regulatory requirements.


                   5  EC2 dedicated instances 

• Instances run on hardware that’s dedicated to you

• May share hardware with other instances in same account

• No control over instance placement (can move hardware after Stop / Start)

-  Dedicated instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. 

- Your dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. 

- Dedicated instances may share hardware with other instances from the same AWS account that are not dedicated instances. 

- Dedicated instances cannot be used for existing server-bound software licenses.



EPV : A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines.

Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?

ANS : dedicated instances 



                   6  savings plans 

- Savings Plans are a flexible pricing model that offer low prices on EC2, Fargate and Lambda usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term. 

- Savings Plans provide you the flexibility to use the compute option that best suits your needs and automatically save money, all without having to perform exchanges or modifications. When you sign up for a Savings Plan, you will be charged the discounted Savings Plans price for your usage up to your commitment.

- Savings Plans allow you to easily reduce your bill by making a commitment to compute usage (e.g. $10/hour) instead of making commitments to specific instance configurations or compute services.

- Amazon Web Services offers two types of Savings Plans - Compute Savings Plans and EC2 Instance Savings Plans.

1 Flexible

-- Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to:

   - EC2 instance usage regardless of instance family, size, Availability Zone, Region, OS, or tenancy
   - Fargate usage
   - Lambda usage for Duration, Provisioned Concurrency, and Provisioned Duration

   EG : For example, with Compute Savings Plans, you can change from C4 to M5 instances,, shift a workload from EU (Ireland) to EU (London), or move a workload from EC2 to Fargate or Lambda at any time and automatically continue to pay the Savings Plans price.



2 Significant Discounts

  - EC2 Instance Savings Plans provide the lowest prices, offering savings up to 72% in exchange for commitment to usage of individual instance families in a region (e.g. M5 usage in N. Virginia). 

  - This automatically reduces your cost on the selected instance family in that region regardless of AZ, size, OS or tenancy. 

  -  EC2 Instance Savings Plans give you the flexibility to change your usage between instances within a family in that region.

  EG : For example, you can move from c5.xlarge running Windows to c5.2xlarge running Linux and automatically benefit from the Savings Plan prices.


• Get a discount based on long-term usage (up to 72% - same as RIs)

• Commit to a certain type of usage ($10/hour for 1 or 3 years)

• Usage beyond EC2 Savings Plans is billed at the On-Demand price

• Locked to a specific instance family & AWS region (e.g., M5 in us-east-1)

• Flexible across:
   • Instance Size (e.g., m5.xlarge, m5.2xlarge)
   • OS (e.g., Linux, Windows)
   • Tenancy (Host, Dedicated, Default)

- it has same as RI but difernet starategy



IMP to KNOW : You can use Dedicated Hosts and Dedicated instances to launch Amazon EC2 instances on physical servers that are dedicated for your use.

-- An important difference between a Dedicated Host and a Dedicated instance is that a Dedicated Host gives you additional visibility and control over how instances are placed on a physical server, and you can consistently deploy your instances to the same physical server over time.

-- As a result, Dedicated Hosts enable you to use your existing server-bound software licenses and address corporate compliance and regulatory requirements.




                   7  EC2 Capacity Reservations

• Reserve On-Demand instances capacity in a specific AZ for any duration

• You always have access to EC2 capacity when you need it

• No time commitment (create/cancel anytime), no billing discounts

• Combine with Regional Reserved Instances and Savings Plans to benefit from billing discounts

• You’re charged at On-Demand rate whether you run instances or not

Suitable for short-term, uninterrupted workloads that needs to be in a "specific AZ"


Which purchasing option is right for me? 


• On demand: coming and staying in resort whenever we like, we pay the full price

• Reserved: like planning ahead and if we plan to stay for a long time, we may get a good discount.

• Savings Plans: pay a certain amount per hour for certain period and stay in any room type (e.g., King, Suite, Sea View, ...)

• Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms.You can get kicked out at any time

• Dedicated Hosts: We book an entire building of the resort

• Capacity Reservations: you book a room for a period with full price even you don’t stay in it



-------------------------- Spot Fleets

• Spot Fleets = set of Spot Instances + (optional) On-Demand Instances

• The Spot Fleet will try to meet the target capacity with price constraints

   • Define possible launch pools: instance type (m5.large), OS, Availability Zone
   • Can have multiple launch pools, so that the fleet can choose
   • Spot Fleet stops launching instances when reaching capacity or max cost

• Strategies to allocate Spot Instances:

   • lowestPrice: from the pool with the lowest price (cost optimization, short workload)
   • diversified: distributed across all pools (great for availability, long workloads)
   • capacityOptimized: pool with the optimal capacity for the number of instances
   • priceCapacityOptimized (recommended): pools with highest capacity available, then select the pool with the lowest price (best choice for most workloads)

• Spot Fleets allow us to automatically request Spot Instances with the lowest price


---------------------------------------------- EC2Families/instance type 

- General Instance = for general purpose

- Memory instannce = if u need more memory for ur application

- CPU instances = more CPU's

- storage = for storage purpose

- GPU = for heavy machines, Graphicd etc 

----instace Type == CPU + Memory

- during the scalability , no data loss is occurs , coz data is stored in EBS volumes, downtime is there 

- if u have HA , no downtime


---------Brustable performnce instance 

- it is billable 

- when there is lot of traffic coming to the ec2instance(t2.micro) it is not sufficient to handle 

- so aws will give cpu credits , ec2 will enter into the brustable mode and it give high performance for limited of time only

- "cpu credits" deped on the type of instances

- onlt t2 and t3 tyes supports "brustble prformance"


=================================== Volumes ==========================================

- nothing but hard disks

- root volm device name = /dev/sda1


- -------------------------------two types of volumes 

- 1 EBS Volumes 

- persistent storage/ permanent storage

- if u stop and start the ec2 instance DATA is not lost 

- EBS volume max sixex is = 16tb

- EBS is billable 

- if u reboot = data is not lost 



--------Types of EBS Volumes

- General Purpose (gp2,gp3) - SSD = general purpose

-- gp2 is backed by solid-state drives (SSDs) and is suitable for a broad range of transactional workloads, including dev/test environments, low-latency interactive applications, and boot volumes. It supports max IOPS/Volume of 16,000.


- Provisional IOPS (io1, io2) = Highly performance

-- Provisioned IOPS SSD (io1) is backed by solid-state drives (SSDs) and is a high-performance Amazon EBS storage option designed for critical, I/O intensive database and application workloads, as well as throughput-intensive database workloads. 

-- io1 is designed to deliver a consistent baseline performance of up to 50 IOPS/GB to a maximum of 64,000 IOPS and provide up to 1,000 MB/s of throughput per volume. Therefore, the io1 volume type would be able to meet the requirement of 25,000 IOPS per volume for the given use-case.



- Throughtput(st1) - HDD = frequntly access data with cheaper price

-- st1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. It supports max IOPS/Volume of 500.


- cold(sc1)- HDD = not frequently access data with cheaper price

 -- sc1 is backed by hard disk drives (HDDs). It is ideal for less frequently accessed workloads with large, cold datasets. It supports max IOPS/Volume of 250.


- Magnetic(standard)-HDD = Previous generation



• EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)

• When in doubt always consult the AWS documentation – it’s good!

• Only gp2/gp3 and io1/io2 Block Express can be used as boot volumes


---------------------------- Detailed explanation of Volumes


1 Solid state drive (SSD) volumes

   - General Purpose SSD volumes = gp3 , gp2

   - Provisioned IOPS SSD volumes = io2 Block Express , io1


--  General Purpose SSD volumes = gp3 , gp2

  • Cost effective storage, low-latency

  • System boot volumes,Virtual desktops, Development and test environments

  • 1 GiB - 16TiB

• gp3:
  
  • Baseline of 3,000 IOPS and throughput of 125 MiB/s
  • Can increase IOPS up to 16,000 and throughput up to 1000 MiB/s independently

• gp2:

  • Small gp2 volumes can burst IOPS to 3,000
  • Size of the volume and IOPS are linked, max IOPS is 16,000
  • 3 IOPS per GB, means at 5,334 GB we are at the max IOPS


  --- The following is a summary of the use cases and characteristics of SSD-backed volumes.

  Durability     =  99.8% - 99.9% durability (0.1% - 0.2% annual failure rate)

  Use cases      =  Transactional workloads , Virtual desktops , Medium-sized, single-instance databases , Low-latency interactive applications , Boot volumes , Development and test environments

  Volume size    =  1 GiB - 16 TiB

  Max IOPS per volume  = 16,000 (64 KiB I/O) |   16,000 (16 KiB I/O)

  Max throughput per volume = 1,000 MiB/s | 250 MiB/s 

  Amazon EBS Multi-attach = Not supported

  NVMe reservations = Not supported

  Boot volume = Supported




--- Provisioned IOPS (PIOPS) SSD = io2 Block Express , io1

• Critical business applications with sustained IOPS performance

• Or applications that need more than 16,000 IOPS

• Great for databases workloads (sensitive to storage perf and consistency)

• io1 (4 GiB - 16TiB):
   
   • Max PIOPS: 64,000 for Nitro EC2 instances & 32,000 for other
   • Can increase PIOPS independently from storage size

• io2 Block Express (4 GiB – 64 TiB):
  
   • Sub-millisecond latency
   • Max PIOPS: 256,000 with an IOPS:GiB ratio of 1,000:1

• Supports EBS Multi-attach


The following is a summary of the use cases and characteristics of SSD-backed volumes.

                                                              io2 Block Express                                                                      io1

Durability                                            99.999% durability (0.001% annual failure rate)                                        99.8% - 99.9% durability (0.1% - 0.2% annual failure rate)

Use cases                                             Workloads that require: Sub-millisecond latency,Sustained IOPS performance,            Workloads that require sustained IOPS performance or more than 16,000 IOPS , I/O-intensive database workloads
                                                        More than 64,000 IOPS or 1,000 MiB/s of throughput
                                              
Volume size                                           4 GiB - 64 TiB                                                                             4 GiB - 16 TiB

Max IOPS per volume                                    256,000 (16 KiB I/O)                                                                      64,000 (16 KiB I/O)

Max throughput per volume                              4,000 MiB/s                                                                               1,000 MiB/s

Amazon EBS Multi-attach                                Supported                                                                                  Supported

NVMe reservations                                      not Supported                                                                              not Supported

Boot volume                                            Supported                                                                                   Supported




2 Hard Disk Drives (HDD) 

 -- st1 

 --	sc1

 • Cannot be a boot volume

 • 125 GiB to 16TiB

 • Throughput Optimized HDD (st1)
   
   • Big Data, Data Warehouses, Log Processing
   • Max throughput 500 MiB/s – max IOPS 500

• Cold HDD (sc1):
   
   • For data that is infrequently accessed
   • Scenarios where lowest cost is important
   • Max throughput 250 MiB/s – max IOPS 250



3 Previous generation volumes =  Magnetic(standard)

 -- Magnetic (standard) volumes are previous generation volumes that are backed by magnetic drives. 

 -- They are suited for workloads with small datasets where data is accessed infrequently and performance is not of primary importance. 

 -- These volumes deliver approximately 100 IOPS on average, with burst capability of up to hundreds of IOPS, and they can range in size from 1 GiB to 1 TiB.


 The following table describes previous-generation EBS volume types.

    Volume type  =  standard

    Use cases	   =  Workloads where data is infrequently accessed
   
    Volume size	 =  1 GiB-1 TiB

    Max IOPS per volume	 = 40–200
  
    Max throughput per volume	 = 40–90 MiB/s
   
    Boot volume	   =  supported







---------------------------------------------  which volume should use when?

- for gneral purpose - gp2,gp3 

-  if u need HP = io1,io2

--- "gp2 is default EBS volume type" , now gp3 is default volume in new aws console

--- IOPS = input and output per second , how mant inputs and outputs u are getting from harddisk

Note : io1,io2 , gp3 are iops configurable 

-- The more iops , the more Bill

-- GP2 has default IOPS = 1:3 =1gb:3IOPS -> it is not IOPS configurable

-- Root volume supports (gp2,gp3,io1,io2 and standard) except sc1 n st1

-- Additional volume supports all types of volumes


-- generally u cant attach volume to multiple instace at the same time 

- but io1 , io2 can be multi-attached at the same time in same AZ

-- up to 16 EC2 instance at a time can be attached 

-- EBS Multi-Attch vs Elastic FIle System


                  EFS            |      EBS-Multi-Attach

Stoarge Type =   file system            Block Storage

Service Type =   Fully manages          Needs To be provisioned

Accessibility =  Regional                Zonal

durability   =   Data Replicated across Multiple AZ's Within a region   |  Data Replicated across same AZ's 

Data Scalability = Unlimited             Limited

pricing Model =    pay for what u use    Pay for provisioned Capacity

Storage Tiers =    Stad and Std-IA       Not supported 

Instance Scalabilty =  can scale to hundreds to thousnds |        Currently limited to 16


-- U can Not attach volumes to Non-Nitro instance Like t2.micro , u can do with t3.micro for Multi attach 

-- for more info look at multi attach documentation.



--------------------------------- Amazon EBS pricing

-- With Amazon Elastic Block Store (EBS), you pay only for what you provision. Volume storage for all EBS volume types is charged by the amount of GB you provision per month until you release the storage. Costs increase for EBS volumes that support additional input/output operations per second (IOPS) and throughput beyond baseline performance.

-- AWS Free Tier includes 30 GB of storage, 2 million I/Os, and 1 GB of snapshot storage with Amazon Elastic Block Store (EBS).



    Volume Type                                                     Price

General Purpose SSD (gp3) - Storage	                            $0.08/GB-month

General Purpose SSD (gp3) - IOPS	                              3,000 IOPS free and $0.005/provisioned IOPS-month over 3,000

General Purpose SSD (gp3) - Throughput	                        125 MB/s free and $0.040/provisioned MB/s-month over 125

General Purpose SSD (gp2) Volumes	                              $0.10 per GB-month of provisioned storage

Provisioned IOPS SSD (io2) - Storage	                          $0.125/GB-month

Provisioned IOPS SSD (io2) - IOPS	                              $0.065/provisioned IOPS-month up to 32,000 IOPS

                                                                $0.046/provisioned IOPS-month from 32,001 to 64,000 IOPS
  	                                                            $0.032/provisioned IOPS-month for greater than 64,000 IOPS

Provisioned IOPS SSD (io1) Volumes	                            $0.125 per GB-month of provisioned storage AND $0.065 per provisioned IOPS-month                                                           

Throughput Optimized HDD (st1) Volumes	                        $0.045 per GB-month of provisioned storage

Cold HDD (sc1) Volumes                                          $0.015 per GB-month of provisioned storage



Examples for pricing : 


Example 1 – General Purpose SSD (gp3) Volumes

-- Volume storage for General Purpose SSD (gp3) volumes is charged by the amount you provision in GB per month until you release the storage. All gp3 volumes include a free baseline performance of 3,000 provisioned IOPS (input/output operations per second) and 125 provisioned MB/s throughput. 

-- Additional IOPS and throughput can be provisioned independently and are charged by the amount you provision in IOPS per month and MB/s per month until you release the IOPS or throughput. Provisioned storage, provisioned IOPS, and provisioned throughput for gp3 volumes will be billed in per-second increments, with a 60-second minimum.


Q : For example, let’s say that you provision a 2,000 GB volume for 12 hours (43,200 seconds) in a 30-day month. Additionally, you provision 10,000 IOPS and 500 MB/s for your volume.

ANS : 

(A) Gp3 volume charge:

    In a region that charges $0.08 per GB-month, you would be charged:

    ($0.08 per GB-month * 2,000 GB * 43,200 seconds / (86,400 seconds/day * 30-day month)) = $2.667

(B) Gp3 IOPS charge:
    
    In a region that charges $0.005 per provisioned IOPS-month, you would be charged:
    
    ($0.005 per provisioned IOPS-month * (10,000 IOPS provisioned – 3,000 IOPS in the free baseline performance) * 43,200 seconds /(86,400 seconds /day * 30-day month))= $0.583

(C) Gp3 baseline performance charge:

    In a region that charges $0.06 per provisioned MB/s-month, you would be charged:

    ($0.06 per provisioned MB/s-month * (500 MB/s provisioned – 125 MB/s in the free baseline performance) * 43,200 seconds /(86,400 seconds /day * 30 day-month))= $0.375

(D) Total charges for example 1:

      Gp3 volume charge = $2.667
      Gp3 IOPS charge = $0.583
      Gp3 baseline performance charge = $0.375

      Total: $3.625 per 30-day month


Example 2 – EBS General Purpose SSD (gp2) Volumes

Q : Volume storage for General Purpose SSD (gp2) volumes is charged by the amount you provision in GB per month until you release the storage. Provisioned storage for gp2 volumes will be billed in per-second increments, with a 60-second minimum. I/O is included in the price of the volumes, so you pay only for each GB of storage you provision.


Gp2 volume charge: 

   For example, let's say that you provision a 2,000 GB volume for 12 hours (43,200 seconds) in a 30-day month. 

   In a region that charges $0.10 per GB-month, you would be charged:($0.10 per GB-month * 2,000 GB * 43,200 seconds / (86,400 seconds/day * 30 day-month)) = Total: $3.33 per 30-day month



Example 3 – EBS Provisioned IOPS SSD (io1) Volumes

-- Volume storage for EBS Provisioned IOPS SSD (io2 and io1) volumes is charged by the amount you provision in GB per month until you release the storage. With Provisioned IOPS SSD (io1 and io2) volumes, you are also charged by the amount you provision in IOPS (input/output operations per second) per month. 

-- Provisioned storage and provisioned IOPS for io1 and io2 volumes will be billed in per-second increments, with a 60-second minimum.



For example, let’s say that you provision a 2,000 GB volume for 12 hours (43,200 seconds) in a 30-day month. 

In a region that charges $0.125 per GB-month, you would be charged $4.167 for the volume ($0.125 per GB-month * 2,000 GB * 43,200 seconds / (86,400 seconds/day * 30-day month)).

 
Additionally, you provision 1,000 IOPS for your volume. 

In a region that charges $0.065 per provisioned IOPS-month, you would be charged $1.083 for the IOPS that you provisioned ($0.065 per provisioned IOPS-month * 1,000 IOPS provisioned * 43,200 seconds /(86,400 seconds /day * 30-day month)).

For this example, the charges would be:

$5.25 ($4.167 + $1.083).



Example 4 – EBS Throughput Optimized HDD (st1) Volumes

Volume storage for Throughput Optimized HDD (st1) volumes is charged by the amount you provision in GB per month until you release the storage. Provisioned storage for st1 volumes will be billed in per-second increments, with a 60-second minimum. I/O is included in the price of the volumes, so you pay only for each GB of storage you provision.

For example, let's say that you provision a 2,000 GB volume for 12 hours in a 30-day month. 

 In a region that charges $0.045 per GB-month, you would be charged $1.50 for the volume ($0.045 per GB-month * 2,000 GB * 43,200 seconds / (86,400 seconds /day * 30 day-month)).



Example 6 – EBS Cold HDD (sc1) Volumes

Volume storage for Cold HDD (sc1) volumes is charged by the amount you provision in GB per month until you release the storage. Provisioned storage for sc1 volumes will be billed in per-second increments, with a 60-second minimum. I/O is included in the price of the volumes, so you pay only for each GB of storage you provision.

For example, let's say that you provision a 2,000 GB volume for 12 hours (43,200 seconds) in a 30-day month

In a region that charges $0.015 per GB-month, you would be charged $0.50 for the volume ($0.015 per GB-month * 2,000 GB * 43,200 seconds / (86,400 seconds/day * 30-day month)).





---------------------LAB-------------------

-- create 2 instances with t3.micro configuration

-- create one volume with configuration of io2 , as same zone as instance that u have created 

-- now attach volume to the both instances

-- in order to working with these part we have to follow some configurations , we have to convert these volumes to the file systems 

-- how to do?

-- connect both instances

-- perform some cmnds 
sudo su
lsblk    -- to check the volume is attached or not 
-- u can able to see the volume that is attached like eg : nvme1n1
--  check file system it is there or not 
file -s /dev/nvme1n1 
-- we do not have any file systems here 
NOTE : convert file systems on one server only not in 2 serves
now create file system 
mkfs -t xfs /dev/nvme1n1 
-- now creat directory
mkdir <anyname>
now do mount /dev/nvme1n1 testtf(ur name of directory)
df -h to see check the mounting 
-- enter into directory 
cd testtf
create some files in inti
touch subbu.txt
-- now the same file should be present in the another server also so chekc 
-- go to 2nd server do sudo su then  create one folder
mkdir <name>
do mount here 
/dev/nvme1n1 <name of ur directory in second server>
go to directory and do ls to check the file is there or not 

-- the file is present in 2nd server also , here it is replicating the data from one server to another server 

== these are some limitations , so always go for the EFS 



2  Instace store volumes 

- not persistent / Temporry storage

- if u stop and start the e2 instance , data is lost 

- it is free volumes

- You can't detach an instance store volume from one instance and attach it to a different instance

- If you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI.

- Emphemeral storage -- another name of ISV 

EPV : An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host instance. Instance store is ideal for the temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.As Instance Store based volumes provide high random I/O performance at low cost (as the storage is part of the instance's usage cost) and the resilient architecture can adjust for the loss of any instance, therefore you should use Instance Store based Amazon EC2 instances for this use-case.

- based on the size of instance, instance store volume will provided by aws 

- u can add instace store volume as additional volume , while u creatinf image for instances , 



--------------


-  if u reboot = data is not lost 

-  the reason behind the aws is that 


NOTE -- whenever if u store ur data in the EBS(central storage) , here once you do


- start and stop = Jump = data is not lost 

- Reboot = no jump = Data is not lost 

- terminate = Data lost 

Explanation : here in the EBS volume it is centraled stored so, when u do start and stop the ec2 instance it will jump from one host machine to another host machine ,due to centrally storage it will get the data volume without any loss


--- NOTE : when you store data in the "INSTANCE STORE"

- stop and start = jump = Data is LOST 

Explanation : when there is instance store , a hard disk is attached by aws to the instance store ,so when there is jump happen here it will jump to another host machine but hard disk will connected to the host machine only , so data is getting lost 

-- Reboot = no jump = Data is not Lost



----------------to know the ec2 is launched properly or not 

- they are 2 types of status checks 

1 instance status check ( software check)  = check the EC2 VM

     -- These are performed by AWS on the instance itself to check if the guest operating system is reachable and running properly. 

     -- A failed instance check could mean a problem with the software configuration of the instance like an incorrect startup script.


2 system status check (hardware check) -- ip, network etc..... 

    -- These are performed by AWS and check if the underlying host system of the instance is reachable and functioning properly.

    -- A failed system check could indicate an issue with the hardware host. 

    -- The main system check is a reachability check to confirm network packets can reach the instance



NOTE: if u get 1/2 or 0/2 status check are passed , just STOP and START the ec2 instance

or 

 terminate it and launch fresh 


-- instance store volume give high performance 

-- EBS volumes are network drives with good but limited performnae



=============================================== DeleteOnTermination ================================================


NOTE : if u terminate the instance all ur root volumes get terminated by default coz, "delete on termination" is checked/enabled / Yes

-  if u want to retain the root volume do uncheck  "delete on termination" while launching te ec2 instance 

- if u terminate ec2 , by default "additional volume will not be deleted coz "DOT" is unchecked/disabled/No


========= IMP : If the instance is already running, you can set DeleteOnTermination to False using the "command line."
    
EG : To modify the deleteOnTermination attribute of the root volume through command line 

-- The following modify-instance-attribute example sets the deleteOnTermination attribute for the root volume of the specified Amazon EBS-backed instance to false. By default, this attribute is true for the root volume.

     cmd : 

     aws ec2 modify-instance-attribute --instance-id i-1234567890abcdef0 --block-device-mappings "[{\"DeviceName\": \"/dev/sda1\",\"Ebs\":{\"DeleteOnTermination\":false}}]"

      This command produces no output




=========================================================== EC2 user data ===============================

-- User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file.


EPV : By default, scripts entered as user data are executed with root user privileges

EXP : 

-- Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script.

-- Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.


By default, user data runs only during the boot cycle when you first launch an instance

EXP : By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance.

-- You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.



--------------------------------- EC2 User Data

• It is possible to bootstrap our instances using an EC2 User data script.

• bootstrapping means launching commands when a machine starts

• That script is only run once at the instance first start

• EC2 user data is used to automate boot tasks such as:

   • Installing updates
   • Installing software
   • Downloading common files from the internet
   • Anything you can think of

• The EC2 User Data Script runs with the root user


EG :


#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html



=====================================SNAPSHOTS===============

-- Backup of the volume is called "SNAPSHOTS"

-- Snapshots are incremental backups 

-- u can create volume from the snapshots 

-- snapshot is point in time copy of the volume 

-- snapshots does not contain any A.Z's

-- EBS volumes ---> snpshots --> ebs volums

-- u can't attach snapshot to the ec2, u have to create a volume from snapshot

-- u can't login into the snapshot

-- snapshots are stored in S3(provided by AWS)

-- snapshots are regional

-- by default snapshots are PRIVATE , if require u can make public

-- u can copy the snapshots across the region in the same account , and aws accounts also usng aws id (private )

-- ebs volms can't moved from one AZ to another AZ but u can take snapshots and use in anaother AZ

-- by default volumes, snapshots are not encrypted

-- Decryption is handled by AWS

-- NOT encryptd --> NOT Encrypted

-- encrypted --> encrypted

-- NOT Encrypted --> encrypted(Copy Option in ec2 u can make encryption)

-- all encryption keys are stored in KMS (key mangement service)

-- NOTE:  encryption snapshots cannot be shared to other accounts 



------------ instace store volumes are created from a template stored in S3

-- to create a snapshot u no need to stop the ec2

-- DATA LIFE CYCLE MANAGER : it is used to take snapshots automatically / sechedule

-- these volumes will get identified by using TAGS

-- RETEntion period = 7 days

------------------- EBS snapshot standard and Archive tier 

-- move the snapshots to Archieve tier i.e 75% cheaper

-- it takes 24 - 72 hrs for restoring from archieve tier

---------------RECYCLE BIN

-- setup rules to retain deleted snpshots so u can recover them after accidental deletion, (retention period 1 day to 1 yr)

------------Fast snapshot Restore(FSR)

-- it is bilable

-- forceful initilization of snaphat to have no latency on the first use


-----------How to encrypt an unencrypted EBS Volume

- Create an EBS snapshot of volume
- encryption the EBS snapshot (using Copy)
- create new EBS volume from snapshots
- now u can attach the encrypted volume to the original instance



------------------------------------- Pricing of SNAPSHOTS 

-- Amazon EBS Snapshots are point-in-time copies of your block data. EBS Snapshots in the Standard tier are stored incrementally, which means you are billed only for the changed blocks stored. EBS Snapshots in the Archive tier are full copies of your block data, which means you are billed for all the blocks stored and not just the changed blocks. 

-- Snapshots in the Archive tier have a minimum retention period of 90 days. Archived snapshots that are deleted or permanently restored to the Standard tier before 90 days will incur a pro-rated charge equal to the storage charge for the remaining days. Additional charges apply for retrievals from the Archive tier.


EBS Snapshots Storage Pricing : 

Standard	       $0.05/GB-month 

Archive	         $0.0125/GB-month


EBS Snapshots Restore Pricing :

Standard	     Free

Archive	       $0.03 per GB of data retrieved


Amazon EBS Fast Snapshot Restore :

-- Fast Snapshot Restore (FSR) allows you to promptly restore fully provisioned EBS volumes from snapshots, regardless of the size of the volume or snapshot. You can enable FSR on snapshots you own or those shared with you.

-- FSR is charged in Data Services Unit-Hours (DSU-Hours) for each snapshot and each Availability Zone in which it is enabled. DSUs mean that you are billed per minute with a one-hour minimum. You will continue to incur charges on an FSR-enabled snapshot until you disable it.


Fast Snapshot Restore            $0.75 per 1 DSU hour on each snapshot and in each AZ it is enabled


EG : EBS Fast Snapshot Restore

-- Example pricing is based on the US-East (N. Virginia) region. Fast Snapshot Restore (FSR) is charged in Data Services Unit-Hours (DSU-Hours) for each snapshot and each Availability Zone in which it is enabled.

-- DSUs are billed per minute with a one-hour minimum. You will continue to incur charges until you disable FSR on a snapshot. The price of 1 DSU-Hour is $0.75.

1 For example, you enable FSR on an EBS Snapshot in three Availability Zones (AZs) and you disable it after 90 minutes. 

The price of one DSU-Hour is $0.75. Since FSR was enabled on one snapshot for 90 minutes in three AZs, you will be billed as 1 snapshot * 3 AZs * 1.5 DSU-hours at $0.75 per DSU-Hour or $3.375.


2 As another example, you enable FSR on an EBS Snapshot in one Availability Zone, and you disable it after 45 minutes

The price of 1 FSR DSU-Hour is $0.75. Since there is a one-hour minimum, you will be billed as 1 snapshot * 1 AZ * one DSU-Hour at $0.75 per DSU-Hour or $0.75.


3 Let’s consider another example where you enable FSR on three snapshots in two Availability Zones, and you disable it after 2.5 hours.

Since FSR was enabled for 2.5 hours (150 minutes) on 3 snapshots and in 2 AZs, you will be billed as 3 snapshots * 2 AZs * 2.5 DSU-hours at $0.75 per FSR DSU-Hour or $11.25.



===============================IMAGES===================

-- copy of the os is called IMAGE 

-- in AWS AMI = amazon machine images

-- template of os is called "AMI"

-- AMI --> copy of entire ec2 instance(includes volumes)

-- AMI contains OS or OS+apps

-- AMI's stored in aws S3

-- copy of the image includes all configuration that we did original istance

-- 1 AMI, can be used to launch multiple ec2 instane

-- it does not have any AZ

-- by default AMI are private 

-- AMI are regional 

-- AMI can be copied from one region to another region

-- AMI can be shared from one a to another aws account

--------------All public images are located in "market place"

-- GOLDEN AMI : images are created automatically 

-- creating image builder --> through : "ec2 image builder"

-- images are backed by either ebs volumes or ISV 



IMP NOTE : whenever if u create image(it has 2 volumes by default) , automatically 2 snapshots are generated 


-- no need to stop the ec2 to take snapshot but it is recommended to take snapshot after stop the ec2 


EPV : The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B.

ANS : 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B

- An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. 

- When the new AMI is copied from Region A into Region B, it automatically creates a snapshot in Region B because AMIs are based on the underlying snapshots. Further, an instance is created from this AMI in Region B. Hence, we have 1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B.




EPV : The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system.

ANS : Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions

EXP : For the current use case, you need to create an AMI such that the application stack is already set up. But AMIs are bound to the Region they are created in. So, you need to copy the AMI across Regions for disaster recovery readiness.

-- Copying a source AMI results in an identical but distinct target AMI with its own unique identifier. In the case of an Amazon EBS-backed AMI, each of its backing snapshots is, by default, copied to an identical but distinct target snapshot. 

-- (The sole exceptions are when you choose to encrypt or re-encrypt the snapshot.) You can change or deregister the source AMI with no effect on the target AMI. The reverse is also true. 

-- There are no charges for copying an AMI. However, standard storage and data transfer rates apply. If you copy an Amazon EBS-backed AMI, you will incur charges for the storage of any additional Amazon EBS snapshots.

-- AWS does not copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI. After the copy operation is complete, you can apply launch permissions, user-defined tags, and Amazon S3 bucket permissions to the new AMI.



==============================KEY-PAIR=========================



-- it is used to retrieve the password of the ec2 instance , we do not have any key-pair by default, we have to create it

-- the extention of the key-pair is ".pem"

-- aws has public key and we have .pem private key both called "key-pair"

-- u can attach key-pair to multiple instances but not many instances atthe same time

-- u can create multiple key pairs

-- ec2 instace can have only 1key-pair attached at any point

-- once the .pem file is attached,u cannot change the .pem file to the instace

-- every time u retrive the password of the same ec2 , u wil get same password 

-- keep ur .pem files in secure place



-- for windows:

- ip: provided by aws

- usename for windows : Administrator

- password : you will get it through key-pair

- to connect windows , u can use RDP protocol rdp/3389

- how to connect? 

ans : remote desktop connection client 



-- for Linux :

- username for windows : ec2-user

- password : you will get it through key-pair

-to connect linux , u can use RDP protocol ssh/22

- how to connect? 

ans : we use putty or mobaxtrem 

-- putty does not support .pem file , it supports PPK file 

-- puttyGen --> convert pem to PPK



-- do not share pem files to any one 


-to login into ec2 so many people u have instead of giving pem file in aws use "Directory servie"

-- join the all ec2 to the active directory , by this people can login without pasword 

-- keypairs only with admins 

-- thse types are called "domain user" login type 

=================================================Cluster network instances (placement groups)==================================

• Sometimes you want control over the EC2 Instance placement strategy

• That strategy can be defined using placement groups

• When you create a placement group, you specify one of the following strategies for the group:

  1 • Cluster—clusters instances into a low-latency group in a single Availability Zone

  2 • Spread—spreads instances across underlying hardware (max 7 instances per group per AZ)

  3 • Partition—spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)



-- cluster = grp of servers ---> this group is called Placement group"

-- generally , when u launch a new ec2 , the ec2 service will place the instance such a way that all ur instance are spread out across diferent hardware

-- the speed b/w instaces is 20Gbps



-- we have 3 cluster network instance

1  Cluster Placement Group : 

   - grouping the einstance in same rack same AZ, high performnace , low HA

   • Pros: Great network (10 Gbps bandwidth between instances with Enhanced Networking enabled - recommended)
   • Cons: If the rack fails, all instances fails at the same time

   • Use case:
      • Big Data job that needs to complete fast
      • Application that needs extremely low latency and high network throughput



2  Spread Placement Group : ec2 instance are spread across AZ's, high HA, critical Applications

    - per 1 AZ = 7 EC2 instances u can launch 

    • Pros:
       • Can span across Availability Zones
       • Reduced risk is simultaneous failure
       • EC2 Instances are on different physical hardware

    • Cons:
       • Limited to 7 instances per AZ per placement group

    • Use case:
       • Application that needs to maximize high availability
       • Critical Applications where each instance must be isolated from failure from each other


3  Partition Placement roup : 

  • Up to 7 partitions per AZ

  • Can span across multiple AZs in the same region 

  • Up to 100s of EC2 instances

  • The instances in a partition do not share racks with the instances in the other partitions

  • A partition failure can affect many EC2 but won’t affect other partitions

  • EC2 instances get access to the partition information as metadata

  - Across AZ, Maz partition = 7 

  - each partition has 100's of ec2 instances

  • Use cases: HDFS, HBase, Cassandra, Kafka


note : placement group recommended to have same homogeneous instance type

- when you are lunching the ec2 , you can select which placement group u want to keep the instance 



==============================Security Groups========================

-- firewall = security group : which stops unauthorized access to the network

-- allow/deny

-- Sg has 2 types of rules 

1 inbound rules : allows traffic towards ec2

2 outbound rules : which allows the traffic outside ec2


-- by default , u dont have any inbound rules , it deny by default, u have give inbound rules

-- by deafult outbound rules are allowed 

-- it is not possible to deny protocol in SG , coz by default inbound rules are deny

-- in SG , we can only ALLOW protocols not DENY

-- every ec2 should have 1 sg

-- ec2 has a default SG

-- they are 3 types of source to access

1 custom : certain network

2 anywhere : through out the world 

3 MyIp : ur own computer 


-- u can attach multiple sg's

-- 1 SG --> attach to multile 


IMP : if u allow any protocol in inbound rule , u no need to allow that on outbound rule ----> these is called "STATEFUL"

Eg: SG

- if u allow any protocol in inbound rule , u must allow that on outbound rule  also ----> these is called "STATELESS"

-- Eg: NACL's

-- inbound n outbound rules 

- default SG is there 

-- SG will hit after NACL 

-- by default , inbound rules are deny

-- u cant deny on SG

-- SG is Instance lvel

-- if u careate any new sg, inbound rules are deny , outbound rules are allowed 

-- SG are STATEFUL

-- if u allow any inbound rule, u no need to allow on ob rule 

-- SG are instances level


EPV :  An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS.

ANS : we have 3 answers 

The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443

The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80

The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432


EXP : PostgreSQL port = 5432  HTTP port = 80  HTTPS port = 443

-- The traffic goes like this : The client sends an HTTPS request to ALB on port 443. This is handled by the rule - "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443"

-- The Application Load Balancer then forwards the request to one of the Amazon EC2 instances. This is handled by the rule - "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80"

-- The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432. This is handled by the rule - "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432"



==================================NACL(network Acces control list)=============

-- it is another layer of security to the ec2 

-- if u want tight the security go for NACL

-- like SG , NACL has inbound and outbound rules


-- NACL will hit first then move to SG

-- in VPC they aare multiple Subnets

-- 1 subnet is associated to 1 AZ

-- 1 subnet can't be in multiple AZ at the same time 

-- 1 AZ can have mltiple subnets

-- 1 NACL can have multiple subnets

-- NACL is subnet level and SG is ec2 level 

-- in NACL u can deny but in S.G u cant 
           
-- inbound n outbound rules 

- default NACL is there 

-- NACL will hit first 

-- by default , inbound rules are allowed

-- u can deny on NACL and allow also

-- NACL is subnet level

-- if u create any new NACL, inbound rules,outbound rules are DENY

-- NACL are STATELESS

-- if u allow any inbound rule, u need to allow on ob rule also


• NACL are like a firewall which control traffic from and to subnets

• One NACL per subnet, new subnets are assigned the Default NACL

IMP ------------ • You define NACL Rules:

• Rules have a number (1-32766), higher precedence with a lower number
• First rule match will drive the decision
• Example: if you define #100 ALLOW 10.0.0.10/32 and #200 DENY 10.0.0.10/32, the IP address will be allowed because 100 has a higher precedence over 200
• The last rule is an asterisk (*) and denies a request in case of no rule match
• AWS recommends adding rules by increment of 100

• Newly created NACLs will deny everything

• NACL are a great way of blocking a specific IP address at the subnet level


------------------------- Default NACL

• Accepts everything inbound/outbound with the subnets it’s associated with

• Do NOT modify the Default NACL, instead create custom NACLs


------------------------------ LAB NACL 

-- create one instance with the user data

#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html


-- make sure u have allowed inbound rules for SSH and HTTP

-- now go to NACL --> add HTTP rule in inbound rules and make it as DENY , give rule number less than 100 , so now try in browser it will gives u error , coz here rule number came into picture , less the rule number more the priority

-- if u give rule number > 100 it will allow coz, u have given rule number > 100 , so small number has high priorities

-- now go to outbound rules , change from allow to deny 

-- now try in browser , it won’t give u value coz , in NACL it is stateless , u have to give both inbound and outbound rules , if u deny any one it won’t work 



NOTE : in SG , it is stateful , no need to give inbound and outbound rules , even if u did not give outbound rules it will give o/p




------------------------------------ Security Group vs. NACLs

        Security Group                                                                                     NACL

Operates at the instance level                                                         Operates at the subnet level

Supports allow rules only                                                              Supports allow rules and deny rules

Stateful: return traffic is automatically allowed, regardless of any rules             Stateless: return traffic must be explicitly allowed by rules (think of ephemeral ports)

All rules are evaluated before deciding whether to allow traffic                       Rules are evaluated in order (lowest to highest) when deciding whether to allow traffic, first match wins

Applies to an EC2 instance when specified by someone                                   Automatically applies to all EC2 instances in the subnet that it’s associated with





=================================================AUTO-SCALING======================================================

-- Scale out and scale in ec2 instance based on demand

-- scale out = adding instances 1,2,3,4,5....

-- scale in = remove instance 4,3,2.......

-- the above 2 are horizontal scaling

-- Scale Up = increase size of resource eg : from t2.micro to t2.large

-- Scale down = decrease the size of resource eg : from t2.micro to t2.nana

-- the above 2 are vertical scaling


EVP : A DevOps engineer at an IT company just upgraded an Amazon EC2 instance type from t2.nano (0.5G of RAM, 1 vCPU) to u-12tb1.metal (12.3 TB of RAM, 448 vCPUs). How would you categorize this upgrade?

ANS : This is a scale up example of vertical scalability

EXP : Vertical scalability means increasing the size of the instance. For example, your application runs on a t2.micro. Scaling up that application vertically means running it on a larger instance such as t2.large. Scaling down that application vertically means running it on a smaller instance such as t2.nano. Scalability is very common for non-distributed systems, such as a database. There’s usually a limit to how much you can vertically scale (hardware limit). In this case, as the instance type was upgraded from t2.nano to u-12tb1.metal, this is a scale up example of vertical scalability.


-- ASG are free , u only need to pay for undelying instances

-- elb does health checks to the application 

-- Cloudwatch will monitor the ec2 istance


-- 3 types of scaling Options:

1  Manual Scaling : manually changed the min,max capacity


2 Schedule Scaling : u can schduled the scaling based on the period or a day 


3 Dynamic Scaling : based on metrics , cpu > 70% -- based on the load



--------------------------LAUNCH TEMPLATE/LAUNCH CONFIGURATION


-- how does ASG know that scalout ec2 will have appn of urs 

-- u can specify this launch template , this is create automatically all things in new ec2 

-- Custom AMI(app) or AWS ami , volumes, SG, 

-- ASG = ELB + EC2 instance + Launch template + SNS




=============================ELASTIC LOAD BALANCER==================

-- it is serverless 

-- http n https 

-- which is used to spread traffic across all the instances

-- elb is maintined by AWS

-- elb is service for us , not a server , u can not login into the elb

-- elb can be access using DNS NAME or URL 

-- health check code is 200

-- elb has a ip addres but this is dynamic not static

-- aws always recommend to use the elb dns not the IP address

--  it is integrated with many AWS service Like EC2, ASG,AMAZON ECS , ACM, Cloudwatch , R53, AWS WAF , GLobal Accelerator



EPV : An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website.

ANS ;

-- The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer

-- The route for the health check is misconfigured


EXP : 

-- You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port.

-- Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.




--------------------------------------------------------------------------- types of L.B's

-------------------- 1 classic load balancer : 2009

• Suppor ts TCP (Layer 4), HTTP & HTTPS (Layer 7)

• Health checks are TCP or HTTP based

• Fixed hostname XXX.region.elb.amazonaws.com

- supports http , https, and TCP

- also know as previous generation

- u can not create routing rules 

- The Classic Load Balancer supports SSL offloading.

-  Layer 7 load balancer works at the application layer




------------------------------- 2  Application Load Balancer : 2016

- Application Load Balancer automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and AWS Lambda functions.

- It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.

- Choose an Application Load Balancer when you need a flexible feature set for your applications with HTTP and HTTPS traffic.

- works on layer 7 (http)

- latest generation

- default choose is ALB

- HTTP and HTTPS

- best for micro-services and container based appn eg : docker and AMZN ECS

- An Application Load Balancer cannot be assigned an Elastic IP address (static IP address).

- Application Load Balancer cannot use EC2 based health checks , it uses ALB health Checks

- You should note that the Application Load Balancer also supports Transport Layer Security (TLS) offloading. 

- it works on round robin algorithm and 

- Uses the least outstanding requests routing algorithm based on request count and target response for HTTP and HTTPS listeners

- u can't give fixed ip's in the ALB , but u can give in NLB (elastic ip's)



-- -----------it has routing features 


-- in ALB we r creating rules , we have target groups , so according to the traffic the request will send to different target groups 


1  host based Routing : https://subbu.com

2  Path Based Routing  :   https://subbu.com/admin

3  String Parameter Routing :  https://subbu.com/course=aws?


note : this is not possible in the CLB , so it is not used now 


Tartget Groups for ALB 

-- Ec2 instances   : can be managed by ASG  - HTTP
-- ECS Tasks       : managed by ECS itself  - HTTP
-- Lambda FUnction : HHTP request translate into a JSON Document
-- IP Address      : must be private IP's 

-- ALB can route trafffic to multiple target groups 

-- health checks are at target group level



----------------------- Application Load Balancer + dynamic port mapping

-- Application Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones (AZs).

-- Dynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster.





EPV : A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance.

ANS : The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check

EXP :

-- An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management.

-- If the Auto Scaling group (ASG) is using EC2 as the health check type and the Application Load Balancer (ALB) is using its in-built health check, there may be a situation where the ALB health check fails because the health check pings fail to receive a response from the instance.

-- At the same time, ASG health check can come back as successful because it is based on EC2 based health check. Therefore, in this scenario, the ALB will remove the instance from its inventory, however, the Auto Scaling Group will fail to provide the replacement instance. 

-- This can lead to the scaling issues mentioned in the problem statement.



IMP : to avoid this problem , 

-- It is recommended to use ALB based health checks for both Auto Scaling group and Application Load Balancer. 

-- If both the Auto Scaling group and Application Load Balancer use ALB based health checks, then you will be able to avoid the scenario mentioned in the question.





------------------------------------------ 3 Network load balancer : 2017

- Choose a Network Load Balancer when you need ultra-high performance, TLS offloading at scale, centralized certificate deployment, support for UDP, and static IP addresses for your applications.

- Handle millions of request per seconds

- u willget fixed ip addres here (used for whitelist an IP purpose)

- Network Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using this IP, while allowing you to scale your application behind the Network Load Balancer using an ASG.

- works on layer 4

- latest generation

- TCP AND UDP, TLS 

- Extremet high performance

- network level 

-  "it provide 1 static ip per AZ" and supports assign Elastic Ip (helpful for whitelistining Specific IP)

- the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number.


-------NLB Target groups

- ec2 Instances
- IP address - must be private 
- ALB 
- lambda function 

-- health check support the TCP,HTTP and HTTPS protocol


IMP NOTE : NLBs support HTTP, HTTPS and TCP health checks:

-- But NLBs only accept either selecting EC2 instances or IP addresses directly as targets. You can't provide a URL to your endpoints, only a health check path (if you're using HTTP or HTTPS health checks).

                   difference between endpoint URL and health check path?

                   ANS : A URL includes the hostname. The health check path is only the path portion. 

                         URL = https://i-0123456789abcdef.us-west-2.compute.internal/index.html  

                         health check path = /index.html

-- You can use HTTP/HTTPS ONLY when Target is ALB. By default it is TCP.

-- HealthCheckProtocol :

The protocol the load balancer uses when performing health checks on targets. The possible protocols are HTTP, HTTPS, and TCP. The default is the TCP protocol. If the target type is ALB, the supported health check protocols are HTTP and HTTPS.


------------------------------------- LAB for NLB 


STEP 1 

-- create one VPC with the CIDR range , here i took 192.168.0.0/16 for demo purpose

-- now create 2 public subnets and 2 private subnets 

-- 2 public subnets (nlb-public-subnet-1a in 1a zone , nlb-public-subnet-1b, in 1b zone)

-- 2 private subnets (nlb-pprivate-subnet-1a in 1a zone , nlb-private-subnet-1b, in 1b zone)

-- create IGW and attach to the VPC

-- create public and private route table 

-- now do subnet association for both public and private route tables 

-- now go to public route table and edit routes , allow all traffic and add IGW and do make save changes 



STEP : 2 

-- create one ec2 instance , here i choose ubuntu  with user data , u can choose any one

-- make sure that ec2 instance , deployed in ur VPC that u have created now 

-- in the security group make sure to add 22 and 80 port numbers  -----> very very imp 

-- now add userdata , in usedata section 

#!/bin/bash
yes | sudo apt update
yes | sudo apt install apache2
echo "<h1>Server Details</h1><p><strong>Hostname:</strong> $(hostname)</p><p><strong>IP Address:</strong> $(hostname -I | cut -d" " -f1)</p>" > /var/www/html/index.html
sudo systemctl restart apache2


-- check in the browser by copying public ip of ec2 instance 

-- we also need to set up custom pages so one I'm going to set it 

-- we need these custom paths because we will be setting up our application load balancer and also we will be setting up our Network load balancer 

-- so I will be able to show you the difference between how application load balancer behaves when we have a custom path and how we can set those custom rules in application load balancer 

-- but at the same time that is not possible uh with the network load balancer 

-- so that's a one more difference which we need to check when we are setting up our application load balancer as well as Network load balancer 



STEP - 3 

-- now go back to the ec2 instance and get connect 

-- check status of apache 

     sudo systemctl status apache2


-- now create two directories
  
         sudo mkdir /var/www/html/foo
         sudo mkdir /var/www/html/bar

-- check it is there are not 

         ls /var/www/html


-- now create index.html file in the both directories

        sudo touch /var/www/html/foo/index.html
        sudo touch /var/www/html/bar/index.html


-- check the file is there or not 

         ls /var/www/html/foo
         ls /var/www/html/bar


-- now add content in file 

       sudo vi /var/www/html/foo/index.html
           <h1>Hello from /foo demo-nlb-ec2-1</h1><p>This is a sample hello message in /foo.html. 

       sudo vi /var/www/html/bar/index.html
           <h1>Hello from /bar demo-nlb-ec2-1</h1><p>This is a sample hello message in /bar.html.</p>"


-- now do restart ur apache 

       sudo systemctl restart  apache2


-- now do verify the pages 

       ur ip address/foo and /bar


------ create another instance with the ubuntu , and create all these stuff , same process 


Step 4 : 


-- now setup our Target group

-- go to target group sections --> make ec2 instances as targets --> Protocol : Port = TCP (coz we are using NLB) --> choose ur VPC and create TG


Step 5 :


-- now setup our NLB 

-- create nlb , with ur vpc and select 1a and 1b public subnets 

-- add target group n create NLB

-- once it get create copy DNS and paste in the browser 

-- check ur r getting o/p or not 

-- now check with path based routing like /foo and /bar 

-- getting .................



STEP 6 : Key Differences: NLB vs. ALB | Path-Based Routing with ALB

-- here i can able to acess my target groups through the NLB 

-- but the difference is we cannot set up some specific  URL based rule with our Network load balancer , which means here go to load balancer --> listeners --> c.o add listeners , here u won’t find any rules which you can Define for your url pattern

   - application load balancer there are some URL patterns like if URL ends with Foo and URL ends with bar then it should put,it should put some rules over here and then it should redirect to the respective Target group so so that it's properly working for URL patterns

   - but in the network load balancer it is not possible it will only give you one URL and that URL will redirect to the ec2 instances

   - so although we have set up the foo and bar HTML pages but I need more granularity and I can set up this rule from the URL itself into load balancer

   - so the it can direct that request to the Target group

   - so that's the difference or the feasibility we have in our application load balancer where we can put some rule patterns on the URL uh URL parameters which are coming in our request 

   - so here if it comes into as a foo then we'll forward that request to the Foo Target group if it is coming with a bar then we'll Target that request to our bar Target group 

   - so that's only possible with our application load balancer and it is not possible with our Network load balancer 



STEP 7

-- to check it practically , create ALB 

-- so for this we need to make some changes 

-- change  the instance 1 name  = foo

-- change  the instance 2 name  = bar

-- now connect instance 1 (foo) --> go to directory cd /var/www/html/ --> sudo rm -rf bar --> sudo systemctl restart  apache2

    - now verify with /bar in browser  , u can’t get 


-- now connect instance 2 (bar) --> go to directory cd /var/www/html/ --> sudo rm -rf foo --> sudo systemctl restart  apache2

    - - now verify with /bar in browser  , u can’t get 


-- now we have to create Target groups for this instances

-- create target group for the foo instance (use HTTP protocol , coz we are doing with ALB) , choose ur VPC then choose foo instance and create target group

--  do same with the bar instance 


-- so now we need to set up our application load balancer 

-- once u create see the difference b/w NLB and ALB 

-- go to Listeners and rules --> c.o HTTP:80 --> c.o add rule --> name = foo-rule --> add condition --> choose path = /foo --> select foo target group --> priority = 1 --> create 


-- go to Listeners and rules --> c.o HTTP:80 --> c.o add rule --> name = bar-rule --> add condition --> choose path = /bar --> select bar target group --> priority = 2 --> create 


-- now copy DNS of ALB and check with /path base routing 








EPV : A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB.As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?

ANS : Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance

EXP : 

-- If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. The load balancer rewrites the destination IP address from the data packet before forwarding it to the target instance.

-- If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port. Note that each network interface can have its security group. The load balancer rewrites the destination IP address before forwarding it to the target.



EPV : The systems administrator at a company wants to set up a highly available architecture for a bastion host solution.

As a solutions architect, which of the following options would you recommend as the solution?

ANS : Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group

EXP : Including bastion hosts in your VPC environment enables you to securely connect to your Linux instances without exposing your environment to the Internet. 

-- After you set up your bastion hosts, you can access the other instances in your VPC through Secure Shell (SSH) connections on Linux. 

-- Bastion hosts are also configured with security groups to provide fine-grained ingress control.

IMP : You need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port 22. They must be publicly accessible.

-- Here, we use a Network Load Balancer, which supports TCP traffic, and will automatically allow you to connect to the Amazon EC2 instance in the backend.







----------------------------------------- 4 Gateway Load Balancer ;2020

- Choose a Gateway Load Balancer when you need to deploy and manage a fleet of third-party virtual appliances that support GENEVE. These appliances enable you to improve security, compliance, and policy controls.

- works on layer 3

- deply , manage and scale a fleet of 3rd party network virtual applications in AWS

- f u want to set up any firewall,prevention system etc

- it uses GENEVE protocol on 6081


======================Sticky Sessions (session Affinity)======================

-- it is possible to implement stickness so that same client is always redirected to the same instance behind a Load Balancer 
-- this work for CLB, ALB, NLB
-- the "cookies" used for stickness has an expiration date 


Use Case: make sure the user does not lost his session data 

-- Session stickiness: Advantages  

-- Session stickiness offers a number of benefits that can improve your web application’s performance, including:

1 Minimized data exchange – When using sticky sessions, servers within your network don’t need to exchange session data, a costly process when done on scale.

2 RAM cache utilization – Sticky sessions allow for more effective utilization of your application’s RAM cache, resulting in better responsiveness.

disadvantage

-- Enabling Stickiness may bring imbalance to the load iver the backend ec2 instance 

-- That said, sticky sessions also make it more difficult to keep servers in balance. A server can become overloaded if it accumulates too many sessions, or if specific sticky sessions require a high number of resources. This could result in your load balancer having to shift a client to a different server mid-session, resulting in data loss.



Note : NLB works without "cookies"




---------------------- cookies Names 

1 Application cookies

A Custom Cookie 

-- generated by the target 
-- can include any custom attribute requested by the appn 
-- cookie name must be specified individually for each target group 
-- Do not use "AWSALB, AWSALBAPP or AWSALBTG " 

B Appication cookie 

-- Generated by load balancer
-- cookie name is "AWSALBAPP"


2 Duration Based cookies 

-- Generated by load balancer
-- cookie name is "AWSALB" for ALB  and AWSELB for CLB 


----- create lB and go to attributes and create stickiness if u want 


==============================Cross Zone Load Balancing============================

1 With Cross Zone Load Balancing : each load balancer distrubuted evenly across all registered instance evenly in all Az's

Eg : u have 2 AZ's so load balancer will route traffic to each AZ evenly 

-- AZ-1 has 2 insatcnes and AZ-2 has 8 instances , so here the traffic is distributed by LB is 505 to each AZ 

-- 50% to AZ-1 so it has 2 instances , lb spread traffic to 2 instances as 10 -10 % to each instances and 

-- in AZ-2 we have 8 isnatnces so 10-10% to 8 instances , lb distribution the traffic evenly 

-- so here total 20% traffic to AZ-1 and 80% traffic to AZ-2 , total traffic is distributed by LB is 100% 


2 Without Cross Zone Load Balancing : each load balancer distrubuted in the instance of node of ELB 

-- eg : from above example AZ-1 get 25% for each instances (2 * 25% = 50% for AZ-1)

-- 6.5% each for AZ-2 (50% for AZ-2)


IMP to know 

1 ALB 

-- For ALB it is enable by Default ( u can be disable at target group level)

-- No charges for inter AZ data 

2 NLB and Gateway loadbalancer : disabled by default 

-- u have to pay for inter AZ data if eneble

3 CLB 

-- disabled by default 
-- No charges for inter AZ data if enabled 



=====================================SSL/TLS - Basics ==============================

• An SSL Certificate allows traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption)

• SSL refers to Secure Sockets Layer, used to encrypt connections

• TLS refers to Transport Layer Security, which is a newer version

• Nowadays, TLS cer tificates are mainly used, but people still refer as SSL

• Public SSL certificates are issued by Certificate Authorities (CA)

• Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc...

• SSL certificates have an expiration date (you set) and must be renewed


----------------------------- Load Balancer - SSL Certificates

       HTTPS(encrypted)                      HTTP Over private VPC 
       Over www
Users ----------------->      Load balancer  ---------------------->     ec2 instances 
      <-----------------                     <----------------------  


• The load balancer uses an X.509 certificate (SSL/TLS server certificate)

• You can manage certificates using ACM (AWS Certificate Manager)

• You can create upload your own certificates alternatively

• HTTPS listener:
  • You must specify a default certificate
  • You can add an optional list of certs to support multiple domains
  • Clients can use SNI (Server Name Indication) to specify the hostname they reach
  • Ability to specify a security policy to support older versions of SSL /TLS (legacy clients)


------------------------------- SSL – Server Name Indication (SNI)

• SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)

• It’s a “newer” protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake

• The server will then find the correct certificate, or return the default one

Note:

  • Only works for ALB & NLB (newer generation), CloudFront
  • Does not work for CLB (older gen)


Elastic Load Balancers – SSL Certificates


------------------------------- Elastic Load Balancers – SSL Certificates

• Classic Load Balancer (v1)
  
   • Support only one SSL certificate
   • Must use multiple CLB for multiple hostname with multiple SSL certificates

• Application Load Balancer (v2)

   • Supports multiple listeners with multiple SSL certificates
   • Uses Server Name Indication (SNI) to make it work

• Network Load Balancer (v2)
 
   • Supports multiple listeners with multiple SSL certificates
   • Uses Server Name Indication (SNI) to make it work



============================================== Connection Draining========================

• Feature naming

   • Connection Draining – for CLB
   • Deregistration Delay – for ALB & NLB

• Time to complete “in-flight requests” while the instance is de-registering or unhealthy

• Stops sending new requests to the EC2 instance which is de-registering

• Between 1 to 3600 seconds (default: 300 seconds)

• Can be disabled (set value to 0)

• Set to a low value if your requests are short




==================================TYPES OF IP's=================

1 public IP  : it is not manditory ,this is optional , it is dynamic , it will change when u do startand stop the ec2


2 Private IP : by deafult u will get private ip ,when u launch ec2 ,it is manditory 



3 Elastic IP : the ip wont change when u start n stop the ec2 , it is static ip address

- it is same as public 

- 5 EIP's are free 

- if u not associated with any ec2 instance, aws will do chrge for ideal eip's




-- with in the AWS 2 ec2 wants to talk , the privatae ip is used 

-- instance meta-data = data about ec2 instance 

- from console , u get meta-data from Details section



-- from CLI , follow this below URL 

- http://169.254.169.254/latest/meta-data/



-- USer-Data = Bootstrap Scripting 

- the script which u have provided will run at the boot time of the ec2 instances 


-- user data will run only for the first time of launching the ec2 

linux = shell script

windows = powershell





==================================Global Accelerator(GA)====================================


-- managed by AWS 

-- it use aws network , it uses aws edge locations 

-- it has low latency and high performance 

-- if u want to setup with static ip we choose "GA"

-- it provide 2 static ip's

-- it is billable 

-- global service 

-- it will reduce 60% traffic to the end-users
 

-- unicast ip : one server holds one ip address 

-- Anycast ip : all server holds same ip address and client is routed to the nearest one 




-- eth0 --> private IP

-- eth1 --> public ip



IMP to Know : 

-- As your application architecture grows, so does the complexity, with longer user-facing IP lists and more nuanced traffic routing logic.

-- AWS Global Accelerator solves this by providing you with two static IPs that are anycast from our globally distributed edge locations, giving you a single entry point to your application, regardless of how many AWS Regions it’s deployed in.

-- This allows you to add or remove origins, Availability Zones or Regions without reducing your application availability. 

-- Your traffic routing is managed manually, or in console with endpoint traffic dials and weights.

-- If your application endpoint has a failure or availability issue, AWS Global Accelerator will automatically redirect your new connections to a healthy endpoint within seconds.

-- By using AWS Global Accelerator, you can:

   1 Associate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users.
   
   2 Easily move endpoints between Availability Zones or AWS Regions without needing to update your DNS configuration or change client-facing applications.

   3 Dial traffic up or down for a specific AWS Region by configuring a traffic dial percentage for your endpoint groups. This is especially useful for testing performance and releasing updates.

   4 Control the proportion of traffic directed to each endpoint within an endpoint group by assigning weights across the endpoints.






----------------------------HOW GA works with LB -------

1 Global traffic is received by a static IP address provided by the accelerator.

2 Global accelerator optimizes the route for every client, minimizing the number of hops to reach Amazon’s network to reduce latency.

3 The request is received by the load balancer’s matching listener and routed accordingly.

-- To optimize application availability, performance, and security, create an accelerator for your load balancer.

-- The accelerator directs traffic over the AWS global network to static IP addresses that serve as fixed endpoints in the nearest Region to the client.

-- AWS Global Accelerator is protected by AWS Shield Standard, which minimizes application downtime and latency from DDoS attacks.

-- You can create an accelerator during load balancer creation or after creation from its Integrations tab or from the AWS Global Accelerator console.

-- The accelerator is created in your account, with the load balancer as an endpoint.


================================================= WAF (web application Firewall)

-- To optimize application security, associate either a predefined AWS WAF web ACL or an existing web ACL to your Application Load Balancer. 

-- This association can be made during load balancer creation or after creation from its Integrations tab or from the AWS WAF console.

-- When you choose the predefined AWS WAF web ACL, a new web ACL is created in your account and associated to your load balancer. The predefined web ACL includes 3 rules to offer protections against the most common security threats.

-- You can adjust or add rules to that web ACL as desired over time. Charges apply  for each web ACL created, the number of rules, and the number of requests processed.

-- If your load balancer is connected to a CloudFront distribution that uses AWS WAF protections, you may still want to consider AWS WAF protections on the load balancer if it can be reached outside of CloudFront. 


--------------- How AWS WAF works with Application Load Balancers

-- Requests are received by the load balancer’s matching listener.

-- The listener first forwards the request to AWS WAF for inspection against the specified web ACL rules.

-- Once a request meets a condition defined in your web ACL rules, AWS WAF instructs the load balancer to either block or allow the request based on the action you define.

-- If a request is allowed, the request is then evaluated by the listener rules and routed accordingly.





-------------------------------7 steps to create ec2 

1 select AMI (linux , windows .......) 

2  instance type 

3 instance configuration = how may instance , public ip , user-dta 

4 select storage = ebs ,root volume additional volm

5 Select SG 

6 add Tag 

7 create PEM file and review 




===============================EC2 console============

-- ec2 global view :u can view resoure globally 

-- events : any maintainenece is there , it will show u in the events sections 

-- launch tmplate : i ASG we use L.T ,it contain configurations 


-- capacity reservations : u can reserve the cpacity ,when u need requriment 

- for only reserving u no need to pay until u will use this 

============================EFA === elastic fabric Adapter 

- it is a network device that u can attach to ur instance to reduce latency and increase throughput for distributed high performnace computing(HPC) and ML 

• Great for inter-node communications, tightly coupled workloads

• Leverages Message Passing Interface (MPI) standard

• Bypasses the underlying Linux OS to provide low-latency, reliable transport

• Improved ENA for HPC, only works for Linux

-- advanced 

-- works for only linux

-- EFA devices provide all Elastic Network Adapter (ENA) devices functionalities plus a new OS bypass hardware interface that allows user-space applications to communicate directly with the hardware-provided reliable transport functionality.


=============================== Elastic Network Interfaces (ENI) =============

• Logical component in a VPC that represents a virtual network card

• The ENI can have the following attributes:
   • Primary private IPv4, one or more secondary IPv4
   • One Elastic IP (IPv4) per private IPv4
   • One Public IPv4
   • One or more security groups
   • A MAC address

• You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover

• Bound to a specific availability zone (AZ)

-- An Elastic Network Interface (ENI) is a logical networking component in a VPC that represents a virtual network card. 

-- You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The ENI is the simplest networking component available on AWS and is insufficient for HPC workflows.


============================ AWS ENA (Elastic Network Adapter)

-- The Elastic Network Adapter (ENA) is designed to provide Enhanced Networking to your EC2 instances. With ENA, you can expect high throughput and packet per second (PPS) performance, as well as consistently low latencies on Amazon EC2 instances.

-- Elastic Network Adapter (ENA) devices support enhanced networking via single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities.

-- Although enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies, still EFA is a better fit for the given use-case because the EFA device provides all the functionality of an ENA device, plus hardware support for applications to communicate directly with the EFA device without involving the instance kernel (OS-bypass communication) using an extended programming interface.





----------------- hibernate mode : to put ec2  in sleep mode 

when to use ? 

- when u have big machines , so much data inside in it , that time u do hibernate it will not charged for hibernate 


===================ec2 Hibernation==============================

--  Amazon has introduced a new EC2 Hibernation feature which when enabled the Hibernation it saves the contents from the instance's memory that is RAM to your Amazon EBS root volume.

-- in genreal whenevr we do 

Stop : if we stop the ec2 instance , the data on ebs volume is persisted

Start : if we start the ec2 instance , the data on ebs volume is restored

Terminate : it delets the ebs volume also 

Hibernation : saves the content from the instance memory (RAM)to ur AMazon root volume


-- the main theme of Hibernation is that when u do not want restart the whole processer 

Eg : in our daily life u can do some work in ur personal computer and do shut-down when it is over , again when u do start ur system it will take some time to start and in the backend it is booting up the system , so to avoid this we can use Hibernation

-- AWS persists the instance's Amazon EBS root volume and any attached Amazon EBS data volumes.

-- Here is the pre-requisites for using the hibernation feature:

1 EC2 hibernate instance root volumes must have encryption enabled to ensure the security of the content stored at EBS.

2 Hibernation is not supported with all of Instance types but it is supported with following instance types: C3, C4, C5, M3, M4, M5, R3, R4, R5, and T2.

3 Instance RAM size should be less than 150GB

4 AMIs supported: Amazon Linux 2 AMI, Amazon Linux AMI, Ubuntu, and Windows

5 EC2 instance hibernate must be backed by EBS volumes, instance stores aren’t supported.

6 Available for the On-demand , Reserved instance and Spot instance

-- 


-- Limitations on the EC2 instances enabled for hibernation

1 You cannot increase/decrease the size of hibernated instance.

2 Cannot enable snapshot or AMIs from instance in hibernation state.

3 You can’t enable hibernation after launch.

4 If an EC2 instance is enabled for auto-scaling group or used by Amazon ECS, then you cannot hibernate that instance.

5 An EC2 instance cannot be kept hibernated for a period of more than 60 days. To keep the instance for longer than 60 days, you must start the hibernated instance, stop the instance, and start it.

6 To hibernate an instance that was launched using your own AMI, you must first configure your AMI to support hibernation.


-------------------LAB-----------------------

-- create one instance A with hibernation , do encrypt ur EBS volume while creating and use aws key for encryption

-- create another instance B  , without hibernation 

-- do connect A and type uptime , it will shows the time from how much u are active state 

-- do same for instance B 

-- do stop the instances and wait for 1-2 min 

-- do start the instance again 

-- connect the instance A and do uptime it will give the time from the time when u started the instance A , coz it is hibernated.

--  do same with the instance B , when u do for the instance B , it will show the time from 0 , coz it is not hibernated 

Use Cases : 

-- Long-running processing 

-- saving the RAM state
-- Services that take time to initiliaze 





-- termination protection 

- enable : u cant terminate 

- disable : u can able to terminate
 

-- same stop protection also -- u cant stop the server 


-- EBS optimized instance:  whenwver ur data in volumes that will store in central EBS 

- like the same way some ec2 will have optimized with the ebs , and it will give high performance


---------Nitro Enclave 

- A Nitro Enclave is a "trusted execution environment (TEE)" in which you can securely process sensitive data. It extends the security and isolation characteristics of the AWS Nitro System and allows you to create isolated compute environments within Amazon EC2 instances. If no value is specified the value of the source template will still be used. If the template value is not specified then the default API value will be used.

- Nitro Enclaves are not compatible with instance types that have less than 2 vCPUs.

- it is chargable

----------------

-- if public ip address changes for ec2 , public dns also will change 


----------------------imp points:


NOTE : aws use "xen" virtualization

-- instance screen shot : to know it is reboot is done or not 

-- while creating the image(ami) u cant change root volume properties except size of ebs 

-- while creating the image we can also add additional volume 

-- u can also select instance store volume as a additional volume 

-- if u ec2 root volume is not encrypted while creating the image the root volume is also not encrypted

-- while creating the image additional volumes can be encrypted

-- during the image creaton process ec2 create snapshot of each of the above volumes 

-- once u check in AMI , the image is created 

-- u can copy the image and create image across regions 

-- copy AMI provide 2 things ( copy imge to other region and image can be encrypted)

-- deregister = delete

-- by deafult images are private , u can also transfer to anothr account using account id 

-- deprecation = out of date


--- recycle bin 

-- u need to create retention rule in recycle bin 

-- R.B can be implemented on snapshots and AMI 

-- not all snapshots and every ami wil not go into recycle bin

-- u can specify whiich one to retention by using tags 

-- if u encrypt snapshot u cant share to others 



---------- u can create image from the snapshot also which has os





======================volumes practicals==========================

--  create 2 nstaces and automatically 2 root volmes will be created 

-- u can deattch root volume only after stop instance 

-- force deattch : this is additional volumes to forcefully deattach 

-- u can increase size of volume without stop the ec2 

-- select volume and go for modify volume and give ur cpaacity as per requriment 

----create lifecycle policy

-- automatically create snapshots and images 

-- Enable cross-Region copy to copy snapshots created by this schedule to up to three additional Regions. if u want only

-- Enable cross-account sharing to share the snapshots created by this schedule with other AWS accounts.


NOTE : if u lost PEM files , how to recover password ? 

- crate new pem and instance 

- new instance has root volume 

- now deattach the old root volume and attach to th new instace as  a additional volume now new instance has 2 root volumes 

- go to old root volume and modify one file in d-drive and deattach from new instance and attach to old instance and u can login 


---------------EBS practicals with volumes


1 Ceate instance -A

2 create one additional volume(give root name as xvd(f-p)) and attach to the instace-A

note: it should be in the same A.Z

3 now connect instace-A and try to mount.

4 check if it attached any extra volumes or not by using command lsblk

5 if no volume is attached then , it will looks like

NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0   8G  0 disk 
├─xvda1   202:1    0   8G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi


6 if any volume is attached , then it looks like

NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0   8G  0 disk 
├─xvda1   202:1    0   8G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi
xvdf      202:80   0   1G  0 disk --------------attached volume


7 Now switchto root user ( only root user can able to mount) ---- sudo su

8 now create file direcory by using command
 
   sudo mkfs -t ext4 /dev/xvdf 

   - here it is xvdf so /dev/xvdf 

9 now create a directory to mount the volume  by using command 

 sudo mkdir <<your foldername>>

10 now mount your EBS Volume by suing command

sudo mount /dev/xvdf <<foldername>>

11 now check it is mounted successfully or not by using command  

    lsblk

12 if it is mounted, then it looks like

NAME      MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0   8G  0 disk 
├─xvda1   202:1    0   8G  0 part /
├─xvda127 259:0    0   1M  0 part 
└─xvda128 259:1    0  10M  0 part /boot/efi
xvdf      202:80   0   1G  0 disk /home/ec2-user/MyRules

here MyRules = FolderName


13 now switch to mount point directory

cd MyRules

14 create some files in it and save , (touch hello.txt)

eg: hello.text and write some txt inside it and save

check to see all the files by usng command ls -ll, it will show you all files that you have created

15 Now detach the volume and create snapshot from it and go to snapshot and create one volume on different A.Z from 1st instance and attach this new created volume to the 2nd instanc ( volume and instance should be in same A.Z in EBS)

16 now connect the instace-2 and enter command to check any file system is there or not by using command

sudo file -s /dev/xvdf  

17 once you enter above command you will get like this , if any file system is presnet

/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=ee2699ae-ba2c-4805-941a-f390c4344a37 (needs journal recovery) (extents) (64bit) (large files) (huge files)

18 folow the same steps from  9-13 , and afer enter ls -ll command to see all files

19 once you did successfully mounted then you will able to see the files that you have creted in the 1st instance time.



===========================LOAD BALANCER=============================

-- LB is regional 

-- in linux all start with /Root

-- u have so many likw /var /etc /usr /home /tmp /bin 

-- when u login in linux ur home diectory is created 

-- /var = all file logs will be there 

-- /tmp = temporary 

-- /bin = all the softwares available 

--/etc = all ur system files will be there 

-- /usr = all things related to user 



 haredware --> kernal --> shell--> cmd = linux structure 

-- all the shells under /bin

-- bash shell is new one , located in /bin/bash


-- when u enter any cmd in linux like touch , ls , it wil go to shell --> kernal --> hardware then get reply to cmd 

-- u cant execute any cmnds without shell script

eg for shell script (user data)

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html



exlanation 

#!/bin/bash  -- shebang 
yum update -y 
yum install -y httpd --  --- installing httpd service

systemctl start httpd --start 

systemctl enable httpd -- auto start ttpd service

echo "<h1>Hello World from $(hostname -f)</h1>" > 

/var/www/html/index.html --default directory httpd service will create 

> = redirect , it will create file 

-- whatveer in echo with > symbol will ceate index.html file automatically

-- whatever the data in echo all the datais stored in index.html file 

-- put this script in userdata


-- for L.B listeners should be http/80 and https/443

-- the facing should be internet facing = public a

-- internal facing is private and access within the vpc only

-- ACM == amazon certificate manager , where u can buy certificates


----------------------

-- create 2 instance with the user data 

-- one instance in 1a AZ and another instance is 1b AZ

-- now go to SG allow http 

-- check th user data is working properly or not by copying public ip of ec2 both and pste in brower u will get o/p 

-- once u check and it is working prprly , u can create LB now 

-- once u create LB , ucan copy  dns and paste in the browser , see hoe taffic is distributed across the multiple instance 

-- if u have heavy applications , make ideal time out more in attributes section 

- all the logs stored in s3 buckets 

-- WAF : Protects your web applications by enabling AWS WAF directly on your load balance

- it is used to secure the web applications ( DDos attacks like hacking ,sql injections or any hacking methods ) 


---- ----------------------------------in Target Groups

--  in attributes 

1  Deregistration delay (draining interval) : it means for eg we have 2 instance , if one goes down 2nd will take care , if 2nd also goes down there is downtime so if u donot want downtime we implement ASG 

- during the scaling time (scale in) removing the server1 , some client may be connecte to server 1 so connection wil be there , so ASG wont delete the srver1 immediately 

- so the concept deregister delay means , it wait for 300 seconds , all in-flight(requests which are in progres) request wil get completed after that only it will deregistered from the target group 


2  Slow start duration  : when scale out happen , LB has to sent trafffic to new server 


3    u will find in target group attributes section 

    1 Load balancing algorithm : round robin method it uses (1,2,3,1,2,3)

    2 Least outstanding request : this means u have server 1 and server 2 based n the request like if serve 1 getting 5 request and server2 is 20 requests , the nxt request is going to server 1 coz it is getting low requests , it is based on the least getting requests 


4 Stickiness : when ever u want to stick with to the specific server 

- if u enable stickness , ur request sent to only one server for the particular period of time 


-------------------------------------- do for /path based routing 

-- create 2 more servers 

-- with admin in the user-data 

-- create 2 ec2 with userdata

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/admin

-- once u created jst copy public ip and paste in browser  "192.87.98.2/admin"

-- now add target groups in LB 

-- once u create TG and attach to LB and create rule 


-- go to ELB --> listeners and rules --> edit rules --> add condition(path) /admin --> 

-- u have one option like return fixed response : 

select path rule --> select return fixd response --> enter response code and write ur text 


	
Forward to target group

Admin-tg : 1 (100%)
Group-level stickiness: Off


-- once u created rules here , check in browser by giving /admin 

-- this is called path based routing 

-- limits for ALB 

-  total 100 ules per ALB 

-  5 Conditions values per rule 

-  5 wildcards per rule

-  5 weighted target group pe rule 


-- for https we have certificates 

-- go to ACM and c.o request certificate 

-- select request a public certificate -->nxt

-- provide Domain name : eg : *.subbu.com ------ here * means any sub-domain

-- u cna purchase the domain first and then create record

-- then in target groups select https in target groups and add liteners 



===============================AUTO SCALING GROUPS========================================


• In real-life, the load on your websites and application can change

• In the cloud, you can create and get rid of servers very quickly

• The goal of an Auto Scaling Group (ASG) is to:
   • Scale out (add EC2 instances) to match an increased load
   • Scale in (remove EC2 instances) to match a decreased load
   • Ensure we have a minimum and a maximum number of EC2 instances running
   • Automatically register new instances to a load balancer
   • Re-create an EC2 instance in case a previous one is terminated (ex: if unhealthy)

• ASG are free (you only pay for the underlying EC2 instances)

IMP : When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the earlier ones. This way, rebalancing does not compromise the performance or availability of your application.

-- Because Amazon EC2 Auto Scaling attempts to launch new instances before terminating the earlier ones, being at or near the specified maximum capacity could impede or completely halt rebalancing activities.




----------------------- Auto Scaling - CloudWatch Alarms & Scaling

• It is possible to scale an ASG based on CloudWatch alarms

• An alarm monitors a metric (such as Average CPU, or a custom metric)

• Metrics such as Average CPU are computed for the overall ASG instances

• Based on the alarm:
  • We can create scale-out policies (increase the number of instances)
  • We can create scale-in policies (decrease the number of instances)






-- create a Load Balancer with  empty TG

-- u have to create Launch template (user-data), u can edit LT once u created , u havve create new LT if u want new version of ur appn 

-- c.o on ASG and Give name as you want and select Launch Template(IMP:Make sure AMI Version is with Kernal 5.10.....) ,select the version of template you want

-- using with LT, create ASG 

-- ASG use LT to launch ec2 instance 

-- if u want ur big Application like tomact on ec2 instance(ASG), cretae ec2 first ,deploy the appn and create AMI 

-- use this custom AMI in LT instead on Linux and create LT 

-- u provide min , max and desired capacity while creating ASG 

-- ASG will launch ec2 in TG automatically 

-- Always Turn on ALB Health checks 'coz, 

Elastic Load Balancing monitors whether instances are available to handle requests. When it reports an unhealthy instance, EC2 Auto Scaling can replace it on its next periodic check.

-- here we need to use sns ( simle notification servie) 

-- in SNS , we have TOPIC and SUBSCRIPTION 

-- in sns u need to crate TOPIC , once u created u need to subscribe

-- u can add this topic in ASG creation

-- create LB with simple TG no need to add any instances as targets ,this job will do by ASG for us during scale out and scale in process

-- now launch template with user-data

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html

-- create auto scaling group 

-- give LT and subnets as u preffered 

-- in notification topic u jst create topic to get notifications 

-- crate ASG 

-- what this ASG will do is that , it wil take LT and it will launch ec2 automatically according to ur desired capacity , so LT has user data so then our ec2 have index.html appn,all the ec2 instances will registered in the target groups automatically  and these TG are linked to Load balancer , so if u can access LB u will get appn 

-- all the things u have do in the launch template only , based on these LT ec2 will created 

-- go n check in target group 

-- in targets all ec2 are gettig registered 

-- There are 3 types of scaling 

1  manual scaling : u can change manually capapcity 

2  schedule Scaling : do schedule when u want to scale out happens when u feel the traffic will more o the certain days or time 

3  dynamic scalng : automatically happens 


-- u can check in activity section how scale out and scle in happens 

-- once it reach to max capaicty it wont go more than that as th given capacity , u can edit once if u want more capacity to scale out 




---- there are 2 types of Scaling policies in ASG 


1 Dynamic scaling policies : 


Amazon EC2 Auto Scaling supports the following types of dynamic scaling policies:

• Dynamic Scaling

    • Target Tracking Scaling
         • Simple to set-up
         • Example: I want the average ASG CPU to stay at around 40%

         EPV : An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received.

         ANS :  Use a target tracking scaling policy based on a custom Amazon SQS queue metric

         EXP : -- If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively. 
         
          -- You may use an existing CloudWatch Amazon SQS metric like ApproximateNumberOfMessagesVisible for target tracking but you could still face an issue so that the number of messages in the queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue.

          -- The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain.

          -- To calculate your backlog per instance, divide the ApproximateNumberOfMessages queue attribute by the number of instances in the InService state for the Auto Scaling group. Then set a target value for the Acceptable backlog per instance.

          -- To illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. 

          -- This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value.




    • Simple / Step Scaling
         • When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units
         • When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1

      - simple scaling : 
      
      -- With simple scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process. 
         
      -- The main issue with simple scaling is that after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms.

      -- This implies that the application would not be able to react quickly to sudden spikes in orders.

      - step scaling policy 

      -- With step scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process.

      -- When step adjustments are applied, they increase or decrease the current capacity of your Auto Scaling group, and the adjustments vary based on the size of the alarm breach. 

      -- step scaling would try to approximate the correct number of instances by increasing/decreasing the steps as per the policy. This is not as efficient as the target tracking policy where you can calculate the exact number of instances required to handle the spike in orders.




    • Scheduled Scaling

       • Anticipate a scaling based on known usage patterns
       • Example: increase the min capacity to 10 at 5 pm on Fridays


 
--- How dynamic scaling policies work

A dynamic scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM. The metrics that are used to invoke the alarm state are an aggregation of metrics coming from all of the instances in the Auto Scaling group. (For example, let's say you have an Auto Scaling group with two instances where one instance is at 60 percent CPU and the other is at 40 percent CPU. On average, they are at 50 percent CPU.) When the policy is in effect, Amazon EC2 Auto Scaling adjusts the group's desired capacity up or down when the threshold of an alarm is breached.

When a dynamic scaling policy is invoked, if the capacity calculation produces a number outside of the minimum and maximum size range of the group, Amazon EC2 Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits. Capacity is measured in one of two ways: using the same units that you chose when you set the desired capacity in terms of instances, or using capacity units (if instance weights are applied).

* Example 1: An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a dynamic scaling policy that adds 3 instances. When invoking this policy, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size.

* Example 2: An Auto Scaling group has a minimum capacity of 2, a current capacity of 3, and a dynamic scaling policy that removes 2 instances. When invoking this policy, Amazon EC2 Auto Scaling removes only 1 instance from the group to prevent the group from becoming less than its minimum size.


3  Predictive scaling policies : 

• Predictive scaling: continuously forecast load and schedule scaling ahead


Predictive scaling forecasts load based on your Auto Scaling group's history. It scales out the group in advance of forecasted load, so that new instances are ready to serve when the load arrives.


Predictive scaling works with CPU utilization, network in/out traffic, the request count to an Application Load Balancer target group, and custom metrics.

You can use predictive scaling to improve availability for applications whose workloads have predictable daily or weekly cycles.

As a best practice, consider using both dynamic scaling and predictive scaling. Predictive scaling uses forecasts to make decisions about when to add capacity according to a metric's historical trends while dynamic scaling makes adjustments in response to real-time changes in a metric's value.


------------- Good metrics to scale on :

-- CPU utilization       : AVg CPU across ur instance 
-- REquestCountPertarget : to makesure the no.of request per ec2 instance is stable
-- AVG Network in/out    : if u r appn is network bound 


IMP NOTE : the ASG can not go over the max capacity that u have mentioned during the creation time 


-- instance Refresh : if u want new version of ur apn , then cretae new LT coz u cant edit old LT so create New LT and attach to ASG 

-- this is how u can do updates in real time in ASG 

Start an instance refresh to perform rolling updates on the Auto Scaling group's instances. Only one instance refresh can be active at a time.


-- when ec2 terminating it is showing like "draining" once it finishes all connctions , then only it will get removed from the TG (it will complete in-flight request and get removed)




EPV : As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs.

ANS : Set the minimum capacity to 2

Use Reserved Instances (RIs) for the minimum capacity


EXP : Here, even though our ASG is deployed across 3 Availability Zones (AZs), the minimum capacity to be highly available is 2. When we specify 2 as the minimum capacity, the ASG would create these 2 instances in separate Availability Zones (AZs). 

-- If demand goes up, the ASG would spin up a new instance in the third Availability Zone (AZ). Later as the demand subsides, the ASG would scale-in and the instance count would be back to 2.


-- Reserved Instances (RIs) provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. 

-- These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. Since minimum capacity will always be maintained, it is cost-effective to choose reserved instances than any other option.

-- In case of an Availability Zone (AZ) outage, the instance in that Availability Zone (AZ) would go down however the other instance would still be available. The ASG would provision the replacement instance in the third Availability Zone (AZ) to keep the minimum count to 2.





EPV : A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases.

As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?

ANS : The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6

EXP : Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. This is why the minimum capacity should be 4 instances and not 2. Auto Scaling group will launch 2 instances each in both the AZs and this redundancy is needed to keep the service available always.




----------------------------------------------------- scaleout practicals 

-- (IMP:Make sure AMI Version is with Kernal 5.10.....) ,select the version of template you want

-- conect one instace to SSH and install manual load to the EC2 instance by install stress commands manually

--  enter the command

 sudo amazon-linux-extras install epel -y 

-- once avove command is successfully installed , then enter this command

  sudo yum install stress -y

-- once you installed, then try to push the load manually by entering the command

  stress -c 5

--  wait for sometime(4-5 min) to updating the data , then check in ASG and see how the changs are happening

-- u can see the scale out is happening and meet the max capacity



EVP: ----------------------------------------------------------------- Launch Configuration vs Launch Template

Launch Configuration :

-- Launch configuration is an instance configuration template that an Auto Scaling Group uses to launch EC2 instances.

-- Launch configuration is similar to EC2 configuration and involves the selection of the Amazon Machine Image (AMI), block devices, key pair, instance type, security groups, user data, EC2 instance monitoring, instance profile, kernel, ramdisk, the instance tenancy, whether the instance has a public IP address, and is EBS-optimized.

-- Launch configuration can be associated with multiple ASGs

-- Launch configuration can’t be modified after creation and needs to be created new if any modification is required.

-- Basic or detailed monitoring for the instances in the ASG can be enabled when a launch configuration is created.

-- By default, basic monitoring is enabled when you create the launch configuration using the AWS Management Console, and detailed monitoring is enabled when you create the launch configuration using the AWS CLI or an API

-- AWS recommends using Launch Template instead.


EPV : A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Configuration (LC1) with "dedicated" instance placement tenancy but the VPC (V1) used by the Launch Configuration LC1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Configuration (LC2) with "default" instance placement tenancy but the VPC (V2) used by the Launch Configuration LC2 has the instance tenancy set to dedicated.

Which of the following is correct regarding the instances launched via Launch Configuration LC1 and Launch Configuration LC2?


ANS : The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have dedicated instance tenancy

EXP : By default, all instances in the VPC run as shared tenancy instances. Amazon EC2 Auto Scaling also supports Dedicated Instances and Dedicated Hosts. 

-- When you create a launch configuration, the default value for the instance placement tenancy is null and the instance tenancy is controlled by the tenancy attribute of the VPC.

-- If you set the Launch Configuration Tenancy to default and the VPC Tenancy is set to dedicated, then the instances have dedicated tenancy.

-- If you set the Launch Configuration Tenancy to dedicated and the VPC Tenancy is set to default, then again the instances have dedicated tenancy.


--------- Launch Configuration Tenancy vs VPC Tenancy


Launch Configuration Tenancy                    VPC Tenancy = default                                 VPC Tenancy = dedicated

1 not specified                                   shared - tenancy instance                           dedicated instance

2 default                                         shared - tenancy instance                           dedicated instance

3 dedicated                                       dedicated instance                                 dedicated instance







Launch Template :

-- A Launch Template is similar to a launch configuration, with additional features, and is recommended by AWS.

-- Launch Template allows multiple versions of a template to be defined.

-- With versioning, a subset of the full set of parameters can be created and then reused to create other templates or template versions for e.g, a default template that defines common configuration parameters can be created and allow the other parameters to be specified as part of another version of the same template.

-- Launch Template allows the selection of both Spot and On-Demand Instances or multiple instance types.

-- Launch templates support EC2 Dedicated Hosts. Dedicated Hosts are physical servers with EC2 instance capacity that are dedicated to your use.

-- Launch templates provide the following features

   - Support for multiple instance types and purchase options in a single ASG.

   - Launching Spot Instances with the capacity-optimized allocation strategy.
   - Support for launching instances into existing Capacity Reservations through an ASG.
   - Support for unlimited mode for burstable performance instances.
   - Support for Dedicated Hosts.
   - Combining CPU architectures such as Intel, AMD, and ARM (Graviton2)
   - Improved governance through IAM controls and versioning.
   - Automating instance deployment with Instance Refresh.


------------------- Auto Scaling Group Attributes

• A Launch Template (older “Launch Configurations” are deprecated)

  • AMI + InstanceType
  • EC2 User Data 
  • EBSVolumes
  • Security Groups 
  • SSH Key Pair
  • IAM Roles for your EC2 Instances 
  • Network + Subnets Information 
  • Load Balancer Information


• Min Size / Max Size / Initial Capacity

• Scaling Policies




EVP : Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ) us-east-1a as it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ) us-east-1a like so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour.

ANS : As Per the default termination policy, the first priority is given to any allocation strategy for On-Demand vs Spot instances. As no such information has been provided for the given use-case, so this criterion can be ignored. 

-- The next priority is to consider any instance with the oldest launch template unless there is an instance that uses a launch configuration. So this rules out Instance A.

-- Next, you need to consider any instance which has the oldest launch configuration. This implies Instance B will be selected for termination 

-- Instance C will also be ruled out as it has the newest launch configuration. Instance D, which is closest to the next billing hour, is not selected as this criterion is last in the order of priority.




EVP : The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance. ? 

ANS : they are several chances that 

1 The health check grace period for the instance has not expired

EXP : Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on Amazon EC2 status checks and Elastic Load Balancing (ELB) health checks until the health check grace period expires.


2 The instance maybe in Impaired status

EXP : Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch.

3 The instance has failed the Elastic Load Balancing (ELB) health check status

EXP : By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. 

-- As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB.




====================================================================CLOUD WATCH=======================================================

-- CloudWatch is all about Alarms,Events and Logs

-- it is regional only 

-- CW is used to monitor performance of all as resource 

-- to monitor the resource , CW need Host level metrics also known as default metrics 

1 CPU 

2 Network 

3 Disk --- volume 

4 Status Check 

-- memory not comes under HLM 

-- memory is custom metrics 

-- 2 types of moitoring 

1 Basic and 2 Detailed Moitoiring 

-- Basic is free nad it will tke every 5 min data

-- deatiled monitoiring : every 1 min data points , billable


-- u create alaram in CW 

-- alarms can do actions like (terminate , reboot ,stop , recover ) 

-- Alarm has 3 States : 

1 In Alarm : > 90 

2 OK : < 90

3 Insufficient : ec2 stopped due to some reasons



-- we also have concept called " Composite Alarms"

- CW alarms are single metric

- Composite Alarms are monitoring the states of multiple alarms 

 eg: AND or OR conditions 


• Helpful to reduce “alarm noise” by creating complex composite alarms




EPV : A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console.

Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?


ANS : Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance

EXP : You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots the instance. 

-- The reboot alarm action is recommended for Instance Health Check failures (as opposed to the recover alarm action, which is suited for System Health Check failures).



============================EVENTS/Event Bridge=========================

-- where all th events are stored 

-- u can use these events for many use cases 

eg: if u want to get notification once ec2 is stoped 

ec2 --> stopped --> event -->route to the target (SNS(topic))


-- u can also do scheduled/Cron Job 

eg: for stopped and started 

 for this we have to create 2 lambda functions one is for stop and another one is for start 

--  NameSpace : group of metrics / collections of related metrics

-- if u want to get all ec2 logs at one place , u have to add CW agent in each ec2

-- all the logs visible in CW logs 

-- by default CW agent not able to push logs to CW logs 

-- by using Roles u can do push the logs to CW LOgs 

---- in CW we have Cannary/cannaries : used to applications monitoring websites endpoints


=============Practiclas Alarms Events and lambda ========================


-- ----CloudWatch


-- crate 2 instances 

-- now these ec2 not in use so ,if u want to stop the ec2 which is not in use ,

-- create alarm first --> c.o + in instances alaram status

-- create alarm as per ur requriments and see how it is stopped the ec2 instance 

-- as of now we have created alarm in ec2 console 

-- we can also create alaram in CloudWatch 

-- go to CloudWatch 

-- c.o create alarm --> select metric --> ec2 --> Per-Instance Metrics-->select ec2 u want copy id --> slect hostlevel metric --> it wil create one namespace like aws/ec2 

-- check how alarm works 

-- IMP : Alarm actions (stop,terminated,reboot) enable and disable at any time 


---------------Events/eventBridge

-- in EB , we can create Rules 

-- u can receive notificatio if someone stop ec2 

-- create rule in event bridge as per ur requrimnts



--------------- automation lambda  

-- if u want to stop ec2 at exact time and start exact time we use lambda for the automation 


1 create policy to stop ec2 

2 create role and attach policy 

3 attach this role to lambda -- trusted entity 

4 event schedule 

-- to folow these steps u can stop ec2 

-- lambda with vpc = can acces aws sevices like rds,s3 etc 

-- lambda without vpc = public 

-- now go to iam inline create policy 

{  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:Start*",
        "ec2:Stop*"
      ],
      "Resource": "*"
    }
  ]
}


------ now create a role 

-- select lambda and attach policy that u have created 

----------- now create Lambda function 

- lambda is region only 

- it is serverless

- create functio and attach role to the lambda and write code for stop function in python 

import boto3
region = 'us-west-1'
instances = ['i-12345cb6de4f78g9h', 'i-08ce9b2d7eccf6d26']
ec2 = boto3.client('ec2', region_name=region)

def lambda_handler(event, context):
    ec2.stop_instances(InstanceIds=instances)
    print('stopped your instances: ' + str(instances))


-- --------create rule in event bridge 

-- create rule with cron job/schedule 

-- create cron job and if u wnt to know it is working or not jst refresh the lambda page u wil get triggers 

-- if u test ur code , if it will get executed successfully , the ec2 instacnes will get into actions(stop,terminated) automtically 


----- Lambda Limits 

- memory allocation = 128MB-10GB

- max Execution time = 900 seconds(15 min) 

- environmenta variable = 4kb in size 

- disk capacity in the function containers (in /tmp) like libraries = 512MB-10GB

- Concurrency execution = 1000(can be increased) , one lambda fun cab be executed 1k times 



-------- deployement 

- lambda function deployement size (compressed .zip) = 50MB

- size of uncompressed deployement = 250MB

- Memory allocation is very imp in lambda functions 

- lambda will be only billed onexeccution times 




===========================Cloudwatch Theory=============================

-- 

-- it is a service that collects and manages operational data 

-- operationl data and ny dtaa that u collected by an environment either detailing how it performs , how it normally runs or any logging data it generates 

---- we have 3 teminologies 

1  Metics : collects data of AWS Products , Apps and even on-premises servers

2  Cloudwatch Logs : logs of AWS products , Aps, on-premises

3  CloudWatch Events : AWS Services and Schedules 

Eg : it generates C.W events when EC2 stops , start or anything 


-----------some  terminologies to understand for the Cloudwatch 


1  NameSpaces : Containers for monitoring data , it is a way to keep things seperate 

-- NameSpace has got a name , it can be anythng as long as it stays within the rule set 

-- All aws data goes to aws namesoace ---> AWS/Service

-- Namespace contains related metric 


2  Metric : it is a collection of related Data Points , in the time ordered structures 

Eg: cpu usage network IN/OUT 


3  Data Point : let us say we have metric called CPU Utilization , everytime any server measures its utilixation and send it into cloudwatch that goes into the CPU utilization metrics and each one of those measures so everytime the server reports the cpu that measure is called "Data Point" 


--it has 2 components 

1 timestamp 

2 value 


Note : CPU utilization metric could contain data from many servers ,so how do we seperate data for this? so use " Dimesions"


4  Dimensions : these are Name Value Pairs that seperate data point for different thngs or perspective withn the same metric 


-- while sending data points to cloud watch ,AWS also send in , these two 

A) Name = InstanceID , Value =I-xxxx

B) name = InstanceType , value =t2.micro.......


5  ALARM : CW also allows to take actions based on metrics which is done using Alarms 

-- two states 

A) OK --> Everything is working fine 

B) ALARM --> Something bad has happened 






==============================================CLoudWatch LOGS=============

--- u ave to install CW Agent 

-- by deafult CW agent do not have permission to push logs to cloudwatch logs 

-- so we should create ROLE here , and give CWFULL access policy and this role is attach to the ec2 

1  create IAM ole and give CW permissions 

2  attach The role to the ec2 instance ,make sure ur kernal version is 5.10....

3  login to te ec2 and install CW agent 

4  configure the files 

5  start the CW agent service 

6  see the logs in CW loogs 

-- crtae IAM role 

-- give CLOUDWATCHFILLACES permissions and attach role to the ec2 

-- now loginto the ec2 and enter cmnds 

1 make sure ur kernal version is 5.10....


- login as root user like sudo -s
2 in real time u always do Yum Update 

3 yum install -y awslogs  --- to intall CW logs

4 once  u install cw packages , u have to create 2 files 

- go to cd /etc/awslogs/

- press enter 

- once u do ls , two files will be crated 

-  1  awscli.conf and  2 awslogs.conf

- do sudo cat awscli.conf

- change region as per ur requriemnt

- do cat another file awslogs.conf

- file = var/logs/messages (this is appn log path) 

- log_stream_name = from ehre logs coming 

5 now u have to start awslogs , once u start awslogs automatically the log group generated once u start 

6 start the cloudwaatch Agent service

- systemctl start awslogsd 

- press nter 

7 once u pres nter the backend process will run and all the system logs will push to the CW logs 

8 go n check in log groups 

- /var/log/messages  log group created 

- we have given log_stream as Instance _id open n check the log group 

- once u open instnce id u can check all the system logs 

9  u can also push appn logs to CW agent 


- insted of var/log/messages u put log path   



--- u can do monitor for appliction using canaries 

- application monitoring --> synthesis canaries --> create canary for ur application and check the appn is working or not 

- select heartbeat canary for simple eg



=========================Lightsail================

- everything is availabe here 

- open lightsail

- here if we create instnce it is called "lightsail instances"

- create instance sleect wordpress

- here backup of lightsail instance is called "snapshot" 

- it only give 32GB only 

- once u create lightsail instance copy ip and paste in browser 



Expore topic : till now we only see Host level metrics , now find how to get metrics for memory 

- By default, we cannot monitor Memory metrics on EC2 Instances.

- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-scripts-intro.html

- use this link for the documentation 

or u can do in another process

- create ec2 instance 

- crate one role and attach policies 

-  attach policies cloudwatchfullacces and AmazonSSMFullAccess

-  why SSM? 

ANs: i have to store particular value(json document which is used to fetch the memory utilixation from aws ec2 and send it to AWS cloudwatch ) in ssm

-  create a parameter in the system manager(system manager --> application --> parameter) with the name (u can give any name) 

eg :  /alarm/AWS-CWAgentLinConfig 

- go system manager -->paramter store --> give name --> in the value place copy json script 

{
	"metrics": {
		"append_dimensions": {
			"InstanceId": "${aws:InstanceId}"
		},
		"metrics_collected": {
			"mem": {
				"measurement": [
					"mem_used_percent"
				],
				"metrics_collection_interval": 60
			}
		}
	}
}


-- ceate ec2 and attach role to ec2 and create with userdata with cloudwatch agent to install

userdata: 

#!/bin/bash
wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip
unzip AmazonCloudWatchAgent.zip
sudo ./install.sh
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:/alarm/AWS-CWAgentLinConfig -s


-- check whether clouwtch agent is instaled or not by using 

-- sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status

-- if it is running it is success

-- now go to CLoudwatch and --> all metrics --> CW Agent --> give instacne id and chck logs of memory



=========================================================== AWS CloudTrail =============================

• Provides governance, compliance and audit for your AWS Account

• CloudTrail is enabled by default!

• Get an history of events / API calls made within your AWS Account by:

• Console
• SDK
• CLI
• AWS Services

• Can put logs from CloudTrail into CloudWatch Logs or S3

• A trail can be applied to All Regions (default) or a single Region.

• If a resource is deleted in AWS, investigate CloudTrail first!

-- By default, CloudTrail trails created via the AWS Management Console will have global service events enabled. It is recommended that you only have one trail allocated to global service events per account in order to reduce duplicate events.


EPV : AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible.




--------------------------- CloudTrail Events

1 Management Events:

• Operations that are performed on resources in your AWS account

• Examples:
 • Configuring security (IAM AttachRolePolicy)
 • Configuring rules for routing data (Amazon EC2 CreateSubnet)
 • Setting up logging (AWS CloudTrail CreateTrail)

• By default, trails are configured to log management events.

• Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)


2  Data Events:

• By default, data events are not logged (because high volume operations)

• Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject): can separate Read and Write Events

• AWS Lambda function execution activity (the Invoke API)


3  CloudTrail Insights Events:

• Enable CloudTrail Insights to detect unusual activity in your account:
 • inaccurate resource provisioning
 • hitting service limits
 • Bursts of AWS IAM actions
 • Gaps in periodic maintenance activity

• CloudTrail Insights analyzes normal management events to create a baseline

• And then continuously analyzes write events to detect unusual patterns
 • Anomalies appear in the CloudTrail console
 • Event is sent to Amazon S3
 • An EventBridge event is generated (for automation needs)


-------------- CloudTrail Events Retention

• Events are stored for 90 days in CloudTrail

• To keep events beyond this period, log them to S3 and use Athena


------------------------ LAB 

-- open cloud trail --> event history u can able to see all the events which u have done in the aws account through the root user or normal user 

-- for lab , lets create a new cloud trail 

-- create new s3 bucket , all the API events that will send to s3 and stored in this bucket 

-- Log file SSE-KMS encryption  and Log file validation = uncheck no need for this demo 

-- management events --> create trail that's it 

-- wait for 5 min atleast to see the current time and date iin the trail 

-- after 5 min go n check in the s3 bucket --> it will create log files 

-- in the mean while create one simple ec2 instance for example purpose\

-- check in the history it will shows u the instance running in the trail

-- this is how u will get all the API calls that U have made in ur AWS Account 



=============================================================== AWS Config ===============

-- AWS Config Record and evaluate configurations of your AWS resources

-- AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.

• Helps with auditing and recording compliance of your AWS resources

• Helps record configurations and changes over time

• Questions that can be solved by AWS Config:
 • Is there unrestricted SSH access to my security groups?
 • Do my buckets have any public access?
 • How has my ALB configuration changed over time? 

• You can receive alerts (SNS notifications) for any changes

• AWS Config is a per-region service

• Can be aggregated across regions and accounts

• Possibility of storing the configuration data into S3 (analyzed by Athena)


-------------------------- Config Rules

• Can use AWS managed config rules (over 75)

• Can make custom config rules (must be defined in AWS Lambda)
 • Ex: evaluate if each EBS disk is of type gp2
 • Ex: evaluate if each EC2 instance is t2.micro

• Rules can be evaluated / triggered:
 • For each config change
 • And / or: at regular time intervals

• AWS Config Rules does not prevent actions from happening (no deny)

Pricing: no free tier, $0.003 per configuration item recorded per region, $0.001 per config rule evaluation per region


----------------------------- LAB 

-- Customize AWS Config to record configuration changes for all supported resource types, or for only the supported resource types that are relevant to you. Globally recorded resources (RDS global clusters and IAM users, groups, roles, and customer managed policies) may be recorded in more than this Region. You are charged based on the number of configuration items recorded. 

-- open AWS config in AWS 

-- this is not free 

-- Recording method = All resource types with customizable overrides

-- choose role created by aws ( u no need to create aws role , aws will create for you)

-- and also bucket will create for you 

-- in rules search for the SSH and select this ssh this will check Checks whether security groups that are in use disallow unrestricted incoming SSH traffic.

-- create config , now config going to monitor the entire AWS environment 

--  changes that made in AWS will send to s3 

-- wait for 1-3 min to get details 

-- u will see the noncomplianct resources on dashboard

-- for me there are 4 noncompliance records are there so , open resources(manage resource) and delete ssh connection in Security groups 

-- now wait 1-2 min and check , now i do not have any noncompliance resources 

-- u can also check resources in left side , u can check time line of the resource when n how the changes happen 

-- it is best for auditing 



---------------------------------- IMP : once u delete rules in console , it won’t delete and u will get charge , so to delete this 

-- create one instance and connect instance install aws cli and login with ur keys 

-- follow these steps 

1 Turn off Recording for that region using the console

2 Delete the Rule by going to actions, delete rule

3 Use the AWS CLI and delete the default recording by

aws configservice delete-configuration-recorder --configuration-recorder-name default --region <region-name>

4 Delete the service linked role created for AWS Config  ( search for AWSConfigService role in roles)


-- Refresh the Config home page to make it appear fresh.

-- If necessary delete the config bucket and its objects.



================================================= CloudWatch vs CloudTrail vs Config ========================

• CloudWatch 

  • Performance monitoring (metrics, CPU, network, etc...) & dashboards
  • Events & Alerting
  • Log Aggregation & Analysis

• CloudTrail
  • Record API calls made within your Account by everyone 
  • Can define trails for specific resources
  • Global Service

• Config

 • Record configuration changes
 • Evaluate resources against compliance rules
 • Get timeline of changes and compliance






=======================================Elastic Beanstalk==================

-- it is use for easy and wuixk deloyment of appn in aws

-- it is PAAS

-- backbone of beanstalk is EC2

-- beanstalk is free but what veer the resource launchs in backend that will be charged 

-- Beanstalk has deployment Modes (presets)

1 Single Ec2 instance : it will launch only one instcnce but it is not good for production environment 

2 High Availability : here u jst give configuration , automaticaly beanstak will prepare for u , it is preffered for the production environment 


Note :there is one more Custoom preset : when u have ur own configuration is called "custom reset"

----------------------Architecture of Elastic Beanstalk


--firt thing u have to create is appn 

-- inside appn u have environment 

-- inside the environment u have ur ec2 instance 

-- once u create env-url wil generated 

-- u can also have multiple environments 

-- if one ec2 is there in environment then it is called single deployment PRESET


-- if u go for High availablity , then u have ELB, ASG top of it and u can have multple instances 

-- u can acccess using env-URL 

-----------------for eg u have latest verion of ur appn 


-- beanstalk has deloyment Method/policy 

1  it has All at Ocne = all instaces go and update at once 

- here there is downtime , no xtra cost 

2 Rolling with additional batches : u wil give batch wise 

- No downtime here , it has  extra cost  

3 Immutable : if u have ASG running , it has 3 ec2 

- if  u g for immutable and ask for deploy new version then it wil create temporary ASG n then new version deployed in Temp ASG once it deployed succesfully then only it will move to original ASG 


4 rcently AWS cam with new one called traffic Spliting 



traffic Spliting  : split traffic b/w ec2 one by one 


5 Blue Green deployment : 

- eg if ur changing the complte platform , but deal is customer never face the downtime 

- All the previoud methods touch customer environment whic is not good 


- here u will create new environment and deloy new version of appn 

- in Aws it has feature called SWAP URL , once u SWAP ,the customer redirected to new platform 

- the existing one is called BLUE ENVIRONMENT and New one is called GREEN Environment 

- it has extra cost 


--------------Practical Elastic bean stalk(blue-green deployment) ----------


-- it is regional 

-- it is all about configuration 

-- There’s no additional charge for Elastic Beanstalk. You pay for Amazon Web Services resources that we create to store and run your web application, like Amazon S3 buckets and Amazon EC2 instances.

-- c. o create appn 

-- u have 2 environments here 


1 Web server environment: Run a website, web application, or web API that serves HTTP requests.

2 Worker environment : Run a worker application that processes long-running workloads on demand or performs tasks on a schedule.


-- choose web server 

-- Environment information 

- name :ELASTICBEANSTALK-tomcat-env

- copy n paste in url(domain) section below 

- crate one role which has ec2full access


-- step 3 : do not enable ublic ip for ec2 instances  


-- health report - basic 

--once u created ElasticBS succesfully it will crate one ec2 instance, SG, EIP and also instance added to Asg automatically 

-- u can all the events in event section below 

-- once u open domain u will get ur first application in tomacat platform 

-- if error chckc SG configuration or copy public ip and paste in browser 


------ now create new environment , now we are changing platform python in appn 


-- same steps jst change platform  

-- once u launc new environment with python , click on domain link now u eill get in green colour for the python 


-- now u want ur customers redirected to the new version(python) 

-- do SWAP URL 

-- go to application level --> go inside tomcat server --> swap environment domain --> slect swap platform 

-- once u swap url all the customers will get in the green colour , here there is no downtime , url is not changed 



-- save ur configuration for the furute purpose if nay one canges ur configuraations 

-- done with Ebeanstalk



=============Simple storage service (S3)================================

-- it is object based storage service 

-- in s3 u can store all kind of files 

-- S3 cost is depend on size of the object and data transfer 

-- u can only downloaded, upload and access files from s3

-- u cant instal os,db in s3

-- the files can not executed 

-- s3 is unlimited storage 

-- s3 is cheaper then ec2

-- s3 is serverless

-- s3 is global 

-- An object consists of the following:

    - Key – The name that you assign to an object. You use the object key to retrieve the object.
    - Version ID – Within a bucket, a key and version ID uniquely identify an object.
    - Value – The content that you are storing.
    - Metadata – A set of name-value pairs with which you can store information regarding the object.
    - Subresources – Amazon S3 uses the subresource mechanism to store object-specific additional information.
    - Access Control Information – You can control access to the objects you store in Amazon S3.
  

IMP : Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata.


-- bucket = container objects 

-- object = file 

---- key = name of the Object 

- The key is the FULL path:

eg : s3://my-bucket/my_file.txt

     s3://my-bucket/my_folder1/another_folder/my_file.txt

-- The key is composed of prefix + object name

eg : s3://my-bucket/my_folder1/another_folder/my_file.txt

-- buckets are regional 

-- bucket name are universal or UNique 

-- all ur buckets shown in same consl only even if u create buckets acrss region 

-- no nested buckets not possible 

-- u can create folders in bucket

-- by default , all buckets are private , if require u cna make public 

-- MAX 100 buckets per account, u can increase by conctact AWS ====imp

-- every file has URL to accesss 

eg : htps://bucket1.s3.ap-south-1.amazonaws.com/photos/puppy.jpg

-- sub-folder are called Prefix 

-- .jpg called suffix 

-- puppy.jpg called object 

-- photos/puppy.jpg === KEY 

-- it is WORM Model == write once read many 

-- ucan not get back once u delete but u can by using versioning 

-- 2 types of buckets

1 General purpose buckets are the original S3 bucket type and are recommended for most use cases and access patterns. General purpose buckets also allow objects that are stored across all storage classes, except S3 Express One Zone.


2  Directory buckets use the S3 Express One Zone storage class, which is recommended if your application is performance sensitive and benefits from single-digit millisecond PUT and GET latencies.




---------------------- Amazon S3 Use cases

• Backup and storage
• Disaster Recovery
• Archive
• Hybrid Cloud storage • Application hosting
• Media hosting
• Data lakes & big data analytics • Software delivery
• Static website


-------------------------------------------  S3 Bucket Policies

----- JSON based policies

- Resources: buckets and objects
- Effect: Allow / Deny
- Actions: Set of API to Allow or Deny
- Principal:The account or user to apply the policy to


--- Use S3 bucket for policy to:
- Grant public access to the bucket
- Force objects to be encrypted at upload
- Grant access to another account (Cross Account)



-------- types of Access control                       AWS Account-Level control                    user-level control

1       IAM policies                                    NO                                           YES

2       ACL's                                           Yes                                           NO

3       Bucket Policies                                 Yes                                           Yes



EPV: A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?

Ans : Use a bucket policy to grant permission to users in its account as well as to users in another account

EXP : A bucket policy is a type of resource-based policy that can be used to grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. For cross-account permissions to other AWS accounts or users in another account, you must use a bucket policy.



EPV : A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy.

{
 "Version": "2012-10-17",
 "Id": "S3PolicyId1",
 "Statement": [
   {
     "Sid": "IPAllow",
     "Effect": "Allow",
     "Principal": "*",
     "Action": "s3:*",
     "Resource": "arn:aws:s3:::examplebucket/*",
     "Condition": {
        "IpAddress": {"aws:SourceIp": "54.240.143.0/24"},
        "NotIpAddress": {"aws:SourceIp": "54.240.143.188/32"}
     }
   }
 ]
}

ANS : It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket

EXP : Let's analyze the bucket policy one step at a time:

-- The snippet "Effect": "Allow" implies an allow effect. The snippet "Principal": "*" implies any Principal. The snippet "Action": "s3:*" implies any Amazon S3 API.

-- The snippet "Resource": "arn:aws:s3:::examplebucket/*" implies that the resource can be the bucket examplebucket and its contents. Consider the last snippet of the given bucket policy: "Condition": { "IpAddress": {"aws:SourceIp": "54.240.143.0/24"}, "NotIpAddress": {"aws:SourceIp": "54.240.143.188/32"} } This snippet implies that if the source IP is in the CIDR block "54.240.143.0/24" (== 54.240.143.0 - 54.240.143.255), then it is allowed to access the examplebucket and its contents. 

-- However, the source IP cannot be in the CIDR "54.240.143.188/32" (== 1 IP, 54.240.143.188/32), which means one IP address is explicitly blocked from accessing the examplebucket and its contents.




----------------------------------------------- Amazon S3 – Static Website Hosting 


• S3 can host static websites and have them accessible on the Internet

• The website URL will be (depending on the region)

  - http://bucket-name.s3-website-aws-region.amazonaws.com

  - http://bucket-name.s3-website.aws-region.amazonaws.com


• If you get a 403 Forbidden error, make sure the bucket policy allows public reads!



-------------------------------------------------- S3 Versioning 

-- when u have critical data it is help to bacjup

-- Versionong is like a backup tool 

-- by deafult versioning is not enabled 

-- this cna be also do while creating bucket 

-- enable on bucket level and applied to objects 

-- Version ID is always Unique 

-- if u delete original ,it will have marker (delete marker) applied to  new version 

-- to restore object , delete the mrker and ur object is restored automatically (latest version) 

-- but if u want to previous version to be restored , u have to download it nd upload it again 

-- it is possible to download version files 

-- it is not posible to download delete marker , u can only delete it 

-- delete marker is applied to only latetst version, not for old/previous versions 

-- once u can enable versioning u can't disable it but u can do suspend it 

-- u can not restore the files when u suspend the versioning 

-- when the version is suspended it wont effect on previus objects but coming objects get effected 

-- It is enabled at the bucket level


----IMP POINTS 

-- min object size = 0 bytes , MAx objct size = 5TB

-- you can have unlimited number of objects having 5TB each in a single bucket 

-- if u have file 5TB u can not upload it in one shot , so in aws recommened MPU(multi-part-upload) 

-- this will done through the CLI not from the console 

-- aws recommended , if u have object > 100MB , go for MPU 




==============================Storage classes============================

-- it is manditory to slect storage classes while u uploading the objects in s3 bucket 


• Amazon S3 Standard - General Purpose
• Amazon S3 Standard-Infrequent Access (IA) 
• Amazon S3 One Zone-Infrequent Access
• Amazon S3 Glacier Instant Retrieval
• Amazon S3 Glacier Flexible Retrieval
• Amazon S3 Glacier Deep Archive
• Amazon S3 Intelligent Tiering

-- Can move between classes manually or using S3 Lifecycle configurations


---------------- 1 Standared frequently Access(FA) General Purpose

- this is used for frequently access data 

- it is default Storage Class 

- no retrival charges apply 

- Low latency and high throughput

- Sustain 2 concurrent facility failures

- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's 

- Min Object size = 0 bytes

- Use Cases: Big Data analytics, mobile & gaming applications, content distribution...


---------------- 2 Standared Infrequently Access(IA) 

-  For data that is less frequently accessed, but requires rapid access when needed

- retrival charges apply

- cheaper than FA 

- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's 

- Min Object size = 128kB

- Min Duration = 30 days 

- Use cases: Disaster Recovery, backups


---------------- 3 Reduce Redundancy Storage(RRS)

- it is Access frequently not accessed data but not critical 

- no retrival charges 

- AWS does not recommend to use this storage class

- cheaper than others 


- Availability :anytime  is 99.99%

- Durability  : long time is 99.99%


------------------ 4 One Zone IA

- infrequently access data but not critical , in a single AZ; 

- data lost when AZ is destroyed

- Retrival Charges Apply 

- Availability :anytime  is 99.5%

- Durability  : long time is 11 9's 

- Min Object size = 128KB

- Min Duration = 30 days 

- Use Cases: Storing secondary backup copies of on-premises data, or data you can recreate


------------------ 5 Intelligent Tier 

-- Small monthly monitoring and auto-tiering fee

-- Moves objects automatically between Access Tiers based on usage

-- There are no retrieval charges in S3 Intelligent-Tiering

- Frequent Access tier (automatic): default tier
- Infrequent Access tier (automatic): objects not accessed for 30 days
- Archive Instant Access tier (automatic): objects not accessed for 90 days
- Archive Access tier (optional): configurable from 90 days to 700+ days
- Deep Archive Access tier (optional): config. from 180 days to 700+ days

- it is only for FA and IA

- Availability :anytime  is 99.99%

- Durability  : long time is 11 9's 

- Min Duration = 30 days 


----------------------------------Glacier 

- it is used for in-frequently acces data , Low-cost object storage meant for archiving / backup

- Pricing: price for storage + object retrieval cost

- Archiving Data

- inside Glacier we create vault 

- Vault : container of Archives 

- Archieve : object/ .zip

Note: Archieve can be upto 40 TB

- unlimited number of archives in 1 vault 

- 1000 vaults

- retrival Charges Apply 

1 Amazon S3 Glacier Instant Retrieval

  - Millisecond retrieval, great for data accessed once a quarter
  - Minimum storage duration of 90 days


2 Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):

  - Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free
  - Minimum storage duration of 90 days

3 Amazon S3 Glacier Deep Archive – for long term storage:

  - Standard (12 hours), Bulk (48 hours)
  - Minimum storage duration of 180 days





================================================= S3 Advanced

----- Amazon S3 – Moving between Storage Classes

-- You can transition objects between storage classes

-- For infrequently accessed object, move them to Standard IA

-- For archive objects that you don’t need fast access to, move them to Glacier or Glacier Deep Archive

-- Moving objects can be automated using a Lifecycle Rules


---------------------------------life cycle Rules

NOte : it is possible to move objcts from one storage class to another storage clss automatically by crated Life cycle Rules 

- Life cycle rules can be applied for entire bucket level or for prefix(sub-folder) 

-- Life cycle Rules can be created for current versions and previous versions also 

-- In LCR we have Transistion and expiration 

- Transistion : configure objects to transition to another storage class
- Move to Glacier for archiving after 6 months

FA --> IA(30 days) --> Glacier(60 dyas) 

here            oth day --> 30th day --> 60th day 

- expiration : Delete after 365 days , configure objects to expire (delete) after some time


-- In an S3 Lifecycle configuration, you can define rules to transition objects from one storage class to another to save on storage costs.

-- When you don't know the access patterns of your objects, or if your access patterns are changing over time, you can transition the objects to the S3 Intelligent-Tiering storage class for automatic cost savings. 


IMP FOR EPV : Amazon S3 supports a waterfall model for transitioning between storage classes,

S3 Standard storage 

    S3 Standard-IA storage class 
     
          S3 Intelligent-Tiering storage class

               S3 One Zone-IA storage class

                    S3 Glacier Instant Retrieval storage class

                           S3 Glacier Flexible Retrieval storage class 

                                    S3 Glacier Deep Archive storage class.

1  Supported lifecycle transitions

-- Amazon S3 supports the following lifecycle transitions between storage classes using an S3 Lifecycle configuration. You can transition from the following:

-  The S3 Standard storage class to any other storage class.

-  The S3 Standard-IA storage class to the S3 Intelligent-Tiering, S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or S3 Glacier Deep Archive storage classes.

-  The S3 Intelligent-Tiering storage class to the S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or S3 Glacier Deep Archive storage classes.

-  The S3 One Zone-IA storage class to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes.

-  The S3 Glacier Instant Retrieval storage class to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes.

-  The S3 Glacier Flexible Retrieval storage class to the S3 Glacier Deep Archive storage class.

-  Any storage class to the S3 Glacier Deep Archive storage class.


2  Unsupported lifecycle transitions

-- Amazon S3 does not support any of the following lifecycle transitions. You can't transition from the following:

- Any storage class to the S3 Standard storage class.

- Any storage class to the Reduced Redundancy Storage (RRS) class.

- The S3 Intelligent-Tiering storage class to the S3 Standard-IA storage class.

- The S3 One Zone-IA storage class to the S3 Intelligent-Tiering, S3 Standard-IA, or S3 Glacier Instant Retrieval storage classes.


Constraints : 


-- When you transition objects from the S3 Standard or S3 Standard-IA storage classes to S3 Intelligent-Tiering, S3 Standard-IA, or S3 One Zone-IA, the following object size constraints apply:

Larger objects  : For the following transitions, there is a cost benefit to transitioning larger objects:

  - From the S3 Standard or S3 Standard-IA storage classes to S3 Intelligent-Tiering.

  - From the S3 Standard storage class to S3 Standard-IA or S3 One Zone-IA.

-- Objects smaller than 128 KiB : For the following transitions, Amazon S3 does not transition objects that are smaller than 128 KiB:

  - From the S3 Standard or S3 Standard-IA storage classes to S3 Intelligent-Tiering or S3 Glacier Instant Retrieval.

  - From the S3 Standard storage class to S3 Standard-IA or S3 One Zone-IA.


ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html




EPV : You have an Amazon S3 bucket that contains files in two different folders - s3://my-bucket/images and s3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ).

ANS : Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days

Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days






---------------------------- Amazon S3 – Lifecycle Rules (Scenario 1)

Q Your application on EC2 creates images thumbnails after profile photos are uploaded to Amazon S3.These thumbnails can be easily recreated, and only need to be kept for 60 days.The source images should be able to be immediately retrieved for these 60 days, and afterwards, the user can wait up to 6 hours. How would you design this?
 
 Ans : -- S3 source images can be on Standard, with a lifecycle configuration to transition them to Glacier after 60 days

       -- S3 thumbnails can be on One-Zone IA, with a lifecycle configuration to expire them (delete them) after 60 days


----------------------------- Amazon S3 – Lifecycle Rules (Scenario 2)

-- A rule in your company states that you should be able to recover your deleted S3 objects immediately for 30 days, although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable within 48 hours.

Ans : Enable S3 Versioning in order to have object versions, so that “deleted objects” are in fact hidden by a “delete marker” and can be recovered

-- Transition the “noncurrent versions” of the object to Standard IA
-- Transition afterwards the “noncurrent versions” to Glacier Deep Archive


----------------------------- Amazon S3 Analytics – Storage Class Analysis

-- Help you decide when to transition objects to the right storage class

-- Recommendations for Standard and Standard IA

-- Does NOT work for One-Zone IA or Glacier

-- Report is updated daily

-- 24 to 48 hours to start seeing data analysis

--  S3 bucket --> Storage Class Analysis --> .csv report 


================================= S3 Glacier Vault Lock =================

-- Adopt a WORM (Write Once Read Many) model

-- A vault is a container for storing archives on Glacier 

-- When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault.

-- Vault Lock is only for Glacier and not for Amazon S3

-- Create a Vault Lock Policy

-- Lock the policy for future edits (can no longer be changed or deleted)

-- Helpful for compliance and data retention



EPV : A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.

ANS : Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls





--------------------------------- Object Lock (versioning must be enabled)

-- Adopt a WORM (Write Once Read Many) model

-- Block an object version deletion for a specified amount of time

-- Retention mode - Compliance:
     • Object versions can't be overwritten or deleted by any user, including the root user
     • Objects retention modes can't be changed, and retention periods can't be shortened

-- Retention mode - Governance:
     • Most users can't overwrite or delete an object version or alter its lock settings
     • Some users have special permissions to change the retention or delete the object

-- Retention Period: protect the object for a fixed period, it can be extended

-- Legal Hold:
     • protect the object indefinitely, independent from retention period
     • can be freely placed and removed using the s3:PutObjectLegalHold IAM permission


EPV : 1) When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version
     
       - You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version.

       - Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.

      2) Different versions of a single object can have different retention modes and periods 

        - Like all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods.

        - For example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. 

        - In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days.

-- permanent lock and certain period lock 

-- if u want to get logs of ur bucket u have to enable " server Access Logs"

-- create seperate bucket for logs and all logs will stored here 

-- server access logs are Bucket level 

-- it is very hard to read logs when u have more logs (1000000000's)


============================Athena===================


-- to avoid above problem we can use "ATHENA" 

-- it analyze the logs directly from S3 



=================================CORS(cross-origin Resource sharing)============================


-- it is small JSON script 

-- when u host static website in s3 and u have index.html website inside u have puppy.jpg stored in bucket 1 

-- now in bucket 2 u put puppy.jpg, now u are caling from the bucket 1 

-- u wont get coz by default u cant share b/w two other buckets 

-- to avoid this we can use CORS 

-- enable CORS in 2nd Bucket 

-- once u eneble cors it will get puppy.jpg

=============================CRR(Cross-Region Replication)============

-- u have bucket in mumbai and u have another bucket in singapore 

-- whatever the thing in b1 it will get relicate in singapore also 

-- for this u have to create CRR rules in Bucket 1

-- CRR is not enebled by default 

-- it is BUcket-level

-- CRR can be shared to different Accounts possible 


Cost : cross-region replication incurs request and transfer fees of $0.005/1000 requests and $0.02/1GB transferred. Standard storage pricing on the replicated side apply and differ by region.  Currently for US locations N. California is 10-60% more expensive than N. Virginia and Oregon, depending on S3, S3-IA, or Glacier.

EG : For an example, replicating 2500GB data containing 5000files breaks down as follows:

5 * $0.005 + 2500 * $0.02 = $50.025 access & transfer fees


-------------------------------- LAB 

Relication Rules for CRR

-- one bucket files in bk1 , this files replicate to bkt 2

-- go to management 

-- enable versioning is maditory 

--public or private it is does not matter 

-- create 2 buckets in two different regions 

-- go to bkt 1 -->> management n create relication rules select destination path 

-- ask aws to create new IAM role

-- now upload some file sin bkt 1 n do refresh in blt 2 u will get same files in bkt 2 


-- if u get delete object in bkt 1 , it won't get delete in the bucket 2 (same for CRR and SRR)


============SRR(same region replication) ==================


-- if u are replicating same region 

-- it is bucket level 


NOTE : versioning is manditory to have CRR/SRR


VERY IMP for EPV :


-- if u want to get logs of ur bucket u have to enable " server Access Logs"

-- object level logs can be captured in CloudTrail



---------------------Amazon S3 – Security encryption(security)

-- it will be done in 2 ways 

1 in-transist : data is moving by using Https -- for this use ACM

-- ACM : is where u can generate HTTPS certs(in-transit)

2 Data at rest : can be done whle data at rest through KMS 

-- KMS : is where u can cretae Encryption keys for data At rest 



=====================S3 has 3 types of Encryptions ==================

1 server-side encryption : 

- we have 3 types 

------ A  SSE-S3 (aws Managed Key) -- default common encryption method we use ,

- it is used Algorith called AES-256 (advanced Encryption Standard)

- by deafult , bucket encryption is enabled 

- Object is encrypted server-side

- Must set header "x-amz-server-side-encryption": "AES256"


explanation : Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in AWS data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. For example, if you share your objects by using a presigned URL, that URL works the same way for both encrypted and unencrypted objects. Additionally, when you list objects in your bucket, the list API operations return a list of all objects, regardless of whether they are encrypted.


-- When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.


------ B SSE-KMS ( AWS KMS KEY)

- KMS advantages: user control + audit key usage using CloudTrail

- Object is encrypted server side

- Must set header "x-amz-server-side-encryption": "aws:kms"

explanation : All Amazon S3 buckets have encryption configured by default, and all new objects that are uploaded to an S3 bucket are automatically encrypted at rest. Server-side encryption with Amazon S3 managed keys (SSE-S3) is the default encryption configuration for every bucket in Amazon S3. To use a different type of encryption, you can either specify the type of server-side encryption to use in your S3 PUT requests, or you can set the default encryption configuration in the destination bucket.

---- SSE-KMS Limitation

-- If you use SSE-KMS, you may be impacted by the KMS limits

-- When you upload, it calls the GenerateDataKey KMS API

-- When you download, it calls the Decrypt KMS API

-- Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)

-- You can request a quota increase using the Service Quotas Console


------- c SSE-C (Customer provided Keys) 

- Server-Side Encryption using keys fully managed by the customer outside of AWS

- Amazon S3 does NOT store the encryption key you provide

- HTTPS must be used

- Encryption key must provided in HTTP headers, for every HTTP request made

explanation : Server-side encryption is about protecting data at rest. Server-side encryption encrypts only the object data, not the object metadata. By using server-side encryption with customer-provided keys (SSE-C), you can store your own encryption keys. With the encryption key that you provide as part of your request, Amazon S3 manages data encryption as it writes to disks and data decryption when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing that you need to do is manage the encryption keys that you provide.

When you upload an object, Amazon S3 uses the encryption key that you provide to apply AES-256 encryption to your data. Amazon S3 then removes the encryption key from memory. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key that you provided matches, and then it decrypts the object before returning the object data to you.



EPV : A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?

ANS : Configure the bucket policy to deny if the PutObject does not have an "x-amz-server-side-encryption" header set

EXP : 

-- Server-side encryption is about data encryption at rest—that is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.

-- As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.

-- To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.

-- In order to enforce object encryption, create an Amazon S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. 

-- There are two possible values for the x-amz-server-side-encryption header

    1 AES256, which tells S3 to use S3-managed keys, 

    2 aws:kms, which tells Amazon S3 to use AWS KMS–managed keys.






2 Client Side Encryption 

-- Use client libraries such as Amazon S3 Client-Side Encryption Library

-- Clients must encrypt data themselves before sending to Amazon S3

-- Clients must decrypt data themselves when retrieving from Amazon S3

-- Customer fully manages the keys and encryption cycle


Client-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects.

The Amazon S3 Encryption Client works as an intermediary between you and Amazon S3. After you instantiate the Amazon S3 Encryption Client, your objects are automatically encrypted and decrypted as part of your Amazon S3 PutObject and GetObject requests. Your objects are all encrypted with a unique data key. The Amazon S3 Encryption Client does not use or interact with bucket keys, even if you specify a KMS key as your wrapping key.


3 In-transit Encryption 

-- Encryption in flight is also called SSL/TLS

-- Amazon S3 exposes two endpoints:
• HTTP Endpoint – non encrypted
• HTTPS Endpoint – encryption in flight

-- HTTPS is recommended

-- HTTPS is mandatory for SSE-C

-- Most clients would use the HTTPS endpoint by default

Encrypting Data-at-Rest and Data-in-Transit

To protect data in transit, AWS encourages customers to leverage a multi-level approach. All network traffic between AWS data centers is transparently encrypted at the physical layer. All traffic within a VPC and between peered VPCs across regions is transparently encrypted at the network layer when using supported Amazon EC2 instance types. At the application layer, customers have a choice about whether and how to use encryption using a protocol like Transport Layer Security (TLS). All AWS service endpoints support TLS to create a secure HTTPS connection to make API requests.


----------IMP : -------s3 data Consistency Models 

-- Read after write for PUTS of New Objects 

-- eventually Consistency for OVERWRITES of PUTS and DELETES 



======================================Pre-signed URL===================== 

-- Generate pre-signed URLs using the S3 Console, AWS CLI or SDK

-- URL Expiration
• S3 Console – 1 min up to 720 mins (12 hours)
• AWS CLI – configure expiration with --expires-in parameter in seconds (default 3600 secs, max. 604800 secs ~ 168 hours)

-- Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET / PUT

-- Examples:

• Allow only logged-in users to download a premium video from your S3 bucket
• Allow an ever-changing list of users to download files by generating URLs dynamically
• Allow temporarily a user to upload a file to a precise location in your S3 bucket

if u want to give access for some time only then AWS it has "pre-signed URL" which is like temporary Acess for certain perios of time to the user through a temorary Object URL



================S3 transfer Acceleration============

-- it is billable 

-- it used CDN network to upload very speed in other regions 

-- Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region

-- Compatible with multi-part upload


EPV : There are no S3 data transfer charges when data is transferred in from the internet. Also with S3TA, you pay only for transfers that are accelerated. if it is not accelerate you no need to pay any money 



==================== S3 Byte-Range Fetches ===========

-- Parallelize GETs by requesting specific byte ranges
-- Better resilience in case of failures
-- Can be used to speed up downloads
-- Can be used to retrieve only partial data (for example the head of a file)


===================== S3 Select & Glacier Select =================

-- Retrieve less data using SQL by performing server-side filtering

-- Can filter by rows & columns (simple SQL statements)

-- Less network transfer, less CPU cost client-side 


=================S3 Requester pays==============

-- in geenral , bucket ownr pay all the S3 storage data trnsfer cost associated to their Bucket

-- with S3 Requester pays buckets , the requester instead of the bucket owner pay the cost of h request and data download from the bucket 

-- helpful if u want to share the large data sets

-- The requester must be authenticated in AWS , so that AWS knows where to charge( in their account), cannot be anonymous 



================S3 event notifications ===================


-- if someone is trying to adding . doing encryption etc... anything in the s3 u will get notifications 

-- s3 send notifications to SNS 

=================S3 Batch Operations==================

-- if u wnat to perform bulk operations on existing S3 object with a single request , example:

• Modify object metadata & properties 
• Copy objects between S3 buckets
• Encrypt un-encrypted objects
• Modify ACLs,tags
• Restore objects from S3 Glacier
• Invoke Lambda function to perform custom action on each lambda object

-- A job consists of a list of objects, the action to perform, and optional parameters

-- S3 Batch Operations manages retries, tracks progress, sends completion notifications, generate reports ...

-- You can use S3 Inventory to get object list and use S3 Select to filter your objects



------------------------------------- LAB 

IMP NOTE : Do not create your own IAM Roles and policies for this project , You just go with "create new Role" option while doing this process


-- create 2 buckets , versioning is must 

-- now in bucket 1 , upload some files 

-- our main aim is to replicate the existing objects from one bukcet to another bucket

-- now in the bucket 1 --> management --> create replication rule 

-- give rule name 

-- Destination : select destination bukcet 

-- IAM role = ask aws to create a role 

-- once u click on save , pop-ip will come and select "Yes, replicate existing objects."

-- Job run options = select any one 

if u choose Automatically run the job when it's ready , it will run automatically once it will get finished 

if u choose Wait to run the job when it's ready , u have to run job manually 

-- Generate completion report = create new bucket to store the report or just simply give bkt 1 for practice only 

-- ask aws to create new IAM role for this job 

-- click on save , once u save this it will create job operations for u and wait for some time , once it get finished , all the objects will get replicated from source to dest bucket

-- now upload file in bucket 1 and it will replicate in dest bucket 'coz we have created replication 



------------------- LAB 2 

-- if u have create only replication rule(SRR, CRR ) already , jst click on view configuration and Click on create manifestjob and provide the details like the above example

-- set Priority b/w 1-5 




=============================================== S3 sync command ===========================================

-- The aws S3 sync command uses the CopyObject APIs to copy objects between Amazon S3 buckets. 

-- The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. 

-- The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. 

-- The sync command on a versioned bucket copies only the current version of the object—previous versions aren't copied. 

-- By default, this preserves object metadata, but the access control lists (ACLs) are set to FULL_CONTROL for your AWS account, which removes any additional ACLs. If the operation fails, you can run the sync command again without duplicating previously copied objects.

  
   EG : You can use the command like so:

   aws s3 sync s3://DOC-EXAMPLE-BUCKET-SOURCE s3://DOC-EXAMPLE-BUCKET-TARGET





=================================================== S3 Data Transfer Pricing – Analysis for USA ==================

• S3 ingress: free

• S3 to Internet: $0.09 per GB

• S3 Transfer Acceleration: 

   • Faster  transfer times (50 to 500% better)
   • Additional cost on top of Data Transfer Pricing: +$0.04 to $0.08 per GB

• S3 to CloudFront: $0.00 per GB

• CloudFront to Internet: $0.085 per GB (slightly cheaper than S3)

   • Cachingcapability(lowerlatency)
   • ReducecostsassociatedwithS3Requests Pricing (7x cheaper with CloudFront)

• S3 Cross Region Replication: $0.02 per GB



===========S3 Access Points======================

-- it simplify security management for S3 buckets


-- all the people acccesss through the Acces points

-- instead of writing critical bucket policies , u can create access points to each and give the DNS Names to the users to acces their respective folders in buckets


IMP : A.P can be public(internet) or private(VPC) 




================================S3 practicals=============================


-- when u do Elastic Bean stalk , one buccket is created automatically delete policy first then delete bucket 

-- create bucket , when bucket is private , object is also private 

-- if u create a public bucket ,object is still private ( u can make public if u want)

-- ad some pictures in bucket 

-- objetc lock can be enabled only at the time of creating bucket only , later u can not do it 

-- copy URI is used in the CLI 

-- Pre-signed URL worked for both private and public Buckets 

-- create one public bucket 

-- upload some images and try to acces u cant coz it is still private only 

-- select files --> actions--> make acl public ,now u will gwt image 



============S3 versioning practicals =================


-- enable versioning for bucket 

-- upload same 2 images again

-- once u toggled show versions options t will show u L with image which means the latest 

-- do delete image 1 

-- now select versions and delete the delete marker u will get back u images 

-- this is how u will get back files if u enebled versioning 


-- Bucket Versioning : properties 

-------------IMP : ------------- MFA delete : additional layer of security that requries MFA for changing bucket versioning settings and permaenetly delete object versions . this feature can be enabled CLI only 


-- -- in S3 , it is possible suspend the versioning 

-- once u suspend versioning u cant get back  the files once u dlete 

-- even though u can see delete marker options but u can not get back those files once u delete , when u suspend the versioning 

-- existing objects do not have any impact once u suspend the vesioning 

eg: for image 1 u have enebled version previously and nxt suspend versioning once u dleete image 1 u will get back those image co existing files wont get any effect 


IMP :  for image 3 nad 4 u did not enable versioning 

-- now do enable versioning , now dlete image 4 and go n check enable versions dlete marker u will get back ur image 4 , coz now versioning is enabled 

-- once the enable is enabled u will get back all files , irresective of uploading time with versioning enable time suspended time 





==============Server Access logs======================

-- create seperate buckets for logs 

-- now go to original bucket -> properties--> enabled Server Access logs--> choose log bucket as destination and save changes 



===============================S3 event notifications=============

-- if something is hapen in S3 u will get notifications 

-- S3 event notification will be sent to 3 destinations 

1 lambda function 

2 SNS topic 

3 SQS Queue 


-- select SNS topic 

-- here u will get error like 

  """Unable to validate the following destination configurations""


__-----------------------------Explore topic , solve this 

-- create bucket , go to propeties create new notification choose SNS 

-- open SNS and create topic 

-- edit access policy 

-- Eg

{
 "Version": "2012-10-17",
 "Id": "example-ID",
 "Statement": [
  {
   "Sid": "example-statement-ID",
   "Effect": "Allow",
   "Principal": {
     "Service": "s3.amazonaws.com"
   },
   "Action": [
    "SNS:Publish"
   ],
   "Resource": "ADD-YOUR-ARN-HERE",  ---- in acces policy resource paste here
   "Condition": {
      "ArnLike": { "aws:SourceArn": "arn:aws:s3:::ADD-YOUR-BUCKET-NAME-HERE" }
   }
  }
 ]
}

-- replace above policy and save changes 

-- now go to bucket and create event notificatin with SNS topic 

-- now try to uplod files , check u get notifications or not ? 

-- Getting ok.......................




EPV : A photo-sharing company is storing user profile pictures in an Amazon S3 bucket and an image analysis application is deployed on four Amazon EC2 instances. A solutions architect would like to trigger an image analysis procedure only on one of the four Amazon EC2 instances for each photo uploaded.

ANS : Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue

EXP : Here we have to use Amazon S3 Event Notifications (which can send a message to either AWS Lambda, Amazon SNS, or Amazon SQS) to send a message to the Amazon SQS queue. By using Amazon SQS, we know only one Amazon EC2 instance among the four will pick up a message and process it.




====================S3 EventBridge==========================


-- by default s3 events stroed in the Event Bridge , u have to enabled explicitly 



=========== AWS CloudTrail data events ==============

--if u want object level logs u will enebled this 


========================how to setup static website on S3===============

-- download some html template from google 

-- now we want our html website on our S3 

-- create one public bucket 

-- upload folder of all html files 

-- make all files public , -->actions --> make public ACL 

-- once u upload , then enable static web site hosting ,in  demo bucket 

-- access ur webite through url 



============ACCESS POINTS ================


-- exploring topic 


============================S3 CORS practicals ============================


-- create 2 public buckets 

-- create 2 html files 

1 index.html 

2 load.html 


1  

<html>
<head>
<title>AWS s3 Cors Demo </title>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
</head>
<body>
<h1>AWS S3 CORS DEMO</h1>
<div id="loadDiv"></div>
</body>
</html>
<script type="text/javascript">
$("#loadDiv").load("load.html");
</script>


2

this is load from same bucket 



-- upload 2 htm files and make them as public in bucket 1

-- enable static web hosting 

-- now try to access link u will get o/p 2 lines calling load.html 

-- now create another bucket 2 (public bucket) and upload only load.html 

-- make file public 

-- copy object url of load.html from bucket 1


<html>
<head>
<title>AWS S3 CORS DEMO </title>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.1/jquery.min.js"></script>
</head>
<body>
<h1>AWS S3 CORS DEMO</h1>
<div id="loadDiv"></div>
</body>
</html>
<script type="text/javascript">
$("#loadDiv").load("https://cors-dem2.s3.ap-south-1.amazonaws.com/load.html");
</script>

-- now upload back index.html(updated) to s3 again and make public in bucket 1

-- make them public 

-- now do refresh the link in browser  

-- u can not able to see load.html content in the browser coz u are calling from the other bucket and u did not enable CORS so u are not getting content 

-- now go and create CORS in Bucket2 

-- open 2 bkt --> permissions --> CORS c.o learn more --> copy snippet from document in google 

[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [],
        "AllowedMethods": [
            "GET"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    }
]


-- now do refrsh the link u will get content of load.html 


-------------------- Life Cycle Configuration

-- create lifecycle rules acording to your requriment 

-- u can disable rules any time 




-------------------------S3 inventory Configuration

-- create I.C 

-- once u create , it will create one report 



------------NOte 

-- s3 bowser download and do ur s3 actions withot go to console every time 




============================================  AWS Storage Extras

--------------------- AWS Snow Family

-- Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS

-- Data migration:
1 Snowcone
2 Snowball Edge
3 Snowmobile

-- Edge computing:
1 Snowcone
2 Snowball Edge

---------------- Data Migrations with AWS Snow Family

                  Time to Transfer
            100 Mbps     1Gbps      10Gbps
10 TB       12 days      30 hours    3 hours
100 TB      124 days     12 days     30 hours
1PB         3 years      124 days    12 days

-- Challenges:
• Limited connectivity
• Limited bandwidth
• High network cost
• Shared bandwidth (can’t maximize the line)
• Connection stability

-- AWS Snow Family: offline devices to perform data migrations

-- If it takes more than a week to transfer over the network, use Snowball devices!

            1 Snowball Edge (for data transfers)

-- The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. 

-- Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. 

-- AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.

-- Physical data transport solution : moveTBs or PBs of data in or out of AWS

-- Alternative to moving data over the network (and paying network fees)

-- Pay per data transfer job

-- Provide block storage and Amazon S3-compatible object storage

-- Snowball Edge Storage Optimized
   • 80 TB of HDD capacity for block volume and S3 compatible object storage

Migration Size : Up to petabytes, offline

-- Snowball Edge Compute Optimized
   • 42 TB of HDD or 28TB NVMe capacity for block volume and S3 compatible object storage

-- Usecases:large data cloud migrations,DC decommission,disaster recovery

-- no pre installation of data sync


             2 AWS Snowcone & Snowcone SSD

-- Small, portable computing, anywhere, rugged & secure, withstands harsh environments

-- Light (4.5 pounds, 2.1 kg)

-- Device used for edge computing, storage, and data transfer

-- Snowcone – 8 TB of HDD Storage

-- Snowcone SSD – 14 TB of SSD Storage

-- Use Snowcone where Snowball does not fit (space- constrained environment)

-- Must provide your own battery / cables

-- Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data

-- Migration Size : Up to 24 TB, online and offline


              3 AWS Snowmobile

-- Transfer exabytes of data (1 EB = 1,000 PB = 1,000,000 TBs)

-- Each Snowmobile has 100 PB of capacity (use multiple in parallel)

-- High security: temperature controlled, GPS, 24/7 video surveillance

-- Better than Snowball if you transfer more than 10 PB

-- Migration Size : Up to exabytes, offline

-- no pre installation of data sync

-- AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. 

-- Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. 

-- Transferring data with Snowmobile is more secure, fast, and cost-effective. AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location.

-- For datasets less than 10PB or distributed in multiple locations, you should use Snowball.



------------------------------------ Snow Family – Usage Process

1. Request Snowball devices from the AWS console for delivery
2. Install the snowball client / AWS OpsHub on your servers
3. Connect the snowball to your servers and copy files using the client
4. Ship back the device when you’re done (goes to the right AWS facility)
5. Data will be loaded into an S3 bucket
6. Snowball is completely wiped


---------------------------------- What is Edge Computing?

-- Process data while it’s being created on an edge location
• A truck on the road,a ship on the sea,a mining station underground...

-- These locations may have 
• Limited / no internet access
• Limited / no easy access to computing power

--- We setup a Snowball Edge / Snowcone device to do edge computing

-- Use cases of Edge Computing:
• Preprocess data
• Machine learning at the edge 
• Transcoding media streams

-- Eventually (if need be) we can ship back the device to AWS (for transferring data for example)


-------------------------------- Snow Family – Edge Computing

-------- Snowcone & Snowcone SSD (smaller)

• 2 CPUs, 4 GB of memory, wired or wireless access 
• USB-C power using a cord or the optional battery

------- Snowball Edge – Compute Optimized

• 104 vCPUs, 416 GiB of RAM
• Optional GPU (useful for video processing or machine learning) 
• 28 TB NVMe or 42TB HDD usable storage
• Storage Clustering available (up to 16 nodes)

------- Snowball Edge – Storage Optimized

• Up to 40 vCPUs,80 GiB of RAM,80TB storage

--  All: Can run EC2 Instances & AWS Lambda functions (using AWS IoT Greengrass)

-- Long-term deployment options: 1 and 3 years discounted pricing



--------------------------------- AWS OpsHub

-- Historically, to use Snow Family devices, you needed a CLI (Command Line Interface tool)

-- Today, you can use AWS OpsHub (a software you install on your computer / laptop) to manage your Snow Family Device

• Unlocking and configuring single or clustered devices
• Transferring files
• Launching and managing instances running on Snow Family Devices
• Monitor device metrics (storage capacity, active instances on your device)
• Launch compatible AWS services on your devices (ex: Amazon EC2 instances, AWS DataSync, Network File System (NFS))



VERY VERY IMP ------------------------------- Solution Architecture: Snowball into glacier

-- Snowball cannot import to Glacier directly

-- You must use Amazon S3 first, in combination with an S3 lifecycle policy

snowball(import) --> AMZN S3 --> (S3 lifecycle policy) --> Amazon Glacier


=================================EFS Practicals ================================


-- create EFS 

-- throughput : speed b/w ec2 and EFS 

-- EFS is regional 

-- Replicatio is posssible in EFS , cost calulated how much data is transferred 

-- launch 1 ec2 instances 

-- in S.G add NFS inbound rule 

-- connect instance 1 and follow some commands 

1  sudo -s 

2  yum install -y nfs-utils

3   mkdir efs  --- create one folder 

4   now do mount with folder 


mount -t nfs4 fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com:/ efs/


explanation :   fs-0c90179e0f0b6c46b.efs.ap-south-1.amazonaws.com  -- u wil get from efs dns name    and efs is folder name 

-- cd efs 

-- now create some files here 


---- now create 2nd instance in diff A.Z 

-- follow the same steps , here u can give mkdir names anything u want 

-- once u do ls u will get same filesin diff A.Z zone instances also 

-- create one file new in ec2 1 and check in 2nd ec2 u will get that file 

-- same for deletion also 

-- this is how u do with Efs amd u can also do repliction EFS 



=====================================CLI Practicals=========================

-- wherever u want to access AWS , u have to istall AWS CLI 

-- for windows u need to downlaod --> AWSCLI.msi

-- for linux u have to use Linux cmnds 

-- configuring KEYS o the insatnce is not recommmended 

-- so launch another ec2 launch aws cli and create Role and give permission 

-- how to acces key? --- Aws configure 

-- o//p : 3 ways table or json or xml 

-- keys are stored in cd~/.aws 

------------praticals

-- launch instance and SSH in S.G 

-- connect instance , follow cmnds 


-- sudo -s 

--curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
-- unzip awscliv2.zip
-- sudo ./aws/install



-- now configure keys

-- aws configure 

-- give keys and region and select table format as o/p 

-- if u want to know where keys stored do 

cd ~/.aws

ls

cat crendentials 

-- u will get ur keys 

-- this is not good to store 

-- try some sample cmnds 

Eg : 

aws ec2 run-instances --image-id ami-0a0f1259dd1c90938 --count 1 --instance-type t2.micro --key-name terraform-key --security-group-ids sg-09756757ec04083da --subnet-id subnet-02c844dbf0a247651


-- as u give op as table mode u will get o/p in table formate 

-- ec2 is created 

-- like this u can do start stop and so many things u want 

-- NOTE : if u want to get data from the resource 

-- type TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` \
&& curl -H "X-aws-ec2-metadata-token: $TOKEN" -v http://169.254.169.254/latest/meta-data/


u will get these many options 

ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
events/
hostname
identity-credentials/
instance-action
instance-id
instance-life-cycle
instance-type
local-hostname
local-ipv4
mac
managed-ssh-keys/
metrics/
network/
placement/
profile
public-hostname
public-ipv4
public-keys/
reservation-id
security-groups
services/


-- if u want to get any one of these data u can add last of cmnd for eg local-ipv4

-- TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` \
&& curl -H "X-aws-ec2-metadata-token: $TOKEN" -v http://169.254.169.254/latest/meta-data/local-ipv4


-- now create s3 through console 

-- aws s3 mb s3://6pm-test-demo/ --region ap-south-1

-- check in console u can able to see buckets 

-- aws s3 rb s3://6pm-test-demo/ --force  == to remove bucket 

-- it is not recommended to do this way coz keys are stored in ec2 itself so u can not do this way 

-- so create role and give permissions(eg : s3 full access) and run ur cmnds without keys 




========================================= MFA with CLI

• To use MFA with the CLI, you must create a temporary session

• To do so, you must run the STS GetSessionToken API call

• aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600



========================================= AWS SDK Overview

• What if you want to perform actions on AWS directly from your applications code ? (without using the CLI).

• You can use an SDK (software development kit) !

• Official SDKs are...

     • Java
     • .NET
     • Node.js
     • PHP
     • Python (named boto3 / botocore)
     • Go
     • Ruby
     • C++


• We have to use the AWS SDK when coding against AWS Services such as DynamoDB

• Fun fact... the AWS CLI uses the Python SDK (boto3)

• The exam expects you to know when you should use an SDK

• Good to know: if you don’t specify or configure a default region, then us-east-1 will be chosen by default




============================================= AWS Limits (Quotas)

• API Rate Limits

     • DescribeInstances API for EC2 has a limit of 100 calls per seconds

     • GetObject on S3 has a limit of 5500 GET per second per prefix

     • For Intermittent Errors: implement Exponential Backoff

     • For Consistent Errors: request an API throttling limit increase


• Service Quotas (Service Limits)

     • Running On-Demand Standard Instances: 1152 vCPU

     • You can request a service limit increase by opening a ticket

     • You can request a service quota increase by using the Service Quotas API


------------------ Exponential Backoff (any AWS service)


• If you get ThrottlingException intermittently, use exponential backoff

• Retry mechanism already included in AWS SDK API calls

• Must implement yourself if using the AWS API as-is or in specific cases

    • Must only implement the retries on 5xx server errors and throttling
    • Do not implement on the 4xx client errors






======================================================= Amazon FSx – Overview ===========================================================


• Launch 3rd party high-performance file systems on AWS

• Fully managed service


1 FSx for Lustre

2 FSx for Windows File Server

3 FSx for NetApp ONTAP

4 FSx for OpenZFS





---------------- 1 Amazon FSx for Lustre

• Lustre is a type of parallel distributed file system, for large-scale computing

• The name Lustre is derived from “Linux” and “cluster

• Machine Learning, High Performance Computing (HPC)

• Video Processing, Financial Modeling, Electronic Design Automation

• Scales up to 100s GB/s, millions of IOPS, sub-ms latencies

• Storage Options:
     • SSD – low-latency, IOPS intensive workloads, small & random file operations
     • HDD – throughput-intensive workloads, large & sequential file operations


• Seamless integration with S3 
     • Can “read S3” as a file system (through FSx)
     • Can write the output of the computations back to S3 (through FSx)


• Can be used from on-premises servers (VPN or Direct Connect)



----- FSx Lustre - File System Deployment Options


1 • Scratch File System
     • Temporary storage
     • Data is not replicated (doesn’t persist if file server fails)
     • High burst (6x faster, 200MBps per TiB)
     • Usage: short-term processing, optimize costs


2 • Persistent File System

     • Long-term storage
     • Data is replicated within same AZ
     • Replace failed files within minutes
     • Usage: long-term processing, sensitive data





------------------- 2 Amazon FSx for Windows (File Server)

• FSx for Windows is a fully managed Windows file system share drive

• Suppor ts SMB protocol & Windows NTFS

• Microsoft ActiveDirectory integration, ACLs, userquotas

• Can be mounted on Linux EC2 instances

• Supports Microsoft's Distributed File System (DFS) Namespaces (group files across multiple FS)

• Scale up to 10s of GB/s, millions of IOPS, 100s PB of data

• Storage Options:
    • SSD – latency sensitive workloads (databases, media processing, data analytics, ...)
    • HDD – broad spectrum of workloads (home directory, CMS, ...)

• Can be accessed from your on-premises infrastructure (VPN or Direct Connect)

• Can be configured to be Multi-AZ (high availability)

• Data is backed-up daily to S3



--------------------- 3 Amazon FSx for NetApp ONTAP

• Managed NetApp ONTAP on AWS

• File System compatible with NFS, SMB, iSCSI protocol

• Move workloads running on ONTAP or NAS to AWS

• Works with:
      • Linux
      • Windows
      • MacOS
      • VMware Cloud on AWS
      • Amazon Workspaces & AppStream 2.0
      • Amazon EC2, ECS and EKS


• Storage shrinks or grows automatically

• Snapshots,replication,low-cost,compression and data de-duplication

• Point-in-time instantaneous cloning (helpful for testing new workloads)


----- Scheduled replication using NetApp SnapMirror

-- You can use NetApp SnapMirror to schedule periodic replication of your FSx for ONTAP file system to or from a second file system. This capability is available for both in-Region and cross-Region deployments.

-- NetApp SnapMirror replicates data at high speeds, so you get high data availability and fast data replication across ONTAP systems, whether you're replicating between two Amazon FSx file systems in AWS, or from on-premises to AWS.

-- Replication can be scheduled as frequently as every 5 minutes, although intervals should be carefully chosen based on RPOs (Recovery Point Objectives), RTOs (Recovery Time Objectives), and performance considerations.

-- There are two types of SnapMirror replication: Volume-level SnapMirror and SVM Disaster Recovery (SVMDR). Only volume-level SnapMirror replication is supported by FSx for ONTAP.




----------------------- 4 Amazon FSx for OpenZFS

• Managed OpenZFS file system on AWS

• File System compatible with NFS (v3, v4, v4.1, v4.2)

• Move workloads running on ZFS to AWS

• Works with:
      • Linux
      • Windows
      • MacOS
      • VMwareCloudonAWS
      • AmazonWorkspaces&AppStream2.0 
      • AmazonEC2, ECS and EKS


• Up to 1,000,000 IOPS with < 0.5ms latency

• Snapshots, compression and low-cost

• Point-in-time instantaneous cloning (helpful for testing new workloads)







================================AWS Transfer Family=========================

-- Fully managed Service for file transer securely 

-- A fully-managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol

-- Supported Protocols
• AWS Transfer for FTP (File Transfer Protocol (FTP))
• AWS Transfer for FTPS (File Transfer Protocol over SSL (FTPS)) 
• AWS Transfer for SFTP (Secure File Transfer Protocol (SFTP))

-- Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)

-- Pay per provisioned endpoint per hour + data transfers in GB

-- Usage: sharing files, public datasets, CRM, ERP, ...

-- data can be transferred in and out of s3 buckets and EFS 

-- generally we are uploaing our files in S3 through the console  but we are doing through the http , 

-- now w r going to transfer files securely from ur laptop 

-- for this we have to install software called "FileZilla" which is used to transfer fis very securley 

-- b/w ur laptop and s3 or EFS we have to create One "Transfer Server" , this server allows us to do  SFTP,FTP,FTPS, we can choose any one , and we hav to create one user in Transfer server 

-- in FileZilla u can login as with user name 

-- once u create server it will give u endpoint 

-------------------------- practical

-- create public bucket 

-- create one bucket policy through policy generator 

eg :   

{
  "Id": "Policy1704630072890",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1704629976563",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket",
        "s3:PutObject"
      ],
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::transfer-dem",
                       "arn:aws:s3:::transfer-dem/*" ],
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "49.205.119.80/32"
        }
      },
      "Principal": "*"
    }
  ]
}

-- transfer-dem = name 

-- now create transfer server 

-- open aws transfer family in console 

-- create server , selete SFTP --> Srvice managed --> select as per ur requriemnets 

--now add user in server 

-- select server c.o add user --> create one role in IAM --> truste entity = ec2 --> use case = Transfer (imp) --> give s3 full access --> create role 

-- go to puttyGen n create SSH keys(mouseover)  and save private key with .ppk 

-- now download Filezilla 

-- open filezilla --File(top) --> site manager --> new site --> copy end point of server --> selet SFTP n paste endpoint of server and give port number as 22--> logon type : key file --> give ur private file u downoaded -- connect 


-- once u coonect u jst drag do upload ur files u want and go check in s3 in console --- getting files 

-- this is how u can transfer files throug SFTP from ur lap to s3 or EFS 



======================================== AWS DataSync ==================

-- Move large amount of data to and from
• On-premises / other cloud to AWS (NFS, SMB, HDFS, S3 API...) – needs agent
• AWS to AWS (different storage services) – no agent needed

-- You can use AWS DataSync to migrate data located on-premises, at the edge, or in other clouds to Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon FSx for NetApp ONTAP.


-- Can synchronize to:
• Amazon S3 (any storage classes – including Glacier)
• Amazon EFS
• Amazon FSx (Windows, Lustre, NetApp, OpenZFS...)

-- Replication tasks can be scheduled hourly, daily, weekly

-- File permissions and metadata are preserved (NFS POSIX, SMB...)

-- One agent task can use 10 Gbps, can setup a bandwidth limit

-- AWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the AWS DataSync API and Console, and Amazon CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. 

-- AWS DataSync performs data integrity verification both during the transfer and at the end of the transfer.





===========================AWS - Storage Gateway Service==========================

-- Bridge between on-premises data and cloud data

-- it combination of on-premises and AWS Storage , so it is called " Hybrid Storage"

-- it is used to transfer the data from on-premises to AWS vice versa (S3, EBS,Glacier and FSx) 

-- ur lap is also called on-premises 

-- Use cases:
• disaster recovery
• backup & restore
• tiered storage
• on-premises cache & low-latency files access

-- Types of Storage Gateway:
• S3 File Gateway
• FSx File Gateway
• Volume Gateway
• Tape Gateway


-- it is not possible to mount storage service(s3,efs,glacier,FSx) to ur on-premises but by use of storage gateway u can do this 

-- there are 4 types of Storage gateways 


1 if u want to use "s3" create "file gateway"

imp : File Gateway supports "NFS" for LINUX and for WINDOWS it use "SMB"(server message Block)

-- Most recently used data is cached in the file gateway
-- Supports S3 Standard,S3 StandardIA, S3 OneZoneA, S3 IntelligentTiering
-- Transition to S3 Glacier using a Lifecycle Policy
-- Bucket access using IAM roles for each File Gateway
-- SMB Protocol has integration with Active Directory (AD) for user authentication


Application Server <-------(NFS or SMB)---->s3 filegateway <------(HTTPS)------> (AWS cloud )s3 storage class ----> (lifecycle policy) --> S3 glacier



2 if u want to create "EBS" create "volume gateway" 

-- Block storage using iSCSI protocol backed by S3

-- Backed by EBS snapshots which can help restore on-premises volumes!

Imp : volume gateway we ahve 2 types 

1 cached volume : to increase the performance ,it uses protocal "iSCSi" , low latency access to most recent data
you store volume data in AWS, with a small portion of recently accessed data in the cache on-premises. ...

2 Stored Volume : you store the entire set of volume data on-premises and store periodic point-in-time backups (snapshots) in AWS. 
entire dataset is on premise, scheduled backups to S3


3 if u want to create "Glacier" create "tape gateway"

-- no need to maintain physical tape 

-- AWS provide VTL(virtual tape library) to take backups , it uses protocal "iSCSi"

-- Some companies have backup processes using physical tapes (!)

-- With Tape Gateway, companies use the same processes but, in the cloud

-- VirtualTape Library (VTL) backed by Amazon S3 and Glacier

-- Back up data using existing tape-based processes (and iSCSI interface)

-- Works with leading backup software vendors



4 if u want to create "FSx" create "FSx gateway" 

-- Native access to Amazon FSx for Windows File Server

-- Local cache for frequently accessed data

-- Windows native compatibility (SMB, NTFS, Active Directory...)

-- Useful for group file shares and home directories


--------------------prac

-- we do not have on-premises ,so consider ur lap as on-premises and VM as Ec2 and install Storage gateway appliance(agent) on on-premises

-- this is chargable 

-- go to soage gateway and create s3 file gateway 

-- Platform options = ec2 

-- customize ur settings 

-- c.o launch instance , automatically AMI is take here 

-- select Instance type is = m5.xlarge

-- add additional volume of 150 GiB of gp2 

-- c.o checkbox and nxt 

-- copy IP of instance and give in nxt step as a connection 

-- now on-premises and file gate way now connected 

-- activate and configure 

-- now ceate s3 private bucket 

-- now file share we have to create , if u open gateway u will have options like what to creat now we have files so create file share 

-- all the data come to the file share and this data stored in AWS S3 

-- c.o customize configuration , in step 3 u can give accesss client to all (0.0.0.0/0)

-- open file share u will find "mount point"

-- launch t2.micro instance for testing 

-- connect nstance 

-- sudo -s 

-- yum install -y nfs-utils

-- mkdir filesystem

-- go to file share and copy mount point for linux 

-- mount -t nfs -o nolock,hard 172.31.1.243:/store-gate [MountPath]/

-- cd foledername

-- now indirectly iam in S3 only if i put any data and stored this data in S3 

-- do df -h n see s3 bucket is there so u r doing mount 

-- create some files 

-- check in S3 ur file will store in S3 automatically 

-- the file u created all go through Storage gateway appliance(agent) --> storefile gateway--> S3 


-- delete all resource 

-- delete additional volume also it has 150 GIB 



==================================================== Storage Comparison =========================

• S3: Object Storage
• S3Glacier:Object Archival
• EBS volumes: Network storage for one EC2 instance at a time
• Instance Storage: Physical storage for your EC2 instance (high IOPS)
• EFS: Network File System for Linux instances, POSIX filesystem
• FSx for Windows: Network File System for Windows servers
• FSx for Lustre: High Performance Computing Linux file system
• FSx for NetApp ONTAP: High OS Compatibility
• FSx for OpenZFS: Managed ZFS file system
• Storage Gateway: S3 & FSx File Gateway,Volume Gateway (cache & stored),Tape Gateway • Transfer Family: FTP, FTPS, SFTP interface on top of Amazon S3 or Amazon EFS
• DataSync:Schedule datasync from on-premises to AWS,or AWS to AWS
• Snowcone / Snowball / Snowmobile: to move large amount of data to the cloud, physically • Database: for specific workloads, usually with indexing and querying



======================================== diff b/w storage gateway and data sync ============================

1 Description :

DataSync : AWS DataSync is an online data transfer service that simplifies, automates, and accelerates the process of copying large amounts of data to and from AWS storage services over the Internet or over AWS Direct Connect.

Storage Gateway : AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage by linking it to S3. Storage Gateway provides 3 types of storage interfaces for your on-premises applications: file, volume, and tape


2  How it Work ?

DataSync : Uses an agent which is a virtual machine (VM) that is owned by the user and is used to read or write data from your storage systems. You can activate the agent from the Management Console. The agent will then read from a source location, and sync your data to Amazon S3, Amazon EFS, or Amazon Fsx for Windows File Server.

Storage Gateway : Uses a Storage Gateway Appliance – a VM from Amazon – which is installed and hosted on your data center. After the setup, you can use the AWS console to provision your storage options: File Gateway, Cached Volumes, or Stored Volumes, in which data will be saved to Amazon S3.
- You can also purchase the hardware appliance to facilitate the transfer instead of installing the VM

3  Protocols 

DataSync :  AWS DataSync can copy data between Network File Systems (NFS), SMB file servers or self-managed object storages. It can also move data between your on-premises storage and AWS Snowcone, Amazon S3, Amazon EFS, or Amazon FSx,

Storage Gateway : File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as NFS and SMB.
- Volume Gateway stores your data locally in the gateway and syncs them to Amazon S3. It also allows you to take point-in-time copies of your volumes with EBS snapshots which you can restore and mount to your appliance as iSCSI device. 
- Tape Gateway data is immediately stored in Amazon S3 and can be archived to Amazon S3 Glacier or Amazon S3 Glacier Deep Archive.

4  Pricing 

DataSync : You are charged standard request, storage, and data transfer rates to read from and write to AWS services, such as Amazon S3, Amazon EFS, AmazonFSx for Windows File Server, and AWS KMS.

Storage Gateway :  You are charged based on the type and amount of storage you use, the requests you make, and the amount of data transferred out of AWS.


combination : You can use a combination of DataSync and File Gateway to minimize your on-premises’ operational costs while seamlessly connecting on-premises applications to your cloud storage. AWS DataSync enables you to automate and accelerate online data transfers to AWS storage services. File Gateway then provides your on-premises applications with low latency access to the migrated data.


====================================================================RDS==============================

-- RDS is Regional 

-- in RDS we called RDS DB instance 

-- it has Endpoint 

-- It is "PAAS", u can not login but u can connect 

-- it hs 2 topics mainly 

1 Read Replicas 

2 Multi AZ


-- AWS RDS has 7 Engines 

-- generally , how u connect to data base?

we need Hostname /endpont 

- usrname 

- passwd

- port number 

-- the db engine(whole database) we can multiple db's 

-- now , there are some aplications only for the read-base purpose , so many appn are connected to the engine it will get overload 

-- so do create seperate Read Replica , it will use for only Reading -purpose 

-- so all read applins are getting conected to this read -replicas and fetch the data from the replica 

-- we have main server( Master), generaly it has a role , if it does not have read replicas then it is only "instance" , if u create Read replicas from the master then it will become "primary"

-- read replicas are connected through Endpoints 

-- Read replicas can be in multiple Regions 

-- all the wrights will be done in Master server only 

-- when some one writing in the main server ,paralley it does not update in the read replicas so it is called " Asynchrous"

-- RR is used for Increase the performance 

-- but it is not for High availability 

-- Max 5 Read Replicas 

-- ReadReplicas have their own endpoints 

-- endponts are provided by the AWS 

-- u can promote RR to Nrml stand alone DB machines, if u create RR from this it will become Primary 

- u can enable M-AZ for RR also . charges are double 


- RDS does not support for io2 , supports for gp2,gp3, io1


------------ Advantages over using RDS VS deploy database on ec2 

-- RDS is managed Service
-- Automated provisioning , Os patching 
-- continuously backups and restore to specific timestamp (Point in Time Restore)!
-- monitoring dashboards
-- read replicas for improving read performance 
-- Multi AZ setup for disaster recovery
-- storage backed by EBS(gp2 or io1)

-- BUT you can’t SSH into your instances



------------------- RDS - storage Auto Scaling 

-- helps u to increase storage on ur RDS DB instance dynamically
-- when RDS detects u are running out of free database storage , it scales automatically 

IMP : u have to set Maximum storage threshold 

-- automatically modify if storage is 

A : free storage is < 10% of allocated storage
B : low storage atleast before 5 min 
C : 6 hrs have passed since last modification 

-- Useful for applications with unpredictable workloads

-- Supports all RDS database engines



------------------------------------------------ RDS Read Replicas

-- Up to 15 Read Replicas
-- Replication is ASYNC, so reads are eventually consistent
-- Replicas can be promoted to their own DB
-- Applications must update the connection string leverage ead replicas 

----- RDS Read Replicas – Use Cases

--  You have a production database that is taking on normal load
--  You want to run a reporting application to run some analytics
--  You create a Read Replica to run the new workload there
--  The production application is unaffected
--  IMP : Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE)

------ RDS Read Replicas – Network Cost

-- In AWS there’s a network cost when data goes from one AZ to another

-- For RDS Read Replicas within the same region, you don’t pay that fee


-----------------------------------------------------MUlTI-AZ / Cluster (RDS Multi AZ (Disaster Recovery))

-- Mutli AZ is used for the High availability 

-- if u enable M-AZ , one more DB instance created , we can not see that instance , it is handled by the AWS 

-- it is "SYNCHRONOUS" 

-- Increase availability

-- Failover in case of loss of AZ, loss of network, instance or storage failure

-- No manual intervention in apps

-- Not used for scaling

IMP NOTE : The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)

-- u need to pay Double amount when u have enabled Multi-AZ 

-- if something happens to ur main DB server , the failover happens here , the requests will send to backend server 

-- endpoint won't change even in failover time , One DNS name – automatic app failover to standby

-- DB operations will not have failover(for eg :some one delete table it wont get failover , tese are responsible by us )  , anything realted to network, servers etc will have fail over 

-- RDS DB instances can be reserved 

-- Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby

-- Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason




EPV : A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down.

ANS : The CNAME record will be updated to point to the standby database

EXP : 

-- In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. 

-- The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.

-- Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.

-- Failover is automatically handled by Amazon RDS so that you can resume database operations as quickly as possible without administrative intervention. 

-- When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary. 

-- Multi-AZ means the URL is the same, the failover is automated, and the CNAME will automatically be updated to point to the standby database.




-------------------------------------------------------------- Amazon RDS maintenance 

-- Occasionally, AWS performs maintenance to the hardware, operating system (OS), or database engine version for a DB instance or cluster.

 1 Hardware maintenance :

  -- Before Amazon RDS schedules maintenance, you receive an email notification about the scheduled maintenance windows. This includes the time of the maintenance and the Availability Zones that are affected. 

  -- During hardware maintenance, Single-AZ deployments are unavailable for a few minutes. 

  -- For Multi-AZ deployments with an affected Availability Zone, your deployment is unavailable for the time it takes the instance to fail over, usually about 60 seconds. 

  -- If maintenance affects only the secondary Availability Zone, then there's no failover or downtime.


2 OS maintenance

  -- To postpone scheduled OS maintenance, adjust your preferred maintenance window. Or, you can choose Defer upgrade from the Actions dropdown menu in the Amazon RDS console. 

  -- To minimize downtime, modify the Amazon RDS DB instance to a Multi-AZ deployment. For Multi-AZ deployments, OS maintenance applies to the secondary instance. 

  -- The instance fails over, and then the primary instance updates. The downtime is during failover. 
  
  Note: If you upgrade to a Multi-AZ deployment, then you incur higher costs. To determine your costs, use the AWS Pricing Calculator.


3  DB engine maintenance

  -- When you upgrade your DB instance's database engine in a Multi-AZ deployment, maintenance occurs on the primary and replica instance at the same time. 

  -- This is also true for non-Amazon Aurora instances in Multi-AZ deployments. Throughout the maintenance window, both the primary and secondary DB instances in the Multi-AZ deployment are unavailable. 

  -- This operation causes downtime until the upgrade is complete. The duration of the downtime varies based on the size of your DB instance.

    NOTE : If the instance is a read replica, then the database engine version upgrade occurs independently from the source instance. By default, maintenance occurs first on the primary instance and then on the replica. 

  
  -- Upgrades to the database engine level require downtime. Even if your RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances upgrade at the same time.

  -- This causes downtime until the upgrade completes, and the duration of the downtime varies based on the size of your DB instance. For more information, 

    NOTE : If you upgrade a SQL Server DB instance in a Multi-AZ deployment, then both the primary and standby instances are upgraded.

          - Amazon RDS performs rolling upgrades, so the outage is only for the duration of a failover.





--------------------------------------RDS – From Single-AZ to Multi-AZ

-- Zero downtime operation (no need to stop the DB)

-- Just click on “modify” for the database

-- The following happens internally:
- A snapshot is taken
- A new DB is restored from the snapshot in a new AZ
- Synchronization is established between the two databases

workflow = Master Db instance --> snapshot is taken --> restore from the snapshot and create new db in new AZ --> standby Db will created , both db instances are in synchronous Replication



--------------------------------------- RDS Custom

• Managed Oracle and Microsoft SQL Server Database with OS and database customization

• RDS: Automates setup, operation, and scaling of database in AWS

• Custom: access to the underlying database and OS so you can
   • Configure settings
   • Install patches(update or repair it)
   • Enable native features
   • Access the underlying EC2 Instance using SSH or SSM Session Manager

• De-activate Automation Mode to perform your customization, better to take a DB snapshot before

                   • RDS vs. RDS Custom
                       • RDS:         entire database and the OS to be managed by AWS
                       • RDS Custom:  full admin access to the underlying OS and the database

            



--------------------------------------RDS Features 

-- backups are called Snapshots 

-- if u want to take backup of whole Engine it is called "Snapshot" 

-- Snapshots  will takek 2 types 

1 manully 

2 Automatic(schedule)

-- if u want to take only one data base inside engine u have to do manualy write scripts and AWS do not have service for the DB level Operations backups 

-- DB level Operations (tables,SP,DB-level backups, Scripts, Fns,Security etc) are handled by Customer 

-- dB instance Level (instance Backup(Snashots), configurations, restorations , capacity etc)

-- Operations is handled by AWS/Platform is handled by AWS 

-- u can also do Encryption

-- we have storage/ volumes --> gp2,io1

- instance type --> Db insatcne type --> db.t2.micro 

-- SG/VPC/subnet

-- performace insights (Dashboard)

-- backup retention period = Max 35days , default 7 days ( Automatic) 

-- No retention priod for Manual Backups, u need to delete it manually 

-- Snapshots can be exported to S3/ restore it from S3 Also 

-- Snapshots can be copied from from one region to another region 

-- shared from one accnt to another accnt 

-- U can do Scale up DB instance but not on FLY mode ( we need to stop the DB instance, Downtime is Requried) 

-- we have ASG on "storage Level"  min 100GB n max 1000GB


-- AWS proporetary Engine is "AURORA" , own product 

-- is is compatible with MySQL and POSTGRES

-- It is serverless/server base based 

-- it is high chargable 




--------------------------------------------- RDS Event Notifications 


• Notifications that tells information about the DB instance itself (created, stopped, start, ...)

• You don’t have any information about the data itself

• Subscribe to the following event categories: DB instance, DB snapshot, DB Parameter Group, DB Security Group, RDS Proxy, Custom Engine Version

• Near real-time events (up to 5 minutes)

• Send notifications to SNS or subscribe to events using EventBridge RDS DB Instance




----------------------------------------------------------------- Amazon Aurora

-- Aurora is a proprietary technology from AWS (not open sourced)

-- Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database)

-- Aurora is “AWS cloud optimized” and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS

-- Aurora storage automatically grows in increments of 10GB, up to 128 TB.

-- Aurora can have up to 15 replicas and the replication process is faster than MySQL (sub 10 ms replica lag)

-- Failover in Aurora is instantaneous (happens immediately, without any delay). It’s HA (High Availability) native.

-- Aurora costs more than RDS (20% more) – but is more efficient

-- There are no standby instances in Aurora. Aurora performs an automatic failover to a read replica when a problem is detected.


-- Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target

   - If the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails over to a new primary instance in one of two ways:
     
      1 By promoting an existing Aurora Replica to the new primary instance

      2 By creating a new primary instance




------------------------------ Aurora High Availability and Read Scaling

----- 6 copies of your data across 3 AZ:
- 4 copies out of 6 needed for writes
- 3 copies out of 6 need for reads
- Self healing with peer-to-peer replication
- Storage is striped across 100s of volumes

-- One Aurora Instance takes writes (master)

-- Automated failover for master in less than 30 seconds

-- Master + up to 15 Aurora Read Replicas serve reads

-- Support for Cross Region Replication

------------------------  difference b/w Aurora read replicas and MySQL Replicas 

1 Aurora read replicas :

Features 

- Number of replicas                         =  Up to 15
- Replication type                           =  Asynchronous (milliseconds)
- Performance impact on primary              =  Low
- Replica location                           =  In-region
- Act as failover target                     =  Yes (no data loss)
- Automated failover                         =  Yes
- Support for user-defined replication delay =  NO
- Support for different data or schema vs. primary = No 


- When you create a second, third, and so on DB instance in an Aurora-provisioned DB cluster, Aurora automatically sets up replication from the writer DB instance to all the other DB instances. 

- These other DB instances are read-only and are known as Aurora Replicas.

Aurora Replicas have two main purposes. 

    - You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. 
    - Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer.



2 MySQL Read Replicas

Features 

- Number of replicas                         =  Up to 5
- Replication type                           =  Asynchronous (seconds)
- Performance impact on primary              =  High
- Replica location                           =  cross-region
- Act as failover target                     =  Yes (potentially minutes of data loss)
- Automated failover                         =  No
- Support for user-defined replication delay =  NO
- Support for different data or schema vs. primary = Yes


-------------------- Aurora DB Cluster

-- client has WEP and REP 

-- we have Writer endPoint (WEP), which is used  when ever the master fails ,still the client able to communicate and it is automatically redirected to right instance 

-- we have Reader endPoint (REP), it is difficult to connect which replica to the client , to avoid this confusion we have REP , it hs same features of WRP , it connects with load balancer and it connects automatically to all read replicas

NOTE : Load balancing happens at the connections level , not the statement level

-------------------  Features of Aurora

- Automatic fail-over
- Backup and Recovery
- Isolation and security
- Industry compliance
- Push-button scaling
- Automated Patching with Zero Downtime
- Advanced Monitoring
- Routine Maintenance
- Backtrack: restore data at any point of time without using backups


------------------- Aurora Replicas - Auto Scaling

-- whenever too many request for the read replicas , it has more CPU usage inuse , so here the automatic scaling will  happens in aurora and add more read replicas this will bring down CPU usage , this is replcia auto scaling 


--------------------------------------------Aurora Serverless

-- Automated database instantiation and auto-scaling based on actual usage

-- Good for infrequent, intermittent or unpredictable workloads

-- No capacity planning needed

-- Pay per second, can be more cost-effective

workflow = Client --> proxy Fleet(managed by Aurora) --> AMZN aurora ...  AMZN aurora .... 


--------------------------------------------Global Aurora

----- Aurora Cross Region Read Replicas:
- Useful for disaster recovery
- Simple to put in place

------ Aurora Global Database (recommended):
- 1 Primary Region (read / write)
- Up to 5 secondary (read-only) regions, replication lag is less than 1 second
- Up to 16 Read Replicas per secondary region
- Helps for decreasing latency
- Promoting another region (for disaster recovery) has an RTO(Recovery Time Objective) of < 1 minute
- Typical cross-region replication takes less than 1 second


---------- EPV :

-- Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).

-- For Amazon Aurora, each Read Replica is associated with a priority tier (0-15). In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). 

-- If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. 

--  If two or more Aurora Replicas share the same priority and size, then Amazon Aurora promotes an arbitrary replica(chosen at random instead of following a consistent rule) in the same promotion tier.


----------- Diff b/w Multi -AZdeploayment  , Multi-region deployment, Read replicas 



       Multi -AZ deploayment                                        Multi-region deployment                                                  Read replicas


1 Main purpose s highly availability                          Main purpose is disaster recovery and local performance                   Main purpose is Scalabilty


2 NON-Aurora : synchronous replications &                      Asynchronous replication                                                 Asynchronous replication 
Aurora  : asynchronous


3 NON-Aurora : only the primary instance is active             All regions are accessible nd can be used for reads                      all read replicas are assessible and can be used for readcaling
  Aurora : all instances are active


4  NON-Aurora : automated backups are taken from standby       automated backups are taken in each region                                No backups configured by default 
  Aurora : automated backups are taken from shared storage 
  layer


5  Always span atleast 2 AZ ithin a single region               Each region can have a Multi - AZ deployment                             can be within an AZ , Cross AZ or Cross region




-------------------------------------------- RDS Backups and Aurora Backups 

-------- RDS Backups

---  Automated backups:
- Daily full backup of the database (during the backup window)
- Transaction logs are backed-up by RDS every 5 minutes
- => ability to restore to any point in time (from oldest backup to 5 minutes ago)
- 1 to 35 days of retention, set 0 to disable automated backups

---- Manual DB Snapshots
- Manually triggered by the user
- Retention of backup for as long as you want


Trick: in a stopped RDS database, you will still pay for storage. If you plan on stopping it for a long time, you should snapshot & restore instead


--------- Aurora Backups

---- Automated backups
- 1 to 35 days (cannot be disabled)
- point-in-time recovery in that timeframe


---- Manual DB Snapshots

- Manually triggered by the user
- Retention of backup for as long as you want


----------------------------------------------RDS & Aurora Restore options

------ Restoring a RDS / Aurora backup or a snapshot creates a new database

Restoring MySQL RDS database from S3

-- Create a backup of your on-premises database
-- Store it on Amazon S3 (object storage)
-- Restore the backup file onto a new RDS instance running MySQL

Restoring MySQL Aurora cluster from S3

-- Create a backup of your on-premises database using Percona XtraBackup
-- Store the backup file on Amazon S3
-- Restore the backup file onto a new Aurora cluster running MySQL




-------------------------------Aurora Database Cloning

-- Create a new Aurora DB Cluster from an existing one

-- Faster than snapshot & restore

-- Uses "copy-on-write" protocol:
   -  Initially, the new DB cluster uses the same data volume as the original DB cluster (fast and efficient – no copying is needed)
   -  When updates are made to the new DB cluster data, then additional storage is allocated and data is copied to be separated

-- Very fast & cost-effective

-- Useful to create a “staging” database from a “production” database without impacting the production database

IMP : You cannot clone databases across AWS regions. 

-- The clone databases must be created in the same region as the source databases. Currently, you are limited to 15 clones based on a copy, including clones based on other clones.

-- After that, only copies can be created. However, each copy can also have up to 15 clones.





EPV : A gaming company is doing pre-launch testing for its new product. The company runs its production database on an Aurora MySQL DB cluster and the performance testing team wants access to multiple test databases that must be re-created from production data. The company has hired you as an AWS Certified Solutions Architect - Associate to deploy a solution to create these test databases quickly with the LEAST required effort.

ANS : Use database cloning to create multiple clones of the production database and use each clone as a test database

EXP : 

-- You can quickly create clones of an Aurora DB by using the database cloning feature. In addition, database cloning uses a copy-on-write protocol, in which data is copied only at the time the data changes, either on the source database or the clone database.

-- Cloning is much faster than a manual snapshot of the DB cluster.

-- This would allow the performance testing team to have quick access to the production data in an isolated way. 

-- The team can iterate over the various test phases by deleting existing test databases and then cloning the production DB to create new test databases.







--------------------------------------------------------- RDS & Aurora Security

-- At-rest encryption:
-  Database master & replicas encryption using AWS KMS – must be defined as launch time
-  If the master is not encrypted, the read replicas cannot be encrypted
-  To encrypt an un-encrypted database, go through a DB snapshot & restore as encrypted

-- In-flightencryption: TLS-readybydefault,usetheAWSTLSrootcertificatesclient-side

-- IAM Authentication: IAM roles to connect to your database (instead of username/pw)

-- Security Groups: Control Network access to your RDS / Aurora DB

-- No SSH available except on RDS Custom

-- Audit Logs can be enabled and sent to CloudWatch Logs for longer retention



------------------------------AWS RDS PROXY

-- it is fully managed data base proxy for RDS by the AWS 

-- it is chargable 

-- allows apps to pool and share the DB connections established with the data base 

-- Improving database effiency by reducing the stress on the data base resource(Eg CPU, RAM ) and Minimize open connections(and timeouts)

-- Serverles,AutoScaling, HA

-- Support MYSQL, POstgres, MariaDB, MSQL and AURORA 

-- Secret Mangaer is a AWS Service where u can store all Scrects(keys, usernames,passwords etc) 

-- RDS Supports SSM to get credntaials 

-- RDS proxy is "never publicly accessible (must be accessed from VPC) "


Summary: RDS proxy will allow ur appns to pool and share the DB connections established with the Db instead of having every single app connect to rds instace they wil be instead connecting to te proxy and proxy will pool these connctions together into less connections to the RdS DB instance 


-- Everything should be inside the VPC 


=================================RDS PRAC========================================


-- create Db with free tier configuration , MYsql without backups 

-- create repelica option grey out coz

Backups for your instance are currently disabled (retention = 0). Please enable backups before attempting to create a read replica.

-- by deafult the role of data base is Insatnce 

-- if u want to edit anything --> go to modify n do changes u want

-- read replica is only read purpose 

-- u can promote to stand alone  

-- once u create snapshots from data base engine , from snapshot u wl create new engine but u can not give same name for db engine 

-- for eg if someone delete table u can get restore from the napshot but the whole engine u should do backup coz db operations are resonsile by us not AWS 

-- go to SG default and add ur engine port number , mine is MYsQL 3306 

-- copy endpoint of RDS 

-- open mysql workbnch and try to connect 

NOte: if u are using company wifi or some private internet , try to connect to ur mobile hotsopt u will get connect 

-- u can also cange verison of snapshot engine with jst one click 

 NOte : when u want to restore from the S3 ur engines u can get only 2 engines 

AUrora and MYSql 


====================================ELASTIC CACHE=======================

-- it is developer resource mainly

-- it is regional 

-- it is also data base only 

-- Amazon ElastiCache can be used as a distributed in-memory cache for session management.

-- In-memory data base caching Service , high performance, low latency

-- Helps reduce load off of databases for read intensive workloads

-- Cache : all frquently accessed data is stored at this place 

-- it is used for Read Purpose 

-- it can handled Session data(cookies) 

-- it supports Encryption ( IN-TRANSIT and DATA at rest) 

-- Helps make your application stateless

-- Using ElastiCache involves heavy application code changes

-- Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Session stores can be set up using both Memcached or Redis for ElastiCache.



------------------- ElastiCache Solution Architecture - DB Cache

-- Applications queries ElastiCache, if not available, get from RDS and store in ElastiCache.

-- Helps relieve load in RDS

-- Cache must have an invalidation strategy to make sure only the most current data is used in there.


------------------  ElastiCache Solution Architecture – User Session Store

-- User logs into any of the application

-- The application writes the session data into ElastiCache

-- The user hits another instance of our application

-- The instance retrieves the data and the user is already logged in


-------------------- it supports two Engines 

1 Redis : permannent

- it supports HA, failover and backups

- Data is Persistent 

- Multi AZ with Auto-Failover

- Read Replicas to scale reads and have high availability

- Data Durability using AOF(Append Only File) persistence

- Backup and restore features

- Supports Sets and Sorted Sets

- Replication

- Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.




2 MemcacheD --temporary 

- it not supports HA, failover and backups

- Data not Persistent 

- Multi-node for partitioning of data (sharding)

- No high availability (replication)

- No backup and restore

- Multi-threaded architecture

- sharding(pieces)

- Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached.



------------------------------ Choosing between Redis and Memcached


-- Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine.

--  Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Understand your requirements and what each engine offers to decide which solution better meets your needs.


                                                               Memcached                         Redis

      
Sub-millisecond latency	                                         Yes                               Yes

Developer ease of use                                            Yes                               Yes      

Data partitioning	                                               Yes                               Yes   

Support for a broad set of programming languages	               Yes                               Yes 

Advanced data structures	                                        -                                Yes

Multithreaded architecture	                                     yes                                -

Snapshots	                                                        -                                Yes

Replication	                                                      -                                Yes

Transactions                                                      -                                Yes

Pub/Sub	                                                          -                                Yes

Lua scripting	                                                    -                                Yes

Geospatial support	                                              -                                Yes
       
   

EXP :

Sub-millisecond latency
  
  - Both Redis and Memcached support sub-millisecond response times. By storing data in-memory they can read data more quickly than disk based databases.

Developer ease of use

  - Both Redis and Memcached are syntactically easy to use and require a minimal amount of code to integrate into your application.

Data partitioning
  
   - Both Redis and Memcached allow you to distribute your data among multiple nodes. This allows you to scale out to better handle more data when demand grows.

Support for a broad set of programming languages
 
   - Both Redis and Memcached have many open-source clients available for developers. Supported languages include Java, Python, PHP, C, C++, C#, JavaScript, Node.js, Ruby, Go and many others.

Advanced data structures
 
   - In addition to strings, Redis supports lists, sets, sorted sets, hashes, bit arrays, and hyperloglogs. Applications can use these more advanced data structures to support a variety of use cases. For example, you can use Redis Sorted Sets to easily implement a game leaderboard that keeps a list of players sorted by their rank.

Multithreaded architecture

   - Since Memcached is multithreaded, it can make use of multiple processing cores. This means that you can handle more operations by scaling up compute capacity.

Snapshots

   - With Redis you can keep your data on disk with a point in time snapshot which can be used for archiving or recovery.

Replication
 
   - Redis lets you create multiple replicas of a Redis primary. This allows you to scale database reads and to have highly available clusters.

Transactions

   - Redis supports transactions which let you execute a group of commands as an isolated and atomic operation.

Pub/Sub

   - Redis supports Pub/Sub messaging with pattern matching which you can use for high performance chat rooms, real-time comment streams, social media feeds, and server intercommunication.

Lua scripting
 
   - Redis allows you to execute transactional Lua scripts. Scripts can help you boost performance and simplify your application.

Geospatial support

   - Redis has purpose-built commands for working with real-time geospatial data at scale. You can perform operations like finding the distance between two elements (for example people or places) and finding all elements within a given distance of a point.




----------------------------------------------- ElastiCache – Cache Security

-- ElastiCache supports IAM Authentication for Redis

-- IAM policies on ElastiCache are only used for AWS API-level security

-- Redis AUTH:
- You can set a “password/token” when you create a Redis cluster
- This is an extra level of security for your cache (on top of security groups)
- Support SSL in flight encryption

-- Memcached
- Supports SASL-based authentication (advanced)





-- if the application hit cache it wil give data without going to RDS 

-- if Cache is not have data , appn go to RDS and get data whatever the data get from the RDS the application writes in the Cache coz it is missed data , so nxt tym if u search for same data it will get from cache not from RDS (this will do developer) 

-- it use to improv the performance 



------------------------------- Developer Strategies (Patterns for ElastiCache)  

-- they are 3 types of strategies

1  Lazy Loading : load the data , when it is necessary , all the read data is cached, data can become stale(old) in cache

if Cache is not have data , appn go to RDS and get data whatever the data get from the RDS the application writes in the Cache coz it is missed data , so nxt tym if u search for same data it will get from cache not from RDS


2 Write Through : write parallelly both data basess (DB and Cache) , in this 100% similar data on Both RDS and Elastic cache , Adds or update data in the cache when written to a DB (no stale data)

3 Session Store: store temporary session data in a cache (using TTL features)


-------------------------------  Developer Associate (Caching Implementation Considerations)


                         How to apply caching


https://aws.amazon.com/caching/implementation-considerations/  - for more reference


• Is it safe to cache data? Data may be out of date, eventually consistent

• Is caching effective for that data?
 
      • Pattern: data changing slowly, few keys are frequently needed
      • Anti patterns: data changing rapidly, all large key space frequently needed


• Is data structured well for caching?

      • example: key value caching, or caching of aggregations results


-----------------------------------------  Which caching design pattern is the most appropriate?


------------------- 1 Lazy Loading / Cache-Aside / Lazy Population

• Pros

   • Only requested data is cached (the cache isn’t filled up with unused data)
   • Node failures are not fatal (just increased latency to warm the cache)

• Cons
   
   • Cache miss penalty that results in 3 round trips, noticeable delay for that request 

       - First of all, if we get a Cache hit is great. 
       - But if we get a Cache miss,
       - that means there's three network calls being done from your application to ElastiCache,
       - which is a Cache miss from your application to RDS,which is read from Data Base and finally,another write to the cache.
       - And so for your users,when they are reading something,and they're seeing some latency,they're not used to it.
       - And so, that three round trips may be a bad user experience.


    • Stale data: data can be updated in the database and outdated in the cache
      
       - So if your data is updated in RDS,then it will not be necessarily updating in ElastiCache.
       - then it will not be necessarily updating in ElastiCache.
       - And that's what you need to ask yourself is my data Okay to be out of date and eventually consistent?


------- Lazy Loading / Cache-Aside / Lazy Population Python Pseudocode


# Python

def get_user(user_id):

    # Check the cache

    record = cache.get(user_id)

    if record is None:       

       # Run a DB query       

       record = db.query("select * from users where id = ?",user_id)

       # Populate the cache

       cache.set(user_id, record)

    return record

# App code

user = get_user(17)




---------------------------- 2  Write Through – Add or Update cache when database is updated


• Pros:
   
   • Data in cache is never stale, reads are quick
    
   • Write penalty vs Read penalty (each write requires 2 calls)

      - you get a Write penalty versus a Read penalty. 
      - So before when we had a Cache miss, we had to do three network calls in total,and that was a Read penalty.
      - But now when we write to the database, we have a Write penalty.
      - Each write now requires two calls, one to the database and one to the cache.
      - And from the user perspective, a user is expecting more a write to take longer than a read. For example, if you were on the social media and you post something then you expect that post to last maybe a second or two.
      - But if you fetch a profile, you expected to take a split second.
      - So here, the users understand that any write, any changes to the database may take a little bit longer.So there may be better from a user perspective.


• Cons:
    
    • Missing Data until it is added / updated in the DB. Mitigation is to implement Lazy Loading strategy as well

    • Cache churn – a lot of the data will never be read



---------  Write-Through Python Pseudocode


# Python

def save_user(user_id, values):

    # Save to DB 

    record = db.query("update users ... where id = ?", user_id, values)

    # Push into cache

    cache.set(user_id, record)

    return record

# App code

user = save_user(17, {"name": "Nate Dogg"})




------------------------------------------------- Cache Evictions and Time-to-live (TTL)

• Cache eviction(remove/leave) can occur in three ways: 

    • You delete the item explicitly in the cache
    • Item is evicted because the memory is full and it’s not recently used (LRU)
    • You set an item time-to-live (or TTL)


• TTL are helpful for any kind of data:

    • Leaderboards
    • Comments
    • Activity streams 


• TTL can range from few seconds to hours or days

• If too many evictions happen due to memory, you should scale up or out



------------------------------------ Final words of wisdom

• Lazy Loading / Cache aside is easy to implement and works for many situations as a foundation, especially on the read side

• Write-through is usually combined with Lazy Loading as targeted for the queries or workloads that benefit from this optimization

• Setting a TTL is usually not a bad idea, except when you’re using Write- through. Set it to a sensible value for your application

• Only cache the data that makes sense (user profiles, blogs, etc...)






------------------------------------------------- REDIS

-- it hs 2 types of modes 

-- it is like NoSQL database 

1  Cluster Mode Enabled  

-- Cluster = Collection of Shards

-- Shard = Cllection of Nodes/Server

-- Each Shard has 6 nodes = 1 Primary Node , 5 Replica Node

-- u can have 500 Shards per cluster 


2  Cluster Mode disabled

-- it has ony one shard = 1 primary and 5 replias 


----------- ElastiCache – Redis Use Case

-- Gaming Leaderboards are computationally complex

-- Redis Sorted sets guarantee both uniqueness and element ordering

-  Sorted Set is a Set data structure that doesn’t allow duplicate members. At the same time, its members are ordered in ascending order. By combining both, we can define a Sorted Set as an ordered collection of unique members.

-- Each time a new element added, it’s ranked in real time, then added in correct order




-------- imp : Service Quotas ia AWS service where u can find the soft limit of the AWS Resource 

-- if required u can increase the soft limit 





------------------------------- Amazon MemoryDB for Redis


• Redis-compatible, durable, in-memory database service

   - So the difference between Redis and MemoryDB for Redis, that 
   - while Redis' intent is to be used as a cache with some durability, 
   - MemoryDB is really a database that has a Redis-compatible API. 


• Ultra-fast performance with over 160 millions requests/second

• Durable in-memory data storage with Multi-AZ transactional log

• Scale seamlessly from 10s GBs to 100s TBs of storage

• Use cases: web and mobile apps, online gaming, media streaming, ...





=================================AWS DynamoDB===================================

-- Fully managed, highly available with replication across multiple AZs

-- NoSQL database - not a relational database - with transaction support

-- Scales to massive workloads, distributed database

-- Millions of requests per seconds, trillions of row, 100s of TB of storage

-- Fast and consistent in performance (single-digit millisecond)

-- Integrated with IAM for security, authorization and administration

-- Low cost and auto-scaling capabilities

-- No maintenance or patching, always available

-- Standard & Infrequent Access (IA) Table Class

-- DynamoDB does not directly support storing data in Amazon S3 Glacier.

    -- However, there are a few ways you could integrate the two services to meet your use case.

    1 You could store metadata and pointers to archived employee records in DynamoDB. The archived records themselves would be stored in S3 Glacier vaults. DynamoDB would provide fast lookups of metadata and pointers to retrieve archived records from Glacier as needed.


    2 When archiving records to Glacier, you would update the corresponding items in DynamoDB with the archive ID. To retrieve an archived record, your application would first check DynamoDB for the archive ID, then use that to retrieve the record from Glacier.




------------------------------Basics OF Dynamodb 

-- DynamoDB is made of Tables

-- Each table has a Primary Key (must be decided at creation time)

-- Each table can have an infinite number of items (= rows)

-- Each item has attributes (can be added over time – can be null)

-- Maximum size of an item is 400KB

-- Data types supported are:
• Scalar Types – String, Number, Binary, Boolean, Null 
• Document Types – List, Map
• Set Types – String Set, Number Set, Binary Set

-- Therefore, in DynamoDB you can rapidly evolve schemas

- stored on SSD Storage 

- spread across 3 geographically distinct data centers

- eventuall consistent Reads(default) --- ECR 

- Strongly Consistent Reads ---- SCR 

-- this is purely developer service 

-- DdB offers "push button" scaling , meaning that u can scale ur database on the fly, without any downtime 

-- it is serverless 

-- Table is collection of Attributes(Coloumns) and Items(Rows) in Amazon dynamo db 

-- Primary Key is always unique and not null 

-- in  Dynamodb Primary Key is called "partition Key"

-- in Ddb , initially u can create a table with single coloumn and tht should be Primary key 

-- in Ddb , p.k is manditory while creating a table 

-- Composite Key = Primary Key + Sort Key 

-- Sort Key is optional 

-- if u have a primary key for a single coloumn = Duplicate values are not allowed 

-- if u have p.k + sk = duplicate values are allowed 

-- Ddb data stroed in JSON format


----- "Dynamo Db Streams" = what ever the item level changes wil be captured 


IMP to know : 

-- By default, all Amazon DynamoDB tables are encrypted using AWS owned keys, which do not write to "AWS CloudTrail logs"

EXP : 

-- AWS owned keys are not stored in your AWS account.

-- They are part of a collection of KMS keys that AWS owns and manages for use in multiple AWS accounts. 

-- AWS services can use AWS owned keys to protect your data. AWS owned keys used by DynamoDB are rotated every year (approximately 365 days).

-- You cannot view, manage, or use AWS owned keys, or audit their use.

-- However, you do not need to do any work or change any programs to protect the keys that encrypt your data. 

-- You are not charged a monthly fee or a usage fee for use of AWS owned keys, and they do not count against AWS KMS quotas for your account.

-- All DynamoDB tables are encrypted. There is no option to enable or disable encryption for new or existing tables. 

-- By default, all tables are encrypted under an AWS owned key in the DynamoDB service account. 

-- However, you can select an option to encrypt some or all of your tables under a "customer managed key" or the "AWS managed key" for DynamoDB in your account.





-------------------Indexs

-- thse indexes ae writes by developers 

-- 2 types of Indexes

1 LSI(Local secondary index) = partition key + Any Coloumn as Sortkey

use DynamodB Local Secondary index when you need to support querying items with different sorting order of attributes.

2 GSI (gobal secondary index) = Any coloumn as pk + any coloumn as SK 

In short, use DynamoDB Global Secondary Index when you need to support querying non-primary key attribute of a table.



Note : -- lsi can be created only at the time of creating the DynamoDb table, later u cannot Modify/delete the LSI 

-- GSI can be Created, modified, delete any time 


------------------------- LAB for LSI and GSI

-- open dynamodb and create table 

-- partiton key = userid , number type 

-- add sort key also with stirng type 

-- Table settings = Customize settings

-- Read/write capacity settings =  provisioned

-- create LSI with different name like TopScores with number type  and create 

-- now create GSI , partiton key  = GameTitle with string and sort key is TopScore with number 

By default, the global secondary index's capacity is the same as your base table's capacity. You can change the index's capacity in the table's settings after you create the table.

-- Read capacity and Write capacity turnoff and make it 1 for practice purpose 

-- add items , choose json format to add items or form method 

{
  "userid": 101,
  "GameTitle": "Starship X",
  "TopScores": 24,
  "wins": 4,
  "loses": 9
}

add data like this and query data as u want 








------------- provisioned Capacity Units 

-- 2 types 

1 RCU ( read capacity units) 

2 WCU (Write Capacity Units )

-- Ddb performance depend upon RCU and WCU 

-- 1 RCU = 5.2 Million Reads it will do 

-- 1 WCU = 2.5 million Writes 



-- by default DDb giving 5 RCU and 5 WCU 

-- For ECR , 1 RCU = 2 Reads per second of 4KB size

-- For SCR, 1RCU = 1 read per second of 4KB Size 

-- 1 WCU = 1 write per second of 1KB Size 




SQL VS NOSQL 


SQL :

-- it is generally used in RDBMS 

-- structures data can be stored in tables 

-- The Schemas Are Static 

-- schemas are rigid and bound to relationships 

-- Helpful to design complex queries 

-- here we call tables , rows and coloumns 

-- eg : Mysql,oracle,sqlite,Postgres and MS_SQL 

-- it has different types 

Relational and Analytical(OLAP) 


NOSQL :

-- it is generally used in NON-RDBMS 

-- Using JSON data, unstructures data can be stored 

-- The Schemas Are dynamic

-- schemas are no-rigid , they are flexible 

-- no interface to prepare complex queries 

-- here we call collections, collections has Documents 

- Eg : MongoDB, BigTable, Redis, RavenDB, Cassandra, Hbase, Neo4j and couchDB 

--it has diff types 

Coloumn-Family , Graph , Document and Key-value 

------------------

-- our "DynamoDB supports "Document and key-value(JSON) pair" 


---------------Provision Capcity examples 


-------------------------------Example For RCU 

eg : if u have a table and u want to read 100 items per second with Strongly Consistent Reads(SCR) and ur items are 8KB in Size, u would calculate the required provisioned capacity as follows ,

-- we kanow the basic of 



-- For ECR , 1 RCU = 2 Reads per second of 4KB size

-- For SCR, 1RCU = 1 read per second of 4KB Size 

-- 1 WCU = 1 write per second of 1KB Size 

-- to solve this 

Sol : here it is asking 8KB size , what is our size? for SCR is 1RCU = 1 read per second of 4KB Size 


so 8KB/4KB = 2 capacity units 

in the question it is asking foor 100 items per second to read , so 

2 read capacity units per item * 100 reads per second = 200 / 1 = 200 read capacity units 

NOte : if it is ECR then 200 / 2 = 100 Read cpacity units 


----------------------Examle for WCU 


-- if u have a table and u want to write 50 items per second and ur items are 4KB in size , u wuld calculate the required provisioned caapcity as folows 


sol : 4 KB / 1KB  = 4 WCU 

     4 WCU * 50 Units per second = 200/1 = 200 write units 





-------------- for eg u have question like 

Q : 10 Strongly Consistent Reads per seconds of 6 KB each 

sol: here we do not have 6 we have only 4 and 8 KB so,go for near (only up) so 8KB 


ans : 20 RCU 



--------------Dynamo db table prac


-- Ddb n cnsole create table 

-- customize settings -> Ddb std

-- provisionsed


------------------------------------- DynamoDB Accelerator (DAX)

-- Fully-managed, highly available, seamless in- memory cache for DynamoDB

-- Help solve read congestion by caching

-- Microseconds latency for cached data

-- Doesn’t require application logic modification (compatible with existing DynamoDB APIs)

-- 5 minutes TTL for cache (default)


------------------------------------ DynamoDB Accelerator (DAX) vs. ElastiCache

-- ElastiCache = Store Aggregation Result

-- Amazon DynamoDB = Individual objects cache and Query & Scan cache with the help of DynamoDB Accelerator (DAX)



--------------------------------- DynamoDB – Stream Processing

-- DynamoDB stream details = Capture item-level changes in your table, and push the changes to a DynamoDB stream. You then can access the change information through the DynamoDB Streams API.

-- Use cases:
• React to changes in real-time (welcome email to users)
• Real-time usage analytics
• Insert into derivative tables
• Implement cross-region replication
• Invoke AWS Lambda on changes to your DynamoDB table

--------------------------- LAB 


DynamoDb Streams : it is an ordered flow of information amout changes  items in a DynamoDb table. When u enable stream on a table , DynamoDb captures information about every modification to data items n the table 

-- Capture item-level changes in your table, and push the changes to a DynamoDB stream. You then can access the change information through the DynamoDB Streams API.

Stream Records can be used for 

-- Kinesis Data Streams/Client library 
-- AWS Lambda
-- Data is retined for 24 hrs 
-- Limited # of consumers
-- Process using AWS Lambda Triggers, or DynamoDB Stream Kinesis adapter


Uses For : 

-- Analytics purpose
-- Cross Region replication
-- Real time changes

Step 1 : create DynamoDb table

-- create table and give partiton key

-- once u create table , add some items into the table

-- go to table --> exports and streams --> turn on DynamoDB stream --> New and old images --> Turn on streams 


Step 2 : create lambda function 

--  in the same page , choose create trigger

-- create new function --> Use Blue print --> search for DynamoDB and select Process updates made to table with python --> give function name --> choose create a new role --> give DynamoDb details in trigger section --> create function

-- once ur function got created , u will get some error 

--  function --> configuration --> permissions --> c.o role --> attach permissions (policies) --> DynamoDBfullaccess --> do refresh the screen the error will disappear

-- ince ur  function is created go back to the trigger page in DynamoDB table add this function with batch size is 100 for this eg 


Step 3 : Test 

-- go back to the DynamoDB table and update and add some items into the table 

-- do some chganges as you want 

-- the changes will trigger to our lambda function '

-- now go to function --> monitor--> view cloudwatch logs --> now you will able to see all the changes , INSERT,UPDATE,DELETE all the modifications that are captured through the DynamoDB Stream APi and trigger through the lambda and you will able to see in the cloud watch logs what has modification occurs 



------------------------------------------------- DynamoDB Global Tables

--- Global table : used to replicate ur db 

-- Make a DynamoDB table accessible with low latency in multiple-regions

-- Active-Active replication

-- Applications can READ and WRITE to the table in any region

-- Must enable DynamoDB Streams as a pre-requisite

note : To create a replica, you must set your table's and index's throughput capacity to auto scaling, or change the capacity mode to on-demand.


-- with the use of sort key , u can do reuse primary key ( name) multiples times but u can not do use same sort key values


-------------------------------------------------- DynamoDB –TimeTo Live (TTL)

-- Automatically delete items after an expiry timestamp

-- specify  a item attribute that contains expiration time for the item

-- it is background process and u no need to write any scripts , it is automatic process 

-- does not affect read/write traffic

-- can move expired item o cold storage (using dynamodb streams + TTL) by using lambda 


-- Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations, web session handling...


---------------------------------- lab

-- Enable TTL - by defalut , u have to enable , Specify TTL attribute name (with Type Number - epoch time in seconds )

-- DynamoDB has 2 scanner background process 

    1 compares current time to value set in TTL and sets item to expire 

    2 scans for expired items and deletes them 

-- items get removed from LSI and GSI 

-- A delete operations enters in to stream with tag system-delete

-- there is no charge for the internal scan operations 


https://www.youtube.com/watch?v=zy4AiUsWZpI 




------------------------------ DynamoDB – Backups for disaster recovery

-- Continuous backups using point-in-time recovery (PITR)

• Optionally enabled for the last 35 days
• Point-in-time recovery to any time within the backup window 
• The recovery process creates a new table

EPV : A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest.

ANS : Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written

EXP : -- Amazon DynamoDB enables you to back up your table data continuously by using point-in-time recovery (PITR). When you enable PITR, DynamoDB backs up your table data automatically with per-second granularity so that you can restore to any given second in the preceding 35 days.

-- PITR helps protect you against accidental writes and deletes. For example, if a test script writes accidentally to a production DynamoDB table or someone mistakenly issues a "DeleteItem" call, PITR has you covered.



-- On-demand backups

• Full backups for long-term retention, until explicitely deleted
• Doesn’t affect performance or latency
• Can be configured and managed in AWS Backup (enables cross-region copy)
• The recovery process creates a new table
-- All on-demand backups are cataloged, discoverable, and retained until they are explicitly deleted.
-- On-demand backup is created upon request. since an on-demand backup cannot be created pre-emptively to handle data corruption issues that happen once in a while.




------------------------------ DynamoDB – Integration with Amazon S3

-- Export to S3 (must enable PITR)

• Worksforanypointoftimeinthelast35days
• Doesn’taffectthereadcapacityofyourtable
• PerformdataanalysisontopofDynamoDB
• Retainsnapshotsforauditing
• ETLontopofS3databeforeimporting back into DynamoDB
• ExportinDynamoDBJSONorIONformat



dynamoDB ----(export)----> s3 -(query)---->Athena 

-- Import from S3

• ImportCSV,DynamoDBJSONorIONformat
• Doesn’tconsumeanywritecapacity
• Createsanewtable
• ImporterrorsareloggedinCloudWatchLogs

s3(.csv,.json,.ion)------(import)---->dynamoDB



---------------------------------- Amazon DynamoDB Continuous Backups and Point-In-Time Recovery (PITR)


-- The Amazon DynamoDB team is back with another useful feature hot on the heels of encryption at rest. 

-- At AWS re:Invent 2017 AWS launched global tables and on-demand backup and restore of your DynamoDB tables

-- You can enable continuous backups with a single click in the AWS Management Console, a simple API call, or with the AWS Command Line Interface (AWS CLI). 

-- DynamoDB can back up your data with per-second granularity and restore to any single second from the time PITR was enabled up to the prior 35 days.

-- AWS built this feature to protect against accidental writes or deletes. If a developer runs a script against production instead of staging or if someone fat-fingers a DeleteItem call, PITR has you covered. 

-- also built it for the scenarios you can’t normally predict. You can still keep your on-demand backups for as long as needed for archival purposes but PITR works as additional insurance against accidental loss of data. 


PITR :

-- The time it takes to restore a table varies based on multiple factors and restore times are not neccesarily coordinated with the size of the table. If your dataset is evenly distributed across your primary keys you’ll be able to take advanatage of parallelization which will speed up your restores.

-- Pricing for continuous backups is detailed on the DynamoDB Pricing Pages. Pricing varies by region and is based on the current size of the table and indexes.

-- For example, in US East (N. Virginia) you pay $0.20 per GB based on the size of the data and all local secondary indexes.

A few things to note:

-- PITR works with encrypted tables.

-- Just like on-demand backups, there are no performance or availability impacts to enabling this feature.

-- Stream settings, Time To Live settings, PITR settings, tags, Amazon CloudWatch alarms, and auto scaling policies are not copied to the restored table.

-- knew I restored the table all along because every PITR API call is recorded in AWS CloudTrail.


NOTE : PITR is available in the US East (N. Virginia), US East (Ohio), US West (N. California), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Canada (Central), EU (Frankfurt), EU (Ireland), EU (London), and South America (Sao Paulo) Regions starting today.







==================================================== Databases in AWS =========================

--- Choosing the Right Database

• We have a lot of managed databases on AWS to choose from

• Questions to choose the right database based on your architecture:

• Read-heavy, write-heavy, or balanced workload? Throughput needs? Will it

change, does it need to scale or fluctuate during the day?

• How much data to store and for how long? Will it grow? Average object size? How are they accessed?

• Data durability? Source of truth for the data ?

• Latency requirements? Concurrent users?

• Data model? How will you query the data? Joins? Structured? Semi-Structured?

• Strong schema? More flexibility? Reporting? Search? RDBMS / NoSQL?

• License costs? Switch to Cloud Native DB such as Aurora?




------------------------------------ DatabaseTypes

• RDBMS (= SQL / OLTP): RDS,Aurora – great for joins

• NoSQL database – no joins, no SQL : DynamoDB (~JSON), ElastiCache (key / value pairs), Neptune (graphs), DocumentDB (for MongoDB), Keyspaces (for Apache Cassandra)

• Object Store: S3 (for big objects) / Glacier (for backups / archives)

• Data Warehouse (= SQL Analytics / BI): Redshift (OLAP), Athena, EMR

• Search: OpenSearch (JSON) – free text, unstructured searches

• Graphs: Amazon Neptune – displays relationships between data

• Ledger: Amazon Quantum Ledger Database

• Time series:AmazonTimestream

• Note: some databases are being discussed in the Data & Analytics section


-------------------------------------- 1  Amazon RDS – Summary

• Managed PostgreSQL / MySQL / Oracle / SQL Server / DB2 / MariaDB / Custom • Provisioned RDS Instance Size and EBS Volume Type & Size

• Auto-scaling capability for Storage

• Support for Read Replicas and Multi AZ

• Security through IAM, Security Groups, KMS , SSL in transit

• Automated Backup with Point in time restore feature (up to 35 days)

• Manual DB Snapshot for longer-term recovery

• Managed and Scheduled maintenance (with downtime)

• Support for IAM Authentication, integration with Secrets Manager

• RDS Custom for access to and customize the underlying instance (Oracle & SQL Server)

• Use case: Store relational datasets (RDBMS / OLTP), perform SQL queries, transactions


EPV :  A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort.


ANS : - Amazon RDS can automatically back up your database and keep your database software up to date with the latest version.

- However, RDS does not allow you to access the host OS of the database.

- so , you need to use Amazon RDS Custom for Oracle as it allows you to access and customize your database server host and operating system, 

- for example by applying special patches and changing the database software settings to support third-party applications that require privileged access. Amazon RDS Custom for Oracle facilitates these functionalities with minimum infrastructure maintenance effort. 

- You need to set up the RDS Custom for Oracle in multi-AZ configuration for high availability.


-------------------------------------- 2 Amazon Aurora – Summary

• Compatible API for PostgreSQL/MySQL,separation ofs torage and compute

• Storage:data is stored in 6 replicas,across 3 AZ–highly available,self-healing,auto-scaling

• Compute:Cluster of DB Instance across multiple AZ,auto-scaling of ReadReplicas

• Cluster:Custom endpoints for writer and reader DBinstances

• Aurora Serverless – for unpredictable / intermittent workloads, no capacity planning

• Aurora Global: up to 16 DB Read Instances in each region, < 1 second storage replication

• Aurora Machine Learning: perform ML using SageMaker & Comprehend on Aurora

• Aurora Database Cloning: new cluster from existing one,faster than restoring a snapshot

• Use case: same as RDS, but with less maintenance / more flexibility / more performance / more Features


---------------------------------------- 3 Amazon ElastiCache – Summary

• Managed Redis / Memcached (similar offering as RDS, but for caches)

• In-memory data store, sub-millisecond latency

• Select an ElastiCache instance type (e.g., cache.m6g.large)

• Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding)

• Security through IAM, Security Groups, KMS, Redis Auth

• Backup / Snapshot / Point in time restore feature

• Managed and Scheduled maintenance

• Requires some application code changes to be leveraged


Use Case: Key/Value store, Frequent reads, less writes, cache results for DB queries, store session data for websites, cannot use SQL.


---------------------------------------- 4 Amazon DynamoDB – Summary

• AWS proprietary technology, managed serverless NoSQL database, millisecond latency

• Capacity modes: provisioned capacity with optional auto-scaling or on-demand capacity

• Can replace ElastiCache as a key/value store (storing session data for example, using TTL feature)

• Highly Available,Multi AZ by default, Read and Writes are decoupled,transaction capability

• DAX cluster for read cache, microsecond read latency

• Security, authentication and authorization is done through IAM

• Event Processing: DynamoDB Streams to integrate with AWS Lambda, or Kinesis Data Streams

• Global Table feature: active-active setup

• Automated backups up to 35 days with PITR (restore to new table), or on-demand backups

• ExporttoS3withoutusingRCUwithinthePITRwindow,importfromS3withoutusingWCU

• Great to rapidly evolve schemas

• Use Case: Serverless applications development (small documents 100s KB), distributed serverless cache



---------------------------------------------- 5 Amazon S3 – Summary

• S3 is a... key / value store for objects
• Great for bigger objects, not so great for many small objects
• Serverless, scales infinitely, max object size is 5 TB, versioning capability
• Tiers: S3 Standard, S3 Infrequent Access, S3 Intelligent, S3 Glacier + lifecycle policy
• Features: Versioning, Encryption, Replication, MFA-Delete, Access Logs...
• Security: IAM, Bucket Policies, ACL, Access Points, Object Lambda, CORS, Object/Vault Lock • Encryption: SSE-S3, SSE-KMS, SSE-C, client-side,TLS in transit, default encryption
• Batch operations on objects using S3 Batch, listing files using S3 Inventory
• Performance:Multi-partupload,S3TransferAcceleration,S3Select
• Automation: S3 Event Notifications (SNS, SQS, Lambda, EventBridge)
• Use Cases: static files, key value store for big files, website hosting


--------------------------------------------- 6 DocumentDB

• Aurora is an “AWS-implementation” of PostgreSQL / MySQL ...

• DocumentDB is the same for MongoDB (which is a NoSQL database)

• MongoDB is used to store, query, and index JSON data

• Similar “deployment concepts” as Aurora

• Fully Managed, highly available with replication across 3 AZ

• DocumentDB storage automatically grows in increments of 10GB

• Automatically scales to workloads with millions of requests per seconds


--------------------------------------------- 7 Amazon Neptune

• Fully managed graph database

• A popular graph dataset would be a social network
   • Users have friends
   • Posts have comments
   • Comments have likes from users
   • Users share and like posts...

• Highly available across 3 AZ, with up to 15 read replicas

• Build and run applications working with highly connected datasets – optimized for these complex and hard queries

• Can store up to billions of relations and query the graph with milliseconds latency

• Highly available with replications across multiple AZs

• Great for knowledge graphs (Wikipedia), fraud detection, recommendation engines, social networking

-- Neptune is secure with support for HTTPS encrypted client connections and encryption at rest.


--------------------------------------------------- 8 Amazon Keyspaces (for Apache Cassandra)

Apache Cassandra is an open-source NoSQL distributed database

• A managed Apache Cassandra-compatible database service
• Serverless, Scalable, highly available, fully managed by AWS
• Automatically scale tables up/down based on the application’s traffic
• Tables are replicated 3 times across multiple AZ
• Using the Cassandra Query Language (CQL)
• Single-digit millisecond latency at any scale, 1000s of requests per second • Capacity: On-demand mode or provisioned mode with auto-scaling
• Encryption, backup, Point-In-Time Recovery (PITR) up to 35 days

• Use cases: store IoT devices info, time-series data, ...


-------------------------------------------------- 9 Amazon QLDB

• QLDB stands for ”Quantum Ledger Database”
• A ledger is a book recording financial transactions
• FullyManaged,Serverless,Highavailable,Replicationacross3AZ
• Used to review history of all the changes made to your application data over time
• Immutable system: no entry can be removed or modified, cryptographically verifiable
• 2-3x better performance than common ledger blockchain frameworks, manipulate data using SQL
• Difference with Amazon Managed Blockchain: no decentralization component, in accordance with financial regulation rules


-------------------------------------------------- 10 Amazon Timestream

• Fully managed, fast, scalable, serverless time series database
• Automatically scales up/down to adjust capacity
• Store and analyze trillions of events per day
• 1000s times faster & 1/10th the cost of relational databases
• Scheduled queries, multi-measure records, SQL compatibility
• Data storage tiering: recent data kept in memory and historical data kept in a cost-optimized storage
• Built-in time series analytics functions (helps you identify patterns in your data in near real-time)
• Encryption in transit and at rest

• Use cases: IoT apps, operational applications, real-time analytics, 



=============================================================== DMS – Database Migration Service ============================================


• Quickly and securely migrate databases to AWS, resilient, self healing

• The source database remains available during the migration

Source DB ----> EC2 instance Running DMS ---> Target DB

• Supports: 

   • Homogeneous migrations: ex Oracle to Oracle
   • Heterogeneous migrations: ex Microsoft SQL Server to Aurora

• Continuous Data Replication using CDC (change data capture)

• You must create an EC2 instance to perform the replication tasks


-- AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.

-- With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.

-- You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.




------------------- DMS Sources and Targets 

SOURCES: 

• On-Premises and EC2 instances databases: Oracle, MS SQL Server, MySQL, MariaDB, PostgreSQL, MongoDB, SAP, DB2

• Azure: Azure SQL Database

• Amazon RDS: all including Aurora

• Amazon S3

• DocumentDB


TARGETS:

• On-Premises and EC2 instances databases: Oracle, MS SQL Server, MySQL, MariaDB, PostgreSQL, SAP

• Amazon RDS

• Redshift, DynamoDB, S3

• OpenSearch Service

• Kinesis Data Streams

• Apache Kafka

• DocumentDB & Amazon Neptune

• Redis & Babelfish



EPV : A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?

ANS : Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams

EXP : 

-- You can achieve this by using AWS Database Migration Service (AWS DMS). AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in AWS cloud.

-- The given requirement needs the functionality to be implemented in the least possible time. You can use AWS DMS for such data-processing requirements. AWS DMS lets you expand the existing application to stream data from Amazon S3 into Amazon Kinesis Data Streams for real-time analytics without writing and maintaining new code. 

-- AWS DMS supports specifying Amazon S3 as the source and streaming services like Kinesis and Amazon Managed Streaming of Kafka (Amazon MSK) as the target. AWS DMS allows migration of full and change data capture (CDC) files to these services. AWS DMS performs this task out of box without any complex configuration or code development.

-- You can also configure an AWS DMS replication instance to scale up or down depending on the workload.



-------------------------------------- AWS Schema Conversion Tool (SCT) 


• Convert your Database’s Schema from one engine to another

• Example OLTP: (SQL Server or Oracle) to MySQL, PostgreSQL, Aurora

• Example OLAP: (Teradata or Oracle) to Amazon Redshift

• Prefer compute-intensive instances to optimize data conversions

Source DB --------(DMS + SCT)-----> Target DB (different engine)

• You do not need to use SCT if you are migrating the same DB engine

  • Ex: On-Premise PostgreSQL => RDS PostgreSQL
  • The DB engine is still PostgreSQL (RDS is the platform)


------------------------------- AWS DMS – Multi-AZ Deployment

• When Multi-AZ Enabled, DMS provisions and maintains a synchronously stand replica in a different AZ

• Advantages:
  • Provides Data Redundancy
  • Eliminates I/O freezes
  • Minimizes latency spikes


---------------------------- RDS & Aurora MySQL Migrations 

• RDS MySQL to Aurora MySQL 

   • Option 1: DB Snapshots from RDS MySQL restored as MySQL Aurora DB
   • Option 2: Create an Aurora Read Replica from your RDS MySQL, and when the replication lag is 0, promote it as its own DB cluster (can take time and cost $)

• External MySQL to Aurora MySQL

  • Option 1: • Use Percona XtraBackup to create a file backup in Amazon S3 
              • Create an Aurora MySQL DB from Amazon S3

  • Option 2: • Create an Aurora MySQL DB
              • Use the mysqldump utility to migrate MySQL into Aurora (slower than S3 method)

• Use DMS if both databases are up and running


-------------------------- RDS & Aurora PostgreSQL Migrations

• RDS PostgreSQL to Aurora PostgreSQL
 
  • Option 1: DB Snapshots from RDS PostgreSQL restored as PostgreSQL Aurora DB
  • Option 2: Create an Aurora Read Replica from your RDS PostgreSQL, and when the replication lag is 0, promote it as its own DB cluster (can take time and cost $)

• External PostgreSQL to Aurora PostgreSQL

  • Create a backup and put it in Amazon S3
  • Import it using the aws_s3 Aurora extension

• Use DMS if both databases are up and running



=================================================== AWS API Gateway =======================================


- Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. 

- APIs act as the front door for applications to access data, business logic, or functionality from your backend services.

- Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.



How Amazon API Gateway Works:

Amazon API Gateway creates RESTful APIs that:

 - Are HTTP-based.

 - Enable stateless client-server communication.

 - Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE.

Amazon API Gateway creates WebSocket APIs that:

 - Adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server. Route incoming messages based on message content.


-- So Amazon API Gateway supports stateless RESTful APIs as well as stateful WebSocket APIs. 




• AWS Lambda + API Gateway: No infrastructure to manage

• Support for the WebSocket Protocol

• Handle API versioning (v1, v2...)

• Handle different environments (dev, test, prod...)

• Handle security (Authentication and Authorization) 

• Create API keys, handle request throttling

• Swagger / Open API import to quickly define APIs • Transform and validate requests and responses

• Generate SDK and API specifications • Cache API responses


--------------------------------------- API Gateway – Integrations High Level

• Lambda Function
  • Invoke Lambda function
  • Easy way to expose REST API backed by AWS lambda

• HTTP
   • Expose HTTP endpoints in the backend
   • Example: internal HTTP API on premise, Application Load Balancer... 
   • Why? Add rate limiting, caching, user authentications, API keys, etc...

• AWS Service

  • Expose any AWS API through the API Gateway
  • Example: start an AWS Step Function workflow, post a message to SQS
  • Why? Add authentication, deploy publicly, rate control...


  -------------------------------------- API Gateway - Endpoint Types

  • Edge-Optimized (default): For global clients

     • Requests are routed through the CloudFront Edge locations (improves latency) 
     • The API Gateway still lives in only one region
  
  • Regional:

     • For clients within the same region
     • Could manually combine with CloudFront (more control over the caching strategies and the distribution)

  • Private: 

    • Can only be accessed from your VPC using an interface VPC endpoint (ENI)
    
    • Use a resource policy to define access  



 -------------------------------------- Enable Amazon API Gateway Caching

-- You can enable Amazon API caching in Amazon API Gateway to cache your endpoint's responses.

-- With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. 

-- When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. 

-- Amazon API Gateway then responds to the request by looking up the endpoint response from the cache instead of requesting your endpoint. The default TTL value for API caching is 300 seconds. 

-- The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. Using API Gateway Caching feature as we can accept stale data for about 24 hours.


--------------------------------------- AWS Step Functions

-- Build serverless visual workflow to orchestrate your Lambda functions

-- Features: sequence, parallel, conditions, timeouts, error handling, ...

-- Can integrate with EC2, ECS, On-premises servers, API Gateway, SQS queues, etc...

-- Possibility of implementing human approval feature

-- Use cases: order fulfillment, data processing, web applications, any workflow



=================================================== Amazon Cognito ================================

• Give users an identity to interact with our web or mobile application

• Cognito User Pools:
  
   • Sign in functionality for app users
   • Integrate with API Gateway & Application Load Balancer

• Cognito Identity Pools (Federated Identity):

  • Provide AWS credentials to users so they can access AWS resources directly 
  • Integrate with Cognito User Pools as an identity providers

• Cognito vs IAM: “hundreds of users”, ”mobile users”, “authenticate with SAML”



----------------------------------- Cognito User Pools (CUP) – User Features

• Create a serverless database of user for your web & mobile apps • Simple login: Username (or email) / password combination

• Password reset

• Email & Phone Number Verification

• Multi-factor authentication (MFA)

• Federated Identities: users from Facebook, Google, SAML...


----------------------------------- Cognito User Pools (CUP) - Integrations

-- CUP integrates with API Gateway and Application Load Balancer



----------------------------------- Cognito Identity Pools (Federated Identities)

• Get identities for “users” so they obtain temporary AWS credentials

• Users source can be Cognito User Pools, 3rd party logins, etc...

• Users can then access AWS services directly or through API Gateway

• The IAM policies applied to the credentials are defined in Cognito

• They can be customized based on the user_id for fine grained control

• Default IAM roles for authenticated and guest users


------------------------------ LAB ---------------


 Cognito User Pools (CUP) 

 -- create user pool 

 -- open console and create user pool

 -- Cognito user pool sign-in options --> email 

 --  step 2 Required attributes --> add any one for user to register --> name 

 -- step 3 --> Send email with Cognito 

 -- step 5 --> name of userpool --> enable hosted UI  --> give domain name any thing eg demo-user-pool-cognito

 -- Initial app client --> public client name --> Allowed callback URL = localhost or anything like www.flipkart.comß

 -- once u created usewr pool --> open user pool ,u can see users and groups 

 -- create user --> set password , once u create user pool 

 --  go to google and search for amazon cognito ui --> view signup page in documentation 

 https://<your domain>/oauth2/authorize?response_type=code&client_id=<your app client id>&redirect_uri=<your callback url>

 -- go to user pools --> app integration -->  change ur details in the above link 

 
https://demo-user-pool-66.auth.ap-south-1.amazoncognito.com/oauth2/authorize?response_type=code&client_id=juroqdd4a95dac05aom4dvadj&redirect_uri=https://www.amazon.com

-- u will get domain name  in APP integration section

-- once the url is ready , paste the link in the browser 

-- this will pop-up sign in page , this is called hosted UI , enter ur details 

-- once u enter it will ask u to change password 

-- once u enter details , u will redirected to the link which u have given in call back url 

--  so delete cookies in ur browser and try to login again 

-- now try to sign up as new user and see how it works 

-- u can also do MFA for another layer of security 


-------------------------- web identity federation and token management 

-- when ever u are login into website some websites will ask u to login through the google , or facebook or gmail etc this is called Identity federation 

-- u can do in sign-in experience page in user pool 

-- u can create ur own log in page using Amplify , and other SDk's


----------------------- LAB , we are login through the SAML 2.0

-- now demo 

--- open IAM Identity center

--  choose Organization

-- Confirm your identity source = Identity Center directory  , if u want to change u can like Active direcory , etc ..

-- now create user in IICenter , IAM users and IIC users are different , copy details of user 

-- now open cognito --> select add user directories to ur app in drop down list --> create userpool --> choose Federated identity providers --> email --> SAML --> no MFA , this is not applicable for the federation users --> nxt --> Send email with Cognito --> skip Step 4 for now(we will do after creating SAML application in IIC) --> give username for pool --> domain name --> client name --> give callback URL --> advanced app client settings --> OAuth 2.0 grant type = choose only Implicit grant --> create userpool

-- now copy the user pool id , we will use them in SAML creation in IIC 

-- now create SAML 2.O application in IIC 

-- go to IIC --> choose application on left side --> add application --> I have an application I want to set up --> SAMl 2.O --> copy link of "IAM Identity Center SAML metadata file" --> Application ACS URL = <cognito domain url>/saml2/idpresponse (eg : https://myuserpool123.auth.ap-south-1.amazoncognito.com/saml2/idpresponse)--> Application SAML audience = urn:amazon:cognito:sp:<cognito pool id> 

-- once u submit --> under actions --> edit attribute mapping --> 

${user:subject}    format = persistent 

-- add new attribute 

email     ${user:email}    format = basic 

-- save changes 

-- now choose assign users --> select user that we have created --> assign user

-- let's configure IAM identity center as a SAML identity provider n cognito user pool 

-- go to cognito in console --> inside user pool --> sign-in experience --> Federated identity provider sign-in =  add Federated identity provider = SAML --> name of ur SAML --> Metadata document source = Enter metadata document endpoint URL (paste url that u have copied while creating SAML , eg : https://portal.sso.ap-south-1.amazonaws.com/saml/metadata/Mjk4MTMyMzY5NjI5X2lucy0zNzMxZjY2ODg5OTY3NjBh) --> saml attribute provide email--> add identity provider 

-- now u must add this identity provider to userpool app client , to do this 

-- go to cognito in console --> inside user pool --> app integration --> open client --> in hosted UI choose edit --> choose identity provider = SAML name that u have created --> save changes 

-- now do test 

-- go to cognito in console --> inside user pool --> app integration --> open client --> view hosted UI --> here u are do lofin with SAML give credentials and login 

-- it is same like when u want to sign up with some other website it will ask u to sign in with fackbook , google , twitter etc..... same we have done here 

-- by this we have successfully configured IIC as identity provider of ur userpool 




---------------------- Cognito Identity Pools(Federated Identity) which gives temporary access to AWS resources  LAB 


-- go to identity pools in cognito 

-- create identity --> Authenticated access --> amzn cognito user pool --> create new role ( give any name for ur role )--> user pool details --> Role settings = Use default authenticated role --> Claim mapping = Use default mappings --> name of ur identity pool --> create 

-- make sure u have not blocked pop up for ur browsers

-- now once token is generated , u can copy the token and do decode it in JWT website 

-- not gettig 

do in personal lap






==============================================================================CLOUDFRONT=====================================================================


-- it is global servie 

-- Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.

-- Amazon CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers.

-- CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.

-- Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity.

-- we can put it in front of our Auto Scaling group and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won't need to scale as much).

--  here u have s3 bucket which have static website in it 

-- if ur user in U.S , the user connect to our website through edge Location not directly to our website , if he cnnect direct from our region the latency will be high 

-- we need to setup CloudFront it create distributions and origin is = S3 bucket 

-- from edge Location to website the data is stored through the CDN , it is super speed managed by the AWS Network

-- once u create CF , it give nasty URL 

-- Improves read performance, content is cached at the edge

-- Improves users experience

-- 216 Point of Presence globally (edge locations)

-- DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall

-- CloudFront supports HTTP/RTMP protocol based requests,



-------------------------- Amazon CloudFront capabilities on routing, security, and high availability.

1 Amazon CloudFront High availability : 

    - Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover

EXP : 

-- You can set up Amazon CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.

-- To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.


2 Amazon CloudFront Routing : 

   - Amazon CloudFront can route to multiple origins based on the content type

EXP : 

-- You can configure a single Amazon CloudFront web distribution to serve different types of requests from multiple origins.

-- For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a Amazon CloudFront web distribution.


3 Amazon CloudFront security :

  - Use field level encryption in Amazon CloudFront to protect sensitive data for specific content

EXP :

-- Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack.

-- This encryption ensures that only applications that need the data—and have the credentials to decrypt it—are able to do so.

-- To use field-level encryption, when you configure your Amazon CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them.

-- You can encrypt up to 10 data fields in a request. (You can’t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.)




---------- CloudFront – Origins

1 S3 bucket
  • For distributing files and caching them at the edge
  • Enhanced security with CloudFront Origin Access Control (OAC) 
  • OAC is replacing Origin Access Identity(OAI)
  • CloudFront can be used as an ingress (to upload files to S3)

2 Custom Origin (HTTP)
  • Application Load Balancer
  • EC2 instance
  • S3 website (must first enable the bucket as a static S3 website)
  • Any HTTP backend you want




EPV : As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly.

ANS : Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy

EXP :

To restrict access to content that you serve from Amazon S3 buckets, you need to follow the following steps:

   1 Create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution.

   2 Configure your Amazon S3 bucket permissions so that Amazon CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the Amazon S3 bucket to access a file there.

-- After you take these steps, users can only access your files through Amazon CloudFront, not directly from the Amazon S3 bucket.

-- In general, if you’re using an Amazon S3 bucket as the origin for a Amazon CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. 

-- If you restrict access by using, for example, Amazon CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct Amazon S3 URL for the file. 

-- Instead, you want them to only access the files by using the Amazon CloudFront URL, so your content remains protected.



---------------------------------------- Use Amazon CloudFront signed URLs

-- Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee.

-- To securely serve this private content by using Amazon CloudFront, you can do the following:

  - Require that your users access your private content by using special Amazon CloudFront signed URLs or signed cookies.
  - A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. So this is a correct option.


---------------------------------------- Use Amazon CloudFront signed cookies

-- Amazon CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. So this is also a correct option.



-----------------------------------  CloudFront vs S3 Cross Region Replication

CloudFront:
- Global Edge network
- Files are cached for a TTL (maybe a day)
- Great for static content that must be available everywhere

S3 Cross Region Replication:
- Must be setup for each region you want replication to happen
- Files are updated in near real-time
- Read only 
- Great for dynamic content that needs to be available at low-latency in few regions



----------- CloudFront – ALB or EC2 as an origin

-- EC2 : EC2 Instances Must be Public

user <--> edge locatin <--> Allow Public IP of Edge Locations <--> EC2 Instances Must be Public

-- load balancer

user <--> edge locatin(it has public IP's) <--> Allow Public IP of Edge Locations <--> Application Load Balancer Must be Public <-->Allow Security Group of Load Balance<-->EC2 Instances
Can be Private .



------------------- CloudFront Geo Restriction

• You can restrict who can access your distribution

   • Allowlist: Allow your users to access your content only if they're in one of the countries on a list of approved countries.
   • Blocklist: Prevent your users from accessing your content if they're in one of the countries on a list of banned countries.

• The “country” is determined using a 3rd party Geo-IP database

• Use case: Copyright Laws to control access to content


-------------------  CloudFront – Cache Invalidations

-- In case you update the back-end origin, CloudFront doesn’t know about it and will only get the refreshed content after the TTL has expired

-- However, you can force an entire or partial cache refresh (thus bypassing the TTL) by performing a CloudFront Invalidation

-- You can invalidate all files (*) or a special path (/images/*)


-------------------  Global users for our application

-- You have deployed an application and have global users who want to access it directly.

-- They go over the public internet, which can add a lot of latency due to many hops

-- We wish to go as fast as possible through AWS network to minimize latency

--- Unicast IP vs Anycast IP

• Unicast IP: one server holds one IP address
• Anycast IP: all servers hold the same IP address and the client is routed to the nearest one



-------------------------------------------  AWS Global Accelerator

-- Leverage the AWS internal network to route to your application

-- 2 Anycast IP are created for your application

-- The Anycast IP send traffic directly to Edge Locations

-- The Edge locations send the traffic to your application

-- Works with Elastic IP, EC2 instances, ALB, NLB, public or private

-- Consistent Performance
• Intelligent routing to lowest latency and fast regional failover 
• No issue with client cache (because the IP doesn’t change) 
• Internal AWS network

-- Health Checks
• Global Accelerator performs a health check of your applications
• Helps make your application global (failover less than 1 minute for unhealthy) 
• Great for disaster recovery (thanks to the health checks)

-- Security
• only 2 external IP need to be whitelisted 
• DDoS protection thanks to AWS Shield


----------------------------------- AWS Global Accelerator vs CloudFront

-- They both use the AWS global network and its edge locations around the world

-- Both services integrate with AWS Shield for DDoS protection.

1 CloudFront
• Improves performance for both cacheable content (such as images and videos)
• Dynamic content(such as API acceleration and dynamic site delivery) 
• Content is served at the edge

2 Global Accelerator
• Improvesper formance for a wide range of applications over TCP or UDP
• Proxying packets at the edge to applications running in one or more AWS Regions.
• Good fit for non-HTTP use cases,such as gaming(UDP),IoT(MQTT),or Voice over IP 
• Good for HTTP use cases that require static IP addresses
• Good for HTTP use cases that required deterministic,fast regional failover


--------------------------- CloudFront Prac 

-- open S3 and create private Bucket 

-- as u create private bucket , no one can access ur url through the S3 

-- Only access through by Cloud-Front directly coz, it is privtae bucket and we did not enable Static hosting also 

-- go to CF in console 

-- create ditrubtion on CF 

-- Origina Domain = load balancer / S3 -- these are the places wher u can host ur applications 

-- OAC --> Create control settings --> do not change any n create 

-- it is created access from S3 

-- Compress objects automatically : CloudFront can automatically compress certain files that it receives from the origin before delivering them to the viewer. CloudFront compresses files only when the viewer supports it, as specified in the Accept-Encoding header in the viewer request.


-- Default root object - optional = index.html  -----> must and should u have to give this , otherwise u won’t get o/p 


----- once u create distrubtion , the S3 bucket policy wil gwt generated copy that policy and paste in bucket policy 

-- now ur appn is getting deployed all over the world 

-- through the CF url customers will able to connect ur webiste through the edge locations 

-- once u change the content of ur website and do uploud again n if u do refresh u won't get new content 

-- u have too do "invalidate the Cache" 

-- go to CF and create invalidation for /index.html , it will get latest file from the S3 and give latest content to customers 

-- By default, CloudFront caches files in edge locations for 24 hours. 




==============================Route 53===================================

-- it is global service 

-- it is DNS(DOMAIN NAME SERVICE)  Service in AWS

-- it keeps track of all hostnames and IP's

-- it converts IP to Host and Host IP to IP  

-- normally ur browser requires 2 things 

1 Host name 

2 IP 

-- the request wil go to LOcal DNS --> Root Name Server -->> Top level domain --> Name server--> SOA 

-- we only have to take care about server--> SOA  only this is configure in R53 

-- A highly available, scalable, fully managed and Authoritative DNS

-- Ability to check the health of your resources

-- The only AWS service which provides 100% availability SLA

-- Why Route 53? 53 is a reference to the traditional DNS port






-------------------------------------------- R53 Features

-- Domian registration 

-- DNS routing 

-- Health checks

-- Routng policies 

------- DNS ?

-- Domain Name System which translates the human friendly hostnames into the machine IP addresses

eg : www.google.com => 172.217.18.36

-- DNS is the backbone of the Internet

-- DNS uses hierarchical naming structure 
- .com
- example.com
- www.example.com
- api.example.com

--- DNS Terminologies

-- Domain Registrar : Amazon Route 53, GoDaddy, ...

-- DNS Records: A, AAAA, CNAME(Canonical Name), NS, ...

-- Zone File: contains DNS records

-- Name Server: resolves DNS queries (Authoritative or Non-Authoritative)

-- Top Level Domain (TLD): .com, .us, .in, .gov, .org, ...

-- Second Level Domain (SLD): amazon.com, google.com, ...

eg :  http://api.www.example.co  --> it is known as URL 

in this URL 

- .com  --> TOP levle domain
- example.com --> Second Level Domain (SLD)
- www.example.com ---> Sub Domain
- api.example.com ---> FQDN (Fully Qualified Domain Name)
- http ----> protocol


-------------------- Route 53 – Hosted Zones

-- A container for records that define how to route traffic to a domain and its subdomains

-- in R53 , we should create "Hosted Zones"(container of Recoreds) first 

-- Hosted Zone (Domain name = subbu.com ) are same 

• You pay $0.50 per month per hosted zone

-- 2 types of H.Z 

public : contains records that specify how to route traffic on the Internet (public domain names)

private : contain records that specify how you route traffic within one or more VPCs (private domain names)
        
        -- You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs.


IMP : For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:

enableDnsHostnames

enableDnsSupport




EPV (private zone) : A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved?

ANS : Enable DNS hostnames and DNS resolution for private hosted zones

-- DNS hostnames and DNS resolution are required settings for private hosted zones. DNS queries for private hosted zones can be resolved by the Amazon-provided VPC DNS server only.

------------------- 1 DNS hostnames: 

-- For non-default virtual private clouds that aren't created using the Amazon VPC wizard, this option is disabled by default.

-- If you create a private hosted zone for a domain and create records in the zone without enabling DNS hostnames, private hosted zones aren't enabled. To use a private hosted zone, this option must be enabled.

------------------- 2 DNS resolution: 

-- Private hosted zones accept DNS queries only from a VPC DNS server.

-- The IP address of the VPC DNS server is the reserved IP address at the base of the VPC IPv4 network range plus two.

--  Enabling DNS resolution allows you to use the VPC DNS server as a Resolver for performing DNS resolution. Keep this option disabled if you're using a custom DNS server in the DHCP Options set, and you're not using a private hosted zone.




EPV : The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this.

Which of the following settings of the VPC need to be enabled? (Select two)


ANS : enableDnsHostnames and enableDnsSupport

EXP : For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:

enableDnsHostnames

enableDnsSupport



-------------------------- Route 53 – Health Checks

• HTTP Health Checks are only for public resources

• Health Check => Automated DNS Failover:
   - Health checks that monitor an endpoint (application, server, other AWS resource)
   - Health checks that monitor other health checks (Calculated Health Checks)
   - Health checks that monitor CloudWatch Alarms (full control !!) – e.g., throttles of DynamoDB, alarms on RDS, custom metrics, ... (helpful for private resources)


• Health Checks are integrated with CW metrics


----------------------------------- Health Checks – Monitor an Endpoint

• About 15 global health checkers will check the endpoint health

   • Healthy/Unhealthy Threshold – 3(default)
   • Interval – 30 sec (can set to 10 sec – higher cost)
   • Supportedprotocol:HTTP, HTTPS and TCP
   • If > 18% of health checkers report the endpoint is healthy, Route 53 considers it Healthy. Otherwise, it’s Unhealthy
   • Ability to choose which locations you want Route 53 to use

• Health Checks pass only when the endpoint responds with the 2xx and 3xx status codes

• Health Checks can be setup to pass / fail based on the text in the first 5120 bytes of the response

• Configure you router/firewall to allow incoming requests from Route 53 Health Checkers


----------------------------------- Route 53 – Calculated Health Checks

• Combine the results of multiple Health Checks into a single Health Check

• You can use OR, AND, or NOT

• Can monitor up to 256 Child Health Checks

• Specify how many of the health checks need to pass to make the parent pass

• Usage: perform maintenance to your website without causing all health checks to fail




-------------------------- Health Checks – Private Hosted Zone

-- Route 53 health checkers are outside the VPC

-- They can’t access private endpoints (private VPC or on-premises resource)

IMP -- You can create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm itself

-- subbu.com -> elb gives nasty url --> ec2(it has appn)

-- whenever u type subbu.com in browser -> R53--> Hoste Zone --> ELB--> Nasty Url--> Ec2 Instance 



--- whenevr u creat "Public Hosted Zone " 2 records will create automatically 

1 NS(Name Server) Record = Pool of servers ( ~ it give 4 Servers) ,it identifies the server(website) 

2 SOA Record : Admin for Hosted Zones ( it has IP address) 


-- NS and SOA are default records , automatically created and managed by AWS 

-- u can purchase domains in 2 ways 

1 R53 

2  3rd party (go daddy) 


-- if u purchase in R53 ,it wil create hoste zones automaticlaly and  NS and SOA records created automatically 


-- whenever u purchase domain 3rd party domain registery , u should connect t AWS ,'coz ur request is here going to GO-daddy so u should connct to AWS 

-- HOW to conect ? 


-- once u create hosted zones in R53 ,the NS records wil crated (4 servers) , these servers u should updated in th Go-daddy , once u updated requests are redirected to AWS 


-- it has some latency 


-- NS and SOA records cannot be deleted 



----------------------------------Route53 Records------------

-- How you want to route traffic for a domain

-- Each record contains:
- Domain/subdomain Name – e.g., example.com
- Record Type – e.g., A or AAAA
- Value – e.g., 12.34.56.78
- Routing Policy – how Route 53 responds to queries
- TTL – amount of time the record cached at DNS Resolvers

-- Route 53 supports the following DNS record types:
- (must know)A /AAAA / CNAME / NS
- (advanced)CAA/DS/MX/NAPTR/PTR/SOA/TXT/SPF/SRV

---------------------- Route 53 – RecordTypes

-- we have A Record , AAAA Record , Cname recor , Alias Record  , MX Record 

-- Most common use is A record + Alias Record 

1 A Record : URL to IPv4


http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records it contains subbu.com --> Ec2 instance IP 

URL --> ipv4


2  AAAA Reacord : URL to IPv6


3 Alias Record : URL to Any Resource :

http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records(subbu.com --> ec2 ip or elb nasty url)

- url to ANY Resource 

-- Maps a hostname to an AWS resource

-- An extension to DNS functionality

-- Automatically recognizes changes in the resource’s IP addresses

-- Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com

-- Alias Record is always of type A/AAAA for AWS resources (IPv4 / IPv6)

-- You can’t set the TTL 


----------- Route 53 – Alias Records Targets

-- Elastic Load Balancers
-- CloudFront Distributions
-- API Gateway
-- Elastic Beanstalk environments
-- S3 Websites
-- VPC Interface Endpoints
-- Global Accelerator accelerator
-- Route 53 record in the same hosted zone Elastic Load Balancer

----- You cannot set an ALIAS record for an EC2 DNS name

ANS : You actually can create a CNAME (or even A) record without an Elastic IP. But every time your EC2 instance is restarted and so moved to another host system the IP address (and your external hostname) of your instance will change.

-- If you can live with this and accept the fact that during the TTL of your DNS record your instance is not reachable you can use the external IP and create an A record with your subdomain. 
-- But as DNS is not very fast in distributing changes (even with a low TTL you can't make sure every resolver handles the TTL correctly) you don't want to do such things most of the times. 
-- This is why AWS provides the Elastic IP - so your IP address which is in the DNS record never changes but the routing behind this IP address is changed by AWS if you reassign it to another instance (or you reboot your host). 
-- This routing change is only inside the AWS data centers and so it is quite fast (within a few seconds) and your instance is reachable again for all users.


4 CNAME Record : URL to URL : 

http://subbbu.com -- > it reach R53--> Hosted zone (subbu.com) --> records (subbu.com-->ELB Nasty URL)

URL to URL 

--- here 

-- http://subbbu.com  ----- Main Domain / Naked domain / Zone Apex Record 

-- admin.gopi.com , web.subbu.com ----- called Sub-domains 


-- CNAME Records are bilable , where as Alias Rec are free 

-- for Naked domain/Main domain we can not use CNAME , instead use Alias 

-- for sub-domains u can u can use CNAME

-- the target is a domain name which must have an A or AAAA record

-- Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)

-- Example: you can’t create for example.com, but you can create for www.example.com

-- Note: Always choose Alias over CNAME 



EPV : A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com.As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?

ANS : Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com

EXP : 

-- You can create an alias record at the top node of a DNS namespace, also known as the zone apex, however, you cannot create a CNAME record for the top node of the DNS namespace. 

-- So, if you register the DNS name covid19survey.com, the zone apex is covid19survey.com. You can't create a CNAME record for covid19survey.com, but you can create an alias record for covid19survey.com that routes traffic to www.covid19survey.com.


IMP NOTE : 

-- You should also note that Amazon Route 53 doesn't charge for alias queries to AWS resources but Route 53 does charge for CNAME queries.

-- Additionally, an alias record can only redirect queries to selected AWS resources such as Amazon S3 buckets, Amazon CloudFront distributions, and another record in the same Amazon Route 53 hosted zone;

-- however a CNAME record can redirect DNS queries to any DNS record. So, you can create a CNAME record that redirects queries from app.covid19survey.com to app.covid19survey.net.




-------------------------------------------- Route 53 – Records TTL (TimeTo Live)


-- TTL (time to live), is the amount of time, in seconds, that you want DNS recursive resolvers to cache information about a record. 

-- If you specify a longer value (for example, 172800 seconds, or two days), you reduce the number of calls that DNS recursive resolvers must make to Amazon Route 53 to get the latest information for the record.

-- This has the effect of reducing latency and reducing your bill for Route 53 service.

-- However, if you specify a longer value for TTL, it takes longer for changes to the record (for example, a new IP address) to take effect because recursive resolvers use the values in their cache for longer periods before they ask Route 53 for the latest information.

--  If you're changing settings for a domain or subdomain that's already in use, AWS recommends that you initially specify a shorter value, such as 300 seconds, and increase the value after you confirm that the new settings are correct.



-- High TTL – e.g., 24 hr
- Less traffic on Route 53
- Possibly outdated records

-- Low TTL – e.g., 60 sec.
- More traffic on Route 53 ($$)
- Records are outdated for less time 
- Easy to change records


-- Except for Alias records, TTL is mandatory for each DNS record

-- 


==================================================== ROUTING POLICIES

-- Define how Route 53 responds to DNS queries

-- Don’t get confused by the word “Routing”
   - It’s not the same as Load balancer routing which routes the traffic
   - DNS does not route any traffic, it only responds to the DNS queries

-- Route 53 Supports the following Routing Policies

• Simple
• Weighted
• Failover
• Latency based
• Geolocation
• Multi-Value Answer
• Geoproximity (using Route 53 Traffic Flow feature)



1               Simple Routing Policy

-- when u search for soemthing it wil goto R53 

http://subbbu.com --> r53 --> hosted ones --> Records --> ELB DNS name 

-- here we are doing simple routing so it is called "simple Routing Policy"

-- it does not have Health Checks 

-- if any down happens of websites u won't get any response from website 

• Typically, route traffic to a single resource

• Can specify multiple values in the same record

• If multiple values are returned, a random one is chosen by the client

• When Alias enabled, specify only one AWS resource

  



2               FailOver Routing Policy : (Active-passive)

-- u have to create 2 records for this policy 

eg : 

  subbu.com --> ELB DNS Name(Mumbai) -- primary record 

  subbu.com --> ELB DNS Name(ireland) -- Secondary record 


if one record get down , the request will automaticall go to second record 

-- for this u have to setup 2 sites in different regions for high availability if downtime of websites happens

-- if u have maintainnece page ,u go for S3 , u can redirect to S3 ur request  

-- it has health checks



3                  GeoLocation Routing Policy;


-- for eg u have cutomers all over the world if they are in china , japan , india, austria etc

-- here if some one is from japan , search for http://subbu.com but he wants in Japanese language 

-- so we have application in each region , we donot have other options , so in each region we do have  appn in region seperately , in this case CF is not work 

-- if user search for something from japan , he will get reply from japan only 

-- how to do? 

we have to create records for each regions eg : if u have 5 regions then create 5 records 

http://subbu.com-->Mumbai ELB/IP
http://subbu.com-->sydney ELB/IP
http://subbu.com-->us ELB/IP
http://subbu.com-->ALSKA ELB/IP
http://subbu.com-->japan ELB/IP


-- R53 will identify user's/request location automatically and redirect to the correct record 

-- Different from Latency-based!

-- This routing is based on user location

-- Specify location by Continent, Country or by US State (if there’s overlapping, most precise location selected)

-- Should create a “Default” record (in case there’s no match on location)

-- Can be associated with Health Checks


EPV : 1) Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights

- Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from.

- For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights.

2) Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution

- You can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution.

- When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located.

- If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following: Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries.

- 



4                Latency Routing Policy 



-- u have different Regions , u have appn in every region seperately

-- if he made any request it does not matter about the regions 

-- which regions give low latency from there resonxe will get 

-- Latency is based on traffic between users and AWS Regions

-- Can be associated with Health Checks (has a failover capability)



5       Mutli-Value Routing Policy 

--  Same as Simple routing Polcy but it has Health Checks 

-- share the traffic based on the traffic 

-- Use when routing traffic to multiple resources

-- Route 53 return multiple values/resources

-- Can be associated with Health Checks (return only values for healthy resources)

-- Up to 8 healthy records are returned for each Multi-Value query

-- Multi-Value is not a substitute for having an ELB



6      Weighted Routing Policy 

-- Control the % of the requests that go to each specific resource

-- Assign each record a relative weight:

      𝑡𝑟𝑎𝑓𝑓𝑖𝑐(%) = weight for a specific resource / sum of all the weights for all records

-- DNS records must have the same name and type

-- Can be associated with Health Checks

-- Use cases: load balancing between regions, testing new application versions...

-- Assign a weight of 0 to a record to stop sending traffic to a resource

-- If all records have weight of 0, then all records will be returned equally


7      Geoproximity (using Route 53 Traffic Flow feature)


-- Route traffic to your resources based on the geographic location of users and resources

-- Ability to shift more traffic to resources based on the defined bias

-- To change the size of the geographic region, specify bias values:
- To expand (1 to 99) – more traffic to the resource
- To shrink (-1 to -99) – less traffic to the resource

-- Resources can be:
- AWS resources (specify AWS region)
- Non-AWS resources (specify Latitude and Longitude)

IMP -- You must use Route 53 Traffic Flow to use this feature 


----- Route 53 – Traffic flow

• Simplify the process of creating and maintaining records in large and complex configurations

• Visual editor to manage complex routing decision trees

• Configurations can be saved as Traffic Flow Policy
  
   • Can be applied to different Route 53 Hosted Zones (different domain names)
   • Supports versioning





============================================== Routing Policies – IP-based Routing

-- Routing is based on clients’ IP addresses

-- You provide a list of CIDRs for your clients and the corresponding endpoints/locations (user-IP-to-endpoint mappings)

-- Use cases: Optimize performance, reduce network costs

-- Example: route end users from a particular ISP to a specific endpoint




===================================================================R53 prac=======================


-- to do this buy domain in R53 registry or do purchase from any other 3rd party services 

-- i Bought from NameCheap 

-- go to your domain --> manage domain --> custom domain in drop down menu

-- now create one ec2 instance with the user data 

-- make sure you are using linux 2 version 

#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html


-- follow below step only if ur getting apache test page , otherwise continue with next steps

IMP NOTE : If u r getting apache test page I instead of ur content) no need to worry , just follow below things -- Geek Dairy

-- connect Linux machine and login as super user sudo su

-- Method 1

-- removing/renaming Welcome Page

mv /etc/httpd/conf.d/welcome.conf /etc/httpd/conf.d/welcome.conf_backup

-- Make sure that Apache is restarted (as root) with the command:

systemctl restart httpd


-- Method 2

-- allow Indexes in /etc/httpd/conf.d/welcome.conf

-- Without an index at the DocumentRoot, the default Apache Welcome page will display unless /etc/httpd/conf.d/welcome.conf is modified to allow Indexes. Edit /etc/httpd/conf.d/welcome.conf to allow Indexes.

-- Comment the Options line (add a # mark) in /etc/httpd/conf.d/welcome.conf as shown below:

vi /etc/httpd/conf.d/welcome.conf
<LocationMatch "^/+$">
#   Options -Indexes
    ErrorDocument 403 /error/noindex.html
</LocationMatch>

              or 

you can enable Indexes by changing the – to a +

vi /etc/httpd/conf.d/welcome.conf
<LocationMatch "^/+$">
    Options +Indexes
    ErrorDocument 403 /error/noindex.html
</LocationMatch>

-- systemctl restart httpd

-- thats it !


-- now do create another ec2 with same userdata in 1b availability zone

-- make sure you have to give HTTP in SG 

-- now create one Load Balancer (application load balancer) 

-- it is not good to give nasty url of LB so do make over for this link 

-- go to R53 service 

-- You have to create one hosted zone in R53 

-- if You purchase domain in AWS itself automatically it will create Hosted zone for you 

-- if u buy some other places u have to create ur hosted zones 

-- create hosted zone, the name is  ( it should be same ur domain name)

-- mine is  subbucloud.lat

-- create hosted zone with public type 

-- once u created hosted zone it will create 2 Records automatically Ns and SOA

-- NS have 4 servers , u should give these servers in ur custom domain (3rd party) 

-- update ur Name servers in ur custom domain 

-- now create records to do make up of ur url 

-- here u can go with subdomian or with naked (main) Url 


1 simple routing policy

-- type sample is sub-domain and record type is A

-- choose Alias Always on , if u do not on u have to give ur IP but we have LB so choose Alias

-- in route traffic section 

select   alias to application and classic load balancer 

select AZ , LB and Routing policy 

-- create record


2  FailOver Routing Policy

-- FailOver it means in simple words if ur website is facing down period in one Region then , automatically the Load balancer will shift the request into the another another region 

-- in Genaral we have deployed our application in 2 regions for high availability , if one region goes down automatically the request shift to another region through the load balancer 

-- for this we have 2 primary site and secondary site , if primary site is not working then it will redirect to secondary site 

-- we know how to create load balancers in different regions but now we will try with the maintainance page which is stored in S3 

-- create one html files add some content related to the maintainance and do store in the s3 bucket (IMP: ur record name and bucket name should be same  then only it works )

-- make sure ur bucket name is ur domain for easy understanding purpose (ACL is enabled for bucket)

-- upload html file and make file has Public and enable static weh hosting ( in index document u have to give ur index.html extension like maintainace.html

-- create records for load balancer and s3 

-- for s3 record u have to slect end point as s3 website endpoint in dropdown below 

-- check ur url is working 

-- now stop the primary site and observe the changes 

-- if ur primary site is not working then it will shift to secondary site so , here u will get mainatainanace page , through the health checks the LOad balancer know that primary site is down so , it will shift traffic to the nxt site

-- getting maintainance page successfully 


-- u can also do same for geo location policy



========================================VPC(Virual Private Cloud)==================

-- it is like a virtual Data center of cloud 

-- Vpc is Regional, MaxVPC's per Region is 5 

VPC 
Internet Gateway
Public and private subnets  ( 200 subnets per VPC)
NAT Gateway
Route with Route Tables



-------------------------------------  Internet Gateway : it provide internet to the VPC , creat IG and attach to VPC

• Allows resources (e.g., EC2 instances) in a VPC connect to the Internet

• It scales horizontally and is highly available and redundant

• Must be created separately from a VPC

• OneVPC can only be attached to one IGW and vice versa

• Internet Gateways on their own do not allow Internet access... 

• Route tables must also be edited!

-- An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.

-- An Internet Gateway serves two purposes: 
   1 to provide a target in your VPC route tables for internet-routable traffic and 
   2 to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses.

-- Additionally, an Internet Gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic.



EPV : While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway.

Which conditions should be met for internet connectivity to be established? (Select two)

ANS : 

1 The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic

EXP : The network access control list (network ACL) that is associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity.

2 The route table in the instance’s subnet should have a route to an Internet Gateway

EXP : A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.

---------------------------------------------------------- Bastion Hosts

• We can use a Bastion Host to SSH into our private EC2 instances

• The bastion is in the public subnet which is then connected to all other private subnets

• Bastion Host security group must allow inbound from the internet on port 22 from restricted CIDR, for example the public CIDR of your corporation

• Security Group of the EC2 Instances must allow the Security Group of the Bastion Host, or the private IP of the Bastion host


---------------------------------------- NAT Instance (outdated, but still at the exam)

• NAT = Network Address Translation

• Allows EC2 instances in private subnets to connect to the Internet

• Must be launched in a public subnet

• Must disable EC2 setting: Source / destination Check

• Must have Elastic IP attached to it

• RouteTables must be configured to route traffic from private subnets to the NAT Instance

----------- NAT Instance – Comments

• Pre-configured Amazon Linux AMI is available

  • Reached the end of standard support on December 31, 2020

• Not highly available / resilient setup out of the box

  • You need to create an ASG in multi-AZ + resilient user-data script

• Internet traffic bandwidth depends on EC2 instance type

• You must manage Security Groups & rules:

• Inbound:
  • Allow HTTP / HTTPS traffic coming from Private Subnets
  • Allow SSH from your home network (access is provided through Internet Gateway)

• Outbound:
  • Allow HTTP / HTTPS traffic to the Internet


-------------------------------------------------- LAB 

-- create ur own VPC with CIDR 192.168.0.0/16

Tenancy defines how EC2 instances are distributed across physical hardware and affects pricing. There are three tenancy options available: Shared ( default ) — Multiple AWS accounts may share the same physical hardware. Dedicated Instance ( dedicated ) — Your instance runs on single-tenant hardware.

-- when u create vpc , automatically one main route table will created 

-- create 2 subnets (public and private)

-- by default these 2 subnets will route to the main route table 

-- create internet gateway and attach to the vpc

-- create RT public and private 

-- now go to public RT and add Route--> edit routes --> add route --> 0.0.0.0/0--> IGW  and also associate public subnet for public RT 

-- now go to private RT and add Route-->  associate private subnet for private RT 

-- now create NAT instance , ( Must be launched in a public subnet)

-- open ec2 --> browse more AMI's --> search for nat --> select amzn-ami-vpc-nat-2018.03.0.20221018.0-x86_64-ebs linux based instance 

-- create one new SG  allow SSH , All ICMP-IPv4 

-- edit vpc select ur vpc --> select Public subnet -->  Auto-assign public IP = enable --> select SG --> create instance 

-- now go to RT --> private RT --> routes --> edit routes --> add ur NAT instance --> save changes 

-- now turn off source and destination check for the NAT insatnce 

Each EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives. However, a NAT instance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable source/destination checks on the NAT instance.

-- Choose instance --> Actions--> Networking,-->Change source/destination check.

-- now launch instance on  private subnet , select ur defalut SG , disable public ip

-- now we have NAT instance and private instance , now check how will EC2 instances in private subnets to connect to the Internet

-- connect NAT instance , if ur ubable to connect , do connect through the putty

-- now try to connect private server

-- --  to conect private server , u just copy SSH Client id and paste in public istance 

eg : ssh -i "terraform-key.pem" ec2-user@192.168.2.31


-- create one file to store .pem value in the public instance 

-- vi terraform-key.pem and paste content of .pem file 

-- we do ot have read permissions for this , so create prmisions 

     chmod 400 terraform-key.pem

-- now try to connect --> Yes

-- once u connect succesfully ,it will connect to the private server

-- now we are in private server , u  can do install anything here 



------------------------------------------------ NAT Gateway 

----------- Basics of NAT Gateway

-- A NAT gateway supports the following protocols: TCP, UDP, and ICMP.

-- The job of NAT Gateway is provide internet to the Private Subnet and convert Private IP to Public IP 

-- The NAT Gateway Should put in Public subnet 

• AWS-managed NAT, higher bandwidth, high availability, no administration

• Pay per hour for usage and bandwidth

• NAT GW is created in a specific Availability Zone, uses an Elastic IP

• Can’t be used by EC2 instance in the same subnet (only from other subnets)

• Requires an IGW (Private Subnet => NATGW => IGW)

• 5 Gbps of bandwidth with automatic scaling up to 100 Gbps

• No Security Groups to manage / required

-- A NAT gateway supports the following protocols: TCP, UDP, and ICMP.

-- You cannot associate a security group with a NAT gateway.

-- You can use a network access control list (network ACL) to control the traffic to and from the subnet in which the NAT gateway is located.

-- A NAT gateway can support up to 55,000 simultaneous connections to each unique destination.

-- You cannot associate a security group with a NAT gateway. You can associate security groups with your instances to control inbound and outbound traffic.

-- You can use a network ACL to control the traffic to and from the subnet for your NAT gateway. NAT gateways use ports 1024–65535. 

-- You can't route traffic to a NAT gateway through a VPC peering connection.

-- You can't route traffic through a NAT Gateway when traffic arrives over a hybrid connection (Site to Site VPN or Direct Connect) through a Virtual Private Gateway. 

-- You can route traffic through a NAT Gateway when traffic arrives over a hybrid connection (Site to Site VPN or Direct Connect) through a transit gateway.



---------------------- NAT Gateway with High Availability

• NAT Gateway is resilient within a single Availability Zone

• Must create multiple NAT Gateways in multiple AZs for fault-tolerance

• There is no cross-AZ failover needed because if an AZ goes down it doesn't need NAT
(if an AZ goes down, all ec2 in that AZ will also be down, No point connecting ec2 in one AZ with NAT GW in other AZ)



-- Public subnet : it is exposd to the internet , public subnet raffic is roouted to Internet Gateway 

-- Private subnet : it is not exposd to the internet , private subnet raffic is roouted to NAT(NEtwork Addres Translator) Gateway 


EPV : A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture.


ANS : For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433

EXP : The above rules make sure that web servers are listening for traffic on all sources on the HTTPS protocol on port 443. The web servers only allow outbound traffic to MSSQL servers in Security Group B on port 1433.

For security group B: Add an inbound rule that allows traffic only from security group A on port 1433

EXP : The above rule makes sure that the MSSQL servers only accept traffic from web servers in security group A on port 1433.




-- Routing:  they r 2 types of R.T

1 Public RT : all traffic is routed to IGW , public subnet is associated

2 Private RT : ALl traffic is routed to NAT, Private Subnet is Associated 


--------------------------- NAT Gateway vs. NAT Instance

                                       NAT Gateway                                                               NAT Gateway

Availability                      Highly available within AZ (create in another AZ)                            Use a script to manage failover between instances

Bandwidth                         Up to 100 Gbps                                                                Depends on EC2 instance type

Maintenance                       Managed by AWS                                                                Managed by you (e.g., software, OS patches, ...)

Cost                              Per hour & amount of data transferred                                         Per hour, EC2 instance type and size, + network $

Public IPv4                       YES                                                                                   YES

Private IPv4                      YES                                                                                   YES

Security Groups                   NO                                                                                    YES

Use as Bastion Host?              NO                                                                                    YES
     

--------------------Steps to create our own VPC

1 crate VPC

2 Create IG and attachto the VPC 

3 Create Public and private Subnets

4 Create NAT Gateway(in public subnet) 

5 Create public RT --> all traffic is routed to IG , public subnet is associated 

6 create Private RT --> All Traffic is routed to NAT,Private Subnet is Associated 

7 Create a new Security Group, Allow RDP/SSH 

8 launch ec2 in Public subnet (Bastion Server) and another in Private Subnet 

9 try to connecting to private server through Bastion server 


NOTE : IG and NAT(Managed by AWS) Gateways are "Services not Servers" 




--- from ur laptap , u can directly connect to public internnet, but u can not connect private subnet directly 

-- to connect private subnet from internet(from latap , not from company)  , u have to launch one Ec2 in Public Subnet which is called "Baston server / Jump server" ( it is only for learnig purpose not real time scenario) 

-- first login into baston server and connect to private subnet 

-- in real scenario ur company is private network , u are using VPN Connection from company to AWS , so u no need to login into baston server , u can directly acces private servers 

-- if u want to connect client private subnets u first need to login into client's Baston server 






Question : one person came to u and ask , i have ec2 instnce in private subnet and i do not want internet access to my private subnet what u do? but he wants to Access AWS services from private subnet

SOln:

-- By default 3 sunets are there , these are allocated to each AZ , all these are public subnets only ( u rable to login into ec2) that's why it is public subnets 

-- we do not have NAT for Default VPC


but he wants to Access AWS services from private subnet ? 

-- in AWS We have VPC ENDPOINTS : it is use to Access only AWS services without NAT Gateway



----------------------------CIDR(Classless InterDomain Routing) 

-- Classless Inter-Domain Routing – a method for allocating IP addresses

-- private ip series starts with eg: 172.98.90.2/16


--  172.98.90.2 = Base IP 

-- /16 = SubnetMask

-- eg: u can create 3 subnets in VPC

192.168.1.0/24 = s1  -----> it it is routing to IG , so it is called Public subnet (1a AZ)

192.168.2.0/24 = s2 ------> it it is routing to NAT , so it is called private subnet (1B)

192.168.3.0/24 = s3 ------> it it is routing to VPCENDPOINTS , so it is called private subnet (1c)


-- for private purpose it starts with 192, 10, 172 

-- rest all are for public IP's

-- 1 subnet should be in one AZ at the same Time 

--- 1 AZ can have multiple subnets  



---------------------- Public vs. Private IP (IPv4)

• The Internet Assigned Numbers Authority (IANA) established certain blocks of IPv4 addresses for the use of private (LAN) and public (Internet) addresses

• Private IP can only allow certain values:

  • 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)----> big networks

  • 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)-------> AWS default VPC in that range

  • 192.168.0.0 – 192.168.255.255 (192.168.0.0/16) ------->  e.g., home networks




---------------------- VPC – Subnet (IPv4)

• AWS reserves 5 IP addresses (first 4 & last 1) in each subnet

• These 5 IP addresses are not available for use and can’t be assigned to an EC2 instance

• Example: if CIDR block 10.0.0.0/24, then reserved IP addresses are:

   • 10.0.0.0 – Network Address
   • 10.0.0.1 – reserved by AWS for the VPC router
   • 10.0.0.2 – reserved by AWS for mapping to Amazon-provided DNS
   • 10.0.0.3 – reserved by AWS for future use
   • 10.0.0.255 – Network Broadcast Address.AWS does not support broadcast in a VPC, therefore the address is reserved


• Exam Tip, if you need 29 IP addresses for EC2 instances:
  • You can’t choose a subnet of size /27 (32 IP addresses, 32 – 5 = 27 < 29)
  • You need to choose a subnet of size /26 (64 IP addresses, 64 – 5 = 59 > 29)

the number decreasing from 32 , 31, 30 ,29 -------> the more IP's we get , so choose according to ur requriments




------------------------------------- what is SubnetMask :this refers to how many IP's that u get inside subnet

-- we have formula 

-------------------------------   2^32-n


for eg /24 then 


2^32-24 == 2^8 = 256 IP's u wil get , u can launch 256 ec2 u can launch 


NOTE : u have to remove 5 IP's from each subnet maks , here u will get 251 


- 5 IP's are reserved for each subnets 

.0 = Network purpose 

.1 =  VPC Router

.2 = amazon -provided DNS 

.3 = Future purpose 

.255 = BroadCasting 


/32 = only one IP


---------------------------------- VPC in AWS – IPv4

• You can have multiple VPCs in an AWS region (max. 5 per region – soft limit)

• Max. CIDR per VPC is 5, for each CIDR:

 • Min. size is /28 (16 IP addresses)
 • Max. size is /16 (65536 IP addresses)

• Because VPC is private, only the Private IPv4 ranges are allowed:

  • 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)
  • 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)
  • 192.168.0.0 – 192.168.255.255 (192.168.0.0/16)    


• Your VPC CIDR should NOT overlap with your other networks (e.g., corporate)



-------------------------------------------steps to create VPC with CIDR

1  crate VPCwith CIDR (192.168.0.0/16)


2  Create IGW and attach to the VPC 

3  Create Public subnet(192.168.1.0/24)

4  Create Private subnet(192.168.2.0/24)

5  create NAT Gateway (in public subnet, and also we have to provide EIP for NAT) 

6  Create Public RT ---> al traffic is routed to IGW ,public subnet is associated 

7  create Private RT --> all traffic is routed to NAT, rivate Subnet is Associated 

8  Create a new Security Group , allow RDP/SSH 

9 Launch EC2 instance in public Subnet (Bastion Server) and Another in private Subnet 

10 Connect to BAstion first and then into the private server 



----------------------------------------- VPC Endpoints

• Every AWS service is publicly exposed (public URL)

• VPC Endpoints (powered by AWS PrivateLink) allows you to connect to AWS services using a private network instead of using the public Internet

• They’re redundant and scale horizontally

• They remove the need of IGW, NATGW, ... to access AWS Services

• In case of issues:
  • Check DNS Setting Resolution in your VPC 
  • CheckRouteTables

------------- Types of Endpoints

1 Interface Endpoints (powered by PrivateLink)

  • Provisions an ENI (private IP address) as an entry point (must attach a Security Group)
  • Supports most AWS services
  • $ per hour + $ per GB of data processed
  - dynamodb does not support for interface endpoints

2 Gateway Endpoints

  • Provisions a gateway and must be used as a target in a route table (does not use security groups)
  • Supports both S3 and DynamoDB
  • Free


------------------------------------------- Gateway or Interface Endpoint for S3?

• Gateway is most likely going to be preferred all the time at the exam

• Cost: free for Gateway, $ for interface endpoint

IMP : • Interface Endpoint is preferred access is required from on- premises (Site to Site VPN or Direct Connect), a different VPC or a different region



--------------------------------------- LAB for endpoints

-- create VPC with CIDR 192.168.0.0/16


-- One IGW attach to only one VPC at the same time 

-- create IGW , and attach to vpc that u have created 

-- create public subnet , give subnet CIDR as 192.168.1.0/24

-- create private subnet , give subnet CIDR as 192.168.2.0/24

-- now Create NAT Gateway, it should be in the public subnet only 

-- create RT , once new VPC created a Default RT created automatically 

-- create RT public and private 

-- now go to public RT and add Route--> edit routes --> add route --> 0.0.0.0/0--> IGW  and also associate public subnet for public RT 

-- now go to private RT and add Route--> edit routes --> add route --> 0.0.0.0/0--> NAT GAteway  and also associate private subnet for private RT 

-- now create ur own SG's with ur VPC

-- launch 1 instance , in public subnet 

-- launch 1 instance , in private  subnet and disable Public 

-- now connect to public subnet instance , sudo -s

--    try connect to private server 

--  to conect private server , u just copy SSH Client id and paste in public istance 

eg : ssh -i "terraform-key.pem" ec2-user@192.168.2.31


-- create one file to store .pem value in the public instance 

-- vi terraform-key.pem and paste content of .pem file 

-- we do ot have read permissions for this , so create prmisions 

     chmod 400 terraform-key.pem

-- now try to connect --> Yes

-- once u connect succesfully ,it will connect to the private server 

previusly it was root@ip-192-168-1-129 ec2-user 

-- once u connect to private server u will redirected to the private IP of private server like    
[ec2-user@ip-192-168-2-31 ~]$ 


--------------------------------------------------- ENDPOINTS

-- now u are in private server 

-- do sudo -s

-- this private server in the private subnet and Associated to Private Route Table , in Private RT all traffic is routed to NAT Gateway, so it has internet access 

-- test whether it has internet or not 

--  yum install -y git ,it installd so , it has internet

-- but i do not wnat internet access , so go to Private RT and delete entry for NAT 

-- not able to login 

--   i want only access to AWS service from this linux machine without internet access 

-- so first install AWS CLI 

-- we r in private server so enable NAT access to download AWS CLI in Private RT 

-- curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

-- unzip awscliv2.zip

sudo ./aws/install

-- follow above  steps to install 

-- now i do not want to give keys in the ec2 instance itself 'coz it is not safe to give keys here so , attach role to this insatce 

-- IAM -->roles --> Aws service (TE) --> user EC2 --> give full access of which service u want to access , (s3) --> create role 

-- attach role to the private server 

-- go to private server and try to access bucket 

-- create bucket in linux 

      aws s3 mb s3://vpc-bucket --region ap-south-1

-- u can able to create bucket it has NAT mens full internet access , check in s3 bucket is created 


-----------now i do not want intrnet Acces , so delete NAT entry in Private RT 

-- now we do not have internet Access but i want to acces the AWS service like s3 

-- create endpoint in the console 

-- ENDPOINTS are 3 typrs 

1 Interface-Endpoint : it uses ENI(elastic network interface) , it has private link 

2 Gateway Load Balancer :  it uses ENI(elastic network interface) , it has private link 

3 Gateway Endpoints : recommended , it works with Routing tables 

-- use Gateway Endpoint 

-- search for s3  and use Gateway Endpoints --> select VPC and attach RT to private Routing Tables 

-- EndPOINT use AWS inernal network toacces the services 

-- go n check in Private RT it will create one oute for ENDPOINT ( pl-78a54011)

-- pl = prefix list = group of network ranges  

-- we have created ENDPINTS , now try to Access 

-- now try to create bucket 

aws s3 mb s3://vpc-bucket19876 --region ap-south-1

-- u will able to create without NAT and internet 

-- aws s3 ls 

-- it works only to connect for AWS Services only 





------------------------------------------ VPC Flow Logs

• Capture information about IP traffic going into your interfaces:

 • VPC Flow Logs
 • Subnet Flow Logs
 • Elastic Network Interface (ENI) Flow Logs

• Helps to monitor & troubleshoot connectivity issues

• Flow logs data can go to S3, CloudWatch Logs, and Kinesis Data Firehose

• Captures network information from AWS managed interfaces too: ELB, RDS, ElastiCache, Redshift,WorkSpaces, NATGW,Transit Gateway...

• Helps to monitor & troubleshoot connectivity issues. Example: 
   • Subnets to internet
   • Subnets to subnets
   • Internet to subnets


------------------------------- LAB 

-- go to clouwatch and create log group 

-- CloudWatch --> logs --> loggroups --> create log group to store the logs 

-- create VPC with CIDR 192.168.0.0/16

-- create IGW , and attach to vpc that u have created

-- one RT will created when u create VPC , go to route table --> edit routes --> add internet gateway 

-- now create subnet for VPC , jst one for practice purpose 

-- as we know the vpc flowlogs can be created at 
  
   • VPC Flow Logs
   • Subnet Flow Logs
   • Elastic Network Interface (ENI) Flow Logs

-- in this demo we are creating at VPC level 

-- go back to VPC --> actions --> create flowlog for ACCEPT for this demo 

-- create one role for this demo 

-- create ec2 instance with user data  with in the subnet that u have created 

policy 

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogGroups",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    }
  ]
}   

-- trust policy

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "vpc-flow-logs.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
} 

-- attach role to the instance 

-- allow http and SSH for ur SG 

-- now go to CW and check logs 

-- it will create some logs , these are flow logs and has all the information , it looks like 

eg : 

2 298132369629 eni-05988048dea221b5a 3.5.211.132 192.168.1.77 443 36382 6 741 6075864 1708930056 1708930116 ACCEPT OK

2 298132369629 eni-05988048dea221b5a 192.168.1.77 3.5.211.132 36382 443 6 96 5957 1708930056 1708930116 ACCEPT OK


VPC Flow Logs Syntax

2 298132369629 eni-05988048dea221b5a 3.5.211.132 192.168.1.77 443 36382 6 741 6075864 1708930056 1708930116 ACCEPT OK


explanation : 

2                     =  version 
298132369629          =  account-id
eni-05988048dea221b5a =  interface-id
3.5.211.132           =  srcaddr
192.168.1.77          =  dstaddr
443                   =  srcport 
36382                 =  dstport
6                     =  protocol
741                   =  packets
6075864               =  bytes
1708930056            =  start
1708930116            =  end
ACCEPT                =  action
OK                    =  log-status


• srcaddr & dstaddr – help identify problematic IP

• srcaddr & dstaddr – help identify problematic IP

• Action – success or failure of the request due to Security Group / NACL

• Can be used for analytics on usage patterns, or malicious behavior

• Quer y VPC flow logs using Athena on S3 or CloudWatch Logs Insights



------------------------------------------------------------ AWS Site-to-Site VPN

-- A VPN connection refers to the connection between your VPC and your own on-premises network. Site-to-Site VPN supports Internet Protocol security (IPsec) VPN connections.

-- By default, instances that you launch into an Amazon VPC can't communicate with your own (remote/ on-premises) network.

-- VPN connection: A secure connection between your on-premises equipment and your VPCs.

-- VPN tunnel: An encrypted link where data can pass from the customer network to or from AWS.

-- Customer gateway: An AWS resource which provides information to AWS about your customer gateway device.

-------

1  Virtual Private Gateway (VGW)
   • VPN concentrator on the AWS side of the VPN connection
   • VGW is created and attached to the VPC from which you want to create the Site-to-Site VPN connection
   • Possibility to customize the ASN (Autonomous System Number)

2  Customer Gateway (CGW)
   • Software application or physical device on customer side of the VPN connection


-------------- Site-to-Site VPN Connections

• Customer Gateway Device (On-premises)
   • What IP address to use?
      • Public Internet-routable IP address for your Customer Gateway device
      • If it’s behind a NAT device that’s enabled for NAT traversal (NAT-T), use the public IP address of the NAT device

• Important step: enable Route Propagation for the Virtual Private Gateway in the route table that is associated with your subnets

• If you need to ping your EC2 instances from on-premises, make sure you add the ICMP protocol on the inbound of your security groups


---- The following are the key concepts for Site-to-Site VPN:

1 VPN connection: A secure connection between your on-premises equipment and your VPCs.

2 VPN tunnel: An encrypted link where data can pass from the customer network to or from AWS.

     Each VPN connection includes two VPN tunnels which you can simultaneously use for high availability.

3 Customer gateway: An AWS resource which provides information to AWS about your customer gateway device.

4 Customer gateway device: A physical device or software application on your side of the Site-to-Site VPN connection.

5 Target gateway: A generic term for the VPN endpoint on the Amazon side of the Site-to-Site VPN connection.

6 Virtual private gateway: A virtual private gateway is the VPN endpoint on the Amazon side of your Site-to-Site VPN connection that can be attached to a single VPC.

7 Transit gateway: A transit hub that can be used to interconnect multiple VPCs and on-premises networks, and as a VPN endpoint for the Amazon side of the Site-to-Site VPN connection.



--------- Site-to-Site VPN limitations

A Site-to-Site VPN connection has the following limitations.

   -- IPv6 traffic is not supported for VPN connections on a virtual private gateway.

   -- An AWS VPN connection does not support Path MTU Discovery.


--- What is IPSec VPN?

An IPSec VPN is a VPN software that uses the IPSec protocol to create encrypted tunnels on the internet. It provides end-to-end encryption, which means data is scrambled at the computer and unscrambled at the receiving server.


-- What is the meaning of IPSec?

IPSEC stands for IP Security. It is an Internet Engineering Task Force (IETF) standard suite of protocols between 2 communication points across the IP network that provide data authentication, integrity, and confidentiality.



-- we can connect the services only which are present in the VPC only 




------------------------------------- LAB 

STEP 1 (AWS SIDE)

--  for this we do not have on-premises , so use another region as ur on-premises

-- first go to mumbai region --> create vpc with 

-- create one subnet with 10.1.0.0/24

-- now create IGW and attach to the VPC 

-- now create RT and add subnet association and IGW 

STEP 2 : (Customer Side)

-- now go to singapore region , go to vpc and create vpc with 10.2.0.0/16

-- create subnet with 10.2.0.0/24

-- now create IGW and attach to the VPC 

-- now create RT and add subnet association and IGW 

STEP 3 : 

-- now in singapore region , create one ec2 instance (amazon linux 2 , kernal version 5.10)

-- create one new SG , wich allows SSH , ALL TCP and ALL ICMP - IPv4

-- launch instance 


STEP 4 

-- now go to mubai region 

-- VPC --> Virtual private network (VPN) --> create Virtual private Gateway and attach to vpc

-- now create customer gate way , in the palce of IP address (copy IPV4 of singapore ec2 address and paste it), no need to give Certificate 

-- now create site to site vpn connection 

-- Create VPN connection --> Target gateway type = VPG --> Customer gateway =Customer gateway ID --> Routing options = static --> Static IP prefixes = subnet prefic of singapore region (10.2.0.0/16)

-- it will take 4-5 min to create 

-- now open Route table in mumbai region --> route Propagation , it will show u VPW automatically --> edit --> enable --save 

-- now go to site to site vpn connection --> download configuration --> vendor = generic --> download 

-- it will contails all the details which is necessary to the vpn connection 


STEP 5 


-- now go to singapore region 

-- connect ec2 instance 

Installation of Openswan 

openswan : Openswan is an IPsec implementation for Linux. It has support for most of the extensions (RFC + IETF drafts) related to IPsec, including IKEv2, X.509 Digital Certificates, NAT Traversal, and many others.



1 sudo su 

2 yum install openswan -y

3 vim /etc/ipsec.conf
   In /etc/ipsec.conf uncomment following line if not already uncommented:
                " include /etc/ipsec.d/*.conf "

4 vim /etc/sysctl.conf --> add below lines in that file 
   net.ipv4.ip_forward = 1
   net.ipv4.conf.all.accept_redirects = 0
   net.ipv4.conf.all.send_redirects = 0

5 service network restart

6 vim /etc/ipsec.d/aws-vpn.conf

   conn Tunnel1
        authby=secret
        auto=start
        left=%defaultroute
        leftid=Customer end Gateway VPN public IP
        right=AWS Virtual private gateway ID- public IP
        type=tunnel
        ikelifetime=8h
        keylife=1h
        phase2alg=aes128-sha1;modp1024
        ike=aes128-sha1;modp1024
        keyingtries=%forever
        keyexchange=ike
        leftsubnet=Customer end VPN CIDR
        rightsubnet=AWS end VPN CIDR
        dpddelay=10
        dpdtimeout=30
        dpdaction=restart_by_peer
-- here update the values of left , right ids and left and right subnets  , from the download configuration file copy n paste values 
-- in configuration file 
#3: Tunnel Interface Configuration
Outside IP Addresses: = public ip 
inside  IP Addresses: = private ip

now do copy Outside IP Addresses of customer gateway and paste in leftid

now do copy Outside IP Addresses of Virtual Private Gateway	 and paste in right 

also update left and right subnet ids of ur vpc 

left subnet=Customer end VPN CIDR (on-premises )
 rightsubnet = AWS end VPN CIDR (AWS Side)


 conn Tunnel1
        authby=secret
        auto=start
        left=%defaultroute
        leftid=52.221.225.18
        right=3.6.30.187
        type=tunnel
        ikelifetime=8h
        keylife=1h
        phase2alg=aes128-sha1;modp1024
        ike=aes128-sha1;modp1024
        keyingtries=%forever
        keyexchange=ike
        leftsubnet=10.2.0.0/16
        rightsubnet=10.1.0.0/16
        dpddelay=10
        dpdtimeout=30
        dpdaction=restart_by_peer

paste these values in the /etc/ipsec.d/aws-vpn.conf 


7 vim /etc/ipsec.d/aws-vpn.secrets
customer_public_ip aws_vgw_public_ip: PSK "shared secret" -- this is format n update the IP'S

-- copy Outside IP Addresses of customer gateway and paste in customer_public_ip
-- copy Outside IP Addresses of Virtual Private Gateway	 and paste in aws_vgw_public_ip

eg : customer_public_ip aws_vgw_public_ip: PSK "shared secret" 
     52.221.225.18 3.6.30.187: PSK "ochAgBIM5pI3Rtzd1mwUKxW7ioiS0twD"

-- "shared secret"  = IPSec Tunnel #1 --> Pre-Shared Key  

8 Commands to enable/start ipsec service

   chkconfig ipsec on
   service ipsec start
   service ipsec status




-- now go to mumbai region  --> site to site vpn --> tunnel --> tunnel1 is UP 


Step 6 

-- go to mumbai region --> create ec2 instance with ur vpc configuration

-- in Sg add ICMP ipv4 

-- copy private IP of the instance 

--  go to sinagpore instance connect --> ping <private IP of mumbai instance>

u will get responce 


--------------- done --------------------------




============================================== Transferring large amount of data into AWS ==========================================

• Example: transfer 200 TB of data in the cloud. We have a 100 Mbps internet connection.

• Over the internet / Site-to-Site VPN: 

   • Immediate to setup
   • Will take 200(TB)*1000(GB)*1000(MB)*8(Mb)/100 Mbps = 16,000,000s = 185d

• Over direct connect 1Gbps: 

  • Long for the one-time setup (over a month)
  • Will take 200(TB)*1000(GB)*8(Gb)/1 Gbps = 1,600,000s = 18.5d

• Over Snowball:

  • Will take 2 to 3 snowballs in parallel
  • Takes about 1 week for the end-to-end transfer
  • Can be combined with DMS

• For on-going replication / transfers: Site-to-SiteVPN or DX with DMS or DataSync




---------------------------------------------------- AWS VPN CloudHub -------------------------------

• Provide secure communication between multiple sites, if you have multiple VPN connections

• Low-cost hub-and-spoke model for primary or secondary network connectivity between different locations (VPN only)

  -- the hub and spoke model provides a means of distribution that relies on a central location (the hub) and a number of spokes leading out from that hub.


• It’s a VPN connection so it goes over the public Internet

• To set it up,connect multiple VPN connections on the same VGW, setup dynamic routing and configure route tables


-- If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. 

-- This enables your remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. 

-- The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. 

-- This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.





------------------------------------------------- Direct Connect (DX) -----------------------------


IMP to know , they are 5 parts in aws to work on hybrid networking 

1 Direct connect 

2 site 2 site vpn connection

3 client vpn 

4 open vpn (3rd party)

5 bastion host


- apart this DX , remaining are wotk through the internet 

- only DX works with Intranet , which means diect connection to ur server and aws network through the cables(wired connection), all the cost will be paid by customer , it is one time set up




• Provides a dedicated private connection from a remote network to your VPC

• Dedicated connection must be setup between your DC and AWS Direct Connect locations

• You need to setup a Virtual Private Gateway on your VPC

• Access public resources (S3) and private (EC2) on same connection

• Use Cases:
   • Increase bandwidth throughput - working with large data sets – lower cost
   • More consistent network experience - applications using real-time data feeds 
   • Hybrid Environments (on prem + cloud)

• Supports both IPv4 and IPv6

-- AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS. 

-- Using AWS Direct Connect, data that would have previously been transported over the internet is delivered through a private network connection between your on-premises data center and AWS.

-- Data transfer pricing over AWS Direct Connect is lower than data transfer pricing over the internet,


------- Direct Connect Gateway

• If you want to setup a Direct Connect to one or more VPC in many different regions (same account), you must use a Direct Connect Gateway


------ Direct Connect – Connection Types

1 Dedicated Connections: 1Gbps,10 Gbps and 100 Gbps capacity

   • Physical ethernet port dedicated to a customer  
   • Request made to AWS first, then completed by AWS Direct Connect Partners

2 Hosted Connections: 50Mbps, 500 Mbps, to 10 Gbps
  
   • Connection requests are made via AWS Direct Connect Partners 
   • Capacity can be added or removed on demand
   • 1, 2, 5, 10 Gbps available at select AWS Direct Connect Partners

• Lead times are often longer than 1 month to establish a new connection


--------- Direct Connect – Encryption

• Data in transit is not encrypted but is private

• AWS Direct Connect + VPN provides an IPsec-encrypted private connection

• Good for an extra level of security, but slightly more complex to put in place



-- we can connect the all aws services through the direct connection , eg dynamoDB , s3 , ec2 etc....... but in site 2 site , we can connect services only in the vpc 






EPV : To establish a private connection between your virtual private cloud (VPC) and the Amazon EFS(elastic file system) API, you can create an interface VPC endpoint. You can also access the interface VPC endpoint from on-premises environments or other VPCs using AWS VPN, AWS Direct Connect, or VPC peering.

-- AWS Direct Connect provides three types of virtual interfaces: public, private, and transit.

1 Public virtual interface

-- To connect to AWS resources that are reachable by a public IP address such as an Amazon Simple Storage Service (Amazon S3) bucket or AWS public endpoints, use a public virtual interface.

-- With a public virtual interface, you can:

   A Connect to all AWS public IP addresses globally.

   B Create public virtual interfaces in any Direct Connect location to receive Amazon’s global IP routes.

   C Access publicly routable Amazon services in any AWS Region (except the AWS China Region).


2 Private virtual interface

-- To connect to your resources hosted in an Amazon Virtual Private Cloud (Amazon VPC) using their private IP addresses, use a private virtual interface. With a private virtual interface, you can:

   A Connect VPC resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances or load balancers on your private IP address or endpoint.

   B Connect a private virtual interface to a Direct Connect gateway. Then, associate the Direct Connect gateway with one or more virtual private gateways in any AWS Region (except the AWS China Region).

   C Connect to multiple Amazon VPCs in any AWS Region (except the AWS China Region), because a virtual private gateway is associated with a single VPC.

Note: For a private virtual interface, AWS advertises the VPC CIDR only over the Border Gateway Protocol (BGP) neighbor. AWS can't advertise or suppress specific subnet blocks in the Amazon VPC for a private virtual interface.


3 Transit virtual interface

-- To connect to your resources hosted in an Amazon VPC (using their private IP addresses) through a transit gateway, use a transit virtual interface. With a transit virtual interface, you can:

A Connect multiple Amazon VPCs in the same or different AWS account using Direct Connect.

B Associate up to three transit gateways in any AWS Region when you use a transit virtual interface to connect to a Direct Connect gateway.

C Attach Amazon VPCs in the same AWS Region to the transit gateway. Then, access multiple VPCs in different AWS accounts in the same AWS Region using a transit virtual interface.


Note: For transit virtual interface, AWS advertises only routes that you specify in the allowed prefixes list on the Direct Connect gateway. For a list of all AWS Regions that offer Direct Connect support for AWS Transit Gateway, see AWS Transit Gateway support.




EPV : A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes.

ANS : Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region


EXP : For the given use case, the main pricing parameter while using the AWS Direct Connect connection is the Data Transfer Out (DTO) from AWS to the on-premises data center. 

-- DTO refers to the cumulative network traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed.

-- Each query response is 60 megabytes in size and each webpage for the visualization tool is 600 kilobytes in size.

-- If you deploy the visualization tool in the same AWS region as the data warehouse, then you only need to pay for the 600 kilobytes of DTO charges for the webpage.

-- However, if you deploy the visualization tool on-premises, then you need to pay for the 60 MB of DTO charges for the query response from the data warehouse to the visualization tool.



EPV : To support critical production workloads that require maximum resiliency, a company wants to configure network connections between its Amazon VPC and the on-premises infrastructure. The company needs AWS Direct Connect connections with speeds greater than 1 Gbps.

ANS : Opt for two separate AWS Direct Connect connections terminating on separate devices in more than one Direct Connect location

EXP : Maximum resilience is achieved by separate connections terminating on separate devices in more than one location.

-- This configuration offers customers maximum resilience to failure. 

-- You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect locations.



------------------------------------------ Site-to-Site VPN connection as a backup ------

• In case Direct Connect fails, you can set up a backup Direct Connect connection (expensive), or a Site-to-Site VPN connection

    Corporate DC <-------(Direct Connect Primary Connection)------------> vpc
                 <-------(Site-to-Site VPN Backup Connection)------------>






------------------------------------------ Transit Gateway ---------------------------

• For having transitive peering between thousands of VPC and on-premises, hub-and-spoke (star) connection

• Regional resource, can work cross-region

• Share cross-account using Resource Access Manager (RAM)

-- What is AWS Resource Access Manager?

Ans : AWS Resource Access Manager (AWS RAM) helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs) in AWS Organizations, and with IAM roles and IAM users for supported resource types. You can use AWS RAM to share resources with other AWS accounts. This eliminates the need to provision and manage resources in every account. When you share a resource with another account, that account is granted access to the resource and any policies and permissions in that account apply to the shared resource.

• You can peer Transit Gateways across regions

• RouteTables:limit which VPC can talk with other VPC

• Works with Direct Connect Gateway,VPN connections

• Supports IP Multicast (not supported by any other AWS ser vice)

-- You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. 

-- RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own.

-- You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps:

Step 1 : create a Resource Share, 

Step 2 : specify resources,

Step 3 : specify accounts. 

-- RAM is available to you at no additional charge


---------- Transit Gateway: Site-to-Site VPN ECMP

• ECMP = Equal cost multi-path routing

• Routing strategy to allow to forward a packet over multiple best path

• Use case: create multiple Site- to-Site VPN connections to increase the bandwidth of your connection to AWS





-------------------------------- VPC – Traffic Mirroring ----------

• Allows you to capture and inspect network traffic in your VPC

• Route the traffic to security appliances that you manage

• Capture the traffic
   
    • From (Source) – ENIs
    • To (Targets) – an ENI or a Network Load Balancer

• Capture all packets or capture the packets of your interest (optionally, truncate packets)

• Source and Target can be in the same VPC or different VPCs (VPC Peering)

• Use cases: content inspection, threat monitoring, troubleshooting, ...




==========================================VPC PEERING==============================

-- create peering can be done in same region , same account and different account also 

-- create 2 vpc's , one in mumbai and one in ieland regions 

-- Two VPC CIDR should be UNIQUE 

-- u can connect to ur servers n vpc which is another region through the VPC Peering 

-- create 2 vpc in different region 

-- follow above steps for creating all requriments IGW ,create public nad private subnets, NAT , Public and private RT's ssociate route tables and Subnets and SG and 2 instances  for vpc 

-- do the same things in other region CIDR 192.169.0.0/16, u can do in same region or u can do in another account also ( no need to create public instance(baston)) , create private subnet only 

-- You must update route tables in each VPC’s subnets to ensure EC2 instances can communicate with each other

-- once u do all setup 

--  go to mumbai region connect BAston server first ,sudo -s

-- create one file to store terraform-key.pem value , vi terraform-key.pem

-- copy SSH Client link of private server 

-- chmod 400 terraform-key.pem

-- now u are able to connect private server from the baston server 

------- now do sudo -s

-- now frm mumbai u should get connect to the ireland 

-- go and copy SSH CLient of ireland private instance 


-- now go to mumbai location and open peering connection , once u created Peering conection 

-- go to VPC and check in peering and do accept 

-- once u accept , open private RT and dd route of VPC trafic of ireland (192.169.0.0/16) 

-- do same i ireland private RT (192.168.0.0/16) 


-- now go to SG of mumbai --> add 192.169.0.0/16 in inbound rules 

-- do vice versa in ireland SG 192.168.0.0/16

-- once u do u can able to login to irland vpc private server 




-------- if u want to create vpc for ur company 

u have to create 

1 Virtual Private Gateway 

2 Customer Gateway 

3 Site-to-Site Vpn --> ownlaod configuration n give it to the comonay network admin to setup VPN Connction from Company to AWS 



------------------------------------------ What is IPv6? -----------------------------------------

• IPv4 designed to provide 4.3 Billion addresses (they’ll be exhausted soon), it is 32 bit

• IPv6 is the successor of IPv4

• IPv6 is designed to provide 3.4 × 10^38, unique IP addresses ,  128 bit

• Every IPv6 address is public and Internet-routable (no private range)

• Format  x.x.x.x.x.x.x.x (x is hexadecimal, range can be from 0000 to ffff)

• Examples:
   
    • 2001:db8:3333:4444:5555:6666:7777:8888
    • 2001:db8:3333:4444:cccc:dddd:eeee:ffff
    • :: = all 8 segments are zero
    • 2001:db8:: = the last 6 segments are zero
    • ::1234:5678 = the first 6 segments are zero
    • 2001:db8::1234:5678 = the middle 4 segments are zero

---------------------------- IPv6 in VPC -----------------

• IPv4 cannot be disabled for your VPC and subnets

• You can enable IPv6 (they’re public IP addresses) to operate in dual-stack mode

• Your EC2 instances will get at least a private internal IPv4 and a public IPv6

• They can communicate using either IPv4 or IPv6 to the internet through an Internet Gateway


---------------------- IPv6 Troubleshooting 

• IPv4 cannot be disabled for your VPC and subnets

• So, if you cannot launch an EC2 instance in your subnet
  
    • It’s not because it cannot acquire an IPv6 (the space is very large)
    • It’s because there are no available IPv4 in your subnet

• Solution: create a new IPv4 CIDR in your subnet



-------------------------------------------------- Egress-only Internet Gateway -------------------------

• Used for IPv6 only

• (similar to a NAT Gateway but for IPv6)

• Allows instances in your VPC outbound connections over IPv6 while preventing the internet to initiate an IPv6 connection to your instances

• You must update the Route Tables



====================================== VPC Section Summary =======================================

• CIDR – IP Range

• VPC – Virtual Private Cloud => we define a list of IPv4 & IPv6 CIDR

• Subnets – tied to an AZ, we define a CIDR

• Internet Gateway – at the VPC level, provide IPv4 & IPv6 Internet Access

• Route Tables – must be edited to add routes from subnets to the IGW,VPC Peering Connections,VPC Endpoints, ...

• Bastion Host – public EC2 instance to SSH into, that has SSH connectivity to EC2 instances in private subnets

• NAT Instances – gives Internet access to EC2 instances in private subnets. Old, must be setup in a public subnet, disable Source / Destination check flag

• NAT Gateway – managed by AWS, provides scalable Internet access to private EC2 instances, when the target is an IPv4 address

• NACL – stateless, subnet rules for inbound and outbound, don’t forget Ephemeral Ports

• Security Groups – stateful, operate at the EC2 instance level

• VPC Peering – connect two VPCs with non overlapping CIDR, non-transitive

• VPC Endpoints – provide private access to AWS Services (S3, DynamoDB, CloudFormation, SSM) within a VPC

• VPC Flow Logs – can be setup at the VPC / Subnet / ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, analyze using Athena or CloudWatch Logs Insights

• Site-to-Site VPN – setup a Customer Gateway on DC, a Virtual Private Gateway on VPC, and site-to-site VPN over public Internet

• AWS VPN CloudHub – hub-and-spoke VPN model to connect your sites

• Direct Connect – setup a Virtual Private Gateway on VPC, and establish a direct private connection to an AWS Direct Connect Location

• Direct Connect Gateway – setup a Direct Connect to many VPCs in different AWS regions

• AWS PrivateLink / VPC Endpoint Services:

    • Connect services privately from your service VPC to customers VPC
    • Doesn’t need VPC Peering, public Internet, NAT Gateway, Route Tables
    • Must be used with Network Load Balancer & ENI

• ClassicLink – connect EC2-Classic EC2 instances privately to your VPC

• Transit Gateway – transitive peering connections for VPC,VPN & DX

• Traffic Mirroring – copy network traffic from ENIs for further analysis

• Egress-only Internet Gateway – like a NAT Gateway, but for IPv6 targets






============================================================== Networking Costs in AWS per GB - Simplified =====================


• Use Private IP instead of Public IP for good savings and better network performance

• Use same AZ for maximum savings (at the cost of high availability)


IMP : 

1  Free for traffic in same availability zone in same region 

2  Free if using private IP in same availability zone in same region 

3  $0.01 if Using private IP b/w two different availability zones in same region

4  $0.02 if using Public IP / Elastic IP b/w two different availability zones in same region

5  $0.02 Inter-region b/w two different regions 




============================================================ Minimizing egress traffic network cost ======================

• Egress traffic: outbound traffic (from AWS to outside)

• Ingress traffic: inbound traffic - from outside to AWS (typically free)

• Try to keep as much internet traffic within AWS to minimize costs

• Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network



-- Egress cost is high when application in corporate data center 

-- Egress cost is minimized when application in AWS only 



====================================================== Network Protection on AWS ====================

• To protect network on AWS, we’ve seen

   • Network Access Control Lists (NACLs)
   • Amazon VPC security groups
   • AWS WAF (protect against malicious requests)
   • AWS Shield & AWS Shield Advanced
   • AWS Firewall Manager (to manage them across accounts)

• But what if we want to protect in a sophisticated way( highly complicated or developed) our entire VPC?


------------------------------------ AWS Network Firewall 

• Protect your entire Amazon VPC

• From Layer 3 to Layer 7 protection

• Any direction, you can inspect

  • VPC to VPC traffic
  • Outbound to internet
  • Inbound from internet
  • To/fromDirectConnect&Site-to-SiteVPN

• Internally, the AWS Network Firewall uses the AWS Gateway Load Balancer
• Rules can be centrally managed cross- account by AWS Firewall Manager to apply to many VPCs internet


---------------------------- Network Firewall – Fine Grained Controls 


• Supports 1000s of rules

  • IP & port - example: 10,000s of IPs filtering
  • Protocol – example: block the SMB protocol for outbound communications
  • Stateful domain list rule groups: only allow outbound traffic to *.mycorp.com or third-party software repo
  • General pattern matching using regex

• Traffic filtering: Allow,drop,or alert for the traffic that matches the rules

• Active flow inspection to protect against network threats with intrusion - prevention capabilities (like Gateway Load Balancer, but all managed by AWS)

• Send logs of rule matches to Amazon S3, CloudWatch Logs, Kinesis Data Firehose




===================================================== Disaster Recovery & Migrations  ==================================

Disaster Recovery Overview

-- What is Disaster ?

Ans : lets say , let's suppose you hosted your application in a region and unfortunately that region met with the national calamity or disaster like a flood or earthquake , or it could be a data center that lost power or due to cable damage it lost the network connectivity , it could be the most common disaster that is human action like someone actually planned andplotted a bad configuration or it could be damaged due to unauthorized access by deleting your data or even it could erase all your customer information 

IMP NOTE : high Availability is not a Disaster Recovery , High availability means running ur application atleast 2 data centers(multi-AZ), the goal of HA is survive a data center lost 

-- Both are for same purpose but Different

-- first both monitor for failures 

-- second both deploy the resources at multiple locations and 

-- third obviously is automated failover where if one data store or resource fails you have the backups to take over 


----------------- so when it comes to availability 

-- you are more focused towards the components or resources of the workload to serve the customer demand and 

-- so that you can operate continuously without failing and you strive hard to meet the service level agreement of service availability 


----------------  so when it comes to Disaster Recovery

-- you need to focus on the time it takes to recover from a disaster 

-- you need to ensure the workload or resources that you have provisioned meets your availability objectives and

-- the focus is rightly on deploying discrete systems to multiple locations 

-- but the main objective here is also to have a multi-site active-active workload distribution


-- 1  so multi-site active active is a disaster recovery strategy to run the workload that you have in a way that it can serve requests in two or more distinct data centers or regions 

-- so this strategy enables your workload to remain available despite disaster events such as natural disaster technical failures or human action


2  point in time backup so which enables you and your customers to recover the backup data from a specified time within the retention period that you have set 

  -- so let's suppose you have your application hosted on a region with multiple availability zones with a proper scaling mechanism your users are very happy with the application status 

  -- if one of these availability zones that is data centers get affected and stops responding so a load balancer is smart enough to route the traffic to other instances in other availability zones and your users are still fine as they are able to access the application 


------- but what if the whole region gets affected what is going to happen in simple words your high availability is now zero availability isn't it , that's where disaster recovery might come in handy because disaster recovery might tell us to have a deployment strategy of multi-site deployment that is like deploying it in other regions as well as if your application is not responding on the region it can as well respond using other regions 


Step 3 : Are you Resilient enough?

-- when you think of trust in terms of your product you should always remember the word resiliency this word (an ability to recover from or adjust easily to misfortune or change.)

-- it's actual sense refers to the capacity to recover quickly from difficulties

-- it's same when it comes to your infrastructure as well so resiliency is the ability of a workload to recover from infrastructure or service disruptions and dynamically acquire computing esources to meet demand and mitigate disruptions such as misconfigurations or transient network issues 

-- always remember that things can go wrong at any point in time but the most important thing is how fast you can recover from the disaster that's where resiliency strategy comes into the picture.

-- disaster recovery and availability are a very important part of our resiliency strategy where disaster recovery focuses on how workload responds to disaster and how well it can recover from that on the other hand availability focuses on uptime or downtime for your resources over a period of time also known as mean value over a period of time.


resiliency = disaster recovery and availability , 

disaster recovery = RPO and RTO 

    disaster Recovery measures objectives for one-time events 

availability = MTBF (Mean time b/w failures) and MTTR(Mean time to recover)

formulas for 

MTBF = (Total working time - Total Breakdown time) / no. of breakdowns

MTTR = Total  maintainace time / No.of repairs 


------------------------- in order to understand how we can calculate the availability you need to understand how to calculate mtbf and mttr 

------------ 1 MTBF (uptime) = (Total working time - Total Breakdown time) / no. of breakdowns

eg : suppose you had a total working time of your application that you have hosted as 100 hours and out of which you had 5 hours of breakdown time so breakdown time imagine it there's a time period where your users were not able to access your application and there were 10 breakdowns in total so

     your mean time between failures will be 100 - 5 hours of breakdown time divided by 10 breakdowns so that is 9.5

     MTBF = 100 - 5 / 10    =   95 / 10 = 9.5 



------------ 2 MTTR(downtime) = Total  maintainace time / No.of repairs 


-- similarly if you have to calculate mttr or mean time to recover you have to check how much time did you spend to bring up your services or application and divide it by the number of repairs that you had to do 

eg : for example you spend around 10 hours in maintenance and it was around five times that you had to repair the system so mttr will be 10 divided by 5 that is 2 hours 


------------ 3  let's calculate availability yes availability is basically as the word tells you it's the amount or percentage of time or value on how available your services or application is to your user

   -- but you have to remember that it's not always that a disaster should occur to impact availability, it could be like for example 
   
   -- you have created an application design that supports a maximum load of 100 users but what will happen if you get around like 10 000 user requests at the same time so you would ask yourself why is it not working for your application to support that amount of load there is no calamity or earthquake or flood still your application is not available to all the users so you have to take some steps to improve

   IMP : availability is a measure to calculate the amount of time that your application is available for use divided by the total time or total amount of time it has been hosted,
   
   -- so internally it is translated to mean time between failures divided by the total of mean time between failures and mean time to recover

   -- formula for calculate Availability 

        Availability = (Available for time use) / (Toatl time) = MTBF / (MTBF + MTTR)

  -- IMP : 99.9 is three nines so this is one of the most desired availability that services are trying to achieve .

  EG : MTBF = 400 Hours 
       MTTR = 10 hours 

       Availability = MTBF / (MTBF + MTTR) 

       400 / (400 + 10) = 400 / 410 = 0.975 

  -- so u will get 97.5 % availability 


-- but availability can be measured with the response as well and not just time

   Availability = no.of successive Response / Valid Requests 

   -- successive Response = 500 

   -- Valid Requests = 510 requests 

   so , 500 / 510 = 0.980 = 98.0 % availability 
 

Step 4 : Resiliency OF the Cloud

-- resiliency of the cloud so what aws tells us is that resiliency is a shared responsibility between aws and you the customer so you must understand how disaster recovery and availability works in the shared responsibility model

-- you have to understand here is the phrase resiliency of the cloud so all the infrastructure that you see here the hardware software networking and the facilities that run aws cloud services and its resiliency is the responsibility of aws

-- so all these things that you see the hardware software networking and the facilities and all the services that are run on aws cloud and its resiliency is the responsibility of aws 

-- so if you're not able to launch an ec2 instance due to a region failure it's aws's responsibility to bring that up 

-- but if you lost your data that you had in your data stores and you didn't plan for disaster recovery , that is your responsibility to safeguard your data and to have disaster recovery in place that's not aws responsibility your data is your responsibility 


Step 5 : Resiliency IN the Cloud

-- so here if we have to take an example so aws gives you a service called elastic compute cloud that is ec2 but it's your responsibility of what you host there and the data that you store

-- if you don't host your application in multiple availability zones for high availability it's not aws's responsibility that your application went down it's your responsibility 

-- if you are using ephemeral stores it's not aws has a responsibility that you lost your customer data the configuration that you have is your responsibility 

-- the encryption of data is your responsibility, hosting your application in multiple regions and having a failover is your responsibility



Step 6 : Business Continuity Plan (BCP) 

-- aws tells us about bcp is that your disaster recovery strategy should be based on business requirements priorities

-- so "business continuity plan"(BCP) is the process involved in creating a system of prevention and recovery from potential threats to the company

-- when creating a disaster recovery strategy we must plan for recovery objectives and they are recovery time objective rto and recovery point objective that is rpo


Step 7 : Recovery Time Objective (RTO)

-- every BCP follows some Disaster Recovery Strategies

• Backup and Restore
• Pilot Light
• Warm Standby
• Hot Site / Multi Site Approach

        ------------------------------- (Faster RTO) ----------------------- >
        Backup and Restore    Pilot     Light Warm Standby     Hot Site / Multi Site Approach
        < ---------------------------- AWS Multi region --------------------- >

1 Backup and Restore (High RPO)
  
  -- RPO / RTO in hours 
  -- Lower priority use cases 
  -- provisions all aws resources after event 
  -- restoring backups after events 
  -- low cost $

2  Pilot Light 

  • A small version of the app is always running in the cloud
  • Useful for the critical core (pilot light)
  • Very similar to Backup and Restore
  • Faster than Backup and Restore as critical systems are already up

  --  RPO / RTO 10s of minutes 
  --  pilot light most core or critical components and services of your application are replicated in another region like your important data and storage replicas
  --  other services are turned off and used only during the testing time , 
  --  so here you have live data but considering some of the services are kept idle
  --  so it might cost you more than backup and restore but it will surely take less
  --  The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. 
  --  The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on can quickly ignite the entire furnace to heat up a house. 
  --  This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS.
  --  a small part of the backup infrastructure is always running simultaneously syncing mutable data (such as databases or documents) so that there is no loss of critical data.
  --  When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core. For Pilot light, RPO/RTO is in 10s of minutes
  

3  Warm Standby

   • Full system is up and running, but at minimum size
   • Upon disaster, we can scale to production load

   -- RPO / RTO in  minutes 
   -- here your services on another location or region are always running but it's a scaled down version of your actual setup 
   -- so this is very good for business critical applications and when you have a disaster you can scale up your resources and obviously it will cost you more than the previous two setups that we have but it is reasonable with recovery time and the recovery time is in minutes
   
4  Multi Site / Hot Site Approach

   • Very low RTO (minutes or seconds) – very expensive
   • Full Production Scale is running AWS and On Premise

   -- RPO / RTO Real-time 
   -- more cost 


step 8 : Recovery Point Objective (RPO)

-- that is the maximum acceptable amount of time since the last data recovery point 

eg :  imagine this scenario whenever you create a data set you keep a copy of that in another location 

-- so that in case you lose the original data you have a backup from which you can recover your lost data

-- you have to consider what your business outcome is and based on which you have to take the decision of which disaster recovery strategy should you choose 

-- we are talking about data loss before service interruption so having said that we all know that with multi site active active you get near zero data loss because you have near real time data replication so it may be around in seconds 

-- but backup and restore is considered to be most efficient when it comes to cost because your backups are created in the same region as their source and are also copied to another region but it gives you the most effective protection from disasters 

-- but the thing is that it'll take more time that's the only problem 

-- but the interesting thing for us is to see that both warm standby and pilot light have , similar data loss but the cost in complexity is less for pilot light compared to warm standby

-- so here both of them use near real-time data sync because both of them have live data with periodic backups 

-- but the difference in recovery cost comes because in pilot light most of the services are turned off or are kept idle compared to warm standby which has a scaled down version of its actual application  


Step 9: Disaster recovery options in the cloud  ( in -details)

-------- 1 Backup and restore 

-- in this will be to take the backup and restore it to another region along with the infrastructure that you have 

-- it is suitable approach for mitigating againast data loss or corruption

-- Backup strategy that runs periodically or is continuos 

-- rpo is the recovery point objective that will depend on how frequently you take the backup and that's where your point in time backup helps you 

-- for that we are using amazon s3 which gives us the feature of s3 cross region applications so we are storing ebs snapshots and the db snapshots on s3 which will be replicated across the region using crr and with the help of aws backup we can also copy our backups of our configurations across regions

-- with the help of aws backup we can also copy our backups of our configurations across regions

-- even though you use aws backup to restore the data it cannot be done automatically so in order to do that we have to make use of aws apis to automate that when it comes to taking backup and taking restore 

-- we can create jobs using apis which can be executed periodically in time based on a trigger point , so the aws backup sends the notification about the job once the backup is completed so it gets notified to the user using the aws sns notification service

-- then using the sns we can trigger the job by invoking lambda function for testing restore functionality and cleanup actions based on which we can perform the restore operation and up for the ebs block storage 


---------- 2  Pilot Light 

-- here we have the database and storage always active but the application servers which contain the code and configuration are turned off or kept idle 

-- so the first thing that you notice here is that until a disaster occurs we don't route traffic to the disaster recovery region as services and resources are kept idle there or they are turned off

-- but having said that you have the option to quickly provision the full scale of your production environment by switching on the resources that are currently turned off so that's a very good thing as well and

-- for data replication we can create a Aurora replicas by making use of the aws aurora global database that gives us the feature of asynchronous cross region replication 

-- for the routing we can also make use of global accelerators and based on the health check it can direct traffic to the appropriate endpoint 



------------ 3 Warm Standby 

-- this is similar to pilot light 

-- but the main difference here is that you can see the resources and the workload that you have in your standby location are scaled down 

-- but having said that unlike pilot light this environment with warm standby is completely functional but it's a scaled down version of it 

-- so in other words it is like less powerful than the actual environment but don't get confused here it's a copy of the actual environment but just a less powerful one

-- when you have a disaster or if you want to support more users or traffic you can also allow routing to this region by just scaling the resources 

-- route 53 is inactive for the production traffic but if you want you can also route the traffic by by allowing the traffic through this and if you want to support more number of traffic then you can also scale it up



------------- 4 multi site Active active / Hot Site Approach 

-- provides you zero downtime and near zero data loss  

-- with multi-site active active you can run your application in both regions simultaneously and that is the active active strategy

-- but there is one more advantage here so as we mentioned or as we discussed just now with multi-site active active you can serve traffic on multiple regions at the same time that's obviously an advantage 

-- so but having said that you can also serve traffic from a single region that is called hot standby active passive where you have an application hosted on multiple regions but but you can choose the one region that always stays active to serve the traffic




• Any event that has a negative impact on a company’s business continuity or finances is a disaster

• Disaster recovery (DR) is about preparing for and recovering from a disaster

• What kind of disaster recovery?

   • On-premise => On-premise: traditional DR, and very expensive
   • On-premise => AWS Cloud: hybrid recovery
   • AWS Cloud Region A => AWS Cloud Region B

• Need to define two terms:

  • RPO: Recovery Point Objective = The amount of data loss that would be acceptable for an organization as a consequence of a disaster. RPO is typically measured in seconds.
  • RTO: Recovery Time Objective  = The amount of time after a disaster in which business operations need to be resumed and resources need to be available for use. RTO can be in the order of minutes.



----------------------  RPO and RTO 


eg : 

---------------------------------- * ---------------------------------- % ----------------------------- $ --------------->



from * to % = DATA LOSS 

from % to $ = DOWN TIME 


-- here * = RPO (Recovery Point Objective) ,   $ = RTO (Recovery Time Objective) , and % = Disaster 



-------------------------------- Disaster RecoveryTips 

• Backup 

   • EBS Snapshots, RDS automated backups / Snapshots, etc...
   • Regular pushes to S3 / S3 IA / Glacier, Lifecycle Policy, Cross Region Replication
   • From On-Premise: Snowball or Storage Gateway

• High Availability
  
   • Use Route53 to migrate DNS over from Region to Region
   • RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3
   • Site to Site VPN as a recover y from Direct Connect

• Replication
   
   • RDS Replication (Cross Region), AWS Aurora + Global Databases
   • Database replication from on-premises to RDS
   • Storage Gateway

• Automation
  
  • CloudFormation / Elastic Beanstalk to re-create a whole new environment
  • Recover / Reboot EC2 instances with CloudWatch if alarms fail
  • AWS Lambda functions for customized automations



============================================================= AWS Backup ============================

• Fully managed service

• Centrally manage and automate backups across AWS services

• No need to create custom scripts and manual processes

• Supported services:
 • Amazon EC2 / Amazon EBS
 • Amazon S3
 • Amazon RDS (all DBs engines) / Amazon Aurora / Amazon DynamoDB 
 • Amazon DocumentDB / Amazon Neptune
 • Amazon EFS / Amazon FSx (Lustre & Windows File Server)
 • AWS Storage Gateway (Volume Gateway)   

 -- EFS , EC2 , DynamoDB = Service level backups , 
    for eg we always take backups of EFS files 

 -- RDS, EBS , AWS Storage gateway , amazon aurora = service level snapshots , 
    for eg we always take snapshots of EBS volumes 



• Supports cross-region backups

• Supports cross-account backups

• Supports PITR for supported services

• On-Demand and Scheduled backups

• Tag-based backup policies

• You create backup policies known as Backup Plans
 
   • Backup frequency (every 12 hours, daily, weekly, monthly, cron expression)
   • Backup window
   • Transition to Cold Storage (Never, Days,Weeks, Months,Years)
   • Retention Period (Always, Days,Weeks, Months,Years)


AWS Backup ----------Create Backup Plan (frequency, retention policy) -------> Assign AWS service -------Automatically backed up to-----> S3


------- Backup Plan : it allows you to configure when to take the backups , u can define frequency of taking backup and also retention policy 

------- backup vault ; -- it is like a container which stores all ur backups 

-------------------- AWS Backup Vault Lock


• Enforce a WORM (Write Once Read Many) state for all the backups that you store in your AWS Backup Vault

• Additional layer of defense to protect your backups against:
   • Inadvertent or malicious delete operations
   • Updates that shorten or alter retention Periods

• Even the root user cannot delete backups when enabled


----------------- new features 

Announcing New Features :

-- Support for EBS snapshot archive  Explore long-term, lower-cost archive storage lifecycle options for EBS snapshots

-- Restore testing  Periodically validate recovery readiness and gain insights into restore times

-- Jobs dashboard  Monitor backup health through visual representations of aggregated backup, copy, and restore metrics



---------------------------- LAB 

-- create one instance with tags

-- now go to aws backup --> settings --> configure resources select only ec2 and deselect remaining

-- now create on-demand backup

-- backup --> dashboard --> create on-demand backup 

-- select ec2 and instance id

-- give retention period 

-- create new backup vault , with default keys , and give tags

-- IAM role  = default 

-- create backup , wait for dew minutes to create 

-- once ur jobs is created 

-- go to backup vault --> inside ur vault --> one recovery point will get assigned, u can reciver ec2 from this recovery point 

-- instead of creating on-demand backup , we can also create backup plans and configure frequency and rentention period time 

-- go to backup plan --> Build a new plan 

-- Backup window  = give time according to ur time zone 

-- do not choose region for destination 

--  now go to backup plans --> assign resources  with default role 

-- in the tags <key> equals <ur value>

-- assign resources 

-- now go to ur backup valur --> select recovery point and see the backup type = manual 

--- now do restoring 

-- first terminate ec2 instance 

-- now go to backup --> backup vault --> recovery point --> restore 

-- it will create one restore job 

-- once it get completed , go n check the ec2 will get back , restore is successfull

-- this is how we can restore thr resource through aws backup , it is manual backup 

-- after sometime it will automatically starts one job for automated backup that we have created 

-- do check ur restore jobs for every 1 hr for automated restore jobs 

-- this is automated backups 

-- clear all resources that u have created 



============================================== More Solutions Architecture =========================

------------------ High Performance Computing (HPC)

• The cloud is the perfect place to perform HPC

• You can create a very high number of resources in no time

• You can speed up time to results by adding more resources

• You can pay only for the systems you have used

• Perform genomics, computational chemistry, financial risk modeling, weather prediction, machine learning, deep learning, autonomous driving


-------------------- Data Management & Transfer

• AWS Direct Connect:
  • Move GB/s of data to the cloud, over a private secure network

• Snowball & Snowmobile
  • Move PB of data to the cloud

• AWS DataSync
  • Move large amount of data between on-premises and S3, EFS, FSx for Windows


------------------ Compute and Networking

• EC2 Instances:
   • CPU optimized, GPU optimized
   • Spot Instances / Spot Fleets for cost savings + Auto Scaling

• EC2 Placement Groups: Cluster for good network performance(same rack and same AZ)


• EC2 Enhanced Networking (SR-IOV)
   • Higher bandwidth, higher PPS (packet per second), lower latency
   • Option 1: Elastic Network Adapter (ENA) up to 100 Gbps
   • Option 2: Intel 82599 VF up to 10 Gbps – LEGACY

• Elastic Fabric Adapter (EFA)
   • Improved ENA for HPC, only works for Linux
   • Great for inter-node communications, tightly coupled workloads
   • Leverages Message Passing Interface (MPI) standard
   • Bypasses the underlying Linux OS to provide low-latency, reliable transport


------------ Storage

• Instance-attached storage:
   • EBS: scale up to 256,000 IOPS with io2 Block Express
   • Instance Store: scale to millions of IOPS, linked to EC2 instance, low latency

• Network storage:
  • Amazon S3: large blob, not a file system
  • Amazon EFS: scale IOPS based on total size, or use provisioned IOPS
  • Amazon FSx for Lustre:
    • HPC optimized distributed file system, millions of IOPS
    • Backed by S3



=================================== Serverless Overview

-- Serverless is a new paradigm in which the developers don’t have to manage servers anymore...

-- They just deploy code

-- They just deploy... functions !

-- Initially... Serverless == FaaS (Function as a Service)

-- Serverless was pioneered by AWS Lambda but now also includes anything that’s managed: “databases, messaging, storage, etc.”

-- Serverless does not mean there are no servers... it means you just don’t manage / provision / see them


----------------------Serverless in AWS 
• AWS Lambda
• DynamoDB
• AWS Cognito
• AWS API Gateway 
• Amazon S3
• AWS SNS & SQS
• AWS Kinesis Data Firehose 
• Aurora Serverless
• Step Functions
• Fargate
------------------------------------------AWS LAMBDA 

-- Why AWS Lambda ?

-- in general EC2 
• Virtual Servers in the Cloud
• Limited by RAM and CPU
• Continuously running
• Scaling means intervention to add / remove servers

-- in lambda
• Virtual functions – no servers to manage! 
• Limited by time - short executions
• Run on-demand
• Scaling is automated!

-- lambda is serveless - no need to maintain servers by u 

-- we create functions in the lambda , this function is called lambda functions 



---------------------------------Benefits of AWS Lambda

-- Easy Pricing:
• Pay per request and compute time
• Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time

-- Integrated with the whole AWS suite of services

-- Integrated with many programming languages

-- Easy monitoring through AWS CloudWatch

-- Easy to get more resources per functions (up to 10GB of RAM!)

-- Increasing RAM will also improve CPU and network!

-- By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources

-- Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold

-- If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code



------------------------------- AWS Lambda language support

• Node.js (JavaScript)
• Python
• Java (Java 8 compatible)
• C# (.NET Core)
• Golang
• C# / Powershell
• Ruby
• Custom Runtime API (community supported, example Rust)




------------------------------ AWS Lambda Integrations Main ones

API Gateway 
Kinesis 
DynamoDB 
S3
CloudFront
CloudWatch Events 
CloudWatch Logs 
SNS 
SQS 
EventBridge
Cognito

-------------------------- AWS Lambda Limits to Know - per region

-- Execution: 
• Memory allocation: 128 MB – 10GB (1 MB increments)
• Maximum execution time: 900 seconds (15 minutes)
• Environment variables (4 KB)
• Disk capacity in the “function container” (in /tmp): 512 MB to 10GB • Concurrency executions: 1000 (can be increased)

-- Deployment:
• Lambda function deployment size (compressed .zip): 50 MB
• Size of uncompressed deployment (code + dependencies): 250 MB
• Can use the /tmp directory to load other files at startup
• Size of environment variables: 4 KB

---------------------- Lambda by default

-- By default, your Lambda function is launched outside your own VPC (in an AWS-owned VPC)

-- Therefore, it cannot access resources in your VPC (RDS, ElastiCache, internal ELB...)

--------------------- Lambda in VPC

-- You must define the VPC ID, the Subnets and the Security Groups

-- Lambda will create an ENI (Elastic Network Interface) in your subnets




---------------------- Configuring provisioned concurrency


-- In Lambda, concurrency is the number of in-flight requests that your function is currently handling. There are two types of concurrency controls available:


 1 Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges.

 2 Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account.






-----EVENT BRIDGE : whenever u click somethning event is generated everytime 

-- event bridge : it holds all the events of aws services  

-- u jst invoke the lambda funcn with the event bridge and do apply ur fuctions 

-- Lambda is used for the "automation"

-- lambda and event bridge are service 

-- lambda is regional level and it supports multiple languages like java,py,go,ruby 




=================================================What is Amazon CloudSearch


-- it is Managed Service in the AWS Cloud 

-- it makes it simple and cost-effective to setup, manage and scale a search solution for ur website or appn 

-- Supports 34 languages

-- popular Search features such as 

- highliting 

- autocomplete and 

- Geospatial search 


---------------Benefits 

- simple 

- Auto-Scalable 

- Reliable 

- High Performnace 

- Fully Managed 

- Cost-Effective and Secure 





========================================= Connect to AWS EC2 Using AWS SSM Session Manager ,Secure your EC2 by Enabling AWS SSM===========================


• Allows you to start a secure shell on your EC2 and on-premises servers

• No SSH access, bastion hosts, or SSH keys needed

• No port 22 needed (better security)

• Suppor ts Linux, macOS, and Windows

• Send session log data to S3 or CloudWatch Logs


-- Why we need this ?


Let us assume that u have ec2 in a Private Subnet and need to connect to the instance without using SSH over the internet . How will u do it?


Ans : by using  "Using AWS SSM Session Manager"

for this u have to create IAM Role for ec2 instance and attach "AmazonSSMManagedInstanceCore" Policy 


-- U no need to worry about following points 

1 no Ports arre needed to be allowed in SG 

2 U can run instances in Private subnets 

3 there is no need of SSH keys 

4 u can delegate access to manage c2 insatcnes using IAM roles.



Note: By default there are few AMI's that have SSM installed already , it is like a C.W Agent , but it is to be installed 


--- some AMI's that have SSM Agent pre-installed

Amazon Linux Base AMIs dated 2017.09 and later

Amazon Linux 2

Amazon Linux 2 ECS-Optimized Base AMIs

Amazon Linux 2023 (AL2023)

Amazon EKS-Optimized Amazon Linux AMIs

macOS 10.14.x (Mojave), 10.15.x (Catalina), 11.x (Big Sur), and 12.x (Monterey)

SUSE Linux Enterprise Server (SLES) 12 and 15

Ubuntu Server 16.04, 18.04, 20.04, and 22.04

Windows Server 2008-2012 R2 AMIs published in November 2016 or later

Windows Server 2016, 2019, and 2022




to check the status 

-- Amazon Linux        

sudo status amazon-ssm-agent


--  Amazon Linux 2 and Amazon Linux 2023         

sudo systemctl status amazon-ssm-agent


--   SUSE Linux Enterprise Server       

sudo systemctl status amazon-ssm-agent          



-------------------------------------- Systems Manager – Run Command

• Execute a document (= script) or just run a command

• Run command across multiple instances (using resource groups)

• No need for SSH

• Command Output can be shown in the AWS Console, sent to S3 bucket or CloudWatch Logs

• Send notifications to SNS about command status (In progress, Success, Failed, ...)

• Integrated with IAM & CloudTrail

• Can be invoked using EventBridge


------------------------------------- Systems Manager – Patch Manager 

• Automates the process of patching managed instances

• OS updates, applications updates, security updates

• Supports EC2 instances and on-premises servers

• Supports Linux, macOS, and Windows

• Patch on-demand or on a schedule using Maintenance Windows

• Scan instances and generate patch compliance report (missing patches)




----------------------------------- Systems Manager – Maintenance Windows

• Defines a schedule for when to perform actions on your instances

• Example: OS patching, updating drivers, installing software, ...

• Maintenance Window contains
  • Schedule
  • Duration
  • Set of registered instances 
  • Set of registered tasks




----------------------------------------------------------- DEMO SSM
                                        

-- check for linux 2 machine , u need to give keys n SG optional 


-- now we want to connect thruh SSM session but we are not getting option to get connect through the SSM sesion 

-- to do that we know it has aleady pre -installed SSM Agent , verify it is there or not by connecting through ec2-connect 

sudo systemctl status amazon-ssm-agent

it is running 


-- now u have to create role for SSM Session 

IAM --> roles --> creatae role for EC2 --> attach AmazonSSMManagedInstanceCore policy -->create role --> attach this role to the Ec2 instance 

--it will take 5-10 min to connect through SSM session 





=======================  Automatically start and stop ur instance with instance scheduler=========================================

-- launch the instance schedular task hub

-- when u click on launch solutions , it will redirect to the cloud formation it takes url for s3 automatically 

-- leave all the values default , in last step enable checkbox

-- do submit the stack it will apporx five minutes to complete the stack 

-- here automatically it will create dynamodb table , lambda function, IAM , SNS, setup KMS and other things as shown in the diagram 

-- once u done ,now configure Periods and schedules 

-- basically schedule is made up of multiple periods

-- The Instance Scheduler on AWS allows you to automatically start and stop Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) instances.some example schedules that can be adapted to many common use-cases.

-- sample schedules

1  Standard 9-5 working hours

this schedule shows how to run instances on weekdays from 9 AM to 5 PM 

Periods

This period will start instances at 9 AM and stop instances at 5 PM on weekdays (Mon-Fri).


Field	      Value

begintime	09:00
endtime	        16:59
name	        weekdays 9-5
weekdays	mon-fri


Schedule

The schedule combines the three periods into the schedule for tagged instances. The schedule includes the following fields and values.

Field	     Value

name	      london-working-hours
periods	      weekdays 9-5
timezone.     Europe/London


2. Stop instances after 5 PM


Instances can be started freely at any time during the day and this schedule will ensure that a stop command is automatically sent to them at 5 PM ET every day.


Periods

This period will stop instances at 5 PM every day.

Field	Value

endtime	16:59
name	stop-at-5 

Schedule

The schedule name provides the tag value that must be applied to insances and the timezone that will be used.

Field	        Value

name	        stop-at-5-new-york
periods	        stop-at-5
timezone	America/New_York


3. Stop instances over the weekend

This schedule shows how to run instances from Monday 9 AM ET to Friday 5 PM ET. Since Monday and Friday are not full days, this schedule includes three periods to accommodate: Monday, Tuesday-Thursday, and Friday.

Periods

The first period starts tagged instances at 9 AM Monday and stops at midnight. This period includes the following fields and values.


Field	    Value

begintime.   09:00
endtime	     23:59
name	     mon-start-9am
weekdays.    mon

The second period runs tagged instances all day Tuesday through Thursday. This period includes the following fields and values.

Field	       Value

name	       tue-thu-full-day
weekdays	tue-thu


The third period stops tagged instances at 5 PM on Friday. This period includes the following fields and values.

Field	    Value

begintime   00:00
endtime	    16:59
name	    fri-stop-5pm
weekdays    fri

Schedule

The schedule combines the three periods into the schedule for tagged instances. The schedule includes the following fields and values.


Field	  Value  

name	  mon-9am-fri-5pm
periods	  mon-start-9am,tue-thu-full-day,fri-stop-5pm
timezone  America/New_York



-- When deploying the Instance Scheduler Hub Stack, the solution created an Amazon DynamoDB table containing several sample periods and schedules that you can use as a reference to create your own custom periods and schedules. To create a schedule in DynamoDB, modify one of the schedules in the configuration table (ConfigTable) or create a new one.

-- open dynamodb table , cf will create tables for us , go n check tables 

-- for this example , I have taken 

Period = office-hours

Schedule = uk-office-hours

IMp : name of the period is to be same in the schedule period column

-- change time zone in the schedule time zone 

-- go to instance --> Tags 

-- here tags are very important to do the actions so be careful while giving the tags 

Here       key = Schedule      value = Schedule's Name 

Eg:               Schedule             uk-office-hours



-- once u set all these values u will able to see the instance will get stopped . By the specific time we have given 

-- done 




============================================================AMAZON SQS (simple queue service)========================================



-----Intro  to AWS Integration & Messaging


• When we start deploying multiple applications, they will inevitably need to communicate with one another

• There are two patterns of application communication

1) Synchronous communications : (application to application)

2  Asynchronous / Event based : (application to queue to application)

• Synchronous between applications can be problematic if there are sudden spikes of traffic

• What if you need to suddenly encode 1000 videos but usually it’s 10?

• In that case, it’s better to decouple your applications,

  • using SQS: queue model
  • using SNS: pub/sub model
  • using Kinesis: real-time streaming model

• These services can scale independently from our application!


------------------------------- amazon SQS 



-- Amazon SQS provides queues for high-throughput, system-to-system messaging. You can use queues to decouple heavyweight processes and to buffer and batch work. Amazon SQS stores messages until microservices and serverless applications process them.

-- message SQS is a form of asynchronous service to service communication used in serverless and micro services architecture 

-- sender = producers and receivers = consumers

-- messages are stores in the QUEUE until they are processed and deleted 

-- where the sender of he message and the one who is receiving the message or is going to receive the message don't have to interact with the message queue at the same point of time

EG : customers and the chef's in the restaurant don't have to interact with the servers or waiters at the same time 

-- each message is processed only once by a single consumer

-- 

-- it is a message queuing service

How it works ?

workflow 

producers --> AWS SQS --> SQS Queue --> Consumers


-- Amazon SQS allows producers to send messages to a queue. Messages are then stored in an SQS Queue. When consumers are ready to process new messages they poll them from the queue. Applications, microservices, and multiple AWS services can take the role of producers or consumers.


Benefits and features : 

1 Highly scalable Standard and FIFO queues

Queues scale elastically with your application. Nearly unlimited throughput and no limit to the number of messages per queue in Standard queues. First-In-First-Out delivery and exactly once processing in FIFO queues.

2 Durability and availability

Your queues are distributed on multiple servers. Redundant infrastructure provides highly concurrent access to messages.

3 Security

Protection in transit and at rest. Transmit sensitive data in encrypted queues. Send messages in a Virtual Private Cloud.

4 Batching

Send, receive, or delete messages in batches of up to 10 messages or 256KB to save costs.


-- it is the first service launched by the AWS in 2004 , it is very popular for the de-coupling the applications 



--------- it has 2 components Standard and FIFO queues , 


1 Amazon SQS – Standard Queue

• Oldest offering (over 10 years old)

• Fully managed service, used to decouple applications

• Attributes:
  • Unlimited throughput, unlimited number of messages in queue
  • Default retention of messages: 4 days, maximum of 14 days
  • Low latency (<10 ms on publish and receive)
  • Limitation of 256KB per message sent

• Can have duplicate messages (at least once delivery, occasionally)

• Can have out of order messages (best effort ordering)



2 Amazon SQS – FIFO Queue

• FIFO = First In First Out (ordering of messages in the queue)

• Limited throughput: 300 msg/s without batching, 3000 msg/s with Batch

• Exactly-once send capability (by removing duplicates)

• Messages are processed in order by the consumer



---------------- the differences are 

1  Message Order

-- Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they are sent. Occasionally (because of the highly-distributed architecture that allows high throughput), more than one copy of a message might be delivered out of order.

-- FIFO queues offer first-in-first-out delivery and exactly-once processing: the order in which messages are sent and received is strictly preserved.

2  Delivery

-- Standard queues guarantee that a message is delivered at least once and duplicates can be introduced into the queue.

-- FIFO queues ensure a message is delivered exactly once and remains available until a consumer processes and deletes it; duplicates are not introduced into the queue.

3  Transactions Per Second (TPS)

-- Standard queues allow nearly-unlimited number of transactions per second.

-- FIFO queues allow to process up to 3000 messages per second per API action.

4  Regions

-- Standard queues are available in all the regions.

-- FIFO queues are currently available in limited regions only.

5  AWS Services Supported

-- Standard Queues are supported by all AWS services.

-- FIFO Queues are currently not supported by all AWS services like: CloudWatch Events, S3 Event Notifications, SNS Topic Subscriptions, Auto Scaling Lifecycle Hooks, AWS IoT Rule Actions, AWS Lambda Dead Letter Queues.



EPV : A company wants to publish an event into an Amazon Simple Queue Service (Amazon SQS) queue whenever a new object is uploaded on Amazon S3.

Which of the following statements are true regarding this functionality?

ANS :  Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed





What is Visibility timeout ?

-- Visibility timeout sets the length of time that a message received from a queue (by one consumer) will not be visible to the other message consumers.

-- The visibility timeout begins when Amazon SQS returns a message. If the consumer fails to process and delete the message before the visibility timeout expires, the message becomes visible to other consumers. If a message must be received only once, your consumer must delete it within the duration of the visibility timeout.

-- The default visibility timeout setting is 30 seconds. This setting applies to all messages in the queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.

eg : if one consumer is request a meaasge from SQS queue it will give message and it has 30 sec default value to read and deletes it , now if consumer 2 has requested for same message it won't send 'coz the visibility timeout is in active for 30 sec , so after 30 sec it will give message to other consumer 

consumer will be : Applications , lambda functions, ec2 instances, and other aws services 

-- If standard queue, when one consumer pick a message and fails, it will be available for other consumers within the visibility period.

-- FIFO : If a message must be received only once, your consumer must delete it within the duration of the visibility timeout


what is Delivery delay (DelaySeconds)?

 Any messages that you send to the queue remain invisible to consumers for the duration of the delay period.

 -- Receive message wait time(ReceiveMessageWaitTimeSeconds): the maximum amount of time that polling will wait for messages to become available.

 -- Message retention period(MessageRetentionPeriod): amount of time that Amazon SQS retains a message that does not get deleted.

 -- Maximum number of receives per message(maxReceiveCount): If the ReceiveCount for a message exceeds the maximum receive count for the queue, Amazon SQS moves the message to the associated DLQ.



----------------------------------------------------------------- Amazon Simple Queue Service (Amazon SQS) temporary queues


-- Temporary queues help you save development time and deployment costs when using common message patterns such as request-response.

-- You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.

-- The following are the benefits of temporary queues:

   - They serve as lightweight communication channels for specific threads or processes.
   - They can be created and deleted without incurring additional costs.
   - They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.

-- To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client.

-- This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. 

-- The key concept behind the client is the virtual queue. 

-- Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. 

-- Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.






EPV : 

Q : A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order.

ANS : Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate

exp : By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. 

-- Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate.

here we need 1k messages so 4 messages per operation 4 * 300 = 1200 , with batch operations 



---------------------- Long Polling :  with long polling the receive message requests queries all the servers for the message

Amazon SQS - Long Polling

• When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue

• This is called Long Polling

• LongPolling decreases the number of API calls made to SQS while increasing the efficiency and reducing latency of your application

• The wait time can be between 1 sec to 20 sec (20 sec preferable)

• Long Polling is preferable to Short Polling

• Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds



------- short polling : by default  use short pooling not the long pooling , with short pooling the received message requests queries only a subset of their service to find messages that are available to include in the response



--------important thing that you need to understand the difference is in short polling Amazon SQS sends the response right away even if the query found is no message 

------- long pooling Amazon SQS asked you sends and empty response only if the polling wait time expires and that is why we use long polling if we want to save API call costs.




----------------------------------

-- using lambda and SQS combination it is very powerful event driven solutions so, lambda polls SQS and process the messages and these messages are read in the batches and lambda function is invoked once for the each batch so u define the batch size in the lambda configuration , u can specify the batch size b/w 1 - 1000 and default value for the batch  10 

lambda configuration for SQS

 1 SQS Batch processing : 

-- As we discussed SQS messagaes are processed in batches not individually 

-- Lambda deletes the SQS batch  automatically , if all messages e process successfully to avoid re-processing

Failed scrnarios:

-- if one or more messages will fail  lambda ll re-process the batch and u may get duplicate records (that's why Idempotency  important in lambda)

How to avoid re-processing messagaes

-- Reporting batch items Failures feature provided by the AWS 

-- only failure messages will re-process through Reporting batch items Failures


2  Batch Size :

-- messages u want to process  once , default value is 10 

-- keep the batch size 1 as small as , coz lambda will encounter lot of messages , it will keep on creating new lambda instance, u might face throttling issues so it has some limit to create lambda functions ryt? so 

-- it is very important aspect of lambda-SQS integration 

3 Batch Window :

-- time u want to wait before invoking the function. it has to do with SQS Polling 
-- default value is 0 . Which means the lambda will keep on polling SQS to know if it has messages equals o batchsize 

-- Important as it can lead to un-necessary lambda polling hence un-necessary SQS and lambda billing


Handling Failed messages (recommended)

-- if a message s failed, it is retried by lambda until 

- retention period is reached (14 days)
- there is a dead-letter queue 

-- so create DLQ with SQS so that ur lambda does not keep on invoking for 14 days 

eg : ur retried policy number is 5 , it will picked by lambda only for 5 times after 5 times tyhe lambda will send this message to DLQ , this will saves u costs , un-necessary cloud watch logs ....



================================================ LAB 1 ==================================================

----------------------- 1 Standard Queue 

-- open SQS 

-- create Std Queue 

-- Receive message wait time = long polling

-- create std queue , u have 2 sections these are producers and consumers 

-- send message and do poll to receive message 

-- now here we do not have any Dead letter queue , even if the message is fails to send it won't go to DLQ 'coz we do not have DLQ here , after 14 days it will get deleted 

-- now create DLQ 

-- go to SQS and create one more queue put this as main queue and previous queue as DLQ 

-- Receive message wait time = 10 secs

-- enable DLQ and set Maximum receives = 3 or 5 

-- now go to main queue and send messages 

-- now send msg , now in receivers section c.o statrt polling and then c.o stop polling quickly for 3 times ('coz we have given retries = 3 )

-- if u poll after 3 times it won’t appear in the msg section , it will go to the dead letter queue 

-- now go to std queue and do poll for message , now u will get the message which is not poll in the main queue , from this u can do check why it is failing


----------------------- 2 FIFO queues

--  go to SQS 

-- name ends with .fifo and enabled content based deduplication

-- create fifo queue , now send msg with group id , now poll 

-- now send same message and observe , here it do not send another message seperate id 'coz the content is same so it wont allow duplicates 

-- now change the content and send msg , this time it will send the message 'coz we have changed our content 

-- here during the polling period only u can able to delete the messages once it gets polled after u cannot delete in the FIFO queue(The receipt handle has expired.)

-- where u can delete in std queue

-- now try to delete during the polling time , u can able to delete the message 




=================================================LAB 2 =================


AWS SQS Trigger To Lambda Function


step 1 : create one lambda function

-- choose blue print --> search for SQS select that --> name of ur function --> create new role --> create function 

-- once u create function with the new role created by the lambda function , u have to add other policies like  "AmazonSQSFullAccess", "AWSLambdaSQSQueueExecutionRole", to get permissions 


Step 2 : create Queue

-- create queue as standard queue --> give name and create queue

-- now whatever the messages that we are giving here that will be going to trigger lambda 

-- so create messages first 

--  open queue --> below c.o lambda triggers add lambda function --> c.o send and receive messages --> once u trigger successfully with lambda type some messages in message box 

-- once u do go n check in cloudwatch logs to see our messages 


-- create one FIFO queue also 

-- select Content-based deduplication in queue settings 

-- when u try to enter message it will send to lambda 

-- now try to send same message , it wont trigger in lambda 'coz here deduplication is going on through the FIFO , it wont send duplicate data again ,

-- try to change the content , now u will see 

or u can also so with group level deduplication also 

-- the FIFO queue must end with the .fifo only 


===================================================== Amazon SNS ==================================================

• The “event producer” only sends message to one SNS topic

• As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications

• Each subscriber to the topic will get all the messages (note: new feature to filter messages)

• Up to 12,500,000 subscriptions per topic

• 100,000 topics limit


------------- SNS integrates with a lot of AWS services

• Many AWS services can send data directly to SNS for notifications

CloudWatch Alarms / AWS Budgets / Lambda / DynamoDB / S3 Bucket (Events) / Auto Scaling Group (Notifications) / RDS Events / AWS DMS (New Replic) / CloudFormation (State Changes) ------------(publish)--------> SNS


----------- Amazon SNS – How to publish

• Topic Publish (using the SDK)
  • Create a topic
  • Create a subscription (or many)
  • Publish to the topic

• Direct Publish (for mobile apps SDK)
  • Create a platform application
  • Create a platform endpoint
  • Publish to the platform endpoint
  • Works with Google GCM, Apple APNS, Amazon ADM...


------------ Amazon SNS – Security

• Encryption:
  • In-flight encryption using HTTPS API
  • At-rest encryption using KMS keys
  • Client-side encryption if the client wants to perform encryption/decryption itself

• Access Controls: IAM policies to regulate access to the SNS API

• SNS Access Policies (similar to S3 bucket policies)
  • Useful for cross-account access to SNS topics
  • Useful for allowing other services ( S3...) to write to an SNS topic 



------------ SNS + SQS: Fan Out

• Push once in SNS, receive in all SQS queues that are subscribers

• Fully decoupled, no data loss

• SQS allows for: data persistence, delayed processing and retries of work

• Ability to add more SQS subscribers over time

• Make sure your SQS queue access policy allows for SNS to write

• Cross-Region Delivery: works with SQS Queues in other regions


Buying Service -------> SNS Topic -------> SQS Queue(s) ---> services 




----------------- Application: S3 Events to multiple queues

• For the same combination of: event type (e.g. object create) and prefix (e.g. images/) you can only have one S3 Event rule

• If you want to send the same S3 event to many SQS queues, use fan-out


s3 object created -------(Events)-----> Bucket ----> SNS Topic ------(fan-out) ----> sqs queue / lambda


----------------- Application: SNS to Amazon S3 through Kinesis Data Firehose

• SNS can send to Kinesis and therefore we can have the following solutions architecture:



----------------- Amazon SNS – FIFO Topic

• FIFO = First In First Out (ordering of messages in the topic)

Producer -------(send messages 4,3,2,1)--------> SNS ----(receive messages 4,3,2,1)---> Subscribers SQS FIFO

• Similar features as SQS FIFO:
  • Ordering by Message Group ID (all messages in the same group are ordered)
  • Deduplication using a Deduplication ID or Content Based Deduplication

• Can have SQS Standard and FIFO queues as subscribers

• Limited throughput (same throughput as SQS FIFO)


--------------- SNS – Message Filtering

• JSON policy used to filter messages sent to SNS topic’s subscriptions

• If a subscription doesn’t have a filter policy, it receives every message




======================================================== Kinesis Overview =============================================

• Makes it easy to collect, process, and analyze streaming data in real-time

• Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data...

    • Kinesis Data Streams: capture, process, and store data streams

    • Kinesis Data Firehose: load data streams into AWS data stores

    • Kinesis Data Analytics: analyze data streams with SQL or Apache Flink

    • Kinesis Video Streams: capture, process, and store video streams



------------------- 1 Kinesis Data Streams

• Retention between 1 day to 365 days

• Ability to reprocess (replay) data

• Once data is inserted in Kinesis, it can’t be deleted (immutability)

• Data that shares the same partition goes to the same shard (ordering)

• Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent

• Consumers: 
   • Write your own: Kinesis Client Library (KCL), AWS SDK
   • Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics,



---------------- Kinesis Data Streams – Capacity Modes

• Provisioned mode:
  • You choose the number of shards provisioned, scale manually or using API
  • Each shard gets 1MB/s in (or 1000 records per second)
  • Each shard gets 2MB/s out (classic or enhanced fan-out consumer) 
  • You pay per shard provisioned per hour

• On-demand mode:
  • No need to provision or manage the capacity
  • Default capacity provisioned (4 MB/s in or 4000 records per second)
  • Scales automatically based on observed throughput peak during the last 30 days
  • Pay per stream per hour & data in/out per GB


---------------- Kinesis Data Streams Security

• Control access / authorization using IAM policies

• Encryption in flight using HTTPS endpoints

• Encryption at rest using KMS

• You can implement encryption/decryption of data on client side (harder)

• VPC Endpoints available for Kinesis to access within VPC

• Monitor API calls using CloudTrail



-------------------------------- 2 Kinesis Data Firehose

• Fully Managed Service, no administration, automatic scaling, serverless
  • AWS: Redshift / Amazon S3 / OpenSearch
  • 3rd party partner: Splunk / MongoDB / DataDog / NewRelic / ...
  • Custom: send to any HTTP endpoint

• Pay for data going through Firehose

• Near Real Time
  • 60 seconds latency minimum for non full batches
  • Or minimum 1MB of data at a time

• Supports many data formats, conversions, transformations, compression

• Supports custom data transformations using AWS Lambda

• Can send failed or all data to a backup S3 bucket


---------------------------------------------- Kinesis Data Streams vs Firehose


 Kinesis Data Streams                                         Firehose


 • Streaming ser vice for ingest at scale                   • Load streaming data into S3 / Redshift / OpenSearch / 3rd party / custom HTTP

 • Write custom code (producer / consumer)                  • Fully managed

 • Real-time (~200 ms)                                      • Near real-time (buffer time min. 60 sec)

 • Manage scaling (shard splitting / merging)               • Automatic scaling

 • Data storage for 1 to 365 days                           • No data storage

 • Supports replay capability                               • Doesn’t support replay capability



 -------------------------------------------- Ordering data into Kinesis

 • Imagine you have 100 trucks (truck_1, truck_2, ... truck_100) on the road sending their GPS positions regularly into AWS.

 • You want to consume the data in order for each truck, so that you can track their movement accurately.

 • How should you send that data into Kinesis?

 • Answer : send using a “Par tition Key” value of the “truck_id”

 • The same key will always go to the same shard


 -------------------------------------------- Ordering data into SQS

 • For SQS standard, there is no ordering.

 • For SQS FIFO, if you don’t use a Group ID, messages are consumed in the order they are sent, with only one consumer

 • You want to scale the number of consumers, but you want messages to be “grouped” when they are related to each other

 • Then you use a Group ID (similar to Partition Key in Kinesis)


------------------------------------------ Kinesis vs SQS ordering

• Let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO

• Kinesis Data Streams:
  • On average you’ll have 20 trucks per shard
  • Trucks will have their data ordered within each shard
  • The maximum amount of consumers in parallel we can have is 5
  • Can receive up to 5 MB/s of data

• SQS FIFO
  • You only have one SQS FIFO queue
  • You will have 100 Group ID
  • You can have up to 100 Consumers (due to the 100 Group ID)
  • You have up to 300 messages per second (or 3000 if using batching)


-------------------------------------------- SQS vs SNS vs Kinesis

SQS:                                                          SNS:                                         Kinesis

• Consumer“pulldata”                                    • Pushdatatomany subscribers                    • Standard:pulldata (2 MB per shard)

• Data is deleted after being consumed                  • Upto12,500,000subscribers                     • Enhanced-fanout:pushdata (2 MB per shard per consumer)

• Canhaveasmanyworkers (consumers) as we want           • Dataisnotpersisted(lost if not delivered)     • Possibility to replay data

• Noneedtoprovision throughput                          • Pub/Sub                                       • Meantforreal-timebigdata,analytics and ETL

• Orderingguaranteesonlyon FIFO queues                  • No need to provision throughput               • Ordering at the shard level

• Individualmessagedelay capability                     • IntegrateswithSQSforfan- out architecture     • Data expires after X days
                                                          pattern     

                                                        • FIFO capability for SQS  FIFO                 • Provisioned mode or on- demand capacity mode   




================================================= Amazon MQ ======================================


• SQS, SNS are “cloud-native” services: proprietary protocols from AWS

• Traditional applications running from on-premises may use open protocols such as:MQTT,AMQP,STOMP,Openwire,WSS

• When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ

• Amazon MQ is a managed message broker service for

RabbitMQ      ActiveMQ

• Amazon MQ doesn’t “scale” as much as SQS / SNS

• Amazon MQ runs on servers, can run in Multi-AZ with failover

• Amazon MQ has both queue feature (~SQS) and topic features (~SNS)






======================================================AMAZON EVENTBRIDGE============================================


-- Amazon EventBridge is a serverless service that uses events to connect application components together, making it easier for developers to build scalable event-driven applications.

-- Event Bridge is an Event Bus that helps in integrating different AWS Services , and custom applications and SAAS Applications 

-- Earlier we have Cloud watch events , the only negative point is that , it did not support SAAS applications and custom applications outside from the AWS 

-- EventBridge was build for the same purpose , u can create effortless event-driven architectures

Eg 1 : u want to receive SNS notification every time production EC2 instance are terminated , this will be done by EventBridge

Eg 2 : automated deployment using code pipelines at 11PM everyday

Eg 3 : if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


-- these all things will done by the event Bridge and u can schedule the events also 

-- It is fully managed service 

-- pay what u use model 


Different Parts in EventBridge:


Working flow -----


Event Producer --> Event --> Amazon EventBridge event Bus --> Rule ---> AWS Lambda , AWS Kinesis Data Firehose, Amazon Simple Notification Service


--  Event producer : AWS Service / Custom applications / Third Party SaaS providers,

--  From the producers the event will generated , this will transform to the bus and it move to rules , in rules there are multiple rules and this rules will send to multiple targets 

-- A single rule may have multiple targets and max target is 5 ,all targets will process in the event in parallel


Important terms in EventBridge :

1 Event  : An Event indicated a change in an environment 

2 Rule   : A rule matches incoming events and routes them to targets for processing  

3 Target : target application of ur rule. A target process events 

Targets can include ec2 , lambda functions , kinesis streams , Ecs tasks,  , the target receive events in JSON format 

4 Event Bus : An event bus receives events. When u create a rule , u associate it with a specific event bus and rules is matched only to events received by that event bus 

- rule can not be create standAlone , it should have it's parent as EventBus

Components of EventBridge :

1 EventBridge Rule : A rule matches incoming events and sends them to targets for processing.

2 EventBridge Pipes : A pipe connects an event source to a target with optional filtering and enrichment.

3 EventBridge Schedule : A schedule invokes a target one-time or at regular intervals defined by a cron or rate expression.

4 EventBridge Schema registry : Schema registries collect and organize schemas.


EventBus : Usecases

if a premier user deactivates himself from ur product , u want to :

- Send and email for feedback 
- Schedule Customer Representative call
- Send customised offers for that use 


In the above example , the working flow will be like 


User deletes his subscription --> custom event is trigged --> rights custom event would be pass into the EventBus --> rule matching --> then it reaches to the targets , once the rule is matched  here then it sends to targets --> 1 the targets are Feedback service 2 Schedule call service 3 offers service 




EPV : A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication.

As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?


ANS : Use Amazon EventBridge to decouple the system architecture

EXP : Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, but for this use case, EventBridge is the right fit.

-- Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services.

-- Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners. Amazon EventBridge also automatically ingests events from over 90 AWS services without requiring developers to create any resources in their account. 

-- Further, Amazon EventBridge uses a defined JSON-based structure for events and allows you to create rules that are applied across the entire event body to select events to forward to a target.

-- Amazon EventBridge currently supports over 15 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, and Amazon Kinesis Streams and Firehose, among others. At launch, Amazon EventBridge is has limited throughput (see Service Limits) which can be increased upon request, and typical latency of around half a second.





=======================================practicals=======================


Example 1 

1  Schedule AWS Lambda Functions Using EventBridge


-- lambda scheduling use cases 

- Automated backups at EOD of ur applications 

- backend cleaning (including logs and temp files )

- consolidated reports after business hours , so lambda can trigger  Athena queries and can run any database queries and send to business stack holders through SNS 


- to schedule events u have two options 

1 to schedule  at fixed rate -- for every 1 minute 

2 Cron job (10 * * * * )

-- open console 

-- open lambda --> python 3.9 --> create function 

-- write python code to invoke lambda function

import json
from datetime import datetime

def lambda_handler(event, context):
    # TODO implement
    currentTime = datetime.now()
    print("Time at which Lambda invoked" + str(currentTime))


-- give empty JSON to test {}

-- now go to Amazon eventBridge in console 

-- c.o create rule --> schedule --> continue to create rule --> schedule that run at regular intervals--> select 1 minute --> select lambda function --> create rule 

-- u can check in CloudWatch logs , for invoking is there or not 

--  the logs are generated 


-- if u want to monitor ur invocations --> Metrics --> all Metrics --> query --> choose AWS/events --> Myrule name 

- metric name = COUNT(invocation)

- filter by = Rulename = rule name that u have created in lambda function to test the function 

- choose number in right corner to see the no. of invocations 


-------------------------------------------

Example 2 : EventBridge with SNS 


-- AS we know that EventBridge will use for 2 process 

1 event-event process : when the event has occurred according to our rule then it will trigger to the targets 

2 schedule process : do schedule 

-- in the above example we have seen Schedule event

-- now event to event 

- for example the ec2 is stopped then it will send alert to the subscribers on SNS Topic 

-- open SNS in console --> create one topic --> standard --> create topic --> open topic -->create subscription --> add protocol email or phone number 

-- when ever the instance is getting stooped then I would like send an alert to all the subscribers 

-- this is called event-event process 

-- create one ec2 instance 

-- create rule in event bridge --> name --> rule with an event pattern --> in event pattern = select ec2 --> select event type as u want --> in Target1 choose AWS Service = SNS topic select SNS topic --> create rule 

-- do stop the instance 

-- u will get notification once the instance is get stopped 


--------------- now for schedule event 

-- create another rule , that is based on the time to stop the instance 

-- select schedule pattern option --> A schedule that runs at a regular rate, such as every 10 minutes. --> target = terminateinstanceAPI call --> u can also add another target to get notification 
 

-- the instance will get terminated after the time that u have specified 


-- this is simple basic example for schedule pattern 



==========================done=============================

============================================================= Data & Analytics ===================================

===================================================================== 1 AMAZON ATHENA=============================================

what is AWS Athena ?

ANS : -- Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.

-- Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there’s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.

-- Athena is out-of-the-box integrated with Amazon Glue Data Catalog, allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning. You can also use Glue’s fully-managed ETL capabilities to transform data or convert it into columnar formats to optimize cost and improve performance.

Benefits :
 
1  Interactive Performance Even for Large Datasets :

With Amazon Athena, you don't have to worry about having enough compute resources to get fast, interactive query performance. Amazon Athena automatically executes queries in parallel, so most results come back within seconds.

2  Built on Presto, Runs Standard SQL

Amazon Athena uses Presto with ANSI SQL support and works with a variety of standard data formats, including CSV, JSON, ORC, Avro, and Parquet. Athena is ideal for quick, ad-hoc querying but it can also handle complex analysis, including large joins, window functions, and arrays. Amazon Athena is highly available; and executes queries using compute resources across multiple facilities and multiple devices in each facility. Amazon Athena uses Amazon S3 as its underlying data store, making your data highly available and durable.

3  Serverless, No ETL

Athena is serverless. You can quickly query your data without having to setup and manage any servers or data warehouses. Just point to your data in Amazon S3, define the schema, and start querying using the built-in query editor. Amazon Athena allows you to tap into all your data in S3 without the need to set up complex processes to extract, transform, and load the data (ETL).

4  Only Pay for Data Scanned

With Amazon Athena, you pay only for the queries that you run. You are charged ¥41.20 per terabyte scanned by your queries. You can save from 30% to 90% on your per-query costs and get better performance by compressing, partitioning, and converting your data into columnar formats. Athena queries data directly in Amazon S3. There are no additional storage charges beyond S3.


Features :

1  Fast Performance

With Amazon Athena, you don’t have to worry about managing or tuning clusters to get fast performance. Athena is optimized for fast performance with Amazon S3. Athena automatically executes queries in parallel, so that you get query results in seconds, even on large datasets.

2  Easy to Get Started

To get started, log into the Athena console, define your schema using the console wizard or by entering DDL statements, and immediately start querying using the built-in query editor. You can also use Amazon Glue to automatically crawl data sources to discover data and populate your Data Catalog with new and modified table and partition definitions. Results are displayed in the console within seconds, and automatically written to a location of your choice in S3. You can also download them to your desktop. With Athena, there’s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.

3  Serverless, Zero Infrastructure, Zero Administration

Amazon Athena is serverless, so there is no infrastructure to manage. You don’t need to worry about configuration, software updates, failures or scaling your infrastructure as your datasets and number of users grow. Athena automatically takes care of all of this for you, so you can focus on the data, not the infrastructure.

4  Pay per Query

With Amazon Athena, you pay only for the queries that you run. You are charged based on the amount of data scanned by each query. You can get significant cost savings and performance gains by compressing, partitioning, or converting your data to a columnar format, because each of those operations reduces the amount of data that Athena needs to scan to execute a query.

5  Easy to Query, Just Use Standard SQL

Amazon Athena uses Presto, an open source, distributed SQL query engine optimized for low latency, ad hoc analysis of data. This means you can run queries against large datasets in Amazon S3 using ANSI SQL, with full support for large joins, window functions, and arrays. Athena supports a wide variety of data formats such as CSV, JSON, ORC, Avro, or Parquet. You can also connect to Athena from a wide variety of BI tools using Athena's JDBC driver.

6  Highly Available & Durable

Amazon Athena is highly available and executes queries using compute resources across multiple facilities, automatically routing queries appropriately if a particular facility is unreachable. Athena uses Amazon S3 as its underlying data store, making your data highly available and durable. Amazon S3 provides durable infrastructure to store important data and is designed for durability of 99.999999999% of objects. Your data is redundantly stored across multiple facilities and multiple devices in each facility.

7  Secure

Amazon Athena allows you to control access to your data by using Amazon Identity and Access Management (IAM) policies, access control lists (ACLs), and Amazon S3 bucket policies. With IAM policies, you can grant IAM users fine-grained control to your S3 buckets. By controlling access to data in S3, you can restrict users from querying it using Athena. Athena also allows you to easily query encrypted data stored in Amazon S3 and write encrypted results back to your S3 bucket. Both, server-side encryption and client-side encryption are supported.


Different ways to access Athena :

1 AWS console
2 Athena API 
3 Athena CLI 
4 JDBC connection

-- it integrates with AWS Glue Data Catalog , it is ETL Tool

-- it also integrates with Amazon Quicksight for data visualization 


Why Athena got popular:

-- used by data Analysts  query large S3 data statements

-- No need to spin up serveres or Hadoop clusters

 Athena Use cases :

-- analyze CloudTrail/CloudFront/VPC/ELB logs
-- integration trough ODBC/JDBC with other visualization tools
-- Ad-hoc logs analysis

Athena Cost Model : 

-- pay only for queries

- $5 per TB data Scanned
- charged for the no.of bytes scanned (with min 10MB PER Query)
- NO charge for DDL(CREATE,ALTER,DROP) and failed quaries
- charges for cancelled queries (for the data scanned)

-- How to save cost?

- Columnar formats(Parquet and ORC)
- using partitions
- compressions(gzip,snappy)
- number of files(more related performance optimization) : Size of files stored in the s3 , it should be min 10MB to 50MB then Athena should not spend effort in scanning multiple small files 


-- Athena - creating tablesm

-- create table using 

- DDL Commands
- AWS Glue crawler
- JDBC driver
- create table wizard -- through AWS console

Note : only EXTERNAL tables re created in the Athena if u dont provides EXTERNAL keyword , u will get error

-- Athena queries

- quer results and metadata stored in s3 
- u specify the specific folder in s3 to store the output
- stored procedures not supported
- certain DDL Operations are also not supported
- Athena federated query (preview feature) : when u query the data , u provide s3 folder so, with federated queries u can combine the results from multiple data eg dynamoDB, AWS storage service,


---------------Practicals---------------------

Step 1 : create s3 bucket and insert some .csv data 

--  insert some  data in the bucket 

--   create one folder inside bucket and go to google and search for sample .csv files downlaod and upload in the s3 bucket 

Step 2 : Glue

-- open glue in console

-- glue --> from left panel data Catalog --> crawlers --> add crawlers --> give name of ur crawler --> not yet --> add a data source --> choose s3 --> copy uri of ur bucket and paste in S3 path --> click on create new role , give name of ur role and select nxt --> click on create new database --> give name of ur database , nxt --> frequency = on-demand --> create crawler

-- now try to run the crawler , 

--  u can also check the logs in cloudwatch 

-- once u done with the running part , a table will added to the database , table name is same as the folder name of s3 that u have uploaded 

--  if ur data is not match , then u can also do edit ur schema 


step 3 : Athena

-- open Athena in the console

-- in the left panel u will see the database and table , u can check all the coloums of ur table

-- now do queries , before that u should add destination to store our outputs in the s3 bucket 

-- go to s3 buckets create one folder in the bucket to store the outputs

-- now try to query in athena editor

-- SELECT * FROM "weather-database"."weather_csv" limit 10;

above is eg query , u can replace with ur details 

weather-database = database name 

weather_csv = table name


NOTE : if u r getting zero records but query is successfull then do 

ur s3 location is like this 

s3://doc-example-bucket/table1.csv 

-- to get avoid from this error , jst create one sub-folder and upload .csv file in that subfolder 

s3://doc-example-bucket/table1/table1.csv



-- in the crawler location u have to give like this , then u won't get any error

s3://doc-example-bucket/table1/



============================================== 2 Redshift ==================


---- Redshift Overview

-- Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis.

• Redshift is based on PostgreSQL, but it’s not used for OLTP

• It’s OLAP – online analytical processing (analytics and data warehousing)

• 10x better performance than other data warehouses, scale to PBs of data

• Columnar storage of data (instead of row based) & parallel query engine

• Pay as you go based on the instances provisioned

• Has a SQL interface for performing the queries

• BI tools such as Amazon Quicksight or Tableau integrate with it

• vs Athena: faster queries / joins / aggregations thanks to indexes



------------ Redshift Cluster

• Leader node: for query planning, results aggregation

• Compute node: for performing the queries, send results to leader

• You provision the node size in advance

• You can used Reserved Instances for cost savings


------------- Redshift – Snapshots & DR

• Redshift has “Multi-AZ” mode for some clusters

• Snapshots are point-in-time backups of a cluster, stored internally in S3

• Snapshots are incremental (only what has changed is saved)

• You can restore a snapshot into a new cluster

• Automated: every 8 hours, every 5 GB, or on a schedule. Set retention between 1 to 35 days

• Manual: snapshot is retained until you delete it

• You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region


-------------- Redshift Spectrum

• Query data that is already in S3 without loading it

• Must have a Redshift cluster available to start the query

• The query is then submitted to thousands of Redshift Spectrum nodes

-- Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables.

-- Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Amazon Redshift Spectrum queries use much less of your cluster's processing capacity than other queries.





=================================================== 3 Amazon OpenSearch Service ========================

• Amazon OpenSearch is successor to Amazon ElasticSearch

• In DynamoDB, queries only exist by primary key or indexes...

• With OpenSearch, you can search any field, even partially matches

• It’s common to use OpenSearch as a complement to another database

• Two modes: managed cluster or serverless cluster
 
• Does not natively support SQL (can be enabled via a plugin)

• Ingestion from Kinesis Data Firehose, AWS IoT, and CloudWatch Logs • Security through Cognito & IAM, KMS encryption,TLS

• Comes with OpenSearch Dashboards (visualization)

-- Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. 

-- OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), as well as visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions). 

-- Amazon OpenSearch Service currently has tens of thousands of active customers with hundreds of thousands of clusters under management processing trillions of requests per month.



--------------------------------------------------- LAB 

-- create domain 

-- give domain name 

-- Domain creation method = standard 

-- template = dev/test 

-- domain without stanadby

-- select AZ as u want how many 

-- Data nodes --> genreal Purpose instance family --> instance type = t2.small.search or t3.small.search instance --> nodes =1 

-- set network as u want 

-- Fine-grained access control --> create master user 

-- Access policy --> Configure domain level access policy = allow 

-- do create , it will take 15-20 min to create 


-- once it get created --> open url of domian 

-- login with ur credentials which u have given during creation of master user 

-- once u enter details --> add data --> add some sample data -- > u can searcgh as u want 






=========================================================== 4 Amazon EMR=========================

• EMR stands for “Elastic MapReduce”

• EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data

• The clusters can be made of hundreds of EC2 instances

• EMR comes bundled with Apache Spark, HBase, Presto, Flink...

• EMR takes care of all the provisioning and configuration

• Auto-scaling and integrated with Spot instances

• Use cases: data processing, machine learning, web indexing, big data...


-- it is not serverless service , u have to provision EMR and then create EMR jobs



--------------------------------------------- Amazon EMR – Node types & purchasing

• Master Node: Manage the cluster, coordinate, manage health – long running

• Core Node: Run tasks and store data – long running

• Task Node (optional): Just to run tasks – usually Spot

• Purchasing options:
   • On-demand: reliable, predictable, won’t be terminated
   • Reserved (min 1 year): cost savings (EMR will automatically use if available) 
   • Spot Instances: cheaper, can be terminated, less reliable

• Can have long-running cluster, or transient (temporary) cluster




=========================================================== 5 Amazon QuickSight ====================================

• Serverless machine learning-powered business intelligence service to create interactive dashboards

• Fast, automatically scalable, embeddable, with per-session pricing

• Use cases:
 • Business analytics
 • Building visualizations
 • Perform ad-hoc analysis
 • Get business insights using data

• Integrated with RDS, Aurora, Athena, Redshift, S3...

• In-memory computation using SPICE engine if data is imported into QuickSight

• Enterprise edition: Possibility to setup Column-Level security (CLS)


------------------------------- QuickSight – Dashboard & Analysis

• Define Users (standard versions) and Groups (enterprise version)
 • These users & groups only exist within QuickSight, not IAM !!

• A dashboard...
   • is a read-only snapshot of an analysis that you can share
   • preserves the configuration of the analysis (filtering, parameters, controls, sort)

• You can share the analysis or the dashboard with Users or Groups 

• To share a dashboard, you must first publish it

• Users who see the dashboard can also see the underlying data


QuickSight : 

-- Amazon QuickSight is a fully managed cloud-scale business(serverless) intelligence service that is powered by machine learning and allows you to create data visualizations and dashboards for the people you work with, wherever they are. It's a competitor to business intelligence tools like Tableau and Microsoft's Power BI.

-- Amazon QuickSight for software as a service (SaaS); QuickSight lets you easily create and publish interactive BI dashboards that include machine learning-powered insights.

QuickSight Features : 

-- pay per session pricing 

Eg : if a reader access QS for 30 minutes , u will be charged for that session only which is $0.30 , max charge is $5 per month 

-- Scales to tens of thousands of users 

-- ML insights 

-- Support for multiple platforms Android, IOS

-- EAsy Integration with AWS services and other third-party services(jira, service now, salesforce)


Working Flow 

S3 bucket which have data --> AWS lake formation ( aws glue Crawler--> glue catalog) --> Amazon Athena / Amazon EMR/AMazon Redshift Specturm --> Amazon QuickSight


Components of QuickSight :

1 SPICE :

S = Super-fast 
P = Parallel
I = In-memory
C = Calculation
E = Engine

-- automate and it is managed by AWS , once u load the data in the SPICE , u can generate as many visualizations as u want 

-- it is quite Costly 

10GB/User and $0.38/GB afterwards

Advantages :

-- fast for analytical Queries
-- more wait in Direct Queries
-- Data Stored in SPICE can be used multiple times 

Problem : What if source data got refreshed?

once u have data and nxt day u have added some data , that should be also display in visualization , in this process u have to refresh the day everytime and specify the intervals 



2 Direct Queries : (only for DB-source)

-- in this model u can genarate the visualizations instaed of loaded into the SPICE to overcome from the billing 

-- got Direct Queries , the dataSource must be a DataBase Such as RDS, Athena, DynamoDB

Advantages:

-- ONly charged when u create analysis 

-- NO need to refresh the data as we must do in SPICE


-- how to setup QuickSight account for first time users

-- go with enterprise edition 

-- choose Authentication method = Use IAM federated identities & QuickSight-managed users

-- give name of ur account   == SubbuAthena

-- give access to ur s3 bucket

--  done with registration 



===============================================Hands-On=============================================


Step 1 : create s3 bucket and insert some .csv data 

--  insert some  data in the bucket 

--   create one folder inside bucket and go to google and search for sample .csv files downlaod and upload in the s3 bucket 

Step 2 : Glue

-- open glue in console

-- glue --> left panel data Catalog --> crawlers --> add crawlers --> give name of ur crawler --> not yet --> add a data source --> choose s3 --> copy uri of ur bucket and paste in S3 path --> click on create new role , give name of ur role and select nxt --> click on create new database --> give name of ur database , nxt --> frequency = on-demand --> create crawler

-- now try to run the crawler , 

--  u can also check the logs in cloudwatch 

-- once u done with the running part , a table will added to the database , table name is same as the folder name of s3 that u have uploaded 

--  if ur data is not match , then u can also do edit ur schema 


step 3 : Athena

-- open Athena in the console

-- in the left panel u will see the database and table , u can check all the coloums of ur table

-- now do queries , before that u should add destination to store our outputs in the s3 bucket 

-- go to s3 buckets create one folder in the bucket to store the outputs

-- now try to query in athena editor

-- SELECT * FROM "weather-database"."weather_csv" limit 10;

above is eg query , u can replace with ur details 

weather-database = database name 

weather_csv = table name


NOTE : if u r getting zero records but query is successfull then do 

ur s3 location is like this 

s3://doc-example-bucket/table1.csv 

-- to get avoid from this error , jst create one sub-folder and upload .csv file in that subfolder 

s3://doc-example-bucket/table1/table1.csv



-- in the crawler location u have to give like this , then u won't get any error

s3://doc-example-bucket/table1/


Step 4 : open QuickSight 

-- create dataset --> choose athena , it is our use case now --> give name of ur dataSource --> validate connection n create dataSource --> select database --> select table of urs .

-- choose Direct query --> visualize

-- once u do this , in the left panel all the field list will visible and select any one as u want 

-- there will be sigltly delay in Direct queries 



================================================== 6 AWS Glue =============================

• Managed extract, transform, and load (ETL) service 

• Useful to prepare and transform data for analytics

• Fully serverless service


S3 Bucket / Amazon RDS ---(extract)----> Glue ETL ----------(load)---> Redshift Data Warehouse


- AWS Glue job is meant to be used for batch ETL data processing. 

- AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. 

- AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target.



---------------------------- Glue – things to know at a high-level

• Glue Job Bookmarks: prevent re-processing old data

• Glue Elastic Views:
    • Combine and replicate data across multiple data stores using SQL
    • No custom code, Glue monitors for changes in the source data, serverless 
    • Leverages a “virtual table” (materialized view)

• Glue DataBrew: clean and normalize data using pre-built transformation

• Glue Studio: new GUI to create, run and monitor ETL jobs in Glue

• Glue Streaming ETL (built on Apache Spark Structured Streaming): compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)


EPV : 

-- AWS Glue Studio is a graphical interface that makes it easy to create, run, and monitor extract, transform, and load (ETL) jobs in AWS Glue. 

-- DataBrew is a visual data preparation tool that enables you to clean and normalize data without writing any code.

   - In DataBrew, a recipe is a set of data transformation steps that you can author interactively in its intuitive visual interface.


------------------------------------------ LAB 


Solution overview :

-- In our fictitious use case, the requirement is to clean up a synthetic medical claims dataset created for this demo, which has some data quality issues introduced on purpose to demonstrate the DataBrew capabilities on data preparation. Then the claims data is ingested into the catalog (so it’s visible to analysts), after enriching it with some relevant details about the corresponding medical providers coming from a separate source.

-- The solution consists of an AWS Glue Studio visual job that reads two CSV files with claims and providers, respectively. The job applies a recipe of the first one to address the quality issues, select columns from the second one, join both datasets, and finally store the result on Amazon Simple Storage Service (Amazon S3), creating a table on the catalog so the output data can be used by other tools like Amazon Athena.





Source : https://aws.amazon.com/blogs/big-data/use-aws-glue-databrew-recipes-in-your-aws-glue-studio-visual-etl-jobs/#:~:text=AWS%20Glue%20Studio%20is%20a,data%20without%20writing%20any%20code.



STEP 1 :  Create a DataBrew recipe


 --   Start by registering the data store for the claims file. This will allow you to build the recipe in its interactive editor using the actual data so you can evaluate the result of the transformations as you define them.

1  Download the claims CSV file using the following link: alabama_claims_data_Jun2023.csv. (in the above BLOG link , u will find the .csv file for this demo )

2  On the DataBrew console, choose Datasets in the navigation pane, then choose Connect new dataset.

3  Choose the option File upload.

4  For Dataset name, enter Alabama claims.

5  For Select a file to upload, choose the file you just downloaded on your computer.

6  For Enter S3 destination, enter or browse to a bucket in your account and Region.

7  Leave the rest of the options by default (CSV separated with comma and with header) and complete the dataset creation.

8  Choose Project in the navigation pane, then choose Create project.

9   For Project name, name it ClaimsCleanup.

10  Under Recipe details, for Attached recipe, choose Create new recipe, name it ClaimsCleanup-recipe, and choose the Alabama claims dataset you just created.

11  Select a role suitable for DataBrew or create a new one, and complete the project creation.




 This will create a session using a configurable subset of the data. After it has initialized the session, you can notice some of the cells have invalid or missing values.



Observation : In addition to the missing values in the columns Diagnosis Code, Claim Amount, and Claim Date, some values in the data have some extra characters: Diagnosis Code values are sometimes prefixed with “code ” (space included), and Procedure Code values are sometimes followed by single quotes.
Claim Amount values will likely be used for some calculations, so convert to number, and Claim Data should be converted to date type.


-- Now that we identified the data quality issues to address, we need to decide how to deal with each case.
There are multiple ways you can add recipe steps, including using the column context menu, the toolbar on the top, or from the recipe summary. Using the last method, you can search for the indicated step type to replicate the recipe created in this post.



-- Claim Amount is essential for this use case, and the decision is to remove such rows.



12  Add the step Remove missing values.

13  For Source column, choose Claim Amount.

14  Leave the default action Delete rows with missing values and choose Apply to save it.
     
      - The view is now updated to reflect the step application and the rows with missing amounts are no longer there.

 - Diagnosis Code can be empty so this is accepted, but in the case of Claim Date, we want to have a reasonable estimation. The rows in the data are sorted in chronological order, so you can impute missing dates using the previews valid value from the preceding rows. Assuming every day has claims, the largest error would be assigning it to the preview day if it were the first claim that day missing the date; for illustration purposes, let’s consider that potential error acceptable.

 - First, convert the column from string to date type.


15  Add the step Change type.

16  Choose Claim Date as the column and date as the type, then choose Apply.

17  Now to do the imputation of missing dates, add the step Fill or impute missing values.

18  Select Fill with last valid value as the action and choose Claim Date as the source.

19  Choose Preview changes to validate it, then choose Apply to save the step.

      - So far, your recipe should have three steps


20  Next, add the step Remove quotation marks.

21  Choose the Procedure Code column and select Leading and trailing quotation marks.

22  Preview to verify it has the desired effect and apply the new step.

23  Add the step Remove special characters.

24  Choose the Claim Amount column and to be more specific, select Custom special characters and enter $ for Enter custom special characters.

25  Add a Change type step on the column Claim Amount and choose double as the type.

26  As the last step, to remove the superfluous “code ” prefix, add a Replace value or pattern step.

27  Choose the column Diagnosis Code, and for Enter custom value, enter code (with a space at the end).

     - Now that you have addressed all data quality issues identified on the sample, publish the project as a recipe.



28  Choose Publish in the Recipe pane, enter an optional description, and complete the publication.


-- Each time you publish, it will create a different version of the recipe. Later, you will be able to choose which version of the recipe to use.



STEP : 2  Create a visual ETL job in AWS Glue Studio


-- Next, you create the job that uses the recipe. Complete the following steps:

1  On the AWS Glue Studio console, choose Visual ETL in the navigation pane.

2  Choose Visual with a blank canvas and create the visual job.

3  At the top of the job, replace “Untitled job” with a name of your choice.

4  On the Job Details tab, specify a role that the job will use.

      - This needs to be an AWS Identity and Access Management (IAM) role suitable for AWS Glue with permissions to Amazon S3 and the AWS Glue Data Catalog. Note that the role used before for DataBrew is not usable for run jobs, so won’t be listed on the IAM Role drop-down menu here.
      
      - Create ROLE :
            - For role type, choose AWS Service, find and choose Glue, and choose Next: Permissions.
            - On the Attach permissions policy page, choose the policies that contain the required permissions; for example, the AWS managed policy AWSGlueServiceRole for general AWS Glue permissions and the AWS managed policy AmazonS3FullAccess for access to Amazon S3 resources. Then choose Next: Review.
            - For Role name, enter a name for your role; for example, AWSGlueServiceRoleDefault. Create the role with the name prefixed with the string AWSGlueServiceRole to allow the role to be passed from console users to the service. AWS Glue provided policies expect IAM service roles to begin with AWSGlueServiceRole. 



Observation : If you used only DataBrew jobs before, notice that in AWS Glue Studio, you can choose performance and cost settings, including worker size, auto scaling, and Flexible Execution, as well as use the latest AWS Glue 4.0 runtime and benefit from the significant performance improvements it brings. For this job, you can use the default settings, but reduce the requested number of workers in the interest of frugality. For this example, two workers will do.


5  On the Visual tab, add an S3 source and name it Providers.

6  For S3 URL, enter s3://awsglue-datasets/examples/medicare/Medicare_Hospital_Provider.csv

7  Select the format as CSV and choose Infer schema. Now the schema is listed on the Output schema tab using the file header.

   IMP : make sure  the header are in the same format that in the file (if it is capital letters change it to the capital letters )


8  In this use case, the decision is that not all columns in the providers dataset are needed, so we can discard the rest.

9  With the Providers node selected, add a Drop Fields transform (if you didn’t select the parent node, it won’t have one; in that case, assign the node parent manually).


10  Select all the fields after Provider Zip Code.

11  Later, this data will be joined by the claims for Alabama state using the provider; however, that second dataset doesn’t have the state specified. We can use knowledge of the data to optimize the join by filtering the data we really need.


12  Add a Filter transform as a child of Drop Fields.

13  Name it Alabama providers and add a condition that the state must match AL.

14  Add the second source (a new S3 source) and name it Alabama claims.

15  To enter the S3 URL, open DataBrew on a separate browser tab, choose Datasets in the navigation pane, and on the table copy the location shown on the table for Alabama claims (copy the text starting with s3://, not the http link associated). Then back on the visual job, paste it as S3 URL; if it is correct, you will see in the Output schema tab the data fields listed.

16  Select CSV format 

17  As a child of this source, search in the Add nodes menu for recipe and choose Data Preparation Recipe.

18  In this new node’s properties, give it the name Claim cleanup recipe and choose the recipe and version you published before.

19  You can review the recipe steps here and use the link to DataBrew to make changes if needed.

20  Add a Join node and select both Alabama providers and Claim cleanup recipes as the parent.

21  Add a join condition equaling the provider ID from both sources.

22  As the last step, add an S3 node as a target (note the first one listed when you search is the source; make sure you select the version that is listed as the target).

23  In the node configuration, leave the default format JSON and enter an S3 URL on which the job role has permission to write.

24  In the Data Catalog update options section, select the second option Create a table in the Data Catalog and on subsequent runs, update the schema and add new partitions, then select a database(create on DB in glue data catalog) on which you have permission to create tables.

25  Assign alabama_claims as the name and choose Claim Date as the partition key (this is for illustration purposes; a tiny table like this doesn’t really need partitions if further data won’t be added later).

26  Now you can save and run the job.

27  On the Runs tab, you can keep track of the process and see detailed job metrics using the job ID link.

28  The job should take a few minutes to complete.

29  When the job is complete, navigate to the Athena console.

30  Search for the table alabama_claims in the database you selected and, using the context menu, choose Preview Table, which will run a simple SELECT * SQL statement on the table.

31  You can see in the result of the job that the data was cleaned by the DataBrew recipe and enriched by the AWS Glue Studio join.

32  Apache Spark is the engine that runs the jobs created on AWS Glue Studio. Using the Spark UI on the event logs it produces, you can view insights about the job plan and run, which can help you understand how your job is performing and potential performance bottlenecks. For instance, for this job on a large dataset, you could use it to compare the impact of filtering explicitly the provider state before doing the join, or identify if you can benefit from adding an Autobalance transform to improve parallelism.




Conclusion

-- In this demo, we showed how you can use AWS DataBrew to build a recipe using the provided interactive editor and then use the published recipe as part of an AWS Glue Studio visual ETL job. We included some examples of common tasks that are required when doing data preparation and ingesting data into AWS Glue Catalog tables.

-- This example used a single recipe in the visual job, but it’s possible to use multiple recipes at different parts of the ETL process, as well as reusing the same recipe on multiple jobs.

-- These AWS Glue solutions allow you to effectively create advanced ETL pipelines that are straightforward to build and maintain, all without writing any code. You can start creating solutions that combine both tools today.





=============================================== 7 AWS Lake Formation ==========================

• Data lake = central place to have all your data for analytics purposes

• Fully managed service that makes it easy to setup a data lake in days

• Discover, cleanse, transform, and ingest data into your Data Lake

• It automates many complex manual steps (collecting, cleansing, moving, cataloging data, ...) and de-duplicate (using ML Transforms)

• Combine structured and unstructured data in the data lake

• Out-of-the-box source blueprints: S3, RDS, Relational & NoSQL DB...

• Fine-grained Access Control for your applications (row and column-level)

• Built on top of AWS Glue



============================================== Kinesis Data Analytics (SQL application) ==========================

• Real-time analytics on Kinesis Data Streams & Firehose using SQL 

• Add reference data from Amazon S3 to enrich streaming data

• Fully managed, no servers to provision

• Automatic scaling

• Pay for actual consumption rate

• Output:
  • Kinesis Data Streams: create streams out of the real-time analytics queries 
  • Kinesis Data Firehose: send analytics query results to destinations
  -- Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.

  -- It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. 

• Use cases:
  • Time-series analytics 
  • Real-time dashboards 
  • Real-time metrics


EPV : An IT company is working on client engagement to build a real-time data analytics tool for the Internet of Things (IoT) data. The IoT data is funneled into Amazon Kinesis Data Streams which further acts as the source of a delivery stream for Amazon Kinesis Firehose. The engineering team has now configured a Kinesis Agent to send IoT data from another set of devices to the same Amazon Kinesis Firehose delivery stream. They noticed that data is not reaching Kinesis Firehose as expected. As a solutions architect, which of the following options would you attribute as the MOST plausible root cause behind this issue?

ANS : Kinesis Agent cannot write to Amazon Kinesis Firehose for which the delivery stream source is already set as Amazon Kinesis Data Streams

EXP : When an Amazon Kinesis Data Stream is configured as the source of a Kinesis Firehose delivery stream, Firehose’s PutRecord and PutRecordBatch operations are disabled and Kinesis Agent cannot write to Kinesis Firehose Delivery Stream directly. 

-- Data needs to be added to the Amazon Kinesis Data Stream through the Kinesis Data Streams PutRecord and PutRecords operations instead.



========================================================== Machine Learning================================================

----------------------------- 1 Amazon Rekognition

• Find objects, people, text, scenes in images and videos using ML

• Facial analysis and facial search to do user verification, people counting

• Create a database of “familiar faces” or compare against celebrities

• Use cases:
 • Labeling
 • Content Moderation
 • Text Detection
 • Face Detection and Analysis (gender, age range, emotions...)
 • Face Search and Verification
 • Celebrity Recognition
 • Pathing (ex: for sports game analysis)


 ---------- Amazon Rekognition – Content Moderation

 • Detect content that is inappropriate, unwanted, or offensive (image and videos)

 • Used in social media, broadcast media, advertising, and e-commerce situations to create a safer user experience

 • Set a Minimum Confidence Threshold for items that will be flagged

 • Flag sensitive content for manual review in Amazon Augmented AI (A2I)

 • Help comply with regulations


 Image --> Amazon Rekognition --> Confidence Level and Threshold --> Optional Manual review in A2I



 ------------------------------ 2 Amazon Transcribe

 • Automatically convert speech to text

 • Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately

 • Automatically remove Personally Identifiable Information (PII) using Redaction

 • Supports Automatic Language Identification for multi-lingual audio

 • Use cases:
   • transcribe customer service calls
   • automate closed captioning and subtitling
   • generate metadata for media assets to create a fully searchable archive

------------------ LAB 

-- create one s3 bucket and upload audio or video file in it 

-- open transcribe --> create job and copy URI of object and paste in the bucket path --> create job in transribe 

- once it get completed it will convert speech to text 

-- go n check 

----- working ......


---------------------------------------------- 3 Amazon Polly

• Turn text into life like speech using deep learning

• Allowing you to create applications that talk

--------------- Amazon Polly – Lexicon & SSML

• Customize the pronunciation of words with Pronunciation lexicons
  • Stylized words: Su3b4u=> “subbu”
  • Acronyms:AWS=>“AmazonWebServices”

• Upload the lexicons and use them in the SynthesizeSpeech operation

• Generate speech from plain text or from documents marked up with Speech Synthesis Markup Language (SSML) – enables more customization

 • emphasizing specific words or phrases
 • using phonetic pronunciation
 • including breathing sounds, whispering
 • using the Newscaster speaking style


------------LAB 

-- open polly , type text u want n click on listen 

-- turn on SSML , try to add break point in the middle of the text 

eg : <speak>Hi! My name is subbu <break time="2s" /> I will read any text you type here.</speak>

-- it will stop 2 sec and continue 

--- now if u have text AWS and it would like to convert Abbrevations like Amazon web service

-- create one .xml file and add content to it like 

<?xml version="1.0" encoding="UTF-8"?>
 <lexicon version="1.0" 
     xmlns="http://www.w3.org/2005/01/pronunciation-lexicon"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
     xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon 
       http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd"
     alphabet="x-sampa" xml:lang="en-US">

<lexeme>
<grapheme>AWS</grapheme>
<alias>Amazon Web Service</alias>
</lexeme>

 </lexicon>


 -- here the language is very imp , u have to give same as ur polly language like en-US

 -- so u have AWS in text but once u upload lex file in .xml format it will read amazon web service instead of AWS

 -- additional settings --> add lex.xml file 



 ----------------------------------- 4 Amazon Translate


 • Natural and accurate language translation

 • Amazon Translate allows you to localize content - such as websites and applications - for international users, and to easily translate large volumes of text efficiently.



 -------------------------------------- 5 Amazon Lex & Connect

• Amazon Lex: (same technology that powers Alexa)
  • Automatic Speech Recognition (ASR) to convert speech to text
  • Natural Language Understanding to recognize the intent of text, callers 
  • Helps build chatbots, call center bots

• Amazon Connect:
 • Receive calls, create contact flows, cloud-based vir tual contact center
 • Can integrate with other CRM systems or AWS
 • No upfront payments, 80% cheaper than traditional contact center solutions

Phone Call Schedule an Appointment ----(call)---> Connect ------(stream)----> lex Intent recognized -----(invoke)---> lambda ----(schedule)----> CRM


--------------------------- 6 Amazon Comprehend

• For Natural Language Processing – NLP

• Fully managed and serverless service

• Uses machine learning to find insights and relationships in text
 • Language of the text
 • Extractskeyphrases,places,people,brands,orevents
 • Understands how positive or negative the text is
 • Analyzes text using tokenization and parts of speech
 • Automatically organizes a collection of text files by topic 

• Sample use cases:
  • analyze customer interactions (emails) to find what leads to a positive or negative experience
  • Create and groups articles by topics that Comprehend will uncover


----------- Amazon Comprehend Medical

• Amazon Comprehend Medical detects and returns useful information in unstructured clinical text:

 • Physician’s notes
 • Discharge summaries • Test results
 • Case notes 

• Uses NLP to detect Protected Health Information (PHI) – DetectPHI API

• Store your documents in Amazon S3, analyze real-time data with Kinesis Data Firehose, or use Amazon Transcribe to transcribe patient narratives into text that can be analyzed by Amazon Comprehend Medical.


------------------------------------------------ 7 Amazon SageMaker

• Fully managed service for developers / data scientists to build ML models 

• Typically, difficult to do all the processes in one place + provision servers 

• Machine learning process (simplified): predicting your exam score

Historical Data:
# years of experience in IT                                -----------> score ------> ML model <----Train and Tune
# years of experience with AWS Time spent on the course
...



--------------------------------------- 8 Amazon Forecast

• Fully managed service that uses ML to deliver highly accurate forecasts • Example: predict the future sales of a raincoat

• 50% more accurate than looking at the data itself

• Reduce forecasting time from months to hours

• Use cases: Product Demand Planning, Financial Planning, Resource Planning, ...



Historical Time-series Data: ---> Amazon S3 ---> Amazon Forecast -----(produce)--> Forecasting Model --> Future sales of raincoat: $500,000


------------------------------------- 9 Amazon Kendra

• Fully managed document search service powered by Machine Learning

• Extract answers from within a document (text, pdf, HTML, PowerPoint, MS Word, FAQs...) 

• Natural language search capabilities

• Learn from user interactions/feedback to promote preferred results (Incremental Learning) 

• Ability to manually fine-tune search results (importance of data, freshness, custom, ...)



------------------------------------- 10 Amazon Personalize

• Fully managed ML-service to build apps with real-time personalized recommendations

• Example: personalized product recommendations/re-ranking, customized direct marketing

 • Example:User bought gardening tools,provide recommendations on the next one to buy



------------------------------------- 11 AmazonTextract

• Automatically extracts text, handwriting, and data from any scanned documents using AI and ML

• Extract data from forms and tables

• Read and process any type of document (PDFs, images, ...)

• Use cases:

 • Financial Services (e.g., invoices, financial reports)
 • Healthcare (e.g., medical records, insurance claims)
 • Public Sector (e.g., tax forms, ID documents, passports)


------------------------------------ AWS Machine Learning - Summary

• Rekognition: face detection, labeling, celebrity recognition • Transcribe: audio to text (ex: subtitles)

• Polly: text to audio

• Translate: translations

• Lex: build conversational bots – chatbots

• Connect: cloud contact center

• Comprehend: natural language processing

• SageMaker: machine learning for every developer and data scientist • Forecast: build highly accurate forecasts

• Kendra: ML-powered search engine

• Personalize: real-time personalized recommendations • Textract: detect text and data in documents





=========================================================== Other Services =======================================

------------------------ 1  What is CloudFormation

• CloudFormation is a declarative way of outlining your AWS Infrastructure, for any resources (most of them are supported).

• For example, within a CloudFormation template, you say:

  • I want a security group
  • I want two EC2 instances using this security group
  • I want an S3 bucket
  • I want a load balancer (ELB) in front of these machines
  - AWS Cloudformation template is a JSON or YAML-format, text-based file that describes all the AWS resources you need to deploy to run your application. 
  - A template acts as a blueprint for a stack. AWS CloudFormation templates cannot be used to deploy the same template across AWS accounts and regions.

• Then CloudFormation creates those for you, in the right order, with the exact configuration that you specify



EPV : A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc.

ANS : Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions

EXP : AWS CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation.

--  A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. Using an administrator account of an "AWS Organization", you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts of an "AWS Organization" across specified regions.





------------------------ 2 Amazon Simple Email Service (Amazon SES)

• Fully managed service to send emails securely, globally and at scale

• Allows inbound/outbound emails

• Reputation dashboard, performance insights, anti-spam feedback

• Provides statistics such as email deliveries, bounces, feedback loop results, email open

• Supports DomainKeys Identified Mail (DKIM) and Sender Policy Framework (SPF)

• Flexible IP deployment: shared, dedicated, and customer-owned IPs

• Send emails using your application using AWS Console, APIs, or SMTP

• Use cases: transactional, marketing and bulk email communications


------------------------- 3  Amazon Pinpoint

• Scalable 2-way (outbound/inbound) marketing communications service

• Supports email, SMS, push, voice, and in-app messaging

• Ability to segment and personalize messages with the right content to customers

• Possibility to receive replies

• Scales to billions of messages per day

• Use cases: run campaigns by sending marketing, bulk, transactional SMS messages

• Versus Amazon SNS or Amazon SES 

  • In SNS & SES you managed each message's audience, content, and delivery schedule
  • InAmazonPinpoint,youcreatemessagetemplates, delivery schedules, highly-targeted segments, and full campaigns


--------------------------- 4 Cost Explorer

• Visualize, understand, and manage your AWS costs and usage over Time

• Create custom reports that analyze cost and usage data.

• Analyze your data at a high level: total costs and usage across all accounts

• Or Monthly, hourly, resource level granularity

• Choose an optimal Savings Plan (to lower prices on your bill)

• Forecast usage up to 12 months based on previous usage


-------------------------- 5 AWS Batch 

• Fully managed batch processing at any scale

• Efficiently run 100,000s of computing batch jobs on AWS

• A “batch” job is a job with a start and an end (opposed to continuous)

• Batch will dynamically launch EC2 instances or Spot Instances

• AWS Batch provisions the right amount of compute / memory

• You submit or schedule batch jobs and AWS Batch does the rest!

• Batch jobs are defined as Docker images and run on ECS

• Helpful for cost optimizations and focusing less on the infrastructure


---------------- Batch vs Lambda

• Lambda:
  • Time limit
  • Limited runtimes
  • Limited temporary disk space • Serverless


• Batch:
 • No time limit
 • Any runtime as long as it’s packaged as a Docker image
 • Rely on EBS / instance store for disk space
 • Relies on EC2 (can be managed by AWS)


------------------------------------------ 6 AWS Amplify - web and mobile applications

• A set of tools and services that helps you develop and deploy scalable full stack web and mobile applications

• Authentication,Storage,API(REST,GraphQL),CI/CD,PubSub,Analytics,AI/MLPredictions, Monitoring, ...

• Connect your source code from GitHub,AWSCodeCommit,Bitbucket,GitLab,or upload directly






------------------------------------------ 7 AWS Outposts

--  AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises. 

--  By providing local access to AWS managed infrastructure, AWS Outposts enables customers to build and run applications on premises using the same programming interfaces as in AWS Regions, while using local compute and storage resources for lower latency and local data processing needs.

--  An Outpost is a pool of AWS compute and storage capacity deployed at a customer site. AWS operates, monitors, and manages this capacity as part of an AWS Region. 

--  You can create subnets on your Outpost and specify them when you create AWS resources such as EC2 instances, EBS volumes, ECS clusters, and RDS instances.

--  Instances in Outpost subnets communicate with other instances in the AWS Region using private IP addresses, all within the same VPC.


IMP Note You cannot connect an Outpost to another Outpost or Local Zone that is within the same VPC.


-------- Key concepts

-- These are the key concepts for AWS Outposts.


1 Outpost site 

         – The customer-managed physical buildings where AWS will install your Outpost. A site must meet the facility, networking, and power requirements for your Outpost.

2 Outpost capacity

         – Compute and storage resources available on the Outpost. You can view and manage the capacity for your Outpost from the AWS Outposts console.

3 Outpost equipment 

         – Physical hardware that provides access to the AWS Outposts service. The hardware includes racks, servers, switches, and cabling owned and managed by AWS.

4 Outposts racks 

         – An Outpost form factor that is an industry-standard 42U rack. Outpost racks include rack-mountable servers, switches, a network patch panel, a power shelf and blank panels.

5 Outposts servers 

         – An Outpost form factor that is an industry-standard 1U or 2U server, which can be installed in a standard EIA-310D 19 compliant 4 post rack. Outpost servers provide local compute and networking services to sites that have limited space or smaller capacity requirements.

6 Service link 

         – Network route that enables communication between your Outpost and its associated AWS Region. Each Outpost is an extension of an Availability Zone and its associated Region.

7 Local gateway (LGW) 

         – A logical interconnect virtual router that enables communication between an Outpost rack and your on-premises network.

8 Local network interface 

         – A network interface that enables communication from an Outpost server and your on-premises network.





------- AWS resources on Outposts


-- You can create the following resources on your Outpost to support low-latency workloads that must run in close proximity to on-premises data and applications:

1 Compute  
 
    Resource type	                  Racks	       Servers

    Amazon EC2 instances             Yes	         Yes	

    Amazon ECS clusters              Yes           Yes

    Amazon EKS nodes                 Yes	         No


2 Database and analytics

     
         Resource type	                                                                Racks	       Servers
         
        Amazon ElastiCache nodes (Redis cluster, Memcached cluster)                      YES          NO

        Amazon EMR clusters                                                              YES         NO

        Amazon RDS DB instances                                                          YES         NO



3 Networking

                 Resource type	                              Racks	       Servers
                  
                App Mesh Envoy proxy                           YES          YES

                Application Load Balancers                     YES          NO

                Amazon VPC subnets	                           YES          YES

                Amazon Route 53	                               YES          NO



4 Storage

                 Resource type	                              Racks	       Servers
                           
                 Amazon EBS volumes                            YES          NO

                 Amazon S3 buckets                             YES          NO


5 Other AWS services

                 Service                               Racks	       Servers

                 AWS IoT Greengrass	                    YES            YES

                 Amazon SageMaker Edge Manager	        YES             YES





----- Pricing

-- You can choose from a variety of Outpost configurations, each providing a combination of EC2 instance types and storage options. The price for rack configurations includes installation, removal, and maintenance. For servers, you must install and maintain the equipment.

-- You purchase a configuration for a 3-year term and can choose from three payment options: All Upfront, Partial Upfront, and No Upfront. 

-- If you choose the Partial option or the No Upfront payment option, monthly charges will apply. 

-- Any upfront charges apply 24 hours after your Outpost is installed and the compute and storage capacity is available for use. 





----------------------- Local Zones vs Outposts


Outposts:


-- for example , u have a company , ur employess are going to cream stone to have a ice cream during a break time which is three miles away from ur office  , u have noticed that it will get impact on the work , so u have asked cream stone and set up their outlet in ur compnay building ,  to improve productivity and make employees also happy at the same time , here u are providing the facilities and cream stone will set up their services , this what exactly happen in the AWS outposts 

-- so many companies have applications that need single digit millisecond latency not only that in some cases there are regulatory constraints that your data has to be processed in hosts we know that AWS regions are not there in every country .

-- it is possible that the companies in one country may be using the region in the neighboring country but there can be regulatory restrictions that the data which they are processing using the AWS Services it cannot leave the boundary of country

-- so in such cases what the clients do is they will request AWS to come and set up their services AWS services in their own data center 

-- so basically just like you provide the facility to creamstone to set up the shop , your it department will have to provide the Rackspace in your own Data Center and this will be very useful when you want to use applications which need  single digit millisecond latency such as  arvr applications virtualized desktops , gaming development projects , media content creation 




Local Zones :

-- now assume that let us go back to that same  shop example assume that your company is part of an a big huge it Park where there are dozens of companies

-- and every company has a lot of employees who are ice cream lovers so every company is facing the same issue of productivity impact 

-- so these companies May request creamstone to come and set up a outlet in the neighborhood somewhere in the campus in the it Parks campus or just outside.

-- so here in this case creamstone will set up the facility just remember in the other case your company is providing the facility

-- but in this case creamstone will take up their own building and they will set up all the infrastructure and they will start providing the service

-- local zone is exactly same AWS when they find that there is a concentration of their Enterprise clients in a particular City they can actually come  and set up a local Zone with limited set of services uch as ec2 VPC EFS Etc 


-- all the companies in the nearby areas can actually make use of the services in  that local Zone that is called local zone 



IMP to know 

-- Q: How are Local Zones different from Availability Zones? 

ANS : 

-- Local Zones are designed to bring the core services needed for the latency-sensitive portions of your workload closer to end users,

-- while Availability Zones provide access to the full array of AWS services. 

-- Services like Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Block Store (Amazon EBS), Amazon Virtual Private Cloud (Amazon VPC), and others are locally available and can be used to serve end users in geographic proximity with extremely low latency.

-- Other AWS services like Amazon Simple Storage Service (Amazon S3) and Amazon Aurora are accessible privately through VPC over AWS private network. Both Local Zones and Availability Zones allow you to build applications for high availability.






------ conclusion :


-- AWS Outpost is an extension of aws's infrastructure to your data center 

-- whereas local zone is provided from a small data center which is owned and maintained and operated by AWS in your neighborhood , so that all the companies in that area can get low latency services

-- it provides only limited services but still the local zones have proved successful










=============================================== White Papers & Architectures =========================


-- The AWS Well-Architected Framework is basically a body of knowledge that describes the various design principles, key concepts, design and architectural best practices that can help companies design and run highly efficient workloads in the AWS platform. 

-- This framework ensures that the company’s cloud architecture is in accordance with the AWS best practices.

-- It also comes with related AWS features, services and tools that you can utilize to measure the overall efficiency of your design.

-- The AWS Well-Architected Framework will empower you to improve your existing IT infrastructure in terms of your overall operations, security, reliability, efficiency, cost optimization, and sustainability.

-- Having well-architected systems greatly increases the plausibility of business success, which is why AWS created the AWS Well-Architected Framework. 

-- This framework is composed of six pillars that help you understand the pros and cons of the decisions you make while building cloud architectures and systems on the AWS platform. 

-- You will learn the architectural best practices for designing and operating reliable, efficient, cost-effective and secure systems in the cloud by using the framework.

-- This framework also provides a way to consistently measure your architectures against best practices and identify areas for improvement.


-------------------------------- How do you use the AWS Well-Architected Framework?

-- Say, for example, you are developing an online solution that handles sensitive financial information. Your system has passed all the integration tests and is finally ready for production deployment any time soon. However, you still want to ensure that your cloud infrastructure in AWS is indeed secure as part of your corporate security compliance.

-- You can check the security pillar of the AWS Well-Architected Framework that focuses on protecting your data, files, and overall systems.  This includes key topics on data integrity, managing user permissions, and establishing controls to detect security incidents.

-- In essence, you can improve your cloud designs by simply answering the evaluation questions and following the best practices provided by this framework. These questions will shed light on your existing or new architecture in the AWS Cloud. It has questions like:

“How do you protect your data at rest?”
“How do you protect your data in transit?”
“How do you manage identities for people and machines?”
…and so on and so forth.

-- Your answer to these questions can show if your cloud architecture is secure or not. If you responded “I don’t know” in the “How do you protect your data at rest?” question, then that means your architecture is not secure and has a high number of security vulnerabilities. This signifies that you don’t employ encryption and tokenization schemes in your system.

-- The same goes for the “How do you protect your data in transit?” query. If you answer that you do not protect your data in transit, then that indicates your architecture has no firewall rules, network authentication, secure key management, and other mechanisms to keep your sensitive data safe as it traverses through different systems and networks. With this realization, you can now resolve the deficiencies in your system by following the prescriptive guidance provided by the AWS Well-Architected Framework.



--------------- Well Architected Framework General Guiding Principles 

• Stop guessing your capacity needs

• Test systems at production scale

• Automate to make architectural experimentation easier

• Allow for evolutionary architectures

  • Design based on changing requirements

• Drive architectures using data

• Improve through game days
  • Simulate applications for flash sale days





----------------- Well Architected Framework 6 Pillars (PROCSS)

• 1  Operational Excellence 
• 2  Security
• 3  Reliability
• 4  Performance Efficiency 
• 5  Cost Optimization
• 6  Sustainability

• They are not something to balance, or trade-offs, they’re a synergy(the working together of two things to produce an effect greater than the sum of their individual effects.)



1. Operational Excellence


-- The ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.

-- There are four best practice areas and tools for operational excellence in the cloud:

A   Organization – AWS Cloud Compliance, AWS Trusted Advisor, AWS Organizations

B   Prepare – AWS Config

c   Operate – Amazon CloudWatch

D   Evolve – Amazon Elasticsearch Service


Key AWS service:
    AWS CloudFormation for creating templates. (See AWS Management Tools Cheat Sheet)


2. Security

-- The ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies.

-- There are six best practice areas and tools for security in the cloud:

A Security – AWS Shared Responsibility Model, AWS Config, AWS Trusted Advisor

B Identity and Access Management – IAM, Multi-Factor Authentication, AWS Organizations

C Detective Controls – AWS CloudTrail, AWS Config, Amazon GuardDuty

D Infrastructure Protection – Amazon VPC, Amazon CloudFront with AWS Shield, AWS WAF

E Data Protection – ELB, Amazon Elastic Block Store (Amazon EBS), Amazon S3, and Amazon Relational Database Service (Amazon RDS)

F Incident Response – IAM, Amazon CloudWatch Events


Key AWS service:
   AWS Identity and Access Management (IAM)



3. Reliability

-- The ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.

-- There are four best practice areas and tools for reliability in the cloud:

A Foundations – IAM, Amazon VPC, AWS Trusted Advisor, AWS Shield
 
B Change Management – AWS CloudTrail, AWS Config, Auto Scaling, Amazon CloudWatch

C Failure Management – AWS CloudFormation, Amazon S3, AWS KMS, Amazon Glacier

D Workload Architecture –  AWS SDK, AWS Lambda


Key AWS service:
   Amazon CloudWatch



4. Performance Efficiency

-- The ability to use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve.

-- There are four best practice areas for performance efficiency in the cloud:


A Selection – Auto Scaling for Compute, Amazon EBS and S3 for Storage, Amazon RDS and DynamoDB for Database, Route53, VPC, and AWS Direct Connect for Network

B Review – AWS Blog and What’s New section of the website

C Monitoring –  Amazon CloudWatch

D Tradeoffs – Amazon Elasticache, Amazon CloudFront, AWS Snowball, Amazon RDS read replicas.


Key AWS service:
  Amazon CloudWatch



5. Cost Optimization

-- The ability to avoid or eliminate unneeded cost or suboptimal resources.

-- There are five best practice areas and tools for cost optimization in the cloud:

A Cloud Financial Management – Amazon QuickSight, AWS Cost and Usage Report (CUR)

B Cost-Effective Resources – Cost Explorer, Amazon CloudWatch and Trusted Advisor, Amazon Aurora for RDS, AWS Direct Connect with Amazon CloudFront

C Matching supply and demand – Auto Scaling

D Expenditure Awareness –  AWS Cost Explorer, AWS Budgets

E Optimizing Over Time – AWS News Blog and the What’s New section on the AWS website, AWS Trusted Advisor


Key AWS service:
  Cost Explorer


6. Sustainability

-- The ability to increase efficiency across all components of a workload by maximizing the benefits from the provisioned resources.

-- There are six best practice areas for sustainability in the cloud:

A Region Selection – AWS Global Infrastructure

B User Behavior Patterns – Auto Scaling, Elastic Load Balancing

C Software and Architecture Patterns – AWS Design Principles

D Data Patterns – Amazon EBS,  Amazon EFS, Amazon FSx, Amazon S3

E Hardware Patterns – Amazon EC2, AWS Elastic Beanstalk

F Development and Deployment Process – AWS CloudFormation


Key AWS service:
  Amazon EC2 Auto Scaling



---------------------------------------------- AWS Well-Architected Tool ----

• Free tool to review your architectures against the 6 pillars Well-Architected Framework and adopt architectural best practices

• How does it work?
  • Select your workload and answer questions
  • Review your answers against the 6 pillars
  • Obtain advice: get videos and documentations, generate a report, see the results in a dashboard




------------------------------------- LAB 


-- open  AWS Well-Architected Tool

-- Environment = pre-Production --> choose region --> choose lens --> define workload

-- open ur workload and explore the things 

--  milestone : this is one of the most important aspects of using the tool because a milestone records the state of a workload at a particular point of time

-- now do review for the questions according to the pillars

-- do answer the questions , here i have answered all the questions , but i did not check all the boxes 

-- once u complete this , it will show u high risks and medium risks in each pillars 

-- do save this milestone , u can also generate report for this , in this report it has clear details about the answers that u have seleted and what are the improvements to do 

-- here u have some high and medium risks 

-- so now do continue review and check all the boxes , in real timeit won't happen 

-- now u do not have any errors and risks , save mile stone and generate report 

-- this is how u can do with this tool 


----------------------------------------------- Trusted Advisor 

• No need to install anything – high level AWS account assessment

• Analyze your AWS accounts and provides recommendation on 6 categories:

 • Cost optimization 
 • Performance
 • Security
 • Fault tolerance
 • Service limits
 • Operational Excellence


• Business & Enterprise Support plan

   • Full Set of Checks
   • Programmatic Access using AWS Support API

 


------------------------------------------------------- some scenario based question ---------------------------------------------------



1   u have been tasked with encrypting EBS Volumes that are restored from unencrypted EBS snapshots . What should u do ? 

ANS :  To encrypt EBS volumes restored from unencrypted EBS snapshots , u can copy the snapshots and enable encryption with a Symmetric Customer Master key (CMK) while creating an EBS volume using the snapshot. This ensures that the restored volumes are encrypted .


2. U need to limit the maximum number of request from a single IP address. How can u achieve this? 

Ans :  T0 limit the max number of requests from a single IP address, u can create a  “rate-based” rule in was web Application Firewall (WAF) and set the desired rate limit . This helps protect your applications from excessive traffic from Single IP Address.


3  u want to grant the bucket full access to all upload objects in an S3 bucket . What steps should you take ?

Ans :  To grant the bucket owner full access to all uploaded objects in an s3 bucket , u can create a bucket policy that requires users to set the objects Access control list (ACL) to “BUCKET-OWNER_FULL_CONTROL” , This ensures that the bucket owner has complete control over the objects in the bucket 


4. How to protect accidental deletion or overwrite objects in an S3 bucket ? 

Ans:  To protect objects in an S3 bucket from accidental deletion or overwrite , u can enable versioning and enable multi-factor Authentication(MFA) delete. 

Versioning Keeps multiple versions of an object , and MFA delete requires additional authentication before deleting or overwriting objects , adding an extra layer of protection .


5. U want to access resources on both on-premises and aws using on-premises credentials stored in Active Directory . How can u Accomplish this ?

Ans:  To access resource both , u can set up SAML 2.o-Based federation using a Microsoft Active Directory Federation Service (AD FS). This allows users to authenticate with their on-premises credentials and access AWS resources Securely .


6  u need to secure the sensitive data stored in EBS volumes . What should u do ?

Ans: To do this , u can enable EBS Encryption. This encrypts the data at rest, providing an additional layer of security for ur sensitive information 

7 u want to ensure that the data-in-transit and data-at-rest of an amazon S3 bucket is always encrypted . How can u achieve this?]

Ans:  u can enabled Amazon S3 server-Side Encryption or use Client-Side Encryption . These encryption methods protect ur data from unauthorised access during transmission and storage 


8  u need to secure a web application by allowing multiple domains to serve SSL traffic over the same IP address . What steps should u take ? 

Ans :  u can use AWS Certificate Manager (ACM) to generate an SSL certificate. Associate the certificate to a CLOUDFront Distribution and enable Server Name Indication(SNI) , which allows multiple SSL certificates to be served from a single IP address 


9. U want to control access for several buckets by using a gateway endpoint to allow access to trusted buckets . How can u achieve this?

Ans :  u can create an endpoint policy for the trusted s3 buckets . This policy specifies the permissions and access controls for the buckets accessed through the gateway endpoint 


10  u need to setup Asynchronous data replication to another RDS Db instance hosted in another AWS Region . How can you achieve this?

Ans :  u can create a Read Replica . This allows us to to replicate ur data from the primary RDS DB instance to the replica, ensuring data redundancy and availability 


11.  U require	 a parallel file system for “hot” (frequently accessed) data . What AWS services can fulfil this requirement?

Ans :  u can utilize Amazon Fix for lustre . It is a fully managed , high performance file system optimised for workloads that require fast access to data 


12.  U need to implement synchronous data replication across Availability Zones with automatic failover in Amazon RDS . What should u do ?

Ans :  to achieve synchronous data replication across AZ with automatic failover in A RDS , u can enable “MULTI-AZ “ deployment. This configuration automatically replicates ur databases synchronously to a standby instance in a different Availability Zone, ensuring high availability and automatic failover.


IMP Note : synchronous : Multi-AZ  deployment 

Asynchronous : read Replicas 



13. U require a storage service to host “cold”( infrequently accessed) data. Which was service fulfilled this requirement?

Ans :  to host infrequently accessed data u can leverage amazon S3 Glacier . It is a low-cost storage service designed for long-term archiving and backups of data that is accessed less frequently 


14. U need to setup a relational databases with a disaster recovery plan having an RPO of 1 second and RTO of less than 1 minute . What should You use ? 

Ans :  u can utilise Amazon Aurora Global database. It provides low-latency global replication, enabling rapid failover and ensuring minimal data loss 


15. U want to monitor databases metrics and receive email notifications when a specific threshold is breached . How can u accomplish this ? 

Ans :  u can create an SNS (simple Notification Service ) topic and add the topic in the cloud watch alarm . This enables us to to receive alerts via email when specific metric thresholds are crossed 


16.  U need to setup DNS failover to a static website . How can you achieve this?

Ans :  us  R53 


17. U want to implement automated backups for all ur EBS volumes . What should u do ? 

Ans:  u can use Amazon data LifeCycle Manager . This service allows u to automate the creation and recreation of EBS snapshots based on customisable schedules and lifecycle policies 

18. U need to monitor the available swap space of ur ec2 instances . How can u monitor this metric ? 

Ans :  to monitor the available swap space of ur ec2 instances , u can install the Cloudwatch Agent and configure it to monitore the Swaputilization metric . This enables us to to track the swap space usage and take appropriate actions if necessary 


19.  How to implement fanout messaging ?

Ans:  to implement this u can create an SNS topics with a message filtering policy and configure multiple SQS queues to subscribe to the topic 


20.  U require a DB that has a read replication latency of less than 1 Second? 

Ans:  u can use Amazon Aurora with cross-region replicas 


21 A specific type LB that use UDP a the protocol for communications b/w clients and thousands of game servers around the world 

Ans : use network load balancer  for TCP/UDP protocols 


22 how to monitor disk and memory space utilisation of an ec2 instance ?

Ans :  install Cloud watch Agent  on instance  


23. Retrieve a subset of data from a large CSV file stored in the s3 bucket ? 

Ans:  perform S3 select operation based on the bucket’s name and object’s key.

S3 select allows u to select specific portion of data from large objects  reducing the amount of data transfer  and also improving performance at the same time 


24.  U have to upload 1TB file to s3 

Ans : use Multi-part upload API to upload large objects in parts 


25.  Improve the performance of the applications by reducing the response times from milliseconds to microseconds ?

Ans:  use  Amazon DynamoDb Accelerator (DAX) , it is in-memory caching service for the Dynamodb , it significantly reduce the database read latency  and improves application performance 


26.  IMP q . Retrieve the instance Id , Public Keys , and Public IP address of an Ec2 instance . 

Ans:  Access the URL 


TOKEN=‘curl -X PUT ”http://169.254.169.254/latest/api/token” -H ”X-aws-ec2-metadata-token-ttl-seconds: 21600”‘ \ && curl -H ”X-aws-ec2- metadata-token: $TOKEN” -v http://169.254.169.254/latest/meta-data/ 

This is also Called magic Ip Address 


27 Route the internet traffic to the resource based on the location of the user ? 
Ans :  use R53 Geolocation Routing Policy 

28.  Suggest A cost -effective soln for over-provisioning of resource ?
Ans:  configure a target Tracking Scaling in ASG , this automatically adjust no.of instances  based on pre-defined metrics optimizing resource allocation and cost management 

29.  The application data is stored in a tape backup soon . The backup data must be preserved for up to 10 years ?
Ans :  Use Was Storage Gateway to backup the data directly to amazon S3 glacier Deep Archive.

30.  Accelerate the transfer of historical records from on-premises to aws over the internet in a cost-effective manner ? 
Ans:  Use Aws Data-sync and select Amazon S3 Deep Archive as the Destination 
Data -sync optimizes the data transfer , ensuring efficient and reliable  to a long term storage 

31. Globally Deliver the static contents and media files to customers around world with low latency ?
Ans: Store the files in Amazon S3 and create Cloud Front Distribution . Select the S3 bucket as the origin 

32 An application must be hosted to 2 ec2 instances and should continuously run for three years . The cpu utilisation of ec2 instances is expected to be stable and predictable?
Ans:  Deploy the application to a Reserved Instance , it provides long-term cost effective options and predictable workloads 

33. Implement a cost -effective soon for s3 objects that are accessed less frequently.
Ans: Create an Amazon life cycle policy to move the objects to amazon s3 Standard-IA 

34.  Minimise the data transfer costs b/w two instances ?
Ans: Deploy the ec2 instance in the same region 

35. Import the SSL/TLS certificate of applications into Aws ?
Ans: Import into Aws Certificate manager or upload it to AWS IAM 





====================================================================== Examples questions =================================================================================


IMP points to remember before answering to the questions 


A in the question if it is mention "Least operational overhead" Means  the Services which has in-built feature to solve the issue which is mentioned in the question . "using ec2 is completely opposite to this statement"

B if it is "Most Operationally efficient way" = they are talking about "either fully managed or serverless" services
  
C Resiliency = Mostly go for Multi-AZ

D Least amount of infrastructure = Either they are talking about serverless services and fully managed services 

E in the question , if it is mention EKS , then blindly go for EKS in Answers , if not , then ggo for ECS 

F in question if is for AWS services , then choose IAM , if it is not for the AWS services , then go for Amazon cognito user pools for authenticate














1 A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500 GB. Each site has a high-speed Internet connection.
The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket. The solution must minimize operational complexity.
Which solution meets these requirements?

ANS : A. Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.



2 A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.
What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?

Ans : Use Amazon Athena directly with Amazon S3 to run the queries as needed



3  A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.
Which solution meets these requirements with the LEAST amount of operational overhead?

ANS : Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.



4 An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet.
Which solution will provide private network connectivity to Amazon S3?

ANS : Create a gateway VPC endpoint to the S3 bucket.



5 A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time.
What should a solutions architect propose to ensure users see all of their documents at once?

ANS : Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS



6 A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth.
Which solution will meet these requirements?

ANS : Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.



7 A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability.
Which solution meets these requirements?

ANS :  Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SOS) subscriptions. Configure the consumer applications to process the messages from the queues.




8 A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.
How should a solutions architect design the architecture to meet these requirements?

ANS : Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue. 




9 A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.
The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.
Which solution will meet these requirements?

ANS : Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.



10  A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received.
Which solution will meet these requirements?

ANS :  Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.




11 A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management.
What should a solutions architect do to accomplish this goal?

ANS : Use AWS Secrets Manager. Turn on automatic rotation.




12 A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53.
What should a solutions architect do to meet these requirements?

ANS : Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.




13 A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule.




14 A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance.
The database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.
Which solution will meet these requirements?

ANS : Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.




15 A company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud.
Which solution will meet these requirements?

ANS : Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.




16  A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.
Which solution will meet these requirements?

ANS : Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.




17  A company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket.
What should the solutions architect do to meet this requirement?

ANS : Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances. 



18 An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket.
A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.
Which combination of actions will meet these requirements? (Choose two.)

ANS :     A  Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.
 
          B  Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue. 




19 A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets.
A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.




20 A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.
A solutions architect needs to minimize the time that is required to clone the production data into the test environment.
Which solution will meet these requirements?

ANS : Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.




21 An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB. 




22 A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely.
Which storage solution will meet these requirements MOST cost-effectively?

ANS : Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month. 




23 A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling.
How should the solutions architect generate the information with the LEAST operational overhead?

ANS : Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.



24 A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database.
During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort.
Which solution will meet these requirements?

ANS : Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.




25 A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes.
What should a solutions architect do to accomplish this goal?

ANS : Turn on AWS Config with the appropriate rules.




26 Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.

ANS : Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.



27 A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory.
Which solution will meet these requirements?

ANS : Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.




28 A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions.
The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions.
Which solution will meet these requirements?

ANS :  Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.



29 A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance.
Which solution meets these requirements MOST cost-effectively?

ANS : Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.




30 A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check.
What should a solutions architect do to accomplish this?

ANS : Use AWS Config rules to define and detect resources that are not properly tagged.



31 A development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images.
Which method is the MOST cost-effective for hosting the website?

ANS : Create an Amazon S3 bucket and host the website there.



32 A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources.
What should a solutions architect do to meet these requirements?

ANS : Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.




33  A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks.
Which solution meets these requirements?

ANS : Enable AWS Shield Advanced and assign the ELB to it.



34 A company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two Regions.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.




35 A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session. Most Voted




36 A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The company must decrease latency for users who access the website.
Which solution meets these requirements MOST cost-effectively?

Ans : Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.




37 A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website.
The company has noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem.

ANS : Change the storage type to Provisioned IOPS SSD. 




38 A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.
The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.
What is the MOST operationally efficient solution that meets these requirements?

ANS : Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.




39 A company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an upload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.




40 A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges.
What is the MOST cost-effective way for the company to avoid Regional data transfer charges?

ANS : Deploy a gateway VPC endpoint for Amazon S3.




41 A company has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for internal users.
Which solution meets these requirements?

ANS : Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.




42 A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

ANS :  1  Enable versioning on the S3 bucket.
       2  Enable MFA Delete on the S3 bucket




43 A company has a data ingestion workflow that consists of the following:
• An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries
• An AWS Lambda function to process the data and record metadata
The company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job.
Which combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)

ANS :  1 Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.

       2 Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.

  


44 A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week.
What should the company do to guarantee the EC2 capacity?

ANS : Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.




45 A company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the catalog is stored in a durable location.
What should a solutions architect do to meet these requirements?

ANS : Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.




46 A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable.
Which solution will meet these requirements MOST cost-effectively?

ANS : Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select




47 A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.
What should a solutions architect do to meet these requirements?


ANS : Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.




48  A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

ANS :   1 Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.

        2 Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.





49  A company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system structure. The company wants a solution that scales automatically. is highly available, and requires minimum operational overhead.
Which solution will meet these requirements?


ANS : Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage. 





50  A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency.
Which solution will meet these requirements?

ANS : Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.




51 A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.
Which solution will meet these requirements?

ANS : Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.




52 A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.
Which solution will meet these requirements?

ANS : Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.




53  A company is running a popular social media website. The website gives users the ability to upload images to share with other users. The company wants to make sure that the images do not contain inappropriate content. The company needs a solution that minimizes development effort.
What should a solutions architect do to meet these requirements?


ANS : Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.




54 A company wants to run its critical applications in containers to meet requirements for scalability and availability. The company prefers to focus on maintenance of the critical applications. The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload.
What should a solutions architect do to meet these requirements?

ANS : Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. 




55 A company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website so that the requests will use HTTPS.
What should a solutions architect do to meet this requirement?

ANS : Create a listener rule on the ALB to redirect HTTP traffic to HTTPS. 



56  A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG format. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text




57 A company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.
Which storage solution is MOST cost-effective?

ANS : Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.




58 A company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.
What should a solutions architect do to ensure messages are being processed once only?


ANS : Use the ChangeMessageVisibility API call to increase the visibility timeout. 




59 A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.
What should the solutions architect do to meet these requirements?

ANS : Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.




60 A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data.
Which solution will meet these requirements with the LEAST operational effort?


ANS : Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database. 




61 A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service.
The company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service. The company needs to improve the application's availability without writing custom scripts or code.
What should a solutions architect do to meet these requirements?

ANS : Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.




62  A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.
What should the solutions architect recommend to meet these requirements?

ANS : Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.




63 A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.
How can the solutions architect meet this requirement?

ANS : Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.




64 company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to the application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access.
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

ANS : 1 Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.


      2 Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host




65 A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.
How should security groups be configured in this situation? (Choose two.)

ANS :  1  Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.
       2  Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.




66 A company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the application's performance. The application consists of application tiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application.
Which solution meets these requirements and is the MOST operationally efficient?

ANS : Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.




67 A company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several additional systems that provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive.
Which solution offers the MOST reliable data transfer?

ANS : AWS DataSync over AWS Direct Connect 



68 A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.




69 A company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years.
What is the MOST operationally efficient solution that meets these requirements?

ANS : Use AWS Backup to create backup schedules and retention policies for the table.




70 A company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most mornings. In the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very quickly.
What should a solutions architect recommend?

ANS :  Create a DynamoDB table in on-demand capacity mode.




80 A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.
What is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?

ANS : Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key.




81 A solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.
Which design should the solutions architect use?

ANS : Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue. 




82 A company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate Manager (ACM). The company's security team must be notified 30 days before the expiration of each certificate.
What should a solutions architect recommend to meet this requirement?

ANS : Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource. 



83  A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed.
What should the solutions architect recommend?

ANS :  Use Amazon CloudFront with a custom origin pointing to the on-premises servers.




84 A company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.
The production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the development and test EC2 instances when they are not in use.
Which EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?

ANS : Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.



85 A company has a production web application in which users upload documents through a web interface or a mobile app. According to a new regulatory requirement. new documents cannot be modified or deleted after they are stored.
What should a solutions architect do to meet this requirement?

ANS : Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled. 



86 A company hosts an application on AWS Lambda functions that are invoked by an Amazon API Gateway API. The Lambda functions save customer data to an Amazon Aurora MySQL database. Whenever the company upgrades the database, the Lambda functions fail to establish database connections until the upgrade is complete. The result is that customer data is not recorded for some of the event.
A solutions architect needs to design a solution that stores customer data that is created during database upgrades.
Which solution will meet these requirements?

ANS :  Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the Lambda functions to connect to the RDS proxy.




87 A survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible.

ANS : Configure the Requester Pays feature on the company's S3 bucket.




88 A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according to the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution.
What should a solutions architect do to secure the audit documents?

ANS : Enable the versioning and MFA Delete features on the S3 bucket.



89 A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours.
The company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue.

ANS : Create a read replica of the database. Configure the script to query only the read replica.




90 A company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call the Amazon S3 API to store and read objects. According to the company's security regulations, no traffic from the applications is allowed to travel across the internet.
Which solution will meet these requirements?

ANS : Configure an S3 gateway endpoint.




91 A company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on Amazon EC2 instances inside a VPC.
Which combination of steps should a solutions architect take to accomplish this? (Choose two.)

ANS :  1 Configure a VPC gateway endpoint for Amazon S3 within the VPC.
       2 Create a bucket policy that limits access to only the application tier running in the VPC.




92 A company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's elasticity and availability.
The current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes.
A solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay.
Which solution meets these requirements?

ANS : Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.





93 A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.
Each file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.
Which solution meets these requirements with the LEAST operational overhead?

ANS : Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB. 




94 An application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application performance slowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the application's performance quickly.
What should the solutions architect recommend?

ANS : Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.




95 

{
    "Version":"2012-10-17",
    "Id":"EC2TerminationPolicy",
    "Statement":[
        {
            "Effect":"Deny",
            "Action":"ec2:*",
            "Resource":"*",
            "Condition":{
                "StringNotEquals":{
                    "ec2:Region":"us-west-1"
                }
            }
        },
        {
            "Effect":"Allow",
            "Action":"ec2:TerminateInstances",
            "Resource":"*",
            "Condition":{
                "IpAddress":{
                    "aws:SourceIp":"10.200.200.0/24"
                }
            }
        }
    ]
}


ANS : Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200




96 A company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution must be highly available and integrated with Active Directory for access control.
Which solution will satisfy these requirements?

ANS : Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.




97 An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that processes the images and sends the results to users through email.
Users report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email messages.

ANS : Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.




98 A company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.
Which solution meets these requirements?

ANS :  Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.




99 A company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted.
Which solution will meet these requirements with the LEAST operational overhead?

ANS :  Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.




100 A company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.



101 A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.
Which combination of steps should a solutions architect take to automate this task? (Choose two.)

ANS : 

1 Install an AWS DataSync agent in the on-premises data center.

2 Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.




102 A company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the same time. The job processes XML data that is in an Amazon S3 bucket. New data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data during each run.
What should the solutions architect do to prevent AWS Glue from reprocessing old data?

ANS : Edit the job to use job bookmarks.



103 A solutions architect must design a highly available infrastructure for a website. The website is powered by Windows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution that can mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not acceptable for the website.
Which actions should the solutions architect take to protect the website from such an attack? (Choose two.)

ANS : 

1 Use AWS Shield Advanced to stop the DDoS attack.

2 Configure the website to use Amazon CloudFront for both static and dynamic content.



104 A company is preparing to deploy a new serverless workload. A solutions architect must use the principle of least privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function.
Which solution meets these requirements?

ANS : Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: events.amazonaws.com as the principal.




105 A company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be encrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must be rotated every year.
Which solution meets these requirements and is the MOST operationally efficient?

ANS : Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation 



106 A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. The company wants to use these data points in its existing analytics platform. A solutions architect must determine the most viable multi-tier option to support this architecture. The data points must be accessible from the REST API.
Which action meets these requirements for storing and retrieving location data?

ANS : Use Amazon API Gateway with Amazon Kinesis Data Analytics.



107 A company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems.
Which design should a solutions architect recommend?

ANS : Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets



108 A company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability 10 delete the objects.
What should a solutions architect do to meet these requirements?

ANS : Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket’s default retention mode for new objects.



109  A social media company allows users to upload images to its website. The website runs on Amazon EC2 instances. During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. Users are experiencing slow upload requests to the website.
The company needs to reduce coupling within the application and improve website performance. A solutions architect must design the most operationally efficient process for image uploads.
Which combination of actions should the solutions architect take to meet these requirements? (Choose two.)

ANS : 

1 Configure the web server to upload the original images to Amazon S3.

2 Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.




110 A company recently migrated a message processing system to AWS. The system receives messages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity.
Which architecture offers the HIGHEST availability?

ANS : Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled.




111 A company hosts a containerized web application on a fleet of on-premises servers that process incoming requests. The number of requests is growing quickly. The on-premises servers cannot handle the increased number of requests. The company wants to move the application to AWS with minimum code changes and minimum development effort.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.




112 A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company’s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.
The data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.




113 A company has created an image analysis application in which users can upload photos and add photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata.
The application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base.
Which solution meats these requirements?

ANS : Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata. 





114 A medical records company is hosting an application on Amazon EC2 instances. The application processes customer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 instances access Amazon S3 over the internet, but they do not require any other network access.
A new requirement mandates that the network traffic for file transfers take a private route and not be sent over the internet.
Which change to the network architecture should a solutions architect recommend to meet this requirement?

ANS : Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.





115 A company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants anew solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.
Which combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)

ANS :   1 Configure Amazon CloudFront in front of the website to use HTTPS functionality. 

        2 Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled. 





116 A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time.
Which solution will meet this requirement with the LEAST operational overhead?

ANS : Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).




117 A company is building a web-based application running on Amazon EC2 instances in multiple Availability Zones. The web application will provide access to a repository of text documents totaling about 900 TB in size. The company anticipates that the web application will experience periods of high demand. A solutions architect must ensure that the storage component for the text documents can scale to meet the demand of the application at all times. The company is concerned about the overall cost of the solution.
Which storage solution meets these requirements MOST cost-effectively?

ANS : Amazon S3




118  A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.
Which solution will meet these requirements with the LEAST amount of administrative effort?

ANS : Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.




119 A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB.
Which solution can the company use to route traffic to all the EC2 instances?

ANS : Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.




120 A company is running an online transaction processing (OLTP) workload on AWS. This workload uses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from this instance.
What should a solutions architect do to ensure the database and snapshots are always encrypted moving forward?

ANS : Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.





121 A company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own SSL certificate, which is on each instance to perform SSL termination.
There has been an increase in traffic recently, and the operations team determined that SSL encryption and decryption is causing the compute capacity of the web servers to reach their maximum limit.
What should a solutions architect do to increase the application's performance?

ANS : Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.




122 A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to complete payment processing of orders through a third-party web service. The application must be highly available.
Which combination of configuration options will meet these requirements? (Choose two.)

ANS :  1 Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets

       2 Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.





123 A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable.
Which solution will meet these requirements?


ANS : Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.




124 A media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for video processing, 300 TB of very durable storage for storing media content, and 900 TB of storage to meet requirements for archival media that is not in use anymore.
Which set of services should a solutions architect recommend to meet these requirements?

ANS : Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage 





125  A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can tolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and operational overhead.
What should a solutions architect do to meet these requirements?

ANS : Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.




126 A company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.
Which combination of actions should the solutions architect take to accomplish this? (Choose two.)

ANS : 

1 Migrate the PostgreSQL database to Amazon Aurora.

2 Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).





127 An application runs on Amazon EC2 instances across multiple Availability Zonas. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%.
What should a solutions architect do to maintain the desired performance across all instances in the group


ANS : Use a target tracking policy to dynamically scale the Auto Scaling group. 





128 A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company wants to serve all the files through an Amazon CloudFront distribution. The company does not want the files to be accessible through direct navigation to the S3 URL.
What should a solutions architect do to meet these requirements?

ANS : Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission. 





129 A company’s website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company’s website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.
Which combination should a solutions architect recommend to meet these requirements?

ANS : Amazon CloudFront and Amazon S3




130 A company runs an Oracle database on premises. As part of the company’s migration to AWS, the company wants to upgrade the database to the most recent available version. The company also wants to set up disaster recovery (DR) for the database. The company needs to minimize the operational overhead for normal operations and DR setup. The company also needs to maintain access to the database's underlying operating system.
Which solution will meet these requirements?

ANS : Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region.




131 A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region.
Which solution will meet these requirements with the LEAST operational overhead?

ANS : Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.




132 A company runs workloads on AWS. The company needs to connect to a service from an external provider. The service is hosted in the provider's VPC. According to the company’s security team, the connectivity must be private and must be restricted to the target service. The connection must be initiated only from the company’s VPC.
Which solution will mast these requirements?

ANS : Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the target service.



133 A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The on-premises database must remain online and accessible during the migration. The Aurora database must remain synchronized with the on-premises database.
Which combination of actions must a solutions architect take to meet these requirements? (Choose two.)

ANS : 1 Create an AWS Database Migration Service (AWS DMS) replication server.
      2  Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).




134 A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators.
Which solution will meet these requirements?

ANS : Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.




135 A company runs its ecommerce application on AWS. Every new order is published as a massage in a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are processed by a different application that runs on a separate EC2 instance. This application stores the details in a PostgreSQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone.
The company needs to redesign its architecture to provide the highest availability with the least operational overhead.
What should a solutions architect do to meet these requirements?

ANS :  Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.




136 A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.
The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.
What should a solutions architect do to meet these requirements with the LEAST operational overhead?

ANS : Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.




137 A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.
The EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.
Which combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)

ANS :  

1 Use Spot Instances for the data ingestion layer

2 Purchase a 1-year Compute Savings Plan for the front end and API layer. 




138 A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each user a personalized view by using mixture of static and dynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible.
How should a solutions architect design the application to ensure the LEAST amount of latency for all users?

ANS : Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.




139 A company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead.
Which solution will meet these requirements?

ANS : Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.



140 A company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilizalion metrics are spiking when monthly reports run.

ANS : Migrate the monthly reporting to an Aurora Replica.



141 A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are all hosted on the EC2 instance. The application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly.
Which solution will meet these requirements MOST cost-effectively?

ANS : Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.



142 A company has a data ingestion workflow that includes the following components:
An Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new data deliveries
An AWS Lambda function that processes and stores the data
The ingestion workflow occasionally fails because of network connectivity issues. When failure occurs, the corresponding data is not ingested unless the company manually reruns the job.
What should a solutions architect do to ensure that all notifications are eventually processed?


ANS : Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue. Most Voted




143 A company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms.
What should the solutions architect do to meet these requirements?

ANS : Create Amazon CloudWatch composite alarms where possible.



144 A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.
Which solutions will meet these requirements? (Choose two.)

ans :

1 Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.

2  Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.






145 A company uses a three-tier web application to provide training to new employees. The application is accessed for only 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store information and wants to minimize costs.
What should a solutions architect do to meet these requirements?

ANS : Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions as event targets for the rules.



146 A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.
Which action should the company take to meet these requirements MOST cost-effectively?

ANS : Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.





147 A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date.
Which solution will meet these requirements?

ANS : Use S3 Object Lock in governance mode with a legal hold of 1 year.



148 A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs).
Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)

ANS:  1 Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.

      2 Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.





149 A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.

Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

ANS : 1 Configure an Amazon CloudWatch Logs export for the DB cluster.

      2 Use AWS Backup to take the backups and to keep the backups for 5 years.




150 A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application’s traffic recently spiked due to fraudulent requests from botnets.

ANS : 1 Create a usage plan with an API key that is shared with genuine users only. 
      2 Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.



151 A company runs a production application on a fleet of Amazon EC2 instances. The application reads the data from an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and often has intermittent traffic. This application should continually process messages without any downtime.

ANS : Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional capacity.



152 A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic.

What should the solutions architect do to accomplish this?

ANS : Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.



153 A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should.be protected throughout the entire application stack, and access to the information should be restricted to certain applications.

ANS : Configure a CloudFront field-level encryption profile



154 An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers.

A solutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number of open connections. The solutions architect needs to prevent the timeout errors while making the least possible changes to the application.

ANS : Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint. Most Voted




155 A company’s infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company wants to back up its data in a separate Region.

Which solution will meet these requirements with the LEAST operational overhead?

ANS : Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.




156 A solutions architect needs to securely store a database user name and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store.

What should the solutions architect do to meet this requirement?

ANS : Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.





157 A company is designing a cloud communications platform that is driven by APIs. The application is hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to provide external users with access to the application through APIs. The company wants to protect the platform against web exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS attacks.

ANS :  1 Use AWS Shield Advanced with the NLB.
       2 Use AWS WAF to protect Amazon API Gateway.



158 A company has an AWS account used for software engineering. The AWS account has access to the company’s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual private gateway.

A development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company’s data center.

ANS : Configure the Lambda function to run in the VPC with the appropriate security group.




159 A company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.

Which combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)


ANS :  1 Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.
       2 Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Configure key rotation.




160 A company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead.


ANS : Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing. 



161 A hospital wants to create digital copies for its large collection of historical written records. The hospital will continue to add hundreds of new documents each day. The hospital’s data team will scan the documents and will upload the documents to the AWS Cloud.

A solutions architect must implement a solution to analyze the documents, extract the medical information, and store the documents so that an application can run SQL queries on the data. The solution must maximize scalability and operational efficiency.


ANS :  1 Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data.

       2 Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant medical information from the text. 





162 A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs.


ANS : Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication. 




163 A company’s order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs.

ANS: Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.



164 A company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the application needs only data from the last 30 days. The company needs a solution that minimizes cost and development effort.

ANS : Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute.



165 A company has a Microsoft .NET application that runs on an on-premises Windows Server. The application stores data by using an Oracle Database Standard Edition server. The company is planning a migration to AWS and wants to minimize development changes while moving the application. The AWS application environment should be highly available.

Which combination of actions should the company take to meet these requirements? (Choose two.)


ANS : 1 Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.

      2 Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.




166 A telemarketing company is designing its customer call center functionality on AWS. The company needs a solution that provides multiple speaker recognition and generates transcript files. The company wants to query the transcript files to analyze the business patterns. The transcript files must be stored for 7 years for auditing purposes.

ANS : Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.



167 A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.

What should a solutions architect do to meet these requirements?

ANS : Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.




168 The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database.

As the company expands, customers report that their meeting invitations are taking longer to arrive.

ANS : Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue. Most Voted




169 An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.

The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead.

ANS : Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.




170 A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents.

The company decides to host its website on AWS and to use Amazon CloudFront. The company’s solutions architect creates a CloudFront distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin.

ANS : Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.



171 A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company’s account.

Which solution will meet these requirements with the LEAST operational overhead?

ANS : Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateImage API call is detected. 



172 A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices.

The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests.

What should a solutions architect do to address this issue without impacting existing users?


ANS : Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.






173 A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket.

Which solution will meet these requirements?

ANS : Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.





174 A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed.

What should the solutions architect do to ensure that the architecture supports distributed session data management?


ANS : Use Amazon ElastiCache to manage and store session data. 




175 A company offers a food delivery service that is growing rapidly. Because of the growth, the company’s order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following:

• A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application
• Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders

The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event.

A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company’s AWS resources.



ANS : Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.





176 A company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of “application” and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components.

ANS : Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.




177 A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the most cost-effective solution that will not increase retrieval time.

ANS : S3 Intelligent-Tiering




178 A company’s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket.

Which solution will meet these requirements with the LEAST development effort?

ANS : Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.





179 A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.

ANS : Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.





180 A company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects and for all objects that are added to the S3 bucket in the future.

Which solution will meet these requirements with the LEAST amount of effort?

ANS : Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects. 





181 A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy.

What should a solutions architect do to meet these requirements?


ANS : Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.




182 A company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.

Which combination of steps will accomplish this task? (Choose two.)


ANS : 1 Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.

      2 Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.





183 A company’s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application.

Which solution will resolve these issues in the MOST operationally efficient way?


ANS : Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.





184 A solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can pass without receiving a single request. The data processing will take place asynchronously, but should be completed within a few seconds after a request is made.

Which compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?


ANS : An AWS Lambda function




185 A company has hired an external vendor to perform work in the company’s AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company’s AWS account.

How should a solutions architect grant this access to the vendor?


ANS : Create an IAM role in the company’s account to delegate access to the vendor’s IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.




186 A company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign its application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly.

Which combination of steps should the company take to meet these requirements? (Choose two.)

ANS : 1  Create an Amazon Route 53 multivalue answer routing policy.
      2  Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.






187 A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL.

ANS :  Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.






188 A company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company needs a highly scalable solution that minimizes operational overhead.

Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

ANS : 1 Use AWS Glue to process the raw data in Amazon S3.

      2 Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3.





189 A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years.

After the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new CloudTrail logs that are delivered to the S3 bucket has remained consistent.

Which solution will delete objects that are older than 3 years in the MOST cost-effective manner?


ANS : Configure the S3 Lifecycle policy to delete previous versions as well as current versions.






190 A company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors.

After an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions architect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic.

Which solution will meet these requirements?

ANS : Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.




191 A company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own VPC. The company’s operations team needs to be notified when RDP or SSH access to an environment has been established.

ANS :  Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.




192  A solutions architect has created a new AWS account and must secure AWS account root user access.

Which combination of actions will accomplish this? (Choose two.)


ANS : 1 Ensure the root user uses a strong password.
      2 Enable multi-factor authentication to the root user. 





193 A company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit.

ANS : Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.




194 A company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The applications need to be migrated one by one with a month in between each migration. Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync across both databases throughout the migration.


ANS : Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.



195 A company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change to the application.

ANS : Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users’ images.




196 A company wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each account.

What should a solutions architect do to meet this requirement MOST cost-effectively?


ANS : Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.




197 A solutions architect needs to design a new microservice for a company’s application. Clients must be able to call an HTTPS endpoint to reach the microservice. The microservice also must use AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect will write the logic for this microservice by using a single AWS Lambda function that is written in Go 1.x.

ANS : Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.



198 A company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect connection. Corporate office users query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the data warehouse are not cached.


ANS : Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.



199 An online learning company is migrating to the AWS Cloud. The company maintains its student records in a PostgreSQL database. The company needs a solution in which its data is available and online across multiple AWS Regions at all times.

ANS : Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.




200 A company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries.

ANS : Multivalue routing policy 




201 A medical research lab produces data that is related to a new study. The lab wants to make the data available with minimum latency to clinics across the country for their on-premises, file-based applications. The data files are stored in an Amazon S3 bucket that has read-only permissions for each clinic.

ANS : Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic



202 A company is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance contains both the web server and the database software. The company must make its website platform highly available and must enable the website to scale to meet user Demand

ANS : Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.




203 A company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single target group. The instances are in an Auto Scaling group for each environment. The company requires a development environment and a production environment. The production environment will have periods of high traffic.

ANS : Reduce the maximum number of EC2 instances in the development environment’s Auto Scaling group.




204 A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are in private subnets. A solutions architect implements an internet-facing Application Load Balancer (ALB) and specifies the EC2 instances as the target group. However, the internet traffic is not reaching the EC2 instances.

ANS : Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets.





205  A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database support team is reporting slow reads against the DB instance and recommends adding a read replica.

Which combination of actions should a solutions architect take before implementing this change? (Choose two.)


ANS : 1 Enable binlog replication on the RDS primary node.

      2 Allow long-running transactions to complete on the source DB instance.




206 A company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that has been uploaded to Amazon S3. Users report that some submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The company wants to improve system performance and scale the system based on user load.


ANS : Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue.




207 A company’s security team requests that network traffic be captured in VPC Flow Logs. The logs will be frequently accessed for 90 days and then accessed intermittently.

What should a solutions architect do to meet these requirements when configuring the logs?

ANS : Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.





208  A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers.

 ANS : Create security group rules using the security group ID as the source or destination.




209 A company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction.

ANS : Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.




210 A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches.

How should the company move the data to Amazon S3 to meet these requirements?

ANS : Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.




211 A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable.

ANS : Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.




212 A company’s compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders.

The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system.

Which solution will meet these requirements?


ANS : Join the file system to the Active Directory to restrict access.




213 A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones.

The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website.


ANS :  1 Configure Amazon CloudFront to cache multiple versions of the content.

       2 Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header. 



214 A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s Amazon EC2 instances. Both VPCs are in the us-east-1 Region.

The solutions architect must implement a solution to provide the application’s EC2 instances with access to the ElastiCache cluster.

Which solution will meet these requirements MOST cost-effectively?


ANS : Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster’s security group to allow inbound connection from the application’s security group.




215 A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error.

What should a solutions architect implement to overcome these timeout errors?


ANS : Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.




216 A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time.

Which solution meets these requirements and is MOST secure?

ANS :  Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.



217 A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.

Which solution meets these requirements?

ANS : Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.



218 A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing.

Which solution will meet these requirements with the LEAST operational overhead?


ANS : Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.




219 A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture.

ANS : Use Amazon ElastiCache in front of the database.




220 A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit.

ANS : Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.



221 A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary.

ANS : Use an Amazon Aurora global database with a warm standby deployment.




222 A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations.

Which solution will meet these requirements in the MOST operationally efficient way?


ANS : Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.



223  A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning.

How should the scaling be changed to address the staff complaints and keep costs to a minimum?

ANS : Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period. 




224 A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’ s data layer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.

What should a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)


ANS : 1 Configure storage Auto Scaling on the RDS for Oracle instance
      2 Configure the Auto Scaling group to use the average CPU as the scaling metric. 




225 A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive.


ANS : Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.



226 A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data.

ANS : 1 Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month. 

      2 Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription. 




227 A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years.


ANS : Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years. 




228 A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances.


ANS : Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.




229  The company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application.

Which solution will meet these requirements?


ANS :  Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.




230 A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month.

Which solution will meet these requirements MOST cost-effectively?


ANS : Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).




231 A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data.

ANS : 1 Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

      2 Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.







232 A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.


ANS : Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.




233 An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data.


ANS : Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.


234 A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC.

What is the SMALLEST CIDR block that meets these requirements?


ANS : 10.0.1.0/24 




235 A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%.

A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur.

Which solution will meet these requirements?


ANS : Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.





236 A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance.

The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone.

Which solution will make the application highly available?


ANS : Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.




237 A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share.

The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days.


ANS : AWS DataSync




238 A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format.

Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead.

Which combination of solutions will meet these requirements? (Choose two.)



ANS : 1 Deploy Amazon CloudFront for content delivery and caching.
      2 Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.




239 A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share.

The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days.

Which AWS solution will meet these requirements?

ANS : AWS DataSync





240 A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis.

Which solution will meet these requirements with the LEAST operational overhead?

ANS : Use AWS DataSync.





241 A company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs to provide a cost-effective network design that minimizes data transfer charges.

Which solution meets these requirements?

ANS : Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.





242 A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed.

Which solution will accomplish this goal with the LEAST operational overhead?


ANS : Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.





243 A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region.

Which solution will meet these requirements in the MOST operationally efficient way?

ANS : Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.





244 A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings.

ANS : Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.




245 A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue.

What should a solutions architect recommend to meet these requirements?


ANS :  Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime. 





246 A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company’s data science team wants to query ingested data in near-real time.

Which solution provides near-real-time data querying that is scalable with minimal data loss?


ANS : Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.




247 What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?

ANS : Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.




248 A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance.

A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze.

Which system architecture should the solutions architect recommend?


ANS :  Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.





249 A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data.

The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency.

Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?


ANS : Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.





250 A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket.

Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content.

Which solution meets these requirements?


ANS : Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.




251 An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets.

Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)



ANS :  1 Move assets to S3 Intelligent-Tiering after 30 days.
       2 Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.




252   A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet traffic must be blocked.


ANS : Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups. 





253 A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest.

What should a solutions architect do to meet this requirement?


ANS : Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.





254 A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files.

Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?


ANS : Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.






255 A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand.

Which solution meets these requirements?


ANS : Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.




256 A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days.

What should a solutions architect do to meet this requirement with the LEAST operational effort?


ANS : Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.





257  A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures.

As traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead.

Which solution will meet these requirements?



ANS : Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions. 





258 A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster.

The DR plan must replicate data to a secondary AWS Region.

Which solution will meet these requirements MOST cost-effectively?


ANS : Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.





259  A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database.

Which solution will meet these requirements with the LEAST operational overhead?


ANS : Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.







260 A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.

Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity.

Which solution will meet these requirements with the LEAST operational overhead?

ANS : Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.







261 A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.

Which solution will meet these requirements with the FEWEST changes to the code?

Ans : Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.





262 A company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company.

The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage.

Which solution will meet these requirements MOST cost-effectively?



ANS : Compute Savings Plan





263 A company collects data from a large number of participants who use wearable devices. The company stores the data in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is constant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB.

Which solution will meet these requirements MOST cost-effectively?

ANS : Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).




267 A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3.

What should a solutions architect do to meet these requirements?


ANS : Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.




268 A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workflow. The company also wants to minimize operational overhead.

Which solution will meet these requirements?


ANS : Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.







269 A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic.

The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant.

Which solution will meet these requirements MOST cost-effectively?


ANS : Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.






270 A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory.

Which solution will run the batch job within 15 minutes with the LEAST operational overhead?


ANS : Use AWS Batch on Amazon EC2. 




271 A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients.

Which solution will meet these requirements with the LEAST operational overhead?


ANS : Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.




272 A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest.

Which solution will meet these requirements?

ANS : Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys. 



278 A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC.

Which solution will meet these requirements with the FEWEST changes to the code?


ANS : Use an interface endpoint. 




279 A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly.

Which actions should a solutions architect take to meet this requirement? (Choose two.)


ANS :  1 Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.
       2 Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.



280 A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events.

Which solution will meet these requirements?


ANS : Amazon EventBridge event bus





281 A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture.

A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data.

Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)


ANS : 1 Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.

      2 Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS. 







282 A company runs a web application that is backed by Amazon RDS. A new database administrator caused data loss by accidentally editing information in a database table. To help recover from this type of incident, the company wants the ability to restore the database to its state from 5 minutes before any change within the last 30 days.

Which feature should the solutions architect include in the design to meet this requirement?

ANS : Automated backups




283 A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content.

Which solution will meet this requirement with the LEAST operational overhead?

ANS : Implement API usage plans and API keys to limit the access of users who do not have a subscription.




284 A solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods for IAM user passwords.

What should the solutions architect do to accomplish this?


ANS : Set an overall password policy for the entire AWS account.




285 A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns.

Which solution will meet these requirements with the LEAST operational overhead?


ANS : Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).




286 A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS).

Which combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)


ANS : 1 Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key. 
      2 Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.




287 When a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events.

Which solution meets these requirements MOST cost-effectively?

ANS : Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.




288 A company is running several business applications in three separate VPCs within the us-east-1 Region. The applications must be able to communicate between VPCs. The applications also must be able to consistently send hundreds of gigabytes of data each day to a latency-sensitive application that runs in a single on-premises data center.

A solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness.

Which solution meets these requirements?\


ANS : Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway.



289 An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order-processing tasks. These tasks require manual approvals as part of the workflow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers.

ANS : Use AWS Step Functions to build the application.



290 A company recently deployed a new auditing system to centralize information about operating system versions, patching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated.

Which solution achieves these goals MOST efficiently?

ANS : Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated. 




291 A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations.

Which solution will meet these requirements?

ANS : Configure provisioned concurrency for the Lambda function that handles the requests. 




292 A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently.

The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modifications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process.

Which solution will meet these requirements with the LEAST amount of change to the application code?


ANS : Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports. 





293 A company has a three-tier application on AWS that ingests sensor data from its users’ devices. The traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and finally to EC2 instances for the application tier. The application tier makes calls to a database.

What should a solutions architect do to improve the security of the data in transit?


ANS : Configure a TLS listener. Deploy the server certificate on the NLB.




294 A solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks.

Which additional configuration strategy should the solutions architect use to meet these requirements?


ANS : Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.





295 A new employee has joined a company as a deployment engineer. The deployment engineer will be using AWS CloudFormation templates to create multiple AWS resources. A solutions architect wants the deployment engineer to perform job activities while following the principle of least privilege.

Which combination of actions should the solutions architect take to accomplish this goal? (Choose two.)

ANS : 1 Create a new IAM user for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only. Most Voted

      2 Create an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launch stacks using that IAM role.





296 A company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance.

The company wants to optimize customer session management during transactions. The application must store session data durably.

Which solutions will meet these requirements? (Choose two.)


ANS : 1 Use an Amazon DynamoDB table to store customer session information.

      2 Deploy an Amazon ElastiCache for Redis cluster to store customer session information.






297 A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours.

The backup strategy must maximize scalability and optimize resource utilization for this environment.

ANS : Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.






298  A company wants to deploy a new public web application on AWS. The application includes a web server tier that uses Amazon EC2 instances. The application also includes a database tier that uses an Amazon RDS for MySQL DB instance.

The application must be secure and accessible for global customers that have dynamic IP addresses.

How should a solutions architect configure the security groups to meet these requirements?



ANS : Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the security group of the web servers. 





299 A company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is configured with the latest generation DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand.

A database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000.

What should a solutions architect do to improve the application performance?


ANS : Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.





300 An IAM user made several configuration changes to AWS resources in their company's account during a production deployment last week. A solutions architect learned that a couple of security group rules are not configured as desired. The solutions architect wants to confirm which IAM user was responsible for making changes.

Which service should the solutions architect use to find the desired information?


ANS : AWS CloudTrail 





301 A company has implemented a self-managed DNS service on AWS. The solution consists of the following:

• Amazon EC2 instances in different AWS Regions
• Endpoints of a standard accelerator in AWS Global Accelerator

The company wants to protect the solution against DDoS attacks.

What should a solutions architect do to meet this requirement?



ANS : Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.







302 An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance.

A solutions architect needs to minimize the amount of operational effort that is needed for the job to run.

Which solution meets these requirements?


ANS : Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.





303 A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company’s internet connection can support an upload speed of 100 Mbps.

Which solution meets these requirements MOST cost-effectively?


ANS : Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.





304 A financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks might take the application offline.

A solutions architect must design a solution to protect the application from this type of attack.

Which solution meets these requirements with the LEAST operational overhead?

ANS : Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.




305 A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application.

What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?


ANS ;  Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.




306 A customer owns a simple API in a VPC behind an internet-facing Application Load Balancer (ALB). a client application which consumes the API is deployed in a second account in private subnets behind a NAT gateway. When requests to the client application increase, the NAT gateway costs are higher than expected. A solutions architect has configured the ALB to be internal.

Which combination of architectural changes will reduce the NAT gateway costs? (Choose two.)


ANS : 1 Configure a VPC peering connection between the two VPCs. Access the API using the private address.
      2 Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address.





307 While performing PCI auditing on an existing workload deployed on AWS. The review identified a public-facing website running on the same Amazon EC2 instance as a Microsoft Active Directory domain controller that was install recently to support other AWS services.
What should the solutions architect recommend for the new design that would improve the security of the architecture and minimize the administrative demand on IT staff?

ANS : Use AWS Directory Service to create a managed Active Directory. Uninstall Active Directory on the current EC2 instance.




308 A video streaming company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for its content around the world.
Which configuration should the solutions architect choose to ensure that users are served the correct content without violating distribution rights.?


ANS : Configure Amazon Route 53 with a geolocation policy.




309 A multinational company will launch e-commerce multi-tier application and due to the big promotions , the company expects a huge traffic in the launching day.
As solution architect, what is the best approach to prevent any potential failure in the database layer?


ANS : Migrate the database to Amazon RDS in multi AZ




310 A start-up company that offers an intuitive financial data analytics service behind Amazon API Gateway.  the traffic to this service is  unpredictable and varies from 0 requests to over 500 per second. The data size which needs to be persisted in a database is currently less than 1 GB with unpredictable future growth and can be queried using simple key-value requests.

Which combination of AWS services would meet these requirements? (Choose two.)


ANS : 1 AWS Lambda
      2 Amazon DynamoDB





311 company runs a container application by using elastic kuber service the application includes microservices that manage customers and place orders the company needs to Route incoming request to the proper microservices which solution will meet this requirement most cost effectively ?


ANS : Amazon API Gateway , even though ALB is there but go for the API is good choice 




322  




















































































